0.0,1.0,0.3333333333333333,0.0,1.0,0.0,0.0,<h3>Adding Cloudfront trigger to a Lambda using CLI or SDK</h3><p>I'm kind of stcuk trying to adding a Cloudfront Distribution trigger for a lambda but using cli or sdk.<br>So far from my objective I have added the trigger through console creating a new lambda using the blueprint for "cloudfront-modify-response-header" and it looks like it's working properly but I could not retrieve the information about that trigger using cli<br>I tried: </p><br><br><pre><code>aws lambda get-policy --function-name myFunctionName --profile xxxx<br></code></pre><br><br><p>but I only got a policy with something like:</p><br><br><pre><code>{<br>    "Policy": {<br>        "Version": "2012-10-17";<br>        "Id": "default";<br>        "Statement": [<br>            {<br>                "Sid": "replicator.lambda.GetFunction";<br>                "Effect": "Allow";<br>                "Principal": {<br>                    "Service": "replicator.lambda.amazonaws.com"<br>                };<br>                "Action": "lambda:GetFunction";<br>                "Resource": "arn:aws:lambda:us-east-1: 12345678912:function: myFunctionName: 1"<br>            }<br>        ]<br>    };<br>    "RevisionId": "b11abfc7-79a7-489e-9bf7-d501234567899"<br>}<br></code></pre><br><br><p>Where is DistributionID; Event type and other properties that I'm seeing in AWS Console?</p><br><br><p>Best regards; </p><br>
0.0,0.0,1.0,0.0,0.0,0.6666666666666666,0.0,<h3>Using CodeDeploy set permissions using AppSpec or script</h3><p>I am new to CI/CD and just configured my first on-premise deployment app. I encountered that after deployment the files have changed permissions. </p><br><br><p>BTW; Is it a normal behavior?</p><br><br><p>Now I am figuring out how to set the permissions back. I have identified 2 alternative ways: </p><br><br><ol><br><li>Appspec</li><br><li>Shell script triggered by appropriate lifecycle event</li><br></ol><br><br><p>What is your suggestion?</p><br>
0.0,0.0,0.0,0.6666666666666666,1.0,0.6666666666666666,0.0,<h3>how to access google spreadsheet json file from s3 while using aws lambda with django</h3><p>I am using <code>django</code> and deployed my application on <code>aws_lambda</code>. Everything worked fine until i wanted to save the content of the database in a <code>google spreadsheet</code></p><br><p>The problem is how to access/get the <code>json.file</code> (that would normally be located in the same folder as where i am using it) now that i am using <code>aws_lambda</code> in production</p><br><h1>views.py</h1><br><pre><code># how i would normally do it; locally<br><br>scope = [&quot;https://spreadsheets.google.com/feeds&quot;; &quot;https://www.googleapis.com/auth/drive&quot;]<br><br>credentials = ServiceAccountCredentials.from_json_keyfile_name(&quot;secret-json-file.json&quot;; scope)<br><br>gc = gspread.authorize(credentials)<br><br>open_google_spreadsheet = gc.open('data').sheet1<br></code></pre><br><p>but since i am using <code>aws_lambda</code> and i stored that json file on the main folder of my <code>aws s3</code> bucket.</p><br><p>I am trying something like this:</p><br><pre><code><br>s3_client = boto3.client(&quot;s3&quot;)<br><br>response = s3_client.get_object(Bucket=aws_bucket; Key=&quot;secret-json-file.json&quot;)<br><br>data = response[&quot;Body&quot;].read()<br><br># Google spreadsheet<br><br>scope = [&quot;https://spreadsheets.google.com/feeds&quot;; &quot;https://www.googleapis.com/auth/drive&quot;]<br><br>credentials = ServiceAccountCredentials.from_json_keyfile_name(data; scope)<br></code></pre><br><p>I also tried this approach:</p><br><pre><code>AWS_ID = aws_access_key_id<br>AWS_KEY = aws_secret_access_key<br>AWS_S3_BUCKET = aws_bucket<br><br>session = boto3.session.Session(aws_access_key_id=AWS_ID; aws_secret_access_key=AWS_KEY) <br><br>s3 = session.resource(&quot;s3&quot;)<br><br>my_s3_bucket = s3.Bucket(AWS_S3_BUCKET)<br><br>current_s3_file = []<br><br>for s3_file in my_s3_bucket.objects.all():<br>  if s3_file.key == &quot;secret-json-file.json&quot;:<br>    current_s3_file.append(s3_file)<br><br># Google spreadsheet<br><br>scope = [&quot;https://spreadsheets.google.com/feeds&quot;; &quot;https://www.googleapis.com/auth/drive&quot;]<br><br>credentials = ServiceAccountCredentials.from_json_keyfile_name(current_s3_file; scope)<br></code></pre><br><p>Unfortunately both approaches are not successful since i am not able to run the command <code>zappa update production</code> anymore; it crash with <code>timeout</code></p><br><p>Below is the ouput:</p><br><pre><code>Error: Warning! Status check on the deployed lambda failed. A GET request to '/' yielded a 504 response code.<br></code></pre><br><p>Any help would be much appreciated.</p><br>
0.6666666666666666,1.0,0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>Can&#39;t talk to Kafka broker on AWS instance</h3><p>I realize this question comes up a lot.  I've read many threads and blog posts; but here I am.  Long story short; I have a docker container running wurstmeister/zookeeper &amp; wurstmeister/kafka; and then some services running in their own containers.  I'll just mention the NoddJS one for now. Everything works fine at home; using IP addresses (not localhost) so I'm baffled at what the difference is here.  On AWS; it simply "doesn't work" even though it seems to at least connect to the broker in the beginning.  I'm explicitly using internal IPs in the config as I don't want this exposed to anything externally.</p><br><br><p>Reading around; I've tried 2 setups. 1 works at home (KAFKA_ADVERTISED_HOST_NAME).  1 doesn't (KAFKA_ADVERTISED_LISTENERS).  Neither work on my EC2 Linux box:</p><br><br><p>Kafka docker-compose.yml</p><br><br><pre><code>version: '2'<br><br>services:<br>  zookeeper:<br>    image: wurstmeister/zookeeper<br>    ports:<br>      - "2181:2181"<br>    networks:<br>      - my-network<br><br>  kafka:<br>    image: wurstmeister/kafka<br>    depends_on:<br>      - zookeeper<br>    ports:<br>      - "9092:9092"<br>    environment:<br>      KAFKA_ADVERTISED_HOST_NAME: &lt;internal-ip&gt;<br>      KAFKA_ADVERTISED_PORT: "9092"<br>      KAFKA_CREATE_TOPICS: "test:1:1"<br>      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181<br>    volumes:<br>      - /var/run/docker.sock:/var/run/docker.sock<br><br>    networks:<br>      - my-network<br><br>networks:<br>  my-network:<br></code></pre><br><br><p>NodeJS docker-compose.yml</p><br><br><pre><code>version: '2'<br><br>services:<br>  nodejs:<br>    build:<br>      context: ./<br>      dockerfile: Dockerfile<br>    networks:<br>      - kafka_my-network<br>    restart: unless-stopped<br>    ports:<br>      - "1337:3000"<br>    volumes:<br>      - "/tmp:/tmp"<br>      - "/var/log:/var/log"<br><br>networks:<br>  kafka_my-network:<br>    external: true<br></code></pre><br><br><p>Then in NodeJS</p><br><br><pre><code>const kafkaHost = '&lt;internal-ip&gt;:9092';<br>const client = new kafka.KafkaClient({kafkaHost});<br>const producer = new kafka.Producer(client)<br>const kakfaTopic = 'test';<br><br>producer.on('ready'; function() {<br>  console.log(`kafka producer is ready`); // I see this; so I'm assuming all is well<br>  ready = true;<br>});<br><br>producer.on('error'; function(err) {<br>  console.error(err);<br>});<br><br>const payload = [<br>  {<br>    topic: kafkaTopic;<br>    messages: JSON.stringify(myMessages);<br>  }<br>]<br><br>producer.send(payload; function(err; data) {<br>  if (err) {<br>    console.error(`Send error ${JSON.stringify(err}`);<br>  }<br><br>  console.log(`Sent data ${JSON.stringify(data)}`);<br>});<br></code></pre><br><br><p>When I start my NodeJS server; I see that I've connected to a Kafka broker.  I can confirm as well that :9092 is open after checking w/ telnet and/or nc.  Then; when it sends a request; the callback gets an empty error. </p><br><br><p>I realize KAKFA_ADVERTISED_HOST_NAME is deprecated; so in the name of completion; here is my attempt using ADVERTISED_LISTENERS which failed.  With this configuration I seemed to get the same results at home as I did on EC2.</p><br><br><pre><code>version: '2'<br><br>services:<br>  zookeeper:<br>    image: wurstmeister/zookeeper<br>    ports:<br>      - "2181:2181"<br>    networks:<br>      - my-network<br><br>  kafka:<br>    image: wurstmeister/kafka<br>    depends_on:<br>      - zookeeper<br>    ports:<br>      - "9092:9092"<br>    environment:<br>      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://&lt;internal-ip&gt;:9092<br>      KAFKA_CREATE_TOPICS: "test:1:1"<br>      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181<br>    volumes:<br>      - /var/run/docker.sock:/var/run/docker.sock<br><br>    networks:<br>      - my-network<br><br>networks:<br>  my-network:<br></code></pre><br><br><p>EDIT: I will not provide this as a solution; but the bitnami image w/ the following config works.  The main difference is it had a pretty straight forward README that I went through.  I can't be certain if I tried the equivalent config w/ wurstmeister (I tried many - and again; at least one of which in docker containers on my own machine but not on a single EC2 instance).</p><br><br><p>Note that I did list 'kafka' with an internal IP (not loopback) in /etc/hosts for this.  This should be tantamount to using the internal IP explicitly which I had done above.</p><br><br><pre><code>version: '2'<br><br>services:<br>  zookeeper:<br>    image: 'bitnami/zookeeper:3'<br>    ports:<br>      - '2181:2181'<br>    volumes:<br>      - 'zookeeper_data:/bitnami'<br>    environment:<br>      - ALLOW_ANONYMOUS_LOGIN=yes<br>    networks:<br>      - my-network<br><br>  kafka:<br>    image: 'bitnami/kafka:2'<br>    ports:<br>      - '9092:9092'<br>      - '29092:29092'<br>    volumes:<br>      - 'kafka_data:/bitnami'<br>      - /var/run/docker.sock:/var/run/docker.sock   <br>    environment:<br>      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181<br>      - ALLOW_PLAINTEXT_LISTENER=yes<br>      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT;PLAINTEXT_HOST:PLAINTEXT<br>      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092;PLAINTEXT_HOST://:29092<br>      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092;PLAINTEXT_HOST://localhost:29092<br>    depends_on:<br>      - zookeeper<br>    networks:<br>      - my-network<br><br>volumes:<br>  zookeeper_data:<br>    driver: local<br>  kafka_data:<br>    driver: local<br><br>networks:<br>  my-network:<br>    driver: bridge<br></code></pre><br>
0.6666666666666666,0.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>Unable to sort in Elastic Search with Pipeline Aggregation</h3><p>I am trying to sort some data in my ES query. I have tried a lot of things and spend a lot of time; But unable to find the solution. Below is the query. <br>Thanks in advance for help. </p><br><br><pre><code>"size":0;<br>"aggs":{ <br>"abc":{ <br>"composite":{ <br>"sources":[ <br>{ <br>"abc_id":{ <br>"terms":{ <br>"field":"l1.id"<br>}<br>}<br>}<br>];<br>"size":"100"<br>};<br>"aggs":{ <br>"impressions":{ <br>"filters":{ <br>"filters":[ <br>{ <br>"bool":{ <br>"must":[ <br>{ <br>"exists":{ <br>"field":"l3.imp"<br>}<br>}<br>]<br>}<br>}<br>]<br>};<br>"aggs":{ <br>"kpi":{ <br>"bucket_script":{ <br>"buckets_path":{ <br>"imp":"sum_of_imp"<br>};<br>"script":"params.imp"<br>}<br>};<br>"sum_of_imp":{ <br>"sum":{ <br>"field":"l3.imp"<br>}<br>};<br>"sorting_sum_of_imp":{<br>  "bucket_sort": {<br>    "sort": [{<br>      "sum_of_imp":{<br>        "order":"desc"<br>      }<br>    }]<br>  }<br>}<br>}<br>}<br>}<br>}<br>};<br>"query":{ <br>"bool":{ <br>"filter":{ <br>"bool":{ <br>"must":[ <br>{ <br>"term":{ <br>"ar.type":"search"<br>}<br>};<br>{ <br>"range":{ <br>"as_on":{ <br>"gte":"2018-12";<br>"lte":"2019-12"<br>}<br>}<br>}<br>];<br>"must_not":[ <br>]<br>}<br>}<br>}<br>}<br>}<br></code></pre><br>
0.0,0.3333333333333333,0.3333333333333333,0.0,0.3333333333333333,0.6666666666666666,0.3333333333333333,<h3>How to generate the proper RestTemplate usage from curl?</h3><p>I have a working curl command that is below:</p><br><br><pre><code>curl -v  -H "Content-Type:application/octet-stream" \<br>         -H "x-amz-server-side-encryption:aws:kms" \<br>         -H "x-amz-server-side-encryption-aws-kms-key-id:abcdef81-abcd-4c85-b1d8-ee540d0a5f5d" \<br>         --upload-file /Users/fd/Downloads/video.mp4 \<br>         'https://video-uploads-prod.s3-accelerate.amazonaws.com/ABCDEAQGZHEhM55fvvA/ads-aws_userUploadedVideo?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200106T165718Z&amp;X-Amz-SignedHeaders=content-type%3Bhost%3Bx-amz-server-side-encryption%3Bx-amz-server-side-encryption-aws-kms-key-id&amp;X-Amz-Expires=86400&amp;X-Amz-Credential=ABCDEFHLWTCWZ2MUPPBQ%2F20200106%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=037949abcd1234b063c75d3d505dd9120dd3fa9250c1ababa152e91fee123ca0'<br></code></pre><br><br><p>The curl is working properly:</p><br><br><pre><code>* We are completely uploaded and fine<br>&lt; HTTP/1.1 200 OK<br></code></pre><br><br><p>However; when I try to use RestTemplate (i'm using spring boot 1.5.6) I'm not able to make it work. The code I use is:</p><br><br><pre><code>byte[] media = //video in mp4//;<br>String uploadUrl = "https://video-uploads-prod.s3-accelerate.amazonaws.com/ABCDEAQGZHEhM55fvvA/ads-aws_userUploadedVideo?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20200106T165718Z&amp;X-Amz-SignedHeaders=content-type%3Bhost%3Bx-amz-server-side-encryption%3Bx-amz-server-side-encryption-aws-kms-key-id&amp;X-Amz-Expires=86400&amp;X-Amz-Credential=ABCDEFHLWTCWZ2MUPPBQ%2F20200106%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=037949abcd1234b063c75d3d505dd9120dd3fa9250c1ababa152e91fee123ca0";<br><br>RestTemplate restTemplate = new RestTemplate();<br><br>HttpHeaders headers = new HttpHeaders();<br>headers.setContentType(MediaType.APPLICATION_OCTET_STREAM);<br>headers.set("x-amz-server-side-encryption"; encryption);<br>headers.set("x-amz-server-side-encryption-aws-kms-key-id"; awsKmsKeyId);<br><br>HttpEntity entity = new HttpEntity&lt;&gt;(media; headers);<br><br>ResponseEntity&lt;String&gt; respEntity = restTemplate.exchange(uploadUrl; HttpMethod.POST; entity; String.class);<br></code></pre><br><br><p>The error I get from AWS is:</p><br><br><pre><code>&lt;Error&gt;<br>   &lt;Code&gt;AuthorizationQueryParametersError&lt;/Code&gt;<br>   &lt;Message&gt;Error parsing the X-Amz-Credential parameter; the Credential is mal-formed; expecting "&amp;lt;YOUR-AKID&amp;gt;/YYYYMMDD/REGION/SERVICE/aws4_request".&lt;/Message&gt;<br>   &lt;RequestId&gt;51FF099744C43804&lt;/RequestId&gt;<br>   &lt;HostId&gt;FOlLws+txYMP0hKEg7aDjQeeARdn7bJN+lw7q/aGA48hRnr1YEsJrVmRi6oEz+mkpHlTIax5MkI=&lt;/HostId&gt;<br>&lt;/Error&gt;<br></code></pre><br><br><p>My suspicion is that RestTemplate is changing the encoding of the URL. Is there anyway to replicate exactly the same as curl with RestTemplate?</p><br>
0.0,0.0,0.0,0.0,0.6666666666666666,0.6666666666666666,0.0,<h3>Why i get undefined in console?</h3><p>I am new to aws and serverless .I don't know why i get undefined in the variable called - first:<br>When i execute the function the command</p><br><pre><code>sls invoke local -f hello -d '{&quot;first&quot;:2}'<br></code></pre><br><p>The object</p><br><pre><code>{<br>   &quot;first&quot;:2<br>}<br></code></pre><br><p>is send to the serverless function.And it is in the response in the event property.<br>Why when i try to get the value of event.first i get undefined?</p><br><pre><code>'use strict'<br><br>module.exports.hello = (event; context; callback) =&gt; {<br>  const first = event.first;<br>  console.log(&quot;**&quot;; first);<br>  const response = {<br>    statusCode: 200;<br>    body: JSON.stringify({<br>      event: event<br>    });<br>  };<br>  callback(null; response);<br>}<br></code></pre><br><p>When i try event;first i need to get only the number in this case 2;but i get undefined</p><br>
0.0,0.0,0.3333333333333333,0.0,0.6666666666666666,0.6666666666666666,0.0,<h3>CodePipeline Task revisions are creating for ECS Fargate service; even i am not changing any task definitions</h3><p>Is there any work around to not create tasks revision on every new deploy?<br>Or is there a way to remove older tasks revision from codePipeline after creating new one?</p><br><br><p>Thank you!</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.3333333333333333,<h3>Creating a StringLike condition with Terraform</h3><p>I am trying to generate some terraform for an aws IAM policy. The condition in the policy looks like this</p><br><pre><code>&quot;StringLike&quot;: {<br> &quot;kms:EncryptionContext:aws:cloudtrail:arn&quot;: [<br> &quot;arn:aws:cloudtrail:*:aws-account-id:trail/*&quot;<br> ]<br></code></pre><br><p>I am looking at the documentation for <code>aws_iam_policy_document</code>: <a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document" rel="nofollow noreferrer">https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document</a>; but it's not clear to me as to how to write this in terraform. Any help would be greatly apprecaited. This is my attempt</p><br><pre><code>condition {<br>        test = &quot;StringLike&quot;<br>        variable = &quot;kms:EncryptionContext:aws:cloudtrail:arn&quot;<br><br>        values = [<br>            &quot;arn:aws:cloudtrail:*:aws-account-id:trail/*&quot;<br>        ]<br>    }<br></code></pre><br>
0.0,0.6666666666666666,0.0,0.0,0.0,0.3333333333333333,0.0,<h3>AWS API Gateway 403 Forbidden response OPTIONS</h3><p>I am trying to call API of AWS through JEE and I got this error in the Chrome Console</p><br><p>[![enter image description here][1]][1]<br>But when I call same API from postman or when I use it in localhost it works.</p><br><p>Whats wrong ?</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>How to rename MYSQL database in RDS?</h3><p>I have searched enough to find steps to rename a Database name in RDS MySql and unable to find one. Can someone point to the place  that has database renaming instruction on an RDS instance?</p><br>
0.0,0.6666666666666666,1.0,0.0,0.0,0.0,0.0,<h3>How to enable Cloudwatch logging for AWS API GW via Cloudformation template</h3><p>I am trying to enable cloudwatch logs for AWS API Gateway via cloudformation template but it does not enables. I have tried setting up logginglevel to INFO in both Stage description and also Method settings. Any idea on what am I missing?</p><br><br><p>When I manually enable logging through UI; it works. Not working when I try to enable through cloudformation template as below -</p><br><br><p><em>Note: I am just using plain cloudformation template and I have already added role ARN that has permissions to API Gateway in my account to log cloudwatch</em></p><br><br><pre><code>TestDeployment:<br>  Type: AWS::ApiGateway::Deployment<br>  Properties:<br>    Description: API Deployment<br>    RestApiId: testApi<br>    StageName: 'dev'<br>    StageDescription:<br>      Description: Stage - DEV<br>      LoggingLevel: INFO<br>      MethodSettings:<br>        - ResourcePath: "/testresource"<br>          HttpMethod: "POST"<br>          LoggingLevel: INFO<br></code></pre><br>
0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>Return list of _all_ ip addresses using AWS CLI</h3><p>Using the AWS CLI; I'd like to retrieve a list of <em>all</em> IP addresses; whether EIP or statically assigned etc.</p><br><br><p>I've been using describe-instances and describe-addresses but want to know if there is an easier way to get all public IP addresses?</p><br><br><pre><code>aws ec2 describe-addresses --public-ips --region eu-west-1 --query 'Addresses[*].PublicIp'<br><br>aws ec2 describe-instances --region eu-west-1<br></code></pre><br><br><p>I've searched through the AWS documentation; but haven't found anything that encompasses everything.</p><br>
0.0,0.0,1.0,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>How to get Cognito User Pool Name by Pool Id</h3><p>For my Multi tenant Application I am creating Cognito user pools for each tenant with a special naming convention using Javascript SDK.  </p><br><br><p>When adding a user in Cognito User Pool for a tenant; an email notification will be sent.<br>I created a lambda function with a custom email message and configured it as a trigger in Custom Messages in Cognito User Pool.<br>Ref: <a href="https://docs.amazonaws.cn/en_us/cognito/latest/developerguide/cognito-user-pool-settings-message-templates.html" rel="nofollow noreferrer">https://docs.amazonaws.cn/en_us/cognito/latest/developerguide/cognito-user-pool-settings-message-templates.html</a></p><br><br><p>Now my concern here is I need to prepare a link and logo by the tenant name; for that purpose I need to get the Cognito User Pool Name. From the Lambda function; I can get the pool Id. </p><br><br><p>Can anyone help; how to get the Cognito User Pool Name by pool Id or how to pass dynamic custom parameters to my lambda function when it is triggered by Cognito User Pool.?? </p><br>
0.6666666666666666,0.0,0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>Kafka Broker ReplicaFetcherThread uses 100% CPU</h3><p>We have 6 brokers running on AWS EC2 (i3.large with 4vcpu); with approx 1000 partitions assigned to each of them. If we replace a broker (same broker ID); with zero data - and replication starts - regardless of how many num.replica.fetchers; we set (have tried 1-5) - the fetcher threads are always maxing out CPU. Currently we are on Confluent 5.3.2 open source.</p><br><br><pre><code>broker.rack=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)<br>broker.id=$KB_BROKER_ID<br>listeners=PLAINTEXT://$KB_ENIIP:9092<br>advertised.listeners=PLAINTEXT://$KB_ENIIP:9092<br>auto.create.topics.enable=false<br>default.replication.factor=3<br>inter.broker.protocol.version=2.3<br>log.dirs=$KB_LOGDIRDATA<br>log.message.format.version=2.3<br>log.roll.hours=24<br>log.retention.hours=48<br>log.segment.bytes=100000000<br>min.insync.replicas=1<br>num.recovery.threads.per.data.dir=4<br>offsets.retention.minutes=10080<br>num.network.threads=16<br>num.io.threads=20<br>socket.send.buffer.bytes=-1<br>socket.receive.buffer.bytes=-1<br>group.initial.rebalance.delay.ms=10000<br>num.partitions=6<br>transaction.state.log.replication.factor=3<br>zookeeper.connection.timeout.ms=1000000<br>zookeeper.connect=$ZK_CONN<br>zookeeper.session.timeout.ms=10000<br>message.max.bytes=5242880<br>replica.fetch.max.bytes=5242880<br>replica.fetch.response.max.bytes=5242880<br>replica.socket.receive.buffer.bytes=5242880<br>num.replica.fetchers=2<br>confluent.support.metrics.enable=false<br><br><br><br><br>      PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND<br> 6913 kafka     20   0   66.2g   5.3g  64128 R 87.5 17.7  13:11.24 ReplicaFetcherT<br> 6916 kafka     20   0   66.2g   5.3g  64128 R 81.2 17.7   8:34.31 ReplicaFetcherT<br> 6929 kafka     20   0   66.2g   5.3g  64128 R 81.2 17.7   8:55.83 ReplicaFetcherT<br> 6930 kafka     20   0   66.2g   5.3g  64128 R 68.8 17.7   8:04.13 ReplicaFetcherT<br> 6918 kafka     20   0   66.2g   5.3g  64128 S 12.5 17.7   0:41.58 ReplicaFetcherT<br> 6919 kafka     20   0   66.2g   5.3g  64128 S 12.5 17.7   3:13.76 ReplicaFetcherT<br> 6927 kafka     20   0   66.2g   5.3g  64128 S 12.5 17.7   5:05.56 ReplicaFetcherT<br> 6931 kafka     20   0   66.2g   5.3g  64128 R  6.2 17.7   7:30.61 ReplicaFetcherT<br> 6777 kafka     20   0   66.2g   5.3g  64128 S  0.0 17.7   0:00.00 java<br> 6807 kafka     20   0   66.2g   5.3g  64128 S  0.0 17.7   0:02.69 java<br></code></pre><br><br><ol><br><li>Does anyone know why this might be occurring ? (is this by design ?)</li><br><li>Is it possible to limit CPU usage of the replicathreads ?</li><br><li>Is the only possible solution; to throw more CPU at the issue ? I did test an instance with 8vcpu; however the fetcher threads still wanted to consume all CPU.</li><br><li>Can setting more num.io.threads and num.network.threads actually increase CPU usage ?</li><br></ol><br>
0.0,0.3333333333333333,0.6666666666666666,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>error CloudFormation template removing instead of creating new elements</h3><p>I am new to AWS Cloud Formation; well I am reusing 2 templates; the first one works totally fine; it creates a Network Stack for AWS Fargate; please see template #1 below; but the second one (which is failing) supposed to creates the services; instead it is trying to delete most of the elements of the Network Stack; please see template #2 below.</p><br><br><p>I can see in the "Changes Preview" how it is marking to "remove" almost everything that I created before with the Network Stack template; please see image below #3.</p><br><br><p><strong>Does somebody can advise what is wrong with the second template?; thank you.</strong></p><br><br><p>1) <strong>Network Stack</strong></p><br><br><pre><code>AWSTemplateFormatVersion: '2010-09-09'<br>Description: Create a network stack with a public vpc; fargate cluster and load balancer as a parent stack. <br><br>Mappings: <br>  SubnetConfig:<br>    VPC:<br>      CIDR: '10.0.0.0/16'<br>    PublicOne:<br>      CIDR: '10.0.0.0/24'<br>    PublicTwo:<br>      CIDR: '10.0.1.0/24'  <br><br>Resources: <br>  FargateVpc:<br>    Type: AWS::EC2::VPC<br>    Properties:<br>      EnableDnsSupport: true<br>      EnableDnsHostnames: true<br>      CidrBlock: !FindInMap ['SubnetConfig'; 'VPC'; 'CIDR']      <br><br>  PublicSubnetOne:<br>    Type: AWS::EC2::Subnet<br>    Properties:<br>      AvailabilityZone: <br>        Fn::Select:<br>        - 0<br>        - Fn::GetAZs: {Ref: 'AWS::Region'}   <br>      VpcId: !Ref FargateVpc      <br>      CidrBlock: !FindInMap ['SubnetConfig'; 'PublicOne'; 'CIDR']<br>      MapPublicIpOnLaunch: true<br><br>  PublicSubnetTwo:<br>    Type: AWS::EC2::Subnet<br>    Properties:<br>      AvailabilityZone: <br>        Fn::Select:<br>        - 1<br>        - Fn::GetAZs: {Ref: 'AWS::Region'}   <br>      VpcId: !Ref FargateVpc      <br>      CidrBlock: !FindInMap ['SubnetConfig'; 'PublicTwo'; 'CIDR']<br>      MapPublicIpOnLaunch: true      <br><br>  InternetGateway:<br>    Type: AWS::EC2::InternetGateway <br><br>  GatewayAttachment:<br>    Type: AWS::EC2::VPCGatewayAttachment <br>    Properties:<br>      VpcId: !Ref FargateVpc<br>      InternetGatewayId: !Ref InternetGateway<br><br>  PublicRouteTable:<br>    Type: AWS::EC2::RouteTable<br>    Properties:<br>      VpcId: !Ref FargateVpc <br><br>  PublicRoute:<br>    Type: AWS::EC2::Route <br>    Properties:<br>      RouteTableId: !Ref PublicRouteTable<br>      DestinationCidrBlock: '0.0.0.0/0' <br>      GatewayId: !Ref InternetGateway  <br><br>  PublicSubnetOneRouteTableAssociation:<br>    Type: AWS::EC2::SubnetRouteTableAssociation <br>    Properties:<br>      SubnetId: !Ref PublicSubnetOne<br>      RouteTableId: !Ref PublicRouteTable    <br><br>  PublicSubnetTwoRouteTableAssociation:<br>    Type: AWS::EC2::SubnetRouteTableAssociation <br>    Properties:<br>      SubnetId: !Ref PublicSubnetTwo<br>      RouteTableId: !Ref PublicRouteTable          <br><br># ECS Cluster<br>  ECSCluster:<br>    Type: AWS::ECS::Cluster<br><br># ECS Roles<br><br># ECS Roles    <br># This role is used by the ECS tasks themselves.<br>  ECSTaskExecutionRole:<br>    Type: AWS::IAM::Role<br>    Properties:<br>      AssumeRolePolicyDocument:<br>        Statement:<br>        - Effect: Allow<br>          Principal:<br>            Service: [ecs-tasks.amazonaws.com]<br>          Action: ['sts:AssumeRole']<br>      Path: /<br>      Policies:<br>        - PolicyName: AmazonECSTaskExecutionRolePolicy<br>          PolicyDocument:<br>            Statement:<br>            - Effect: Allow<br>              Action:<br>                # Allow the ECS Tasks to download images from ECR<br>                - 'ecr:GetAuthorizationToken'<br>                - 'ecr:BatchCheckLayerAvailability'<br>                - 'ecr:GetDownloadUrlForLayer'<br>                - 'ecr:BatchGetImage'<br><br>                # Allow the ECS tasks to upload logs to CloudWatch<br>                - 'logs:CreateLogStream'<br>                - 'logs:PutLogEvents'<br>              Resource: '*'    <br><br>  # This is an IAM role which authorizes ECS to manage resources on our<br>  # account on our behalf; such as updating our load balancer with the<br>  # details of where our containers are; so that traffic can reach your<br>  # containers.<br>  ECSRole:<br>    Type: AWS::IAM::Role<br>    Properties:<br>      AssumeRolePolicyDocument:<br>        Statement:<br>        - Effect: Allow<br>          Principal:<br>            Service: [ecs.amazonaws.com]<br>          Action: ['sts:AssumeRole']<br>      Path: /<br>      Policies:<br>      - PolicyName: ecs-service<br>        PolicyDocument:<br>          Statement:<br>          - Effect: Allow<br>            Action:<br>              # Rules which allow ECS to attach network interfaces to instances<br>              # on our behalf in order for awsvpc networking mode to work right<br>              - 'ec2:AttachNetworkInterface'<br>              - 'ec2:CreateNetworkInterface'<br>              - 'ec2:CreateNetworkInterfacePermission'<br>              - 'ec2:DeleteNetworkInterface'<br>              - 'ec2:DeleteNetworkInterfacePermission'<br>              - 'ec2:Describe*'<br>              - 'ec2:DetachNetworkInterface'<br><br>              # Rules which allow ECS to update load balancers on our behalf<br>              # with the information about how to send traffic to our containers<br>              - 'elasticloadbalancing:DeregisterInstancesFromLoadBalancer'<br>              - 'elasticloadbalancing:DeregisterTargets'<br>              - 'elasticloadbalancing:Describe*'<br>              - 'elasticloadbalancing:RegisterInstancesWithLoadBalancer'<br>              - 'elasticloadbalancing:RegisterTargets'<br>            Resource: '*'<br><br># Load Balancer Security group<br>  PublicLoadBalancerSG:<br>    Type: AWS::EC2::SecurityGroup<br>    Properties:<br>      GroupDescription: Access to the public facing load balancer from entire internet range<br>      VpcId: !Ref FargateVpc<br>      SecurityGroupIngress:<br>        - CidrIp: 0.0.0.0/0<br>          IpProtocol: -1<br><br># Fargate Container Security Group<br>  FargateContainerSecurityGroup:<br>    Type: AWS::EC2::SecurityGroup<br>    Properties: <br>      GroupDescription: Access to fargate containers<br>      VpcId: !Ref FargateVpc    <br><br>  EcsSecurityGroupIngressFromPublicALB:<br>    Type: AWS::EC2::SecurityGroupIngress<br>    Properties:<br>      Description: Ingress from the public ALB <br>      GroupId: !Ref FargateContainerSecurityGroup   <br>      IpProtocol: -1<br>      SourceSecurityGroupId: !Ref PublicLoadBalancerSG<br><br>  EcsSecurityGroupIngressFromSelf:<br>    Type: AWS::EC2::SecurityGroupIngress<br>    Properties:<br>      Description: Ingress from other containers in the same security group <br>      GroupId: !Ref FargateContainerSecurityGroup<br>      IpProtocol: -1<br>      SourceSecurityGroupId: !Ref FargateContainerSecurityGroup<br><br># Load Balancer<br>  PublicLoadBalancer:<br>    Type: AWS::ElasticLoadBalancingV2::LoadBalancer <br>    Properties:<br>      Scheme: internet-facing <br>      LoadBalancerAttributes: <br>        - Key: idle_timeout.timeout_seconds<br>          Value: '30'<br>      Subnets: <br>        - !Ref PublicSubnetOne <br>        - !Ref PublicSubnetTwo <br>      SecurityGroups: [!Ref 'PublicLoadBalancerSG']      <br><br># Target Group<br>  DummyTargetGroupPublic:<br>    Type: AWS::ElasticLoadBalancingV2::TargetGroup<br>    Properties:<br>      HealthCheckIntervalSeconds: 6<br>      HealthCheckPath: /<br>      HealthCheckProtocol: HTTP<br>      HealthCheckTimeoutSeconds: 5<br>      HealthyThresholdCount: 2<br>      Name: !Join ['-'; [!Ref 'AWS::StackName'; 'drop-1']]<br>      Port: 80<br>      Protocol: HTTP<br>      UnhealthyThresholdCount: 2<br>      VpcId: !Ref 'FargateVpc'<br><br># Listener<br>  PublicLoadBalancerListener: <br>    Type: AWS::ElasticLoadBalancingV2::Listener<br>    DependsOn:  <br>      - PublicLoadBalancer     <br>    Properties:<br>      DefaultActions:<br>        - TargetGroupArn: !Ref 'DummyTargetGroupPublic'<br>          Type: 'forward'<br>      LoadBalancerArn: !Ref 'PublicLoadBalancer'      <br>      Port: 80<br>      Protocol: HTTP    <br><br>Outputs:<br>  VPCId:<br>    Description: The ID of the vpc that this stack is deployed on <br>    Value: !Ref FargateVpc<br>    Export: <br>      Name: !Join [':'; [!Ref 'AWS::StackName'; 'VPCId']]      <br>  PublicSubnetOne:<br>    Description: Public subnet one<br>    Value: !Ref 'PublicSubnetOne'<br>    Export:<br>      Name: !Join [ ':'; [ !Ref 'AWS::StackName'; 'PublicSubnetOne' ] ]<br>  PublicSubnetTwo:<br>    Description: Public subnet two<br>    Value: !Ref 'PublicSubnetTwo'<br>    Export:<br>      Name: !Join [ ':'; [ !Ref 'AWS::StackName'; 'PublicSubnetTwo' ] ]<br>  FargateContainerSecurityGroup:<br>    Description: A security group used to allow Fargate containers to receive traffic<br>    Value: !Ref 'FargateContainerSecurityGroup'<br>    Export:<br>      Name: !Join [ ':'; [ !Ref 'AWS::StackName'; 'FargateContainerSecurityGroup' ] ]      <br># ECS Outputs      <br>  ClusterName:<br>    Description: The name of the ECS cluster<br>    Value: !Ref 'ECSCluster'<br>    Export:<br>      Name: !Join [ ':'; [ !Ref 'AWS::StackName'; 'ClusterName' ] ]<br>  ECSRole:<br>    Description: The ARN of the ECS role<br>    Value: !GetAtt 'ECSRole.Arn'<br>    Export:<br>      Name: !Join [ ':'; [ !Ref 'AWS::StackName'; 'ECSRole' ] ]<br>  ECSTaskExecutionRole:<br>    Description: The ARN of the ECS role<br>    Value: !GetAtt 'ECSTaskExecutionRole.Arn'<br>    Export:<br>      Name: !Join [ ':'; [ !Ref 'AWS::StackName'; 'ECSTaskExecutionRole' ] ]<br>  PublicListener:<br>    Description: The ARN of the public load balancer's Listener<br>    Value: !Ref PublicLoadBalancerListener<br>    Export:<br>      Name: !Join [ ':'; [ !Ref 'AWS::StackName'; 'PublicListener' ] ]          <br>  ExternalUrl:<br>    Description: The url of the external load balancer<br>    Value: !Join [''; ['http://'; !GetAtt 'PublicLoadBalancer.DNSName']]<br>    Export:<br>      Name: !Join [ ':'; [ !Ref 'AWS::StackName'; 'ExternalUrl' ] ]            <br></code></pre><br><br><p>2) <strong>Service Stack</strong></p><br><br><pre><code><br>AWSTemplateFormatVersion: '2010-09-09'<br>Description: Deploy a service on AWS Fargate; hosted in a public subnet of a VPC; and accessible via a public load balancer<br><br># Input Paramters<br>Parameters:<br>  StackName: <br>    Type: String<br>    Default: test-fargate<br>    Description: The name of the parent fargate networking stack<br>  ServiceName:<br>    Type: String<br>    Default: nginx<br>    Description: Name of the ECS service<br>  ImageUrl:<br>    Type: String<br>    Default: nginx<br>    Description: The url of a docker image that contains the application process that<br>                 will handle the traffic for this service<br>  ContainerPort:<br>    Type: Number<br>    Default: 80<br>    Description: What port number the application inside the docker container is binding to<br>  ContainerCpu:<br>    Type: Number<br>    Default: 256<br>    Description: How much CPU to give the container. 1024 is 1 CPU<br>  ContainerMemory:<br>    Type: Number<br>    Default: 512<br>    Description: How much memory in megabytes to give the container<br>  Path:<br>    Type: String<br>    Default: "*"<br>    Description: A path on the public load balancer that this service<br>                 should be connected to. Use * to send all load balancer<br>                 traffic to this service.<br>  Priority:<br>    Type: Number<br>    Default: 1<br>    Description: The priority for the routing rule added to the load balancer.<br>                 This only applies if your have multiple services which have been<br>                 assigned to different paths on the load balancer.<br>  DesiredCount:<br>    Type: Number<br>    Default: 2<br>    Description: How many copies of the service task to run<br>  Role:<br>    Type: String<br>    Default: ""<br>    Description: (Optional) An IAM role to give the service's containers if the code within needs to<br>                 access other AWS resources like S3 buckets; DynamoDB tables; etc<br><br>Conditions: <br>  HasCustomRole: !Not [!Equals [!Ref 'Role'; '']]                 <br><br># Task Definition  <br>Resources: <br>  TaskDefinition:<br>    Type: AWS::ECS::TaskDefinition <br>    Properties: <br>      Family: !Ref 'ServiceName'<br>      Cpu: !Ref 'ContainerCpu'<br>      Memory: !Ref 'ContainerMemory'<br>      NetworkMode: awsvpc<br>      RequiresCompatibilities: <br>        - FARGATE <br>      ExecutionRoleArn:<br>        Fn::ImportValue: <br>          !Join [':'; [!Ref 'StackName'; 'ECSTaskExecutionRole']]  <br>      TaskRoleArn:<br>        Fn::If: <br>          - 'HasCustomRole' <br>          - !Ref 'Role'<br>          - !Ref "AWS::NoValue"         <br>      ContainerDefinitions:<br>        - Name: !Ref 'ServiceName'<br>          Cpu: !Ref 'ContainerCpu'<br>          Memory: !Ref 'ContainerMemory'<br>          Image: !Ref 'ImageUrl'<br>          PortMappings: <br>            - ContainerPort: !Ref 'ContainerPort'      <br><br># ALB Target Group<br>  TargetGroup:<br>    Type: AWS::ElasticLoadBalancingV2::TargetGroup <br>    Properties: <br>      HealthCheckIntervalSeconds: 6<br>      HealthCheckPath: /<br>      HealthCheckProtocol: HTTP <br>      HealthCheckTimeoutSeconds: 5<br>      HealthyThresholdCount: 2<br>      TargetType: ip <br>      Name: !Ref 'ServiceName'<br>      Port: !Ref 'ContainerPort'<br>      Protocol: HTTP <br>      UnhealthyThresholdCount: 2<br>      VpcId:                     <br>        Fn::ImportValue:<br>          !Join [':'; [!Ref 'StackName'; 'VPCId']]<br><br># ALB Rule<br>  LoadBalancerRule:<br>    Type: AWS::ElasticLoadBalancingV2::ListenerRule<br>    Properties:<br>      Actions:<br>        - TargetGroupArn: !Ref 'TargetGroup'       <br>          Type: 'forward'<br>      Conditions:<br>        - Field: path-pattern<br>          Values: [!Ref 'Path']       <br>      ListenerArn: <br>        Fn::ImportValue:<br>          !Join [':'; [!Ref 'StackName'; 'PublicListener']] <br>      Priority: !Ref 'Priority'                         <br><br># ECS or Fargate Service     <br>  Service: <br>    Type: AWS::ECS::Service <br>    DependsOn: LoadBalancerRule <br>    Properties: <br>      ServiceName: !Ref 'ServiceName'<br>      Cluster: <br>        Fn::ImportValue: <br>          !Join [':'; [!Ref 'StackName'; 'ClusterName']] <br>      LaunchType: FARGATE <br>      DeploymentConfiguration: <br>        MaximumPercent: 200<br>        MinimumHealthyPercent: 75<br>      DesiredCount: !Ref 'DesiredCount'    <br>      NetworkConfiguration: <br>        AwsvpcConfiguration:<br>          AssignPublicIp: ENABLED<br>          SecurityGroups: <br>            - Fn::ImportValue:<br>                !Join [':'; [!Ref 'StackName'; 'FargateContainerSecurityGroup']]    <br>          Subnets:<br>            - Fn::ImportValue:<br>                !Join [':'; [!Ref 'StackName'; 'PublicSubnetOne']]                          <br>            - Fn::ImportValue:<br>                !Join [':'; [!Ref 'StackName'; 'PublicSubnetTwo']]    <br>      TaskDefinition:  !Ref TaskDefinition<br>      LoadBalancers:<br>        - ContainerName: !Ref 'ServiceName'<br>          ContainerPort: !Ref 'ContainerPort'<br>          TargetGroupArn: !Ref 'TargetGroup'                                                       <br></code></pre><br><br><p><a href="https://i.stack.imgur.com/IaWMb.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/IaWMb.png" alt="enter image description here"></a></p><br>
0.0,0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,0.0,<h3>AWS Lambda create folder in S3 bucket</h3><p>I have a Lambda that runs when files are uploaded to S3-A bucket and moves those files to another bucket S3-B. The challenge is that I need create a folder inside S3-B bucket with a corresponding date of uploaded files and move the files to the folder. Any help or ideas are greatly apprecited. It might sound confusing so feel free to ask questions.Thank you!</p><br>
0.0,0.0,0.0,0.0,0.6666666666666666,0.6666666666666666,0.3333333333333333,<h3>Jenkins Poll SCM is showing next time to check is the following day</h3><p>I'm attempting to run jenkins on an AWS EC2 instance. I have updated the EC2 instance to the correct timezone and I've updated the JAVA_ARGS in /etc/default/jenkins to have the same timezone as my EC2 instance. I'm not even sure if timezone is the problem. But in my Poll SCM section when I update the settings it shows the next time the job will run is the following day. Here is a screenshot take at 7/25/2020 9:35 my local time. As you can see it shows that it won't run until 7/26/2020 at 1:34.</p><br><p><a href="https://i.stack.imgur.com/1Ai6Y.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/1Ai6Y.png" alt="enter image description here" /></a></p><br><p>I also just now noticed that <em>while</em> it's building (triggered manually) it shows the date as tomorrow</p><br><p><a href="https://i.stack.imgur.com/93QjI.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/93QjI.png" alt="enter image description here" /></a></p><br><p>But once it's done building and I refresh the jenkins page it shows the right date</p><br><p><a href="https://i.stack.imgur.com/5UBVY.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/5UBVY.png" alt="enter image description here" /></a></p><br><p><strong>EDIT</strong><br>Following are screenshots of my user's timezone in the jenkins GUI and the timezone shown in the systemInfo page of the jenkins server</p><br><p><a href="https://i.stack.imgur.com/JYF9X.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/JYF9X.png" alt="enter image description here" /></a><br><a href="https://i.stack.imgur.com/yWvzl.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/yWvzl.png" alt="enter image description here" /></a></p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>How to Identify the exact IAM user permissions used by a Programatic access user</h3><p>In production I just wanted to give only the required permissions in the policy for IAM user.</p><br><p>Currently I have given S3FullAccess Policy for the IAM user; is there any way to identify which all permissions in that s3 bucket are been utilized by the IAMuser; Do we have any kind of logs that states this? So if I could get those permissions I could create a custom policy for the user with only specific permissions that the user requires.</p><br>
0.0,0.6666666666666666,0.6666666666666666,1.0,0.0,0.0,0.0,<h3>Create ClouldFormation stack for the specific folder in S3 bucket</h3><p>I tried out image resize functionality using cloud-formation service. when create the stack asked selected our s3 bucket. after creating the stack it gives cloud-front ApiEndpoint But it gave from the root directory of s3 bucket.</p><br><p>for the example :<br>s3 bucket path: <a href="https://surathbucket1.s3.ap-south-1.amazonaws.com/png.png" rel="nofollow noreferrer">https://surathbucket1.s3.ap-south-1.amazonaws.com/png.png</a></p><br><p>cloud-front path for stack: <a href="https://d1njf1ibm6jv3d.cloudfront.net/png.png" rel="nofollow noreferrer">https://d1njf1ibm6jv3d.cloudfront.net/png.png</a></p><br><p>I need to create a stack for a specific folder in s3 bucket instead of the root directory.</p><br><p>for the example:<br>s3 bucket path: <a href="https://surathbucket1.s3.ap-south-1.amazonaws.com/test/png.png" rel="nofollow noreferrer">https://surathbucket1.s3.ap-south-1.amazonaws.com/test/png.png</a></p><br><p>cloud-front path for stack: <a href="https://d1njf1ibm6jv3d.cloudfront.net/png.png" rel="nofollow noreferrer">https://d1njf1ibm6jv3d.cloudfront.net/png.png</a></p><br><p>I used this template for creating a stack<br><a href="https://docs.aws.amazon.com/solutions/latest/serverless-image-handler/template.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/solutions/latest/serverless-image-handler/template.html</a></p><br><p>anyone know the answer please help me. Thank you</p><br>
0.0,1.0,0.0,1.0,0.6666666666666666,0.0,0.0,<h3>My Lambda can&#39;t connect to my RDS instance</h3><p>I'm trying to create both services within the same VPC and give them appropriate security groups but they I can't make it work.</p><br><pre><code>variable &quot;vpc_cidr_block&quot; {<br>  default = &quot;10.1.0.0/16&quot;<br>}<br><br>variable &quot;cidr_block_subnet_public&quot; {<br>  default = &quot;10.1.1.0/24&quot;<br>}<br><br>variable &quot;cidr_block_subnets_private&quot; {<br>  default = [&quot;10.1.2.0/24&quot;; &quot;10.1.3.0/24&quot;; &quot;10.1.4.0/24&quot;]<br>}<br><br>data &quot;aws_availability_zones&quot; &quot;available&quot; {<br>  state = &quot;available&quot;<br>}<br><br>resource &quot;aws_vpc&quot; &quot;vpc&quot; {<br>  cidr_block = var.vpc_cidr_block<br>}<br><br>resource &quot;aws_subnet&quot; &quot;private&quot; {<br>  count = length(var.cidr_block_subnets_private)<br>  cidr_block = var.cidr_block_subnets_private[count.index]<br>  vpc_id = aws_vpc.vpc.id<br>  availability_zone = data.aws_availability_zones.available.names[count.index]<br>}<br><br>resource &quot;aws_security_group&quot; &quot;lambda&quot; {<br>  vpc_id = aws_vpc.vpc.id<br><br>  egress {<br>    from_port = 0<br>    to_port = 0<br>    protocol = &quot;-1&quot;<br>    cidr_blocks = [&quot;0.0.0.0/0&quot;]<br>  }<br>}<br><br>resource &quot;aws_security_group&quot; &quot;rds&quot; {<br>  vpc_id = aws_vpc.vpc.id<br><br>  ingress {<br>    description = &quot;PostgreSQL&quot;<br>    from_port = 5432<br>    protocol = &quot;tcp&quot;<br>    to_port = 5432<br>//    security_groups = [aws_security_group.lambda.id]<br>  }<br><br>  egress {<br>    from_port = 0<br>    to_port = 0<br>    protocol = &quot;-1&quot;<br>    cidr_blocks = [&quot;0.0.0.0/0&quot;]<br>  }<br>}<br><br>resource &quot;aws_lambda_function&quot; &quot;event&quot; {<br>  function_name = &quot;ServerlessExampleEvent&quot;<br><br>  timeout = 30<br><br>  s3_bucket = &quot;mn-lambda&quot;<br>  s3_key = &quot;mn/v1.0.0/lambda-1.0.0-all.jar&quot;<br><br>  handler = &quot;dk.fitfit.handler.EventRequestHandler&quot;<br>  runtime = &quot;java11&quot;<br><br>  memory_size = 256<br><br>  role = aws_iam_role.event.arn<br><br>  vpc_config {<br>    security_group_ids = [aws_security_group.lambda.id]<br>    subnet_ids = [for s in aws_subnet.private: s.id]<br>  }<br><br>  environment {<br>    variables = {<br>      JDBC_DATABASE_URL = &quot;jdbc:postgresql://${aws_db_instance.rds.address}:${aws_db_instance.rds.port}/${aws_db_instance.rds.identifier}&quot;<br>      DATABASE_USERNAME = aws_db_instance.rds.username<br>      DATABASE_PASSWORD = aws_db_instance.rds.password<br>    }<br>  }<br>}<br><br>resource &quot;aws_db_subnet_group&quot; &quot;db&quot; {<br>  subnet_ids = aws_subnet.private.*.id<br>}<br><br>resource &quot;aws_db_instance&quot; &quot;rds&quot; {<br>  allocated_storage = 10<br>  engine = &quot;postgres&quot;<br>  engine_version = &quot;11.5&quot;<br>  instance_class = &quot;db.t2.micro&quot;<br>  username = &quot;postgres&quot;<br>  password = random_password.password.result<br>  skip_final_snapshot = true<br>  apply_immediately = true<br><br>  vpc_security_group_ids = [aws_security_group.rds.id]<br>  db_subnet_group_name = aws_db_subnet_group.db.name<br>}<br><br>resource &quot;random_password&quot; &quot;password&quot; {<br>  length = 32<br>  special = false<br>}<br></code></pre><br><p>I tried to not clutter the question by only posting the relevant part of my HCL. Please let me know if I missed anything important.</p><br>
0.0,1.0,0.0,1.0,0.0,0.0,0.0,<h3>CloudFront origin-response return status: &#39;403&#39;</h3><p>I have setup CloudFront with S3 origin to serve images from S3. Now Adding Lamnda@Edge for image conversion. which works based on <code>status</code> code (<code>200</code> or <code>404</code>)</p><br><p>But Cloudfront event for origin-response always has <code>status: '403'</code></p><br><p>Here is the sample response (from <code>event.Records[0].cf</code>)</p><br><pre><code>{<br>    config: {<br>        distributionDomainName: 'xxxx.cloudfront.net';<br>        distributionId: 'xxxxx';<br>        eventType: 'origin-response';<br>        requestId: 'YVAViQDNbJcmgRlZFEWjxS2xF5balXwR-Kkv8PN4jXRO4hEPksOaJg=='<br>    };<br>    request: {<br>        clientIp: '81.403.0.141';<br>        headers: {<br>            referer: [Array];<br>            'x-forwarded-for': [Array];<br>            'user-agent': [Array];<br>            via: [Array];<br>            pragma: [Array];<br>            'accept-encoding': [Array];<br>            host: [Array];<br>            'cache-control': [Array]<br>        };<br>        method: 'GET';<br>        origin: {<br>            s3: [Object]<br>        };<br>        querystring: '';<br>        uri: '/images/product/photo/photocell_white.webp'<br>    };<br>    response: {<br>        headers: {<br>            'x-amz-request-id': [Array];<br>            'x-amz-id-2': [Array];<br>            date: [Array];<br>            server: [Array];<br>            'content-type': [Array];<br>            'transfer-encoding': [Array]<br>        };<br>        status: '403';<br>        statusDescription: 'Forbidden'<br>    }<br>} <br></code></pre><br><p>I have S3 bucket is publicly writable (I know its dangerous; but just to get this working).</p><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Sid&quot;: &quot;PublicAccess&quot;;<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Principal&quot;: &quot;*&quot;;<br>            &quot;Action&quot;: &quot;s3:*&quot;;<br>            &quot;Resource&quot;: &quot;arn:aws:s3:::stage.domain.com/*&quot;<br>        }<br>    ]<br>}<br></code></pre><br><p>In CloudFront</p><br><pre><code>Origin Domain Name : stage.domain.com.s3.amazonaws.com<br>Origin Path : /assets<br>Origin ID : S3-stage.domain.com<br></code></pre><br><p>What is wrong here</p><br><p>Lambda Function : <a href="https://pastebin.com/raw/FNd59Tvn" rel="nofollow noreferrer">https://pastebin.com/raw/FNd59Tvn</a></p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>Sagemaker Studio errors a loading screen to clear workspace every-time when connected &quot;open studio&quot;</h3><p>On resuming or restarting the sagemaker-studio I have the below message pop-up.</p><br><p><a href="https://i.stack.imgur.com/OXyNb.png" rel="noreferrer"><img src="https://i.stack.imgur.com/OXyNb.png" alt="Sagemaker-Studio Loading.. clear workspace error screen" /></a></p><br><p>Even after clearing the workspace; it won't open. After few such retries; the Jupiter notebook (sagemaker studio / IDE) opens. I get this message every time I reconnect and approx 15 mins are wasted each time :(</p><br><p>Am I doing something wrong while stopping/closing the sagemaker studio?</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>edit option group those belongs to only rds Group</h3><p>Is there any way I can segregate option group on the basis of resource they are attached to. <br>I have to make changes to the rds group who are attached to RDS (oracle) not to the snapshot and I have to perform this from powershell.</p><br>
0.0,0.0,0.5,0.0,1.0,0.0,0.0,<h3>How can I override the configuration such as Load Balancer and Capacity for my ElasticBeanstalk application</h3><p>I am trying to deploy a sample PHP application on the ElasticBeanstalk using CloudFormation as an attempt to learn the CloudFormation.</p><br><p>The following is my template.</p><br><pre><code>AWSTemplateFormatVersion: '2010-09-09'<br>Description: AWS CloudFormation Sample PHP Application on ElasticBeanstalk Template<br>Parameters:<br>  KeyName:<br>    Description: Name of an existing EC2 KeyPair to enable SSH access to the AWS Elastic<br>      Beanstalk instance<br>    Type: AWS::EC2::KeyPair::KeyName<br>    ConstraintDescription: must be the name of an existing EC2 KeyPair.<br>Mappings:<br>  Region2Principal:<br>    us-east-1:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    us-west-2:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    us-west-1:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    eu-west-1:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    eu-west-2:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    eu-west-3:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    ap-southeast-1:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    ap-northeast-1:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    ap-northeast-2:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    ap-northeast-3:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    ap-southeast-2:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    ap-south-1:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    us-east-2:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    ca-central-1:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    sa-east-1:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    cn-north-1:<br>      EC2Principal: ec2.amazonaws.com.cn<br>      OpsWorksPrincipal: opsworks.amazonaws.com.cn<br>    cn-northwest-1:<br>      EC2Principal: ec2.amazonaws.com.cn<br>      OpsWorksPrincipal: opsworks.amazonaws.com.cn<br>    eu-central-1:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>    eu-north-1:<br>      EC2Principal: ec2.amazonaws.com<br>      OpsWorksPrincipal: opsworks.amazonaws.com<br>Resources:<br>  WebServerRole:<br>    Type: AWS::IAM::Role<br>    Properties:<br>      AssumeRolePolicyDocument:<br>        Statement:<br>          - Effect: Allow<br>            Principal:<br>              Service:<br>                - Fn::FindInMap:<br>                    - Region2Principal<br>                    - Ref: AWS::Region<br>                    - EC2Principal<br>            Action:<br>              - sts:AssumeRole<br>      Path: /<br>  WebServerRolePolicy:<br>    Type: AWS::IAM::Policy<br>    Properties:<br>      PolicyName: WebServerRole<br>      PolicyDocument:<br>        Statement:<br>          - Effect: Allow<br>            NotAction: iam:*<br>            Resource: '*'<br>      Roles:<br>        - Ref: WebServerRole<br>  WebServerInstanceProfile:<br>    Type: AWS::IAM::InstanceProfile<br>    Properties:<br>      Path: /<br>      Roles:<br>        - Ref: WebServerRole<br>  SampleApplication:<br>    Type: AWS::ElasticBeanstalk::Application<br>    Properties:<br>      Description: AWS Elastic Beanstalk Sample PHP Application<br>  SampleApplicationVersion:<br>    Type: AWS::ElasticBeanstalk::ApplicationVersion<br>    Properties:<br>      Description: Version 1.0<br>      ApplicationName:<br>        Ref: SampleApplication<br>      SourceBundle:<br>        S3Bucket:<br>          Fn::Join:<br>            - '-'<br>            - - elasticbeanstalk-samples<br>              - Ref: AWS::Region<br>        S3Key: php-sample.zip<br>  SampleConfigurationTemplate:<br>    Type: AWS::ElasticBeanstalk::ConfigurationTemplate<br>    Properties:<br>      ApplicationName:<br>        Ref: SampleApplication<br>      Description: SSH access to PHP Application<br>      SolutionStackName: 64bit Amazon Linux 2 v3.0.3 running PHP 7.3<br>      OptionSettings:<br>        - Namespace: aws:autoscaling:launchconfiguration<br>          OptionName: EC2KeyName<br>          Value:<br>            Ref: KeyName<br>        - Namespace: aws:autoscaling:launchconfiguration<br>          OptionName: IamInstanceProfile<br>          Value:<br>            Ref: WebServerInstanceProfile<br>  SampleEnvironment:<br>    Type: AWS::ElasticBeanstalk::Environment<br>    Properties:<br>      Description: AWS Elastic Beanstalk Environment running Sample PHP Application<br>      ApplicationName:<br>        Ref: SampleApplication<br>      EnvironmentName: LaravelAppTesting<br>      TemplateName:<br>        Ref: SampleConfigurationTemplate<br>      VersionLabel:<br>        Ref: SampleApplicationVersion<br>Outputs:<br>  URL:<br>    Description: URL of the AWS Elastic Beanstalk Environment<br>    Value:<br>      Fn::Join:<br>        - ''<br>        - - http://<br>          - Fn::GetAtt:<br>              - SampleEnvironment<br>              - EndpointURL<br></code></pre><br><p>As you can see below; the ElasticBeanstalk has configurations such as Load Balancer; Capacity and so on. Now; I am trying to override; Load Balancer settings of Beanstalk. How can I do that?</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>AWS amplify working on the backend as a team</h3><p>I am interested in the AWS Amplify technology; but what I don't understand is how to enable other developers to collaborate on the backend.  I understand with the frontend you can just hook it up to github and it works fine and I've had friends make changes.  How do you work on the backend as a team?  Thanks!</p><br>
0.0,0.0,1.0,0.6666666666666666,0.0,0.0,0.3333333333333333,<h3>Is there a way to pass SSEKMSKeyId and ServerSideEncryption to scrapy FilesPipeline using AWS S3?</h3><p>Is there a way to pass SSEKMSKeyId and ServerSideEncryption to scrapy FilesPipeline using AWS S3 or do I need to write a separate pipeline?</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>SyntaxError (amazon-sagemaker-object-has-no-attribute)</h3><p>I'm running the code cell below; on SageMaker Notebook instance.</p><br><pre><code>pd.concat([train_data['y_yes']; train_data.drop(['y_no'; 'y_yes']; axis=1)]; axis=1).to_csv('train.csv'; index=False; header=False)<br>boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix; 'train/train.csv')).upload_file('train.csv')<br>s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket_name; prefix); content_type='csv')<br></code></pre><br><p>And if I hit; the following error is appearing:</p><br><pre><code>AttributeError: 'SageMaker' object has no attribute 's3_input'<br></code></pre><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>Spark Job Fails with Unknown Error writing to S3 from AWS EMR</h3><p>Can someone help to resolve this issue?</p><br><p>Thank you in advance.</p><br><p>Error logs :</p><br><pre><code>java.io.EOFException: Unexpected EOF while trying to read response from server<br>    at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:402)<br>    at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)<br>    at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1073)<br>20/07/22 22:43:37 WARN DataStreamer: Error Recovery for BP-439833631-172.19.222.143-1595381416559:blk_1073742309_1498 in pipeline [DatanodeInfoWithStorage[172.19.222.182:50010;DS-7783002b-d57a-43a3-9d91-9934e2d063f8;DISK]; DatanodeInfoWithStorage[172.19.223.27:50010;DS-1bc8cea7-9c28-4869-aada-55b7d0b0680c;DISK]; DatanodeInfoWithStorage[172.19.223.199:50010;DS-880d5121-16a2-465e-ad20-ca99f4287770;DISK]]: datanode 0(DatanodeInfoWithStorage[172.19.222.182:50010;DS-7783002b-d57a-43a3-9d91-9934e2d063f8;DISK]) is bad.<br>20/07/22 22:44:58 WARN DataStreamer: Exception for BP-439833631-172.19.222.143-1595381416559:blk_1073742309_1499<br>java.io.EOFException: Unexpected EOF while trying to read response from server<br>    at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:402)<br>    at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)<br>    at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1073)<br>20/07/22 22:44:58 WARN DataStreamer: Error Recovery for BP-439833631-172.19.222.143-1595381416559:blk_1073742309_1499 in pipeline [DatanodeInfoWithStorage[172.19.223.27:50010;DS-1bc8cea7-9c28-4869-aada-55b7d0b0680c;DISK]; DatanodeInfoWithStorage[172.19.223.199:50010;DS-880d5121-16a2-465e-ad20-ca99f4287770;DISK]; DatanodeInfoWithStorage[172.19.222.180:50010;DS-40b1c81b-18d1-4d8d-ab49-11904f3dd23c;DISK]]: datanode 0(DatanodeInfoWithStorage[172.19.223.27:50010;DS-1bc8cea7-9c28-4869-aada-55b7d0b0680c;DISK]) is bad.<br>20/07/22 22:47:00 WARN DataStreamer: Exception for BP-439833631-172.19.222.143-1595381416559:blk_1073742309_1500<br>java.io.EOFException: Unexpected EOF while trying to read response from server<br>    at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:402)<br>    at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)<br>    at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1073)<br>20/07/22 22:47:00 WARN DataStreamer: Error Recovery for BP-439833631-172.19.222.143-1595381416559:blk_1073742309_1500 in pipeline [DatanodeInfoWithStorage[172.19.223.199:50010;DS-880d5121-16a2-465e-ad20-ca99f4287770;DISK]; DatanodeInfoWithStorage[172.19.222.180:50010;DS-40b1c81b-18d1-4d8d-ab49-11904f3dd23c;DISK]; DatanodeInfoWithStorage[172.19.223.55:50010;DS-3e5dc677-cd1d-49fc-b50a-4b058ae298aa;DISK]]: datanode 0(DatanodeInfoWithStorage[172.19.223.199:50010;DS-880d5121-16a2-465e-ad20-ca99f4287770;DISK]) is bad.<br>20/07/22 22:47:03 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.0 GB; free 24.5 GB)<br></code></pre><br>
0.0,0.3333333333333333,0.0,0.3333333333333333,0.0,0.6666666666666666,0.6666666666666666,<h3>Django/React - Static Main.js file with AWS hosting staticfiles</h3><p>I'm creating an app with Django as the backend and React as the front-end. I already have the back-end built out; and now I'm trying to build out the React components. </p><br><br><p>My problem is this- right now I have my static files hosted on AWS. So every time I make a change to a component; I need to run collectstatic through django in order for my Django template to read the updated Main.js file.</p><br><br><p>Has anyone encountered this issue before? I am new to React; so I may be missing a very simple solution.</p><br><br><p>Thanks!!</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Filtering AWS CloudWatch raw log events by multiple values / AWS CLI</h3><p>Given the following query on CloudWatch that extracts logs with messages including "entry 1456" <em>(where 1456 is an ID)</em> how should I extend this to take multiple IDs and what is the corresponding CLI command?</p><br><br><pre><code>fields  @message<br>| filter @message like "entry 1456"<br>| limit 10<br></code></pre><br><br><p>To clarify I'd like to filter with multiple IDs; for instance "like 1456|1257|879". But not sure of the format of regex in such case.</p><br><br><p>And I assume the corresponding CLI command will be sth like:</p><br><br><pre><code>aws logs filter-log-events <br>--log-group-name group_name<br>--app<br>--filter-pattern ........<br></code></pre><br><br><p>Just want to make sure of the best way to formulate this.</p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,1.0,0.0,<h3>How to do database schema migrations as part of AWS CDK deploy?</h3><p>We are running a web application on AWS EC2 that connects to a MariaDB running on AWS RDS. We have AWS CDK scripts set up for creating a new infrastructure including everything from our own services (the web application itself; background workers; RabbitMQ; RDS DB) and AWS services such as ALB; Security groups; DNS; S3 etc. Everything works fine; i.e. an operational infra; all services running; monitoring and autoscaling are set up when I run <code>cdk deploy</code>.</p><br><p>However; at times we need to upgrade MariaDB schema. In old infra with no autoscaling it was simply done by setting the one EC2 instance to maintenance mode; deploying new software and running special manage commands to initiate DB schema upgrades using Alembic. The problem with running just <code>cdk deploy</code> is that the new software versions of the web application may be incompatible with the DB schema until the DB schema has been upgraded. <strong>The software that is running should always correspond with the DB schema</strong>; i.e. there should not be any old instances running when the DB schema has been upgraded.</p><br><ol><br><li><p>How to do <code>cdk deploy</code> in a controlled manner when there is a DB schema upgrade to be done so that there will not be instances running on an older software version that are incompatible with the new DB schema?</p><br></li><br><li><p>How to do DB schema upgrades as a part of <code>cdk deploy</code>?</p><br></li><br></ol><br><p>Regarding question 2: We have considered having a special version of the web application which would not be a part of autoscaling or open to the Internet; i.e. it would be used only for running special manager commands such as DB schema upgrades. Is this unnecessary or does AWS CDK have some mechanisms to do controlled deployment with relational DB schema upgrades?</p><br><p>Also; we plan to use AWS CDK as part of CI/CD; i.e. run it with Jenkins to deploy software to target environment. Previously we have been orchestrating all this using Salt from Jenkins.</p><br><p>NB: I have seen this question: <a href="https://stackoverflow.com/questions/63088820/how-to-perform-database-schema-migrations-after-deploying-with-aws-cdk">How to perform database schema migrations after deploying with AWS CDK?</a> but this different since this is more about software compatibility and I think the answers there missed the actual question.</p><br>
1.0,0.0,0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>How can solve a scheduling problem a .ipnyb notebook in Sagemaker using AWS lambda and Lifecycle Configuration?</h3><p>I want to schedule  my .ipynb file with Amazon Lambda. I am following the steps of this publications <a href="https://towardsdatascience.com/automating-aws-sagemaker-notebooks-2dec62bc2c84" rel="nofollow noreferrer">https://towardsdatascience.com/automating-aws-sagemaker-notebooks-2dec62bc2c84</a>. For notebook instance is working very well starting and stoping; but my .ipynb file is not executing;  i wrote  as the same above mentioned publication in lifecycle configuration.</p><br><p>Just i change these lines with my notebook instance source<br>&quot;NOTEBOOK_FILE=&quot;/home/ec2-user/SageMaker/Test Notebook.ipynb&quot;<br>/home/ec2-user/anaconda3/bin/activate &quot;$ENVIRONMENT&quot;<br>&quot;source /home/ec2-user/anaconda3/bin/deactivate&quot;.</p><br><p>Cloudwatch is working very well for notebook instance; but .ipynb file is not executed.</p><br><p>Can someone help me about my problem!</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>Error in creating table with column name containing dot (.) in Amazon Athena even after escaping the dot with backticks(`)</h3><p>As per <a href="https://docs.aws.amazon.com/athena/latest/ug/tables-databases-columns-names.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/athena/latest/ug/tables-databases-columns-names.html</a>;</p><br><blockquote><br><p><strong>Special characters</strong></p><br><p>Special characters other than underscore (_) are not supported. For<br>more information; see the Apache Hive LanguageManual DDL<br>documentation.</p><br><p><strong>Important</strong></p><br><p>Although you may succeed in creating table; view; database; or column<br>names that contain special characters other than underscore by<br>enclosing them in backtick (`) characters; subsequent DDL or DML<br>queries that reference them can fail.</p><br></blockquote><br><p>So; I tried to create a table using JSON file stored in S3 bucket and one of the keys in JSON contains multiple dots(.); which; as per the information given on the link; should be fine is I used backticks(`) to escape it.</p><br><pre><code>CREATE EXTERNAL TABLE json_table (<br>id string;<br>version string;<br>com`.`org`.`dto`.`Customer string )<br>ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'<br>WITH SERDEPROPERTIES ('ignore.malformed.json' = 'true')<br>LOCATION 's3://narendra-damodardas-modi-test-data/';<br></code></pre><br><p>But it is giving the following error:</p><br><pre><code>line 1:8: no viable alternative at input 'create external' (service: amazonathena; status code: 400; error code: invalidrequestexception; request id: ef586f31-2515-4faa-a9fe-3a0e418235d2)<br></code></pre><br><p>Now; you may say that as per the link provided; it is but obvious that it is not gonna work; but when I do this via Crawler in AWS Glue; everything works fine and I can see the column with dots in it.</p><br><p>As per <a href="https://docs.aws.amazon.com/athena/latest/ug/understanding-tables-databases-and-the-data-catalog.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/athena/latest/ug/understanding-tables-databases-and-the-data-catalog.html</a>;</p><br><blockquote><br><p>Regardless of how the tables are created; the tables creation process<br>registers the dataset with Athena. This registration occurs in the AWS<br>Glue Data Catalog and enables Athena to run queries on the data.</p><br></blockquote><br><p>So; AWS Athena is utilizing AWS Glue behind the scenes and if Glue's crawler is able to add columns containing dots(.) in the JSON key; why Athena's query is not able to do it.</p><br><p>Maybe I am missing something. So; if anyone has experienced something like this in that past and got past the problem; please enlighten me. And if it impossible to do what I am trying to do; please highlight that as well; so that I do not keep wasting my time.</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.3333333333333333,<h3>How log serverless outputs to console?</h3><p>Consider serverless template:</p><br><pre><code>service: my_application<br><br>frameworkVersion: &quot;&gt;=1.38.0 &lt;2.0.0&quot;<br><br>provider:<br>  name: aws<br>  region: us-east-1<br><br>resources:<br>  Resources:<br>    DataDeliveryRole:<br>      Type: AWS::IAM::Role<br>      Properties:<br>        AssumeRolePolicyDocument:<br>          Version: &quot;2012-10-17&quot;<br>          Statement:<br>            - Sid: &quot;&quot;<br>              Effect: Allow<br>              Principal:<br>                Service:<br>                  - iot.amazonaws.com<br>              Action:<br>                - sts:AssumeRole<br>        Path: &quot;/&quot;<br>        Policies:<br>          - PolicyName: my_policy<br>            PolicyDocument:<br>              Version: &quot;2012-10-17&quot;<br>              Statement:<br>                - Effect: Allow<br>                  Action:<br>                    - logs:CreateLogGroup<br>                    - logs:CreateLogStream<br>                    - logs:PutLogEvents<br>                    - logs:PutMetricFilter<br>                    - logs:PutRetentionPolicy<br>                  Resource:<br>                    - &quot;*&quot;<br></code></pre><br><p>Is there a way to log resource like <code>DataDeliveryRole</code> arn into console? Please note that resource name is not set.</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>How to run delete and insert query on S3 data on AWS</h3><p>So I have some historical data on S3 in .csv/.parquet format. Everyday I have my batch job running which gives me 2 files having the list of data that needs to be deleted from the historical snapshot; and the new records that needs to be inserted to the historical snapshot. I cannot run insert/delete queries on athena. What are the options (cost effective and managed by aws) do I have to execute my problem?</p><br>
0.0,0.0,1.0,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>Variable assignment inside AWS ssm does not work</h3><p>I am trying to run below command on AWS SSM:</p><br><pre><code>Parameters = @{<br>           commands = @(<br>           &quot;Write-Host userid is  $userID password $($password)&quot;<br>           '$userID2 = $userID'<br>           '$password2 = $password'<br>           &quot;Write-Host userid is  $userID2 password $($password2)&quot;<br>       ) <br>        }<br></code></pre><br><p>First Write-Host statement prints the correct values of $userID and $password but after the assignment of the new variable; second <code>Write-Host</code> variable prints empty for both variables. Am I doing something wrong? I tried fetching the values of <code>$userID</code> and <code>$password</code> with double quotes as well but no luck.</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.6666666666666666,<h3>AWS Amplify Redirects Gatsby 404 to Root URL</h3><p>I have deployed a multi-page gatsby site both to AWS Amplify and Netlify.</p><br><p>When I go to a page that doesn't exist on my site that was deployed with Netlify; I get the 404 page I have created. But when I go to the same non-existent route on the site with Amplify; it redirects me to the root path.</p><br><p>I have checked the network tab; and I get a 404 status code without any HTTP redirects. So I assume the redirect happens in the JavaScript code?</p><br><p>Does this happen to anyone else?</p><br><p>Here is the default <code>Rewrites and Redirects</code> for my site on Amplify:</p><br><p><a href="https://i.stack.imgur.com/pCl2K.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/pCl2K.png" alt="enter image description here" /></a></p><br>
0.0,0.6666666666666666,0.6666666666666666,0.6666666666666666,0.6666666666666666,0.0,0.0,<h3>Setting up SSH tunnel through AWS SSM proxycommand</h3><h3>My "problem":</h3><br><br><p>I want to connect to my DocumentDB via an SSH tunnel. I'm not sure if it's possible; but I wanted to give it a try.</p><br><br><h3>What's keeping me from doing it:</h3><br><br><p>I can't wrap my head around forwarding the DocumentDB port while supplying both my EC2 pem file as well as that of the DB (generic RDS pem).</p><br><br><h3>My setup:</h3><br><br><p>I've set up my infrastructure in ECS Fargate; connecting to a DocumentDB. The only ports made available to the public are 80 and 443 (80 to re-route to 443) and I want to keep it that way. I've set up a EC2 instance (no outgoing routes) I'm powering up when needed to connect via AWS SSM (and I want to keep it that way). </p><br><br><p>The EC2 Instance is an Amazon Linux 2 AMI.</p><br><br><p>Now I have set up a proxycommand to connect via SSH without having a port opened using SSM in my ~/.ssh/config file.</p><br><br><pre><code># --- SSH over AWS Session Manager ---<br>host i-*<br>    ProxyCommand sh -c "aws ssm start-session --target %h --document-name AWS-StartSSHSession --parameters 'portNumber=%p'"<br></code></pre><br><br><p>That's working as expected and I am able to connect via SSH and copy files from and to the EC2 instance via SCP. I can use this SSH connection (or the unaltered AWS SSM connection) to access my DocumentDB via the mongoshell; but now I'm asking myself if it would be possible to open a SSH tunnel to it. </p><br><br><h3>What I've tried:</h3><br><br><pre><code>ssh -L 8022:my-document-db-instance.amazonaws.com:27017 ssm-user@i-ec2-instance -i ~/.ssh/ec2-instance.pem -v<br></code></pre><br><br><p>The SSH tunnel to the EC2 instance has been created successfully (at least it seems so) and it's not showing any errors; but I can't connect to the database via Robo3T (or mongoshell). I suspect opening the connection in the tunnel is wrong; but I've got no clue how to start it. </p><br><br><p>I've tried opening a tunnel to the EC2 instance; then tunneling from there via Robo3T; but that didn't work as well (partially because I don't know which port to connect to on the EC2). </p><br><br><p>I have to admit I'm not the Linux admin I'd like to be. </p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>Amazon SNS for webhook to client (HTTP)</h3><p>I am building a service through AWS and I can't figure it out how to send webhook to my clients through http(s).</p><br><p>It seems like one solution would be to create an SNS topic for everyone of them; then subscribe their URL to this topic; but I keep thinking this is an overkill solution.. I mean; one topic with one URL for each client..</p><br><p>Is there a solution with SNS that would allow me to send messages directly to an URL with HTTP ? Without having to create a topic for each of the clients then subscribe their url to that topic ?</p><br><p>Thank you a lot</p><br>
0.0,0.0,0.0,0.6666666666666666,0.3333333333333333,1.0,0.0,<h3>Run a Python Script Stored in an AWS S3 Bucket on EC2 and Save Resulting File Back into S3 Bucket</h3><p>I have a Python script stored in an S3 bucket. I'd like to have it run in AWS (an EC2 instance presumably) and save its output (a pickle file) back into the same S3 bucket.</p><br><br><p>In the Python script itself; you specify a filename and just call to_pickle:</p><br><br><pre><code>def metadata_df(search_api;hashtags;since;until;filename;lat_long_only=True):<br><br>    if os.path.exists(filename):<br>        df = pickle.load(open(filename; 'rb'))<br>    else:<br>        df = ...<br><br>    df.to_pickle(filename)<br>    return df<br><br>...<br><br>if __name__ == "__main__":<br>    pickle_name = yesterday+'_'+'tweets.pkl'<br>    metadata_df(api.search; hashtags; since=yesterday;until=today; filename=pickle_name;lat_long_only=True)<br>...<br></code></pre><br><br><p>Wondering how I go about doing this (only need to run this a single time).</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>AWS RDS Postgres error while taking the dump</h3><p>When I try to take PostgresDump (AWS RDS) the following error I am getting:</p><br><pre class="lang-none prettyprint-override"><code>ERROR:  permission denied for relation dms_stats_detailed<br>pg_dump: error: query was: LOCK TABLE table_name IN ACCESS SHARE MODE<br></code></pre><br><p>I am having admin permission though (with Master User).</p><br>
0.3333333333333333,0.0,0.0,0.3333333333333333,0.0,0.0,0.3333333333333333,<h3>Bloomfilter with AWS Services</h3><p>I would like to give my users the ability to swip elements like Tinder.</p><br><p>Since I don't want these elements to show up over and over again; I would put them into a table/list and then check if they are already there.</p><br><p>It was recommended to solve this with a Bloomfilter.</p><br><p>Now; how do I solve this best with an AWS service?<br>I use AWS AppSync and DynamoDB</p><br><p>I could imagine that the user simply gets an item in a table:</p><br><pre><code>{owner: &quot;232-4232-323&quot;; seenCards: &quot;bloomFilterHashes&quot;}<br></code></pre><br><p>Then I would be able to create the filter from the data again and again?</p><br>
0.0,0.3333333333333333,0.6666666666666666,0.0,0.6666666666666666,0.6666666666666666,0.0,<h3>paramiko.ssh_exception.SSHException client.connect format</h3><p>I am trying to log in to a remote machine (EC2). But it keeps saying there is an SSHException and the key is Invalid. </p><br><br><pre><code>paramiko.ssh_exception.SSHException: Invalid key (class: RSAKey; data type: oQIBAAKCAQEApkTX3as35p1TF9W..............<br></code></pre><br><br><p>This is my code:</p><br><br><pre><code>import paramiko<br><br>amznKey = "MIIEoQIBAAKCAQEApkTX3as35p1TF9W............."<br>key = paramiko.RSAKey(data=bytes(amznKey; 'utf-8'))<br>client = paramiko.SSHClient()<br>client.get_host_keys().add('ubuntu@ec2-3-123-12-80.us-east-2.compute.amazonaws.com'; <br>'ssh-rsa'; key)<br>client.connect('ubuntu@ec2-2-134-99-80.us-east-2.compute.amazonaws.com'; username=''; password='')<br>stdin; stdout; stderr = client.exec_command('ls')<br>for line in stdout:<br>    print('... ' + line.strip('\n'))<br>client.close()<br></code></pre><br><br><p>And also; is there a better way to SSH to EC2 with python?</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>how to check the aws s3 url is valid or not</h3><p>I need to check the AWS s3 URL is valid or not using Nodejs; I need only the status code not all the data of the file<br>exa:-<a href="https://test.s3.us-east-2.amazonaws.com/occupancy.csv" rel="nofollow noreferrer">https://test.s3.us-east-2.amazonaws.com/occupancy.csv</a>; I applied request method but it takes all the data from the file..second method AWS s3.headObject  but it only checks the bucket name exist or not <br>tell me is there any method who give the status code that this URL has existed or not</p><br>
0.0,0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,0.3333333333333333,<h3>AWS: Node.js Lambda function inside CloudFormation to upload file to DeviceFarm URL</h3><p>I would like to write Node.js Lambda function inside CloudFormation template that gets a file from S3 and upload it to DeviceFarm URL.</p><br><br><p>My code:</p><br><br><pre><code>// get file from S3:<br>const zipTestsFile = await s3.getObject(params).promise().Body;<br><br>// <br>let deviceFarmUploadParams = {<br>    name: "file.zip";<br>    type: "APPIUM_NODE_TEST_PACKAGE";<br>    projectArn: project.arn<br>};<br>let UPLOAD = await devicefarm.createUpload(deviceFarmUploadParams).promise().then(<br>    function(data){<br>        return data.upload;<br>    };<br>    function(error){<br>        console.error("Creating upload failed with error: "; error);<br>    }<br>);<br>let UPLOAD_ARN = UPLOAD.arn;<br>let UPLOAD_URL = UPLOAD.url;<br></code></pre><br><br><p>How can I upload the file to device farm URL? <br>I can only use Node.js Built-in Modules.</p><br>
0.0,1.0,0.0,0.0,1.0,0.0,0.0,<h3>Why do we connect to our instance through IP address (ssh) instead of using instance id?</h3><p>Why we are connecting our instance through IP address(ssh) instead of using Instance ID?</p><br>
0.0,0.0,0.0,0.0,0.0,0.0,1.0,<h3>AWS media convert thumbnail genrated from video get rotated?</h3><p>In my application; I'm using AWS media-convert jobs to generate the thumbnail from my video uploaded to s3. The video gets uploaded from mobile. It works fine when I record a video from my mobile and uploads it at the same time. I got thumbnail generated properly.</p><br><p>But when I upload a pre-recorded video from the same mobile and upload it the thumbnail for that video get rotated by -90 degree.</p><br><p>Not sure what could be the problem here.</p><br>
0.0,0.0,1.0,0.3333333333333333,0.0,0.3333333333333333,0.0,<h3>codedeploy unable to access s3</h3><p><a href="https://i.stack.imgur.com/7Evg2.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/7Evg2.png" alt="enter image description here" /></a>I have a codepipeline on <strong>Account A</strong> and codedeployment group on <strong>Account B</strong>. I'm seeing the below error once the codedeployment group start the trigger<br><code>The IAM role arn:aws:iam::accountb:role/testcrss does not give you permission to perform operations in the following AWS service: Amazon S3. Contact your AWS administrator if you need help. If you are an AWS administrator; you can grant permissions to your users or groups by creating IAM policies.</code></p><br><p>I was referring to this <a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-create-cross-account.html" rel="nofollow noreferrer">document</a> provided by aws for aws cross-account deployment using codepipeline; do I need to configure anything other than the info provided in the document?</p><br><p><strong>policies attached to testcrss role</strong></p><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Action&quot;: [<br>                &quot;s3:Get*&quot;;<br>                &quot;s3:List*&quot;<br>            ];<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Resource&quot;: &quot;*&quot;<br>        }<br>    ]<br>}<br></code></pre><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Action&quot;: [<br>                &quot;kms:DescribeKey&quot;;<br>                &quot;kms:GenerateDataKey*&quot;;<br>                &quot;kms:Encrypt&quot;;<br>                &quot;kms:ReEncrypt*&quot;;<br>                &quot;kms:Decrypt&quot;<br>            ];<br>            &quot;Resource&quot;: [<br>                &quot;arn:aws:kms:us-east-2:AccountA:key/valuetest&quot;<br>            ]<br>        }<br>    ]<br>}<br></code></pre><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Action&quot;: [<br>                &quot;s3:Get*&quot;<br>            ];<br>            &quot;Resource&quot;: [<br>                &quot;arn:aws:s3:::AccountA bucket/*&quot;<br>            ]<br>        };<br>        {<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Action&quot;: [<br>                &quot;s3:ListBucket&quot;<br>            ];<br>            &quot;Resource&quot;: [<br>                &quot;arn:aws:s3:::AccountA bucket&quot;<br>            ]<br>        }<br>    ]<br>}<br></code></pre><br><p><strong>Bucket policy on Account A</strong></p><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Id&quot;: &quot;SSEAndSSLPolicy&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Sid&quot;: &quot;DenyUnEncryptedObjectUploads&quot;;<br>            &quot;Effect&quot;: &quot;Deny&quot;;<br>            &quot;Principal&quot;: &quot;*&quot;;<br>            &quot;Action&quot;: &quot;s3:PutObject&quot;;<br>            &quot;Resource&quot;: &quot;arn:aws:s3:::AccountAbucket/*&quot;;<br>            &quot;Condition&quot;: {<br>                &quot;StringNotEquals&quot;: {<br>                    &quot;s3:x-amz-server-side-encryption&quot;: &quot;aws:kms&quot;<br>                }<br>            }<br>        };<br>        {<br>            &quot;Sid&quot;: &quot;DenyInsecureConnections&quot;;<br>            &quot;Effect&quot;: &quot;Deny&quot;;<br>            &quot;Principal&quot;: &quot;*&quot;;<br>            &quot;Action&quot;: &quot;s3:*&quot;;<br>            &quot;Resource&quot;: &quot;arn:aws:s3:::AccountAbucket/*&quot;;<br>            &quot;Condition&quot;: {<br>                &quot;Bool&quot;: {<br>                    &quot;aws:SecureTransport&quot;: &quot;false&quot;<br>                }<br>            }<br>        };<br>        {<br>            &quot;Sid&quot;: &quot;&quot;;<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Principal&quot;: {<br>                &quot;AWS&quot;: &quot;arn:aws:iam::AccountB:root&quot;<br>            };<br>            &quot;Action&quot;: [<br>                &quot;s3:Get*&quot;;<br>                &quot;s3:Put*&quot;<br>            ];<br>            &quot;Resource&quot;: &quot;arn:aws:s3:::AccountAbucket/*&quot;<br>        };<br>        {<br>            &quot;Sid&quot;: &quot;&quot;;<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Principal&quot;: {<br>                &quot;AWS&quot;: &quot;arn:aws:iam::AccountB:root&quot;<br>            };<br>            &quot;Action&quot;: &quot;s3:ListBucket&quot;;<br>            &quot;Resource&quot;: &quot;arn:aws:s3:::AccountAbucket&quot;<br>        };<br>        {<br>            &quot;Sid&quot;: &quot;Cross-account permissions&quot;;<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Principal&quot;: {<br>                &quot;AWS&quot;: &quot;arn:aws:iam::AccountB:role/testcrss&quot;<br>            };<br>            &quot;Action&quot;: [<br>                &quot;s3:Get*&quot;;<br>                &quot;s3:List*&quot;<br>            ];<br>            &quot;Resource&quot;: &quot;arn:aws:s3:::AccountAbucket/*&quot;<br>        }<br>    ]<br>}<br></code></pre><br><p><strong>Trust Relationship for Role testcrss</strong></p><br><pre><code>{<br>  &quot;Version&quot;: &quot;2012-10-17&quot;;<br>  &quot;Statement&quot;: [<br>    {<br>      &quot;Sid&quot;: &quot;&quot;;<br>      &quot;Effect&quot;: &quot;Allow&quot;;<br>      &quot;Principal&quot;: {<br>        &quot;Service&quot;: [<br>          &quot;codedeploy.amazonaws.com&quot;;<br>          &quot;ec2.amazonaws.com&quot;<br>        ]<br>      };<br>      &quot;Action&quot;: &quot;sts:AssumeRole&quot;<br>    }<br>  ]<br>}<br></code></pre><br>
0.0,0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,0.0,<h3>AWS CodeBuild nodejs image with aws cli v2 installed</h3><p>We are building our project and we have to use AWS CLI v2 to deploy our project.</p><br><p>The runtime version that we use is this one:</p><br><pre><code>phases:<br>  install:<br>    runtime-versions:<br>      nodejs: 12.x<br></code></pre><br><p>Is there an official AWS CodeBuild nodejs image that we can use that has AWS CLI v2 installed or do we need to create our own. Is there an elegant way to upgrade to v2 for the above runtime?</p><br><p>This seems that it works but it might not be very stable in the future:</p><br><pre><code> # uninstall awscli version 1<br> - pip3 uninstall -y awscli<br> # install awscli version 2<br> - curl &quot;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip&quot; -o &quot;awscliv2.zip&quot;<br> - unzip awscliv2.zip<br> - ./aws/install<br></code></pre><br>
0.0,0.3333333333333333,0.0,1.0,0.3333333333333333,0.3333333333333333,0.3333333333333333,<h3>Django on GAE and AWS RDS PostgreSQL</h3><p>Is it possible to use an AWS RDS PostgreSQL database with a Django app hosted on Google App Engine standard (Python 3)?</p><br><br><p>Currently; any code that tries to connect to the RDS database hangs; but I can connect to the RDS database from my machine.</p><br>
0.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.0,<h3>Boto3 Call to Lambda Function Works in Dev but Not in Production?</h3><p>I'm trying to launch an AWS Lambda asynchronously from a Django Python app using Boto3. The following code works fine on my local development system:</p><br><br><pre><code>import boto3<br>from concurrent.futures import ThreadPoolExecutor<br><br>[.....]<br><br>endpoint_scoring_arn = "arn:aws:lambda:us-east-1:xxx:function:my-lambda-function"<br>data = {<br>    'my_data_param': 'my_data';<br>}<br><br># https://stackoverflow.com/a/39457165/364966<br># https://stackoverflow.com/questions/40377662/boto3-client-noregionerror-you-must-specify-a-region-error-only-sometimes<br>boto3_client = boto3.client('lambda'; region_name='us-east-1')<br>with ThreadPoolExecutor(max_workers=1) as executor:<br>future_execution = [executor.submit(boto3_client.invoke;<br>                FunctionName=endpoint_scoring_arn;<br>                InvocationType="Event";<br>                Payload=json.dumps(data)<br>                )]<br>try:<br>    print("boto3 exception:")<br>    print(future_execution[0]._exception.kwargs['report'])<br>except Exception as e0:<br>    print("No boto3 exception found")<br></code></pre><br><br><p>...but when I run it in production on EC2; the Lambda function never gets run.  There are no error messages in my Django debug.log; or in syslog; and the above code prints out <code>"No boto3 exception found"</code>.</p><br><br><p>What could account for this?</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.0,<h3>Why am I getting &quot;this.combinedGrant is not a function&quot; when synthesizing lambda functions with AWS CDK?</h3><p>I'm using AWS CDK to create my cloud stacks; I've identified that the error &quot;<strong>this.combinedGrant is not a function</strong>&quot; is being produced by the following code but I can't find any references to it:</p><br><pre><code>    let authFunctions = [<br>      {<br>        name: 'user-registration';<br>        handler: 'index.register';<br>        code: './handlers/user-registration';<br>        loginAccess: loginTable.grantFullAccess;<br>        otpAccess: otpTable.grantWriteData<br>      };<br></code></pre><br><p>...</p><br><pre><code>    for (let i in authFunctions) {<br>      let definition = authFunctions[i];<br>      let func = new Function(this; `${definition.name}-function`; {<br>        runtime: Runtime.NODEJS_12_X;<br>        handler: definition.handler;<br>        code: Code.asset(definition.code);<br>        environment: {<br>          LOGIN_TABLE_NAME: loginTable.tableName;<br>          OTP_TABLE_NAME: otpTable.tableName;<br>          MAILGUN_DOMAIN: credentials.mailgun.domain;<br>          MAILGUN_FROM: credentials.mailgun.from;<br>          MAILGUN_API_KEY: credentials.mailgun.api_key;<br>          JWT_SECRET: definition.hasJWTSecret ? &quot;secret-placeholder&quot; : &quot;&quot;<br>        };<br>        layers: [authLayer; utilityLayer];<br>        timeout: Duration.seconds(5)<br>      });<br><br>      definition.loginAccess(func);<br>      definition.otpAccess(func);<br><br>      let api = new RestApi(this; `${definition.name}-api`);<br>      api.root.addMethod('POST'; new LambdaIntegration(func));<br>    }<br><br></code></pre><br>
1.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.0,<h3>Amazon Lambda / API Gateway / Amazon Lex - Error: Handled</h3><p>I've created an API Gateway that links to my Amazon Lex bot. It has worked in the past; but for some reason it displays this error message now. The bot works when I test it in the 'Test Chatbot' feature in Lex.</p><br><br><p>The error occurred after I set the new function as a 'Lambda initialization and validation.'</p><br><br><p><strong>Error Message:</strong></p><br><br><pre><code>START RequestId: e18b78ef-3177-4e2f-999a-33cd48258a60 Version: $LATEST<br>2020-01-26T05:37:01.003Z    e18b78ef-3177-4e2f-999a-33cd48258a60    ERROR   Invoke Error    {"errorType":"Error";"errorMessage":"handled";"stack":["Error: handled";"    at _homogeneousError (/var/runtime/CallbackContext.js:13:12)";"    at postError (/var/runtime/CallbackContext.js:30:54)";"    at done (/var/runtime/CallbackContext.js:57:7)";"    at fail (/var/runtime/CallbackContext.js:67:7)";"    at /var/runtime/CallbackContext.js:105:16";"    at processTicksAndRejections (internal/process/task_queues.js:93:5)"]}END RequestId: e18b78ef-3177-4e2f-999a-33cd48258a60<br>REPORT RequestId: e18b78ef-3177-4e2f-999a-33cd48258a60  Duration: 579.06 ms Billed Duration: 600 ms Memory Size: 128 MB Max Memory Used: 93 MB  <br></code></pre><br><br><p><strong>API:</strong> </p><br><br><pre><code>var AWS = require('aws-sdk');<br>AWS.config.update({region: 'us-east-1'});<br><br>var lexruntime = new AWS.LexRuntime();<br><br>function main(event) {<br>  var params = {<br>    botAlias: 'myBot';<br>    botName: 'myBot';<br>    inputText: event.body.inputText; <br>    userId: 'myUserId'<br><br>  };<br>  return new Promise((resolve; reject) =&gt; {<br>    lexruntime.postText(params; function(err; data) {<br>      if (err) { <br>        var err2 = err.errorMessage<br>        reject(err2)<br>      }<br>      else {     <br>        resolve({data})<br>      }<br>    });<br>  });<br>}<br><br>exports.handler = main;<br></code></pre><br><br><p><strong>New function that I created right before the error message showed up:</strong></p><br><br><pre><code>'use strict';<br><br>function close(sessionAttributes; intentName; slots; slotToElicit; message) {<br>    return {<br>        sessionAttributes;<br>        dialogAction: {<br>            type: 'ElicitSlot';<br>            intentName: intentName;<br>            slots;<br>            slotToElicit;<br>            message;<br>        };<br>    };<br>}<br><br>function delegate(sessionAttributes; slots){<br>    return { <br>        sessionAttributes: sessionAttributes;<br>        dialogAction: {<br>            type: "Delegate";<br>            slots: slots<br>        }    <br>    };<br>}<br><br>function dispatch(intentRequest; callback) {<br>    const sessionAttributes = intentRequest.sessionAttributes;<br>    if(!sessionAttributes.userStatus){<br>        var param1 = {<br>            Email: null<br>        };<br>        intentRequest.currentIntent.slots = param1;<br>        callback(close(sessionAttributes; 'Verify'; intentRequest.currentIntent.slots; 'Email'; { "contentType": "PlainText"; "content": "Please enter your email"}));<br>    }<br>    else {<br>        callback(delegate(sessionAttributes; intentRequest.currentIntent.slots));<br>    }<br>}<br><br>exports.handler = (event; context; callback) =&gt; {<br>    try {<br>        dispatch(event;<br>            (response) =&gt; {<br>                callback(null; response);<br>            });<br>    } catch (err) {<br>        callback(err);<br>    }<br>};<br></code></pre><br><br><p>What is the meaning of the error?</p><br>
0.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.0,<h3>Updating the minimum count for ecs using boto3</h3><p>With AWS boto3 ; we are able to use the update service to set desired count to 0. Is there a way to update the minimum count also. Any suggestions</p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.0,0.6666666666666666,<h3>S3 coping files between buckets on different accounts without modifying any polices?</h3><p>S3 coping files between buckets on different accounts without modifying any polices.<br>S3 coping files between buckets on different accounts without modifying any polices.<br>S3 coping files between buckets on different accounts without modifying any polices</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>Combining AWS step function task and map outputs in to one array</h3><p>I have a task state that outputs the following:</p><br><pre><code>&quot;batch&quot;: {<br>    &quot;batch&quot;: &quot;size&quot;;<br>    &quot;currentTimestamp&quot;: 1596205376<br>  };<br></code></pre><br><p>and a map state out outputs an array:</p><br><pre><code>&quot;batch&quot;: [<br>    {<br>      &quot;batch&quot;: &quot;product-batch-0&quot;;<br>      &quot;currentTimestamp&quot;: 1596205376<br>    };<br>    {<br>      &quot;batch&quot;: &quot;product-batch-1&quot;;<br>      &quot;currentTimestamp&quot;: 1596205376<br>    }<br>]<br></code></pre><br><p>I would like to combine them so that the input to the state that follows the map state is this:</p><br><pre><code>  &quot;batch&quot;: [<br>    {<br>    &quot;batch&quot;: &quot;Size&quot;;<br>    &quot;currentTimestamp&quot;: 1596205376<br>    };<br>    {<br>      &quot;batch&quot;: &quot;product-batch-22&quot;;<br>      &quot;currentTimestamp&quot;: 1596205376<br>    };<br>    {<br>      &quot;batch&quot;: &quot;product-batch-8&quot;;<br>      &quot;currentTimestamp&quot;: 1596205376<br>    }<br>]<br></code></pre><br><p>Is this possible using the input/output processing available in aws step functions? I want to have them contained in one array so they can be processed together in an additional map state later in the state machine.</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>Aws How to know the region of the Bucket using Java SDK</h3><p>I using the following code to get the <code>listBuckets()</code>;</p><br><br><pre><code>        AmazonS3ClientBuilder s3 = AmazonS3ClientBuilder.standard().withRegion("us-east-1");<br>        AmazonS3 s3Client = s3.build();<br>        bucketList = s3Client.listBuckets();<br></code></pre><br><br><p>There are two things here:</p><br><br><ol><br><li>It returns the list of all the Buckets present; irrespective of the region.</li><br><li>Each item of <code>bucketList()</code> which is a <code>Bucket</code> contains only these attributes: </li><br></ol><br><br><pre><code>bucket.getName()<br>bucket.getOwner()<br>bucket.getCreationDate()<br></code></pre><br><br><p>It contains no <em>field/method</em> which can provide which region it lies in.</p><br><br><p>Also; when I try to get the count of <code>Objects</code> in this bucket using </p><br><br><pre><code>s3Client.listObjectsV2(bucket.getName()).getKeyCount())<br></code></pre><br><br><p>I get the following Exception:</p><br><br><pre><code>com.amazonaws.services.s3.model.AmazonS3Exception: The bucket is in this region: us-east-2. Please use this region to retry the request (Service: Amazon S3; Status Code: 301; Error Code: PermanentRedirect;<br></code></pre><br><br><p>I completely understood the error; but I need help with all of the following things:</p><br><br><ul><br><li>How to <code>listBuckets()</code> for a specific region only; without creating an <code>s3Client</code> for every region. Let's say my <code>s3Client</code> is of <code>us-east-1</code>; how do I get a list of all the buckets in region <code>ap-south-1</code>.</li><br><li>How do I know the region of every bucket present in the <code>listBucket()</code> response as there are only <code>name; owner; and creation date</code> attributes.</li><br><li>How do I get list of <code>Objects</code> of some another region using s3Client of some another region.</li><br></ul><br><br><p>I guess it has something to do with <code>headers</code>.</p><br><br><p>Also a bonus help (or else I will have to ask another question)</p><br><br><ul><br><li>While downloading and uploading of objects; how can I display the progress of Upload/ Download of object; let's say my file size is 3GB; I need a way to show that 50% or a half filled progress bar perhaps to show 1.5GB is uploaded/downloaded.</li><br></ul><br>
0.0,0.0,0.0,0.0,1.0,1.0,0.0,<h3>Parallel Invocation of Lambda functions by another lambda function asynchronously in Python</h3><p>I have a master lambda function (say X) which calls another lambda function (say Y) asynchronously. Once one instance of lambda function Y is triggered and executed it dumps data to kinesis or s3 and then there is the final lambda function (say Z) which takes that data from kinesis or s3 and as input and gets triggered. This is the entire flow process and the process is completely event driven. </p><br><br><p>Now the master function (X) is triggered with different inputs for each run. For the first lambda function call (Y) it creates as many number instances as the input to the master and different AWS request id for each instance. But for the second slave lambda function (Z) this is not being created. It is creating only one aws request id (which means the process is not asynchronous). But if I assign a wait time of 20 seconds to the master lambda function (X) then this problem is no more there. I am using Python 3.6 for the process .</p><br><br><p>Why does this happen ? Is this a known issue ? Is there any better approach to do the same thing to achieve the complete end-to-end flow process ?</p><br><br><p>This the code for my master lambda function(X):</p><br><br><p>***************************sample code***********************************</p><br><br><pre><code>def master_handler(event; context):<br>"""<br>Function to trigger slave code<br>"""<br>    client = boto3.client('lambda')<br>    aws_request_id = context.aws_request_id<br>    inputs = ['abc';'def';'ghi']<br>    for i in inputs:<br>        payload = {'current_input': '{}'.format(i);<br>                   'parent_aws_request_id': '{}'.format(aws_request_id)<br>                  }<br>        client.invoke(<br>            FunctionName=&lt;slave function name&gt;;<br>            InvocationType='Event';<br>            Payload=json.dumps(payload)<br>        )<br>        print(f"Trigger executed for {i}")<br>    print(f"Master function execution completed!")<br></code></pre><br>
0.0,1.0,0.0,0.3333333333333333,0.3333333333333333,0.0,0.0,<h3>$_GET variables don&#39;t work my ec2 instance using cloudfront aws</h3><p>On my website; <code>$_GET</code> variables for php do not work (undefined index error). For my behavior settings; I have this set:</p><br><p><a href="https://i.stack.imgur.com/wHn2C.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/wHn2C.png" alt="enter image description here" /></a></p><br><p>Is there another option I have to set?</p><br>
0.0,0.0,0.6666666666666666,0.0,1.0,0.0,0.3333333333333333,<h3>Spinnaker CloudDriver Pod crashing because of IAM issue</h3><p>When deploying Spinnaker to EKS via hal deploy apply; Spinnaker Clouddriver pod goes to CrashLoopBackOff with the following error;</p><br><br><blockquote><br>  <p>Caused by: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: User: arn:aws:sts::xxxxxxxxxxxx:assumed-role/Spinnaker-k8s-Worker-Node-Role/i-yyyyyyyyyyyyyyy is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::xxxxxxxxxxxx:role/Spinnaker-Managed-Role</p><br></blockquote><br><br><p>My Halyard config is like follows;</p><br><br><pre><code>currentDeployment: default<br>deploymentConfigurations:<br>- name: default<br>  version: 1.17.6<br>  providers:<br>    appengine:<br>      enabled: false<br>      accounts: []<br>    aws:<br>      enabled: true<br>      accounts:<br>      - name: my-account<br>        requiredGroupMembership: []<br>        providerVersion: V1<br>        permissions: {}<br>        accountId: 'xxxxxxxxxxxx' # my account id here<br>        regions:<br>        - name: us-east-1<br>        assumeRole: Spinnaker-Clouddriver-Role<br>        lifecycleHooks: []<br>      primaryAccount: my-account<br>      bakeryDefaults:<br>        baseImages: []<br>      defaultKeyPairTemplate: '{{name}}-keypair'<br>      defaultRegions:<br>      - name: us-east-1<br>      defaults:<br>        iamRole: BaseIAMRole<br></code></pre><br><br><p>My <code>Spinnaker-Clouddriver-Role</code> IAM role has <code>PowerUserAccess</code> permissions at the moment and has following as the Trust Relationship</p><br><br><pre><code>{<br>  "Version": "2012-10-17";<br>  "Statement": [<br>    {<br>      "Effect": "Allow";<br>      "Principal": {<br>        "Service": [<br>          "ec2.amazonaws.com";<br>          "ecs.amazonaws.com";<br>          "application-autoscaling.amazonaws.com"<br>        ]<br>      };<br>      "Action": "sts:AssumeRole"<br>    };<br>    {<br>      "Effect": "Allow";<br>      "Principal": {<br>        "AWS": "arn:aws:iam::xxxxxxxxxxx:role/Spinnaker-k8s-Worker-Node-Role"<br>      };<br>      "Action": "sts:AssumeRole"<br>    }<br>  ]<br>}<br></code></pre><br><br><p>How can I get this resolved?</p><br><br><hr><br><br><p>The full log can be found on <a href="https://gist.github.com/agentmilindu/d9d31ee4287c87fb87e5060e0709989d#file-awssecuritytokenserviceexception-log-L3" rel="nofollow noreferrer">https://gist.github.com/agentmilindu/d9d31ee4287c87fb87e5060e0709989d#file-awssecuritytokenserviceexception-log-L3</a></p><br>
0.0,0.0,1.0,1.0,0.0,0.0,0.0,<h3>How to solve s3 access denied on file added to s3 bucket from another account?</h3><p>I have two accounts; A and B. A has a S3 bucket; B has a lambda function which sends a csv to S3 bucket in account A. I am creating these resources using terraform.</p><br><p>After I login to Account A; I am able to see the file added; but not able to Download or Open the file; it says Access Denied. I see the below in the Properties section of the file.</p><br><p><a href="https://i.stack.imgur.com/0r67e.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/0r67e.png" alt="enter image description here" /></a></p><br><p>I did not add any encryption to the file or bucket.</p><br>
0.0,0.0,0.0,0.0,0.0,0.3333333333333333,1.0,<h3>How to add advanced if condition in AWS SES HTML template?</h3><p>Is there a way to create a conditional test that an object meets specific criteria more than exists/doesn't exist in an AWS SES HTML template?  For example; <code>{{#if x &gt; 5}}{{x}}{{/if}}</code> instead of <code>{{#if x}}{{x}}{{/if}}</code>?</p><br><p>I've read the docs here (<a href="https://docs.aws.amazon.com/ses/latest/DeveloperGuide/send-personalized-email-advanced.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/ses/latest/DeveloperGuide/send-personalized-email-advanced.html</a>) and seen a good SO question/answer here (<a href="https://stackoverflow.com/questions/54474310/how-to-add-add-if-condition-in-aws-ses-html-template">How to add add if condition in AWS SES html template?</a>); but neither one talked about an <code>if</code> statement that was more than a check on if an object exists.</p><br>
0.0,0.3333333333333333,1.0,0.0,0.0,0.6666666666666666,0.0,<h3>What should my OAuth2 Redirect URL be with an S3 static site frontend and an EC2 API backend?</h3><p>I'm running the <a href="https://stackoverflow.com/questions/41467395/using-s3-as-static-web-page-and-ec2-as-rest-api-for-it-together-aws/">usual setup</a> with React apps where I'm using S3 and Cloudfront for the frontend for my single page application and then having them query an EC2 backend for any data that they need. This works well.</p><br><br><p>My issue I'm having is with creating the OAuth2 Authorization Code flow. I have it working such that the client is redirected to Google to login; but what I am now confused about is where I should redirect the user to upon successful authentication.</p><br><br><p>The "frontend" is basically <code>www.website.com</code> and is hosted in an S3 bucket configured to serve as a static website; whereas the "backend" is <code>www.website.com/api/</code>. I have Cloudfront set up such that <code>www.website.com/api/</code> calls redirect to the EC2 instance; and every other route redirects to the S3 bucket.</p><br><br><p>In this situation; what do I specify as the redirect URL? It seems to be the case that if I specify the "backend"; then after the backend EC2 route is hit; then redirecting back to the "frontend" doesn't work.</p><br><br><p>Is the recommended way to instead specify the OAuth2 Redirect URL to the "frontend" (the static S3 site); and have a frontend route which then does a request to the EC2 backend? My concern is that the frontend (and hence user) should not be given access to the response data of the OAuth2 response route; but I'm unsure of any alternatives.</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>AWS S3 &#39;RequestTimeTooSkewed&#39; error when uploading multiple files</h3><p>I'm uploading multiple HTML and PNG files in a particular directory structure to my AWS S3 bucket.  The code I wrote to upload these files has worked perfectly fine up until now.</p><br><br><p>Recently; I have been experiencing many "RequestTimeTooSkewed" errors as the upload process nears its completion.  The message "The difference between the request time and the current time is too large" is provided as well.</p><br><br><p>I should mention that I am daily uploading more and more PNG files with each execution.  For example; I used to upload about 500 PNG files.  The next day; I may be uploading 700 PNG files.  I am now uploading about 2;500 PNG files per execution.</p><br><br><p>I understand that this issue occurs when the AWS S3 server's time does not match the machine running the uploading script's time.  I have checked my machine's time against the server's time and found that both times are virtually identical.</p><br><br><p>I am wondering if the uploading files are getting bottlenecked and are initially given an earlier time than when the actual upload begins.</p><br><br><p>If anyone has any idea why these errors are happening; I would love to hear an explanation.  Thank you!!</p><br><br><p>Here is the code responsible for uploading to AWS S3:</p><br><br><pre><code>let fs = require('fs');<br>let rimraf = require('rimraf');<br>let directory = 'reporting/reports/';<br>let aws = require('aws-sdk');<br><br>let date;<br>let screenshotFolder = [];<br>let OSFolder = [];<br>let hubFolder = [];<br><br>aws.config.update({<br>    accessKeyId: '***';<br>    secretAccessKey: '***';<br>    region: 'us-west-2'<br>});<br><br>let s3 = new aws.S3({<br>    apiVersion: "2006-03-01";<br>});<br><br>fs.readdirSync(directory).forEach(file =&gt; {<br>    date = file;<br>});<br><br>directory = directory + date + "/";<br><br>fs.readdirSync(directory).forEach(file =&gt; {<br>    if (file === ".DS_Store") {<br>        rimraf.sync(file);<br>    }<br>});<br><br>if (fs.existsSync(directory + "screenshots/")) {<br>    fs.readdirSync(directory + "screenshots/").forEach(file =&gt; {<br>        screenshotFolder.push(file);<br>    });<br><br>    for (let i = 0; i &lt; screenshotFolder.length; ++i) {<br>        let fileStream = fs.createReadStream(directory + "screenshots/" + screenshotFolder[i]);<br>        fileStream.on('error'; function (err) {<br>            console.log("ERROR: "; err);<br>        });<br>        s3.upload({<br>            Bucket: 'Reporting';<br>            Key: "Reporting/" + date + "/screenshots/" + screenshotFolder[i];<br>            Body: fileStream;<br>            ACL: 'public-read'<br>        }; function (err; data) {<br>            if (err) {<br>                console.log("Screenshot Error: "; err);<br>            }<br>            if (data) {<br>                console.log("Screenshot Success: "; data.Location);<br>            }<br>        }).on('httpUploadProgress'; event =&gt; {<br>            console.log(`Screenshot Uploaded ${event.loaded} out of ${event.total}`);<br>        });<br>    }<br>}<br><br>if (fs.existsSync(directory + "hub/")) {<br>    fs.readdirSync(directory + "hub/").forEach(file =&gt; {<br>        hubFolder.push(file);<br>    });<br><br>    for (let i = 0; i &lt; hubFolder.length; ++i) {<br>        let fileStream = fs.createReadStream(directory + "hub/" + hubFolder[i]);<br>        fileStream.on('error'; function (err) {<br>            console.log("ERROR: "; err);<br>        });<br>        s3.upload({<br>            Bucket: 'Reporting';<br>            Key: "Reporting/" + date + "/hub/" + hubFolder[i];<br>            Body: fileStream;<br>            ACL: 'public-read';<br>            ContentType: 'text/html'<br>        }; function (err; data) {<br>            if (err) {<br>                console.log("Hub Error: "; err);<br>            }<br>            if (data) {<br>                console.log("Hub Success: "; data.Location);<br>            }<br>        }).on('httpUploadProgress'; event =&gt; {<br>            console.log(`Hub Uploaded ${event.loaded} out of ${event.total}`);<br>        });<br>    }<br>}<br><br>fs.readdirSync(directory).forEach(file =&gt; {<br>    if (file !== "screenshots"  &amp;&amp; file !== "hub" &amp;&amp; file !== ".DS_Store") {<br>        OSFolder.push(file);<br>    }<br>});<br><br>for (let i = 0; i &lt; OSFolder.length; ++i) {<br>    fs.readdirSync(directory + OSFolder[i]).forEach(file =&gt; {<br>        fs.readdirSync(directory + OSFolder[i] + "/" + file).forEach(html =&gt; {<br>            let fileStream = fs.createReadStream(directory + OSFolder[i] + "/" + file + "/" + html);<br>            fileStream.on('error'; function (err) {<br>                console.log("OS ERROR: "; err);<br>            });<br>            s3.upload({<br>                Bucket: 'Reporting';<br>                Key: "Reporting/" + date + "/" + OSFolder[i] + "/" + file + "/" + html;<br>                Body: fileStream;<br>                ACL: 'public-read';<br>                ContentType: 'text/html'<br>            }; function (err; data) {<br>                if (err) {<br>                    console.log("OS Error: "; err);<br>                }<br>                if (data) {<br>                    console.log("OS Success: "; data.Location);<br>                }<br>            }).on('httpUploadProgress'; event =&gt; {<br>                console.log(`OS Uploaded ${event.loaded} out of ${event.total}`);<br>            });<br>        });<br>    });<br>}<br></code></pre><br>
0.0,0.3333333333333333,0.0,0.0,1.0,0.0,0.0,<h3>AWS ECS/FARGATE - Service task running but no container instance</h3><p>I'm trying to deploy a docker container image with an API to AWS using ECS/FARGATE and while the service task <strong>appears</strong> to be running; it <strong>doesn't have a container instance associated</strong> with it and so I cannot communicate with my API. I'm not very experienced in AWS in general; so I apologise in advance if I'm making a rookie mistake.</p><br><h3>What I tried</h3><br><ol><br><li>Created the ECR; pushed my docker image</li><br><li>Created the cluster</li><br><li>Created the task definition:</li><br></ol><br><pre><code>{<br>  &quot;ipcMode&quot;: null;<br>  &quot;executionRoleArn&quot;: &quot;arn:...:role/ecsTaskExecutionRole&quot;;<br>  &quot;containerDefinitions&quot;: [<br>    {<br>      &quot;dnsSearchDomains&quot;: null;<br>      &quot;environmentFiles&quot;: null;<br>      &quot;logConfiguration&quot;: {<br>        &quot;logDriver&quot;: &quot;awslogs&quot;;<br>        &quot;secretOptions&quot;: null;<br>        &quot;options&quot;: {<br>          &quot;awslogs-group&quot;: &quot;/ecs/my-task-definition&quot;;<br>          &quot;awslogs-region&quot;: &quot;eu-central-1&quot;;<br>          &quot;awslogs-stream-prefix&quot;: &quot;ecs&quot;<br>        }<br>      };<br>      &quot;entryPoint&quot;: null;<br>      &quot;portMappings&quot;: [<br>        {<br>          &quot;hostPort&quot;: 5001;<br>          &quot;protocol&quot;: &quot;tcp&quot;;<br>          &quot;containerPort&quot;: 5001<br>        }<br>      ];<br>      &quot;command&quot;: null;<br>      &quot;linuxParameters&quot;: null;<br>      &quot;cpu&quot;: 0;<br>      &quot;environment&quot;: [ //environmnet variables  ];<br>      &quot;resourceRequirements&quot;: null;<br>      &quot;ulimits&quot;: null;<br>      &quot;dnsServers&quot;: null;<br>      &quot;mountPoints&quot;: [];<br>      &quot;workingDirectory&quot;: null;<br>      &quot;secrets&quot;: null;<br>      &quot;dockerSecurityOptions&quot;: null;<br>      &quot;memory&quot;: null;<br>      &quot;memoryReservation&quot;: null;<br>      &quot;volumesFrom&quot;: [];<br>      &quot;stopTimeout&quot;: null;<br>      &quot;image&quot;: &quot;&lt;&lt;ECR_ID&gt;&gt;.dkr.ecr.eu-central-1.amazonaws.com/&lt;&lt;CONTAINER_NAME&gt;&gt;:&lt;&lt;TAG_ID&gt;&gt;&quot;;<br>      &quot;startTimeout&quot;: null;<br>      &quot;firelensConfiguration&quot;: null;<br>      &quot;dependsOn&quot;: null;<br>      &quot;disableNetworking&quot;: null;<br>      &quot;interactive&quot;: null;<br>      &quot;healthCheck&quot;: {<br>        &quot;retries&quot;: 3;<br>        &quot;command&quot;: [<br>          &quot;CMD-SHELL&quot;;<br>          &quot;curl -f localhost:5001/status&quot;<br>        ];<br>        &quot;timeout&quot;: 5;<br>        &quot;interval&quot;: 30;<br>        &quot;startPeriod&quot;: null<br>      };<br>      &quot;essential&quot;: true;<br>      &quot;links&quot;: null;<br>      &quot;hostname&quot;: null;<br>      &quot;extraHosts&quot;: null;<br>      &quot;pseudoTerminal&quot;: null;<br>      &quot;user&quot;: null;<br>      &quot;readonlyRootFilesystem&quot;: null;<br>      &quot;dockerLabels&quot;: null;<br>      &quot;systemControls&quot;: null;<br>      &quot;privileged&quot;: null;<br>      &quot;name&quot;: &quot;&lt;&lt;CONTAINER_ID&gt;&gt;&quot;<br>    }<br>  ];<br>  &quot;placementConstraints&quot;: [];<br>  &quot;memory&quot;: &quot;2048&quot;;<br>  &quot;taskRoleArn&quot;: &quot;arn:...:role/ecsTaskExecutionRole&quot;;<br>  &quot;compatibilities&quot;: [<br>    &quot;EC2&quot;;<br>    &quot;FARGATE&quot;<br>  ];<br>  &quot;taskDefinitionArn&quot;: &quot;arn:aws:ecs:eu-central-1:893482549503:task-definition/my-task-definition:12&quot;;<br>  &quot;family&quot;: &quot;my-task-definition&quot;;<br>  &quot;requiresAttributes&quot;: [<br>    {<br>      &quot;targetId&quot;: null;<br>      &quot;targetType&quot;: null;<br>      &quot;value&quot;: null;<br>      &quot;name&quot;: &quot;com.amazonaws.ecs.capability.logging-driver.awslogs&quot;<br>    };<br>    {<br>      &quot;targetId&quot;: null;<br>      &quot;targetType&quot;: null;<br>      &quot;value&quot;: null;<br>      &quot;name&quot;: &quot;com.amazonaws.ecs.capability.docker-remote-api.1.24&quot;<br>    };<br>    {<br>      &quot;targetId&quot;: null;<br>      &quot;targetType&quot;: null;<br>      &quot;value&quot;: null;<br>      &quot;name&quot;: &quot;ecs.capability.execution-role-awslogs&quot;<br>    };<br>    {<br>      &quot;targetId&quot;: null;<br>      &quot;targetType&quot;: null;<br>      &quot;value&quot;: null;<br>      &quot;name&quot;: &quot;com.amazonaws.ecs.capability.ecr-auth&quot;<br>    };<br>    {<br>      &quot;targetId&quot;: null;<br>      &quot;targetType&quot;: null;<br>      &quot;value&quot;: null;<br>      &quot;name&quot;: &quot;com.amazonaws.ecs.capability.docker-remote-api.1.19&quot;<br>    };<br>    {<br>      &quot;targetId&quot;: null;<br>      &quot;targetType&quot;: null;<br>      &quot;value&quot;: null;<br>      &quot;name&quot;: &quot;com.amazonaws.ecs.capability.task-iam-role&quot;<br>    };<br>    {<br>      &quot;targetId&quot;: null;<br>      &quot;targetType&quot;: null;<br>      &quot;value&quot;: null;<br>      &quot;name&quot;: &quot;ecs.capability.container-health-check&quot;<br>    };<br>    {<br>      &quot;targetId&quot;: null;<br>      &quot;targetType&quot;: null;<br>      &quot;value&quot;: null;<br>      &quot;name&quot;: &quot;ecs.capability.execution-role-ecr-pull&quot;<br>    };<br>    {<br>      &quot;targetId&quot;: null;<br>      &quot;targetType&quot;: null;<br>      &quot;value&quot;: null;<br>      &quot;name&quot;: &quot;com.amazonaws.ecs.capability.docker-remote-api.1.18&quot;<br>    };<br>    {<br>      &quot;targetId&quot;: null;<br>      &quot;targetType&quot;: null;<br>      &quot;value&quot;: null;<br>      &quot;name&quot;: &quot;ecs.capability.task-eni&quot;<br>    }<br>  ];<br>  &quot;pidMode&quot;: null;<br>  &quot;requiresCompatibilities&quot;: [<br>    &quot;FARGATE&quot;<br>  ];<br>  &quot;networkMode&quot;: &quot;awsvpc&quot;;<br>  &quot;cpu&quot;: &quot;256&quot;;<br>  &quot;revision&quot;: 12;<br>  &quot;status&quot;: &quot;ACTIVE&quot;;<br>  &quot;inferenceAccelerators&quot;: null;<br>  &quot;proxyConfiguration&quot;: null;<br>  &quot;volumes&quot;: []<br>}<br></code></pre><br><ol start="4"><br><li><p>Created the service; using the task definition. Also connected it to my application load balancer.</p><br></li><br><li><p>In EC2 I added a new <code>ecs-optimized</code> instance and connected it to the cluster by doing:<br><code>ECS_CLUSTER=my-cluster &gt;&gt; /etc/ecs/ecs.config</code> from inside the instance.</p><br></li><br><li><p>I run the task from the service.</p><br></li><br></ol><br><h3>Result</h3><br><ul><br><li>I see the application running from the task logs:<br><a href="https://i.stack.imgur.com/7wNII.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/7wNII.png" alt="Tasks logs" /></a></li><br></ul><br><p>But I cannot get any response from <code>curl</code> from my computer. I am trying to send the requests to my <strong>load balancer</strong> and even the <strong>Public IP</strong> address provided by the task:<br><a href="https://i.stack.imgur.com/66B02.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/66B02.png" alt="Task information" /></a></p><br><ul><br><li>From the cluster view; I see the running task with <strong>no container instance</strong>:</li><br></ul><br><p><a href="https://i.stack.imgur.com/DCNzh.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/DCNzh.png" alt="Running task with no container instance" /></a></p><br><ul><br><li><p>In the ECS Instances task; I see the EC2 assigned with <strong>no running tasks</strong>:<br><a href="https://i.stack.imgur.com/zD1Wl.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/zD1Wl.png" alt="cluster view - ECS instances task" /></a></p><br></li><br><li><p>Also the <strong>health check appears to be failing</strong>; as the task <strong>stops and restarts</strong> every few minutes.</p><br></li><br></ul><br><p>I have followed several tutorials on the matter and none seem to solve the problem. Any tips are welcomed and if some information is missing I'll be happy to add it.</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>DynamoDB ordering/sorting items with query</h3><p>I have a DDB Table which contains items with several attributes.<br>My partition key and sort key are both strings. I want to do a Query which needs both of these attributes; but I also need to sort data using a third attribute which is not included in the primary key.<br>Since DynamoDB only supports sorting with range key; is there any way to be able to achieve this with GSIs etc.</p><br>
0.0,0.0,0.0,0.0,0.0,0.0,1.0,<h3>How to run Appium python tests in parallel on devicefarm</h3><p>Is is possible to run my appium python tests in parallel on aws devicefarm; i know we can select multiple devices from the aws device pool but that runs the same time on all selected devices;<br>i wanted to distribute my tests on multiple devices;</p><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.0,<h3>How to customize EC2 instance in Elastic Beanstalk Environment</h3><p>I have a working Elastic Beanstalk environment(PHP 7.3).</p><br><pre><code>The ec2 uses Amazon linux2.<br>I now have to run some userdata ( yum install ... ; curl ...) on these ec2 before they start. <br>Is this possible ?<br>I wish userdata was included in Launch configuration. <br></code></pre><br><p>Can any provide some guidance here.<br>Just as an FYI; I am using cloudformation.</p><br><p>Thanks !</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>Cannot modify a default parameter group for remaining connection slots on Amazon RDS</h3><p>I keep getting:</p><br><blockquote><br><p>SQLSTATE[08006] [7] FATAL: remaining connection slots are reserved<br>for non-replication superuser connections (SQL: select * from &quot;sheeps&quot;<br>where &quot;name&quot; = foobar limit 1)</p><br></blockquote><br><p>So I thought of modifying the value in the paramaeter group (max_connections default on 270)</p><br><p>But when I change the max_connections to a higher value; I get:</p><br><blockquote><br><p>&quot;Error saving: Cannot modify a default parameter group. (Service:<br>AmazonRDS; Status Code: 400; Error Code: InvalidParameterValue;<br>Request ID: b5179a68-xxxx-44a8-b396-xxx; Proxy: null)&quot;</p><br></blockquote><br><p>I have the max limit Laravel Vapor offers: 3008 mb.</p><br>
0.3333333333333333,0.0,0.6666666666666666,0.0,0.6666666666666666,0.0,0.0,<h3>AWS Lambda function that has been working for weeks; one day timed out for no apparent reason. Ideas?</h3><p>I wrote a simple lambda function (in python 3.7) that runs once a day; which keeps my Glue data catalog updated when new partitions are created. It works like this:</p><br><br><ul><br><li>Object creation in a specific S3 location triggers the function asynchronously</li><br><li>From the event; lambda extracts the key (e.g.: s3://my-bucket/path/to/object/)</li><br><li>Through AWS SDK; lambda asks glue if the partition already exists</li><br><li>If not; creates the new partition. If yes; terminates the process.</li><br></ul><br><br><p>Also; the function has 3 print statements:</p><br><br><ul><br><li>one at the very beginning; saying it started  the execution</li><br><li>one in the middle; which says if the partition exists or not</li><br><li>one at the end; upon successful execution.</li><br></ul><br><br><p>This function has an average execution time of 460ms per invocation; with 128MB RAM allocated; and it cannot have more than about 12 concurrent executions (as 12 is the maximum amount of new partitions that can be generated daily). There are no other lambda functions running at the same time that may steal concurrency capacity. Also; just to be sure; I have set the timeout limit to be 10 seconds. </p><br><br><p>It has been working flawlessly for weeks; except this morning; 2 of the executions timed out after reaching the 10 seconds limit; which is very odd given it's 20 times larger than the avg. duration.</p><br><br><p>What surprises me the most; is that in one case only the 1st print statement got logged in CloudWatch; and in the other case; not even that one; as if the function got called but never actually started the process.</p><br><br><p>I could not figure out what may have caused this. Any idea or suggestion is much appreciated.</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>AWS Glue - Dev Endpoint - &#39;userData.json&#39; error</h3><p>Every time new Dev Endpoint is created (new cluster) - this error show up at first:</p><br><br><pre><code>] util.UserData (UserData.java:getUserData(70)) - Error encountered while try to get user data<br>java.io.IOException: File '/var/aws/emr/userData.json' cannot be read<br></code></pre><br><br><p>How to resolve it?</p><br><br><p>Thannks.</p><br>
0.0,0.0,1.0,0.0,0.0,1.0,0.0,<h3>Only add header on proxied API Gateway request with Lambda authorizer</h3><p>At the moment I have an architecture in mind with AWS ApiGateway + Lambda for server HTML based on if a user is properly authenticated or not. I am trying to achieve this Cognito and a custom Lambda Authorizer. I'd like my Lambda to always return HTML and based on the cookie that is passed; generate HTML for a logged in / logged out state. In my mind that would be ideal to have a separate authorizer that does the token validation and pass a header to the HTML generating Lambda.</p><br><br><p>How can one achieve this?</p><br><br><p>I'm using AWS Sam template to define my CF stack. See my current template:</p><br><br><pre><code>AWSTemplateFormatVersion: '2010-09-09'<br>Transform: 'AWS::Serverless-2016-10-31'<br>Description: A Lambda function for rendering HTML pages with authentication<br>Resources:<br>  WebAppGenerator:<br>    Type: 'AWS::Serverless::Function'<br>    Properties:<br>      Handler: app.handler<br>      Runtime: nodejs12.x<br>      CodeUri: .<br>      Description: A Lambda that generates HTML pages dynamically<br>      MemorySize: 128<br>      Timeout: 20<br>      Events:<br>        ProxyRoute:<br>          Type: Api<br>          Properties:<br>            RestApiId: !Ref WebAppApi<br>            Path: /{proxy+}<br>            Method: GET<br>  WebAppApi:<br>    Type: AWS::Serverless::Api<br>    Properties:<br>      StageName: Prod<br>      Auth:<br>        DefaultAuthorizer: WebTokenAuthorizer<br>        Authorizers:<br>          WebTokenAuthorizer:<br>            FunctionArn: !GetAtt WebAppTokenAuthorizer.Arn<br>  WebAppTokenAuthorizer:<br>    Type: AWS::Serverless::Function<br>    Properties:<br>      CodeUri: .<br>      Handler: authorizer.handler<br>      Runtime: nodejs12.x<br></code></pre><br><br><p>In my authorizer (Typescript) I was thinking of generating a policy that always has an 'allow' effect. But if an authorization token (not cookie-based yet) is missing; it's already returning a 403.<br>See:</p><br><br><pre><code><br>function generatePolicy(principalId: string; isAuthorized: boolean; resource): APIGatewayAuthorizerResult {<br>    const result: APIGatewayAuthorizerResult = {<br>        principalId;<br>        policyDocument: {<br>            Version: '2012-10-17';<br>            Statement: []<br>        }<br>    };<br><br>    if (resource) {<br>        result.policyDocument.Statement[0] = {<br>            Action: 'execute-api:Invoke';<br>            Effect: 'Allow';<br>            Resource: resource<br>        };<br>    }<br><br>    result.context = {<br>        isAuthorized<br>    };<br><br>    return result<br>}<br></code></pre><br>
0.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.0,<h3>Running Sidekiq on seperate EC2 instance using ElasticBeanStalk</h3><p>My question is; is that a major drawback of EBS ? In Heroku; I just need to write two lines in Procfile to do this. Is there any workaround?</p><br>
0.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.0,<h3>HTTPie Command from AWS Lambda (Python3.8)</h3><p>I need to run HTTPie Command from withing the AWS Lambda </p><br><br><ol><br><li>I created a layer for HTTPie  </li><br><li>Added layer to my python function</li><br><li>Running my HTTPie command from AWS Lambda which looks like below</li><br></ol><br><br><pre><code>import subprocess<br><br>def lambda_handler(req; context):<br><br>result = subprocess.call("/opt/python/bin/http GET https://reqres.in/api/users?page=2"; shell=True)<br><br>return result<br></code></pre><br><br><p><strong>Function Logs:</strong></p><br><br><p>/bin/sh: /opt/python/bin/http: Permission denied</p><br><br><p><strong>Requirement:</strong></p><br><br><p>The reason I am using HTTPie rather than requests is that I need to use different authentication mechanisms with my request. With requests I cannot make http calls with other authentication mechanisms which is important business requirement. Following are some of the examples we would be using and HTTPie provides all of those plugins</p><br><br><p>Signature; ApiAuth; AWS Auth; HMAC; JWTAuth; OAuth; OAuth2 and others</p><br><br><p><strong>Solutions tried so far</strong></p><br><br><ul><br><li>Created a package instead of layer; assign that package all<br>permissions; zip it; assign all permissions to zip as well"chmos -R<br>777 folder" and uploaded to lambda as zip. This still gave me same<br>issue.</li><br><li>Added this to my code before using http </li><br><li>result = subprocess.call("chmod -R 777 /opt/python/bin/http"; shell=True)<br><br><ul><br><li>chmod: changing permissions of /var/task/lokesh/bin/http: Read-only file system</li><br><li>/bin/sh: /var/task/lokesh/bin/http: Permission denied</li><br></ul></li><br></ul><br>
0.0,1.0,0.0,0.0,1.0,0.0,0.0,<h3>Running multiple tomcats on same ec2</h3><p>I have 2 ENIs attached to my ec2 instance; both of which have an elastic IP attached to it. I have also installed two tomcat 8s in my ec2 instance and have changed the server.xmls as follows</p><br><br><p>For server 1</p><br><br><br><br><pre><code>&lt;Server port="8005" shutdown="SHUTDOWN"&gt;<br>  &lt;Listener className="org.apache.catalina.startup.VersionLoggerListener" /&gt;<br>  &lt;Listener className="org.apache.catalina.core.AprLifecycleListener" SSLEngine="on" /&gt;<br>  &lt;Listener className="org.apache.catalina.core.JreMemoryLeakPreventionListener" /&gt;<br>  &lt;Listener className="org.apache.catalina.mbeans.GlobalResourcesLifecycleListener" /&gt;<br>  &lt;Listener className="org.apache.catalina.core.ThreadLocalLeakPreventionListener" /&gt;<br><br>  &lt;GlobalNamingResources&gt;<br>    &lt;Resource name="UserDatabase" auth="Container"<br>              type="org.apache.catalina.UserDatabase"<br>              description="User database that can be updated and saved"<br>              factory="org.apache.catalina.users.MemoryUserDatabaseFactory"<br>              pathname="conf/tomcat-users.xml" /&gt;<br>  &lt;/GlobalNamingResources&gt;<br><br>  &lt;Service name="Catalina"&gt;<br><br>    &lt;Connector port="8080" protocol="HTTP/1.1"<br>           address="10.0.0.174"<br>               connectionTimeout="20000"<br>               redirectPort="8443" /&gt;<br>    &lt;Connector port="8009" protocol="AJP/1.3" redirectPort="8443" /&gt;<br><br><br>    &lt;Engine name="Catalina" defaultHost="localhost"&gt;<br><br>      &lt;Realm className="org.apache.catalina.realm.LockOutRealm"&gt;<br>        &lt;Realm className="org.apache.catalina.realm.UserDatabaseRealm"<br>               resourceName="UserDatabase"/&gt;<br>      &lt;/Realm&gt;<br><br>      &lt;Host name="localhost"  appBase="webapps"<br>            unpackWARs="true" autoDeploy="true"&gt;<br><br>        &lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs"<br>               prefix="localhost_access_log" suffix=".txt"<br>               pattern="%h %l %u %t &amp;quot;%r&amp;quot; %s %b" /&gt;<br><br>      &lt;/Host&gt;<br>    &lt;/Engine&gt;<br>  &lt;/Service&gt;<br>&lt;/Server&gt;<br></code></pre><br><br><p>For server 2</p><br><br><pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;<br>&lt;Server port="8004" shutdown="SHUTDOWN"&gt;<br>  &lt;Listener className="org.apache.catalina.startup.VersionLoggerListener" /&gt;<br>  &lt;Listener className="org.apache.catalina.core.AprLifecycleListener" SSLEngine="on" /&gt;<br>  &lt;Listener className="org.apache.catalina.core.JreMemoryLeakPreventionListener" /&gt;<br>  &lt;Listener className="org.apache.catalina.mbeans.GlobalResourcesLifecycleListener" /&gt;<br>  &lt;Listener className="org.apache.catalina.core.ThreadLocalLeakPreventionListener" /&gt;<br><br>  &lt;GlobalNamingResources&gt;<br>    &lt;Resource name="UserDatabase" auth="Container"<br>              type="org.apache.catalina.UserDatabase"<br>              description="User database that can be updated and saved"<br>              factory="org.apache.catalina.users.MemoryUserDatabaseFactory"<br>              pathname="conf/tomcat-users.xml" /&gt;<br>  &lt;/GlobalNamingResources&gt;<br><br>  &lt;Service name="Catalina"&gt;<br><br>    &lt;Connector port="8081" protocol="HTTP/1.1"<br>           address="10.0.0.161"<br>               connectionTimeout="20000"<br>               redirectPort="8444" /&gt;<br>    &lt;Connector port="8010" protocol="AJP/1.3" redirectPort="8444" /&gt;<br><br><br>    &lt;Engine name="Catalina" defaultHost="localhost"&gt;<br><br>      &lt;Realm className="org.apache.catalina.realm.LockOutRealm"&gt;<br>        &lt;Realm className="org.apache.catalina.realm.UserDatabaseRealm"<br>               resourceName="UserDatabase"/&gt;<br>      &lt;/Realm&gt;<br><br>      &lt;Host name="localhost"  appBase="webapps"<br>            unpackWARs="true" autoDeploy="true"&gt;<br><br>        &lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs"<br>               prefix="localhost_access_log" suffix=".txt"<br>               pattern="%h %l %u %t &amp;quot;%r&amp;quot; %s %b" /&gt;<br><br>      &lt;/Host&gt;<br>    &lt;/Engine&gt;<br>  &lt;/Service&gt;<br>&lt;/Server&gt;<br></code></pre><br><br><p>After successfully starting both these instances; I'm able to access server 1 (port 8080) but not server 2 (port 8081). If I shut down server 1 and change default the ports in server 2; then I am able to access server 2. Where am I going wrong here?</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>Can&#39;t seem to get Boto&#39;s dynamodb.update_item to work</h3><p>I'm trying to use this Boto function;<br>But can't seem to get it right..<br>Any idea?<br><a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/dynamodb.html#DynamoDB.Client.update_item" rel="nofollow noreferrer">Doc Link</a></p><br><br><pre><code>    tablename = 'name_table'<br>    response = dynamodb.update_item(<br>        TableName=tablename;<br>        Key={<br>            'name': 'Tony';<br>        };<br>        UpdateExpression="set name= :r";<br>        ExpressionAttributeValues={<br>            ':r': 'Ralph';<br>        };<br>        ReturnValues="UPDATED_NEW"<br>    )<br></code></pre><br><br><p>This are the errors I get :</p><br><br><pre><code>Invalid type for parameter Key.name; value: set(['Tony'; 0]); type: &lt;type 'set'&gt;; valid types: &lt;type 'dict'&gt;<br></code></pre><br><br><p>I've tried to change the name from str to dict; <br>So now I'm with this error</p><br><br><pre><code>ValueError: dictionary update sequence element #0 has length 1; 2 is required<br></code></pre><br><br><p>Where to go from here ?<br>Many thanks..</p><br>
0.0,0.6666666666666666,0.6666666666666666,0.0,1.0,0.3333333333333333,0.0,<h3>Best practice for querying API Gateway (Lambda) from Front End for unauthenticated users</h3><p>Looking to move a legacy application to serverless using Lambda for back-end logic but am unsure what the best practices are for making API Gateway requests from front-end while preventing users from querying the Gateway directly. Users are all unauthenticated but we would still like some sort of rate limiting per user.</p><br><br><p><strong>Approach 1</strong></p><br><br><p>Host static content (html; js; etc.) on S3. JS on pages queries gateway to fetch back-end data and get presigned URLs for images and videos on S3. </p><br><br><p>With this approach I am concerned users will be able to query the Gateway directly instead of going through the UI. This programmatic access could be difficult to prevent and result in unexpected expenses.</p><br><br><p><strong>Approach 2</strong></p><br><br><p>No static content from S3; instead every page will be served via Lambda. Lighter static content could be sent directly from Lambda and larger images and videos would come from presigned S3 URLs.</p><br><br><p>This would prevent users from examining front-end -> back-end requests since they would only receive the fully assembled html. Concerned this approach would increase lambda run time and expense.</p><br>
0.0,0.3333333333333333,1.0,0.0,0.0,0.0,0.0,<h3>AWS - Importing Cloudflare certificate - where to get Certificate chain?</h3><p>I'm trying to import a certificate generated in Cloudflare into AWS. Coludflare provided me with the certificate and private key; but AWS also requires a field called "certificate chain". Where can I get this value?</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>In AWS DynamoDB; why isn&#39;t my ConsistentRead giving me the most up-to-date data?</h3><p>I have a typical REST-ful API doing CRUD operations on a DynamoDB being served by a Python web server written with Flash and Boto3. My get_list function looks like this:</p><br><br><pre><code>def list_items():<br>    table = dbh.Table(table_name)<br>    response = table.scan(ConsistentRead=True)<br>    return { "items": response["Items"] }<br></code></pre><br><br><p>And also an update which looks like this:</p><br><br><pre><code>def update_item (item_id):<br>    table = dbh.Table(table_name)<br>    input_json = request.get_json()<br>    table.put_item(Item = {<br>        "uuid": str(item_id);<br>        "data":input_json<br>        })<br>    return {}<br></code></pre><br><br><p>My web app is calling the API endpoint for <code>update_item</code>; waiting for completion; and then immediately calling the API endpoint for <code>list_items</code> when it gets a return value from <code>update_item</code>. <code>list_items</code> is using <code>ConsistentRead</code>; which I was expecting would wait until the previous writes are done. But the list of items is consistently missing the new updates. A call to list items moments later gives the full update. How can I ensure that if I call these API endpoints back to back I get the most up-to-date results. Or if I can't; how should I structure my web app so that when I edit an item; I can see the updates without sleeping an arbitrary amount of time after an update?</p><br>
0.0,0.0,0.6666666666666666,0.0,1.0,0.0,0.0,<h3>How to change the log level of a kubernetes pod?</h3><p>when i give </p><br><br><pre><code>kubectl logs &lt;pod-name&gt;<br></code></pre><br><br><p>it give logs of the pod whose level is set to <code>info</code> <br>how can i change this log level(both before Deployment and after deployment)?</p><br>
0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.3333333333333333,0.3333333333333333,<h3>How to force Gatsby to redirect a specific URL path to an external site?</h3><p>I have a Gatsby site and due to some specific requirements; I need to redirect anyone who attempts to hit a specific URL path; for which there is no page; to an external site. This URL path is not a page within the site; but it's something that a user may be inclined to type due to documentation that is out of my control.</p><br><p>Here's an example: Let's say the site is located at <a href="https://www.example.com" rel="nofollow noreferrer">https://www.example.com</a>. A user may visit <a href="https://www.example.com/puppies" rel="nofollow noreferrer">https://www.example.com/puppies</a>; which does not exist. My file structure does not contain a <code>src/pages/puppies.js</code> file. However; when that URL is entered; I need to redirect the user to another site altogether; such as <a href="https://www.stackoverflow.com">https://www.stackoverflow.com</a>.</p><br>
0.0,1.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>CloudFront returning content from origin instead of cache</h3><p>We are using <strong>AWS CloudFront</strong> to cache static content(video files storer in AWS S3).<br>In <strong>Cache Behavior Settings</strong> caching by headers; cookies and query string is disabled what should improve caching.</p><br><p>In the same time on <strong>Popular objects</strong> page I noticed that some video files are requested many times but only few requests are returned from cache. Almost all our customers have to use the same region.</p><br><p>Anyway according to <a href="https://aws.amazon.com/cloudfront/features/" rel="nofollow noreferrer">https://aws.amazon.com/cloudfront/features/</a></p><br><blockquote><br><p>AWS CloudFront have 205 Edge Locations and 12 Regional Edge Caches.</p><br></blockquote><br><p><strong>Questions:</strong></p><br><ul><br><li>How in that case file which was requested <strong>867 times</strong> in <strong>2 day period</strong> could be returned from cache <strong>only 36 times</strong>?</li><br><li>How to improve caching performance?</li><br></ul><br><p><a href="https://i.stack.imgur.com/g8OTC.jpg" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/g8OTC.jpg" alt="enter image description here" /></a></p><br><p><a href="https://i.stack.imgur.com/nf108.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/nf108.png" alt="enter image description here" /></a></p><br>
0.0,0.0,0.0,0.0,0.6666666666666666,0.6666666666666666,0.0,<h3>How to add a JRE 1.8 into Eclipse Installer JRE?</h3><p>I am trying deploy my Spring Boot application to Elastic Beanstalk.</p><br><p>The problem that I am facing right now is; my application is developed with Tomcat 9; Java 13 and Project Facet 4.0.</p><br><p>So when I deployed to a AWS environment; I kept getting error. After researching; I found out AWS only support Tomcat 8; Java 8 and Project Facet 3.0.</p><br><p>I managed to change Project Facet 4.0 to 3.0.</p><br><p>But I can't add Java 8 to the project.</p><br><p>I downloaded the JDK 8 from Oracle; and tried open jdk-8u251-macosx-x64.dmg and run JDK 8 Update 251.pkg.</p><br><p>The system keeps giving me error that: <strong>&quot;JDK 8 Update 251.pkg&quot; can't be opened because Apple cannot check it for malicious software. This software needs to be updated.</strong></p><br><p>Anyone know how to fix this problem so that I can run JRE 8 on eclipse?</p><br><p>Here is picture the problem that I have when I deploy the app to AWS.<br><a href="https://i.stack.imgur.com/O5hza.png" rel="nofollow noreferrer">An internal error occurred during: Updating AWS Elastic Beanstalk environment</a></p><br><p>Here is the thread that I tried to follow but without success:<br><a href="https://github.com/aws/aws-toolkit-eclipse/issues/149" rel="nofollow noreferrer">https://github.com/aws/aws-toolkit-eclipse/issues/149</a>.</p><br>
0.0,0.0,1.0,1.0,0.0,0.0,0.0,<h3>AWS: Could not able to give s3 access via s3 bucket policy</h3><p>I am the root user of my account and i created one new user and trying to give access to s3 via s3 bucket policy:</p><br><br><p>Here is my policy details :-</p><br><br><pre><code>{ "Id": "Policy1542998309644"; "Version": "2012-10-17"; "Statement": [ { "Sid": "Stmt1542998308012"; "Action": [ "s3:ListBucket" ]; "Effect": "Allow"; "Resource": "arn:aws:s3:::aws-bucket-demo-1"; "Principal": { "AWS": [ "arn:aws:iam::213171387512:user/Dave" ] } } ]}<br></code></pre><br><br><p>in IAM  i have not given any access to the new user. I want to provide him access to s3 via s3 bucket policy. Actually i would like to achieve this : <a href="https://aws.amazon.com/premiumsupport/knowledge-center/s3-console-access-certain-bucket/" rel="nofollow noreferrer">https://aws.amazon.com/premiumsupport/knowledge-center/s3-console-access-certain-bucket/</a>   But not from IAM ; I want to use only s3 bucket policy.</p><br>
0.0,0.6666666666666666,0.0,0.0,1.0,0.0,0.0,<h3>AWS EKS; How To Hit Pod directly from browser?</h3><p>I'm very new to kubernetes. I have spent the last week learning about Nodes; Pods; Clusters; Services; and Deployments.</p><br><p>With that I'm trying to just get some more understanding of how the networking for kubernetes even works. I just want to expose a simple nginx docker webpage and hit it from my browser.</p><br><p>Our VPC is setup with a direct connect so I'm able to hit EC2 instances on their private IP addresses. I also setup the EKS cluster using the UI on aws for now as <code>private</code>. For testing purposes I have added my cidr range to be allowed on all TCP as an additional security group in the EKS cluster UI.</p><br><p>Here is my basic service and deployment definitions:</p><br><pre><code>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: testing-nodeport<br>  namespace: default<br>  labels:<br>    infrastructure: fargate<br>    app: testing-app<br>spec:<br>  type: NodePort<br>  selector:<br>    app: testing-app<br>  ports:<br>    - port: 80<br>      targetPort: testing-port<br>      protocol: TCP<br>---<br>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: testing-deployment<br>  namespace: default<br>  labels:<br>    infrastructure: fargate<br>    app: testing-app<br>spec:<br>  replicas: 1<br>  selector:<br>    matchLabels:<br>      infrastructure: fargate<br>      app: testing-app<br>  template:<br>    metadata:<br>      labels:<br>        infrastructure: fargate<br>        app: testing-app<br>    spec:<br>      containers:<br>      - name: nginx<br>        image: nginx:1.14.2<br>        ports:<br>        - name: testing-port<br>          containerPort: 80<br></code></pre><br><p>I can see that everything is running correctly when I run:</p><br><p><code>kubectl get all -n default</code></p><br><p>However; when I try to hit the NodePort IP address on port <code>80</code> I can't load it from the browser.</p><br><p>I can hit the pod if I first setup a <code>kubectl proxy</code> at the following url (as the proxy is started on port <code>8001</code>):</p><br><pre><code>http://localhost:8001/api/v1/namespaces/default/services/testing-nodeport:80/proxy/<br></code></pre><br><p>I'm pretty much lost at this point. I don't know what I'm doing wrong and why I can't hit the basic nginx docker outside of the kubectl proxy command.</p><br>
0.0,0.0,0.0,0.0,0.0,0.0,1.0,<h3>Fetching list of things in things group or things from AWS IoT</h3><p>I need list of things in group or list of things from AWS with that I tried to find solution from <a href="https://aws-amplify.github.io/aws-sdk-ios/docs/reference/AWSIoT/Classes/AWSIoTListThingsInThingGroupRequest.html" rel="nofollow noreferrer">AWSIoT Reference</a> So i have used below code to get it. Previously i used to get it using normal API call from our backend service but i need fully use with AWS.</p><br><br><pre><code>   func initializeAWS() {<br><br>       let credentialsProvider = AWSCognitoCredentialsProvider(regionType:AWS_REGION;<br>                                                               identityPoolId:IDENTITY_POOL_ID)<br>       initializeControlPlane(credentialsProvider: credentialsProvider)<br><br>   }<br><br>   func initializeControlPlane(credentialsProvider: AWSCredentialsProvider) {<br><br>       let controlPlaneServiceConfiguration = AWSServiceConfiguration(region:AWS_REGION; credentialsProvider:credentialsProvider)<br><br>       AWSServiceManager.default().defaultServiceConfiguration = controlPlaneServiceConfiguration<br>       iot = AWSIoT.default()<br><br>       let request = AWSIoTListThingsInThingGroupRequest()<br>       request?.thingGroupName = "XXXGroupName"<br>       let output = iot.listThings(inThingGroup: request!)<br>       print("output is \(output.result)")<br>       print("error is \(output.error)")<br><br>   }<br></code></pre><br><br><p>I have used here <code>AWSIoT</code> &amp; <code>AWSIoTListThingsInThingGroupRequest</code> object to get list of things may i know is this right way to fetch ? if it is I'm <code>output</code> and <code>error</code> both objects getting nil. </p><br><br><p>I tried to find solution for the AWS IOT example from Github; I didnt get anything relevant answer to this. Or is there anything in <code>iotDataManager</code> that will give list of things ? Please can you help me on this ? For more info I have raised question on AWS Github <a href="https://github.com/aws-amplify/aws-sdk-ios/issues/2466" rel="nofollow noreferrer">Fetching list of things in things group</a></p><br>
0.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.0,<h3>AWS ECS Task single instance</h3><p>In my architecture when I receive a new file on S3 bucket; a lambda function triggers an ECS task.<br>The problem occurs when I receive multiple files at the same time: the lambda will trigger multiple instance of the same ECS task that acts on the same shared resources.</p><br><p>I want to ensure only 1 instance is running for specific ECS Task; how can I do?<br>Is there a specific setting that can ensure it?</p><br><p>I tried to query ECS Cluster before run a new instance of the ECS task; but (using AWS Python SDK) I didn't receive any information when the task is in PROVISIONING status; the sdk only return data when the task is in PENDING or RUNNING.</p><br><p>Thank you</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>AWS Landing Zone - Rollback complete codepipeline after error occured post core account creation</h3><p>I've tried to setup my AWS organization using AWS Landing Zone. This is what I have done :-</p><br><br><ul><br><li>Deploy the AWS Landing Zone based on the AWS Landing Zone initiation template</li><br><li>Execute CodePipeline created by initiation template</li><br><li>Core accounts were created by CodePipeline; but build gets failed while creating the CoreResources</li><br></ul><br><br><p>Now; I wanted to execute the codepipeline again after doing some changes in Manifest.yaml file.</p><br><br><p>Can someone help me in understanding how can I delete the created organizations i.e. "core" and "application" and core accounts?</p><br><br><p>As far I know deletion of AWS account from Organization is not that straight forward and you have provide payment; plan details before deleting accounts created by Landing Zone. Plus; even after provided all required details AWS won't allow you to delete that account immediately. </p><br><br><p><strong>Is there any way to delete Organizations and core accounts created by AWS Landing Zone immediately?</strong></p><br>
0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>HTML file served as static in AWS Elastic Beanstalk with HTTPS are served as string</h3><p>I've deployed my Node.js + Express API on AWS EB and routed a custom domain with its HTTPS certificate.</p><br><p>Then I've a static HTML file in static directory of this environment &quot;/public&quot; and the HTML file is served greatly if opened on a browser with HTTP.</p><br><p>But when I open the same HTML file with HTTPS it seems to serve the file content as a string in this way:</p><br><pre><code>&lt;html&gt;<br>    &lt;head&gt;&lt;/head&gt;<br>    &lt;body&gt;<br>    &lt;pre style=&quot;word-wrap: break-word; white-space: pre-wrap;&quot;&gt;<br>        &quot;&lt;!DOCTYPE html&gt; ... {all my html page content}&quot;<br>    &lt;/pre&gt;<br>    &lt;/body&gt;<br> &lt;/html&gt;<br></code></pre><br><p>How can i serve this html file with HTTPS?</p><br>
0.0,0.3333333333333333,0.0,1.0,0.0,0.0,0.0,<h3>How do I create a Presigned URL to download a file from an S3 Bucket using Boto3?</h3><p>I have to download a file from my <code>S3 bucket</code> onto my server for some processing. The bucket does not support direct connections and has to use a <code>Pre-Signed URL</code>.</p><br><hr /><br><p>The <code>Boto3 Docs</code> talk about using a presigned URL to upload but do not mention the same for download.</p><br>
0.3333333333333333,0.0,0.6666666666666666,0.0,1.0,0.0,0.0,<h3>How to call secret manager from lambda function</h3><p>I am new to AWS. I am trying to establish my lambda function to the AWS Redshift so that I can query the database. I have stored the credentials in the secret key manager.</p><br><p>I understand that the secret key manager has provided a sample code to retrieve the sercet in the application. However; I have no idea how to get going after copying the code in my lambda function.</p><br><p>handler.py</p><br><pre><code># Use this code snippet in your app.<br># If you need more information about configurations or implementing the sample code; visit the AWS docs:   <br># https://aws.amazon.com/developers/getting-started/python/<br><br>import boto3<br>import base64<br>from botocore.exceptions import ClientError<br><br><br>def get_secret():<br><br>    secret_name = &quot;mykeyname&quot;<br>    region_name = &quot;myregionname&quot;<br><br>    # Create a Secrets Manager client<br>    session = boto3.session.Session()<br>    client = session.client(<br>        service_name='secretsmanager';<br>        region_name=region_name<br>    )<br><br>    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.<br>    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html<br>    # We rethrow the exception by default.<br><br>    try:<br>        get_secret_value_response = client.get_secret_value(<br>            SecretId=secret_name<br>        )<br>    except ClientError as e:<br>        if e.response['Error']['Code'] == 'DecryptionFailureException':<br>            # Secrets Manager can't decrypt the protected secret text using the provided KMS key.<br>            # Deal with the exception here; and/or rethrow at your discretion.<br>            raise e<br>        elif e.response['Error']['Code'] == 'InternalServiceErrorException':<br>            # An error occurred on the server side.<br>            # Deal with the exception here; and/or rethrow at your discretion.<br>            raise e<br>        elif e.response['Error']['Code'] == 'InvalidParameterException':<br>            # You provided an invalid value for a parameter.<br>            # Deal with the exception here; and/or rethrow at your discretion.<br>            raise e<br>        elif e.response['Error']['Code'] == 'InvalidRequestException':<br>            # You provided a parameter value that is not valid for the current state of the resource.<br>            # Deal with the exception here; and/or rethrow at your discretion.<br>            raise e<br>        elif e.response['Error']['Code'] == 'ResourceNotFoundException':<br>            # We can't find the resource that you asked for.<br>            # Deal with the exception here; and/or rethrow at your discretion.<br>            raise e<br>    else:<br>        # Decrypts secret using the associated KMS CMK.<br>        # Depending on whether the secret is a string or binary; one of these fields will be populated.<br>        if 'SecretString' in get_secret_value_response:<br>            secret = get_secret_value_response['SecretString']<br>        else:<br>            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])<br>            <br>    # Your code goes here. <br></code></pre><br><p>How do I check if the connection is established and how do i query from the redshift?</p><br><p>And I understand that we need to have lambda_handler(event;context) in the code.</p><br>
0.0,0.0,0.3333333333333333,0.0,0.0,1.0,0.0,<h3>AWS SQS issue. The specified queue does not exist for this wsdl version</h3><p>I'm facing the following issue.</p><br><br><p>My application wants to use two AWS SQS queues which are in the same region. My Applications running on EC2.</p><br><br><ul><br><li>is there problem to have two AWS SQS queues in the same user &amp; same region ?</li><br><li>When i try to connect from EC2 then it says <strong><em>"com.amazonaws.services.sqs.model.QueueDoesNotExistException: The specified queue does not exist for this wsdl version. (Service: AmazonSQS; Status Code: 400; Error Code: AWS.SimpleQueueService.NonExistentQueue; Request ID: e10c43db-e5db-5143-9"</em></strong></li><br><li>I did try to connect with the same credentials from local environemt. It was able to connect and push the messages to the Queue.</li><br><li>My AWS User has SQS Full Access; even though it says Queue doesn't exist.</li><br></ul><br><br><p>Thanks</p><br>
1.0,0.3333333333333333,0.3333333333333333,0.0,0.0,0.0,0.0,<h3>Aws Vpc flow logs to athena from s3</h3><p><a href="https://i.stack.imgur.com/cbSXB.jpg" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/cbSXB.jpg" alt="enter image description here"></a>I'm planning to push my vpc flow logs which are currently in s3 to be stored in athena. <br>Whenever vpc flow logs push the log file to  s3 event will be triggered to send the data to athena and check if there is any data greater then 30days delete it. </p><br><br><p>Any suggestion?? </p><br><br><p>My opinion: to put the logs in redshift using copy command. </p><br><br><p>Query used :<br>CREATE EXTERNAL TABLE IF NOT EXISTS vpc_flow_logs (<br>  version int;<br>  account string;<br>  interfaceid string;<br>  sourceaddress string;<br>  destinationaddress string;<br>  sourceport int;<br>  destinationport int;<br>  protocol int;<br>  numpackets int;<br>  numbytes bigint;<br>  starttime int;<br>  endtime int;<br>  action string;<br>  logstatus string<br>)<br>PARTITIONED BY (<code>date</code> date)<br>ROW FORMAT DELIMITED<br>FIELDS TERMINATED BY ' '<br>LOCATION 's'<br>TBLPROPERTIES ("skip.header.line.count"="1");</p><br>
0.0,0.0,0.0,0.3333333333333333,0.0,1.0,0.3333333333333333,<h3>Track when a user navigates off of website and update state in dynamoDB</h3><p>Im looking for some way to track when a user connects a disconnects from our angular site. Im looking to Update and store that information in dynamoDB. The main issue Im facing is updating dynamoDB for unexpected closing of the site. So when someone forecloses the bowers; losses internet connection; or even just shuts down their pc. </p><br><br><p>The only solution I can think of is some heart beat api; that after x amount of time without a beat deans that the user has disconnected. Or use a web-socket to track the connect and disconnect.</p><br><br><p>Im just looking for other ideas and what you think would be the simplest approach to this.</p><br>
0.0,1.0,0.6666666666666666,0.0,0.0,0.0,0.0,<h3>Log Response Headers in Access Logs for API Gateway</h3><p>Is it possible to access response headers in API Gateway Access Logs? I am wanting to log a Header in my response <code>Changes</code> for Audit Logging Purposes; so I display a message for each Action as to what effect that Action.</p><br><br><p>My Current Log Format looks like:</p><br><br><pre><code>{<br>  "path": "$context.path";<br>  "requestTime": "$context.requestTime";<br>  "username": "$context.authorizer.username";<br>  "email": "$context.authorizer.email"<br>  "requestId": "$context.requestId";<br>  "status": "$context.status";<br>  "httpMethod": "$context.httpMethod";<br>  "ip": "$context.identity.sourceIp";<br>  "changes": "$$$TODO$$$"<br>}<br></code></pre><br><br><p>In <code>$$$TODO$$$</code> I have tried all sorts of expressions; to no avail; I imagine this might not be possible in the <code>Custom Access Logging</code> and that is my problem and I just don't know it; and I'll be forced to process the more verbose logs. Currently I am sending only these fields to an Elasticsearch cluster which is working nicely; but I'm worried with this new requirement I will have to change to the normal API gateway logs; and then do more filtering and processing with potentially a custom Lambda function.</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>Predict probability on AWS SageMaker AutoPilot endpoint</h3><p>I am testing SageMaker AutoPilot in order to verify how good it is for regular use.</p><br><br><p>Up until now; it seems relatively easy to use it; it trained a model with good results and it was easy to create the endpoint. I would like to get the predicted label and its probability; in order to check if the prediciton is good. However; I could only get the label and I did not find anything about retrieving the probability (predict_proba).</p><br><br><p>Is there any way to get the probability? Thank you!</p><br>
1.0,0.0,0.3333333333333333,0.3333333333333333,0.0,0.3333333333333333,0.0,<h3>Running Pyspark locally to access parquet file in S3 Error: &quot;Unable to load AWS credentials from any provider in the chain&quot;</h3><p>I am trying to access the parquet file that's available in S3 bucket using Pyspark local via Pycharm. I have the AWS toolkit configured in Pycharm and I have the access key and security key added in my <strong>~/.aws/credentials</strong>  yet I see the credentials are not getting accessed. Which throws me the error &quot;Unable to load AWS credentials from any provider in the chain&quot;</p><br><pre><code>import os<br>import pyspark<br>from pyspark.sql import SparkSession<br><br><br>os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34;org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell'<br><br>spark = SparkSession.builder\<br>            .appName('Pyspark').getOrCreate()<br><br>my_df = spark.read.\<br>    parquet(&quot;s3a://&lt;parquet_file_location&gt;&quot;) --Using s3 gives me no file system error<br><br>my_df.printSchema()<br></code></pre><br><p>Is there any alternative approach to try Pyspark locally and access the AWS resources.</p><br><p>Also I should be able to use s3 in parquet path but that seems to throw an error with file system not found. Does any dependency or jar file needs to be added for running the Pyspark locally</p><br>
0.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.0,<h3>AWS Lambda Performance Drops Heavily with Increasing of Concurrent Instances</h3><p>In our application (.net core) there is a complex CPU oriented algorithm. It normally takes 2-3 minutes per single execution. Right now we execute this in a background service sequentially. So we only can manage about 25 successful executions per hour which is not enough when there is high demand. Execution in multiple threads also didn't help given this is a highly CPU oriented job. In fact; it gave even worse results with multiple threads. </p><br><br><p>So I thought of using AWS Lambdas. So I created a Lambda Function capable of executing the logic and it is triggered by an AWS SQS. So whenever I need to execute the logic; a message is pushed to the queue; and Lambda picks and executes it.</p><br><br><p>When there is only one request Lambda also took 2-3 minutes per execution and that is ok. I have set Lambda's timeout as 15 minutes just in case. </p><br><br><p>However; the Problem starts when there is a large number of requests (ex: 1000 withing 5 mins). As expected Lambda increases the number of instances. But that eventually drops the performance of all the instances. In fact; almost all of them can't complete the job within the 15-minute timeout. </p><br><br><p>So I presume all the parallel instances of Lambdas are span out in one/few PCs where they share the same/few CPUs which eventually simulate the condition I initially had with multiple threads. Contrary to my original thought of each instance gets configured memory (Allocated 512 MB. it normally needs less than 180MB) and adequate CPU for it.</p><br><br><p>The package size is 15Mb. Since cold start time is not a big issue for me; so I think provisioned concurrency also wouldn't help me either (not sure). Besides; It needs to be configured with a particular version which will add lots of hassle during subsequent deployments. </p><br><br><p>I hope the problem is clear. Has anyone come across something like this or knows how to get over with this? </p><br><br><p>Thanks.</p><br>
0.0,0.3333333333333333,0.6666666666666666,0.0,1.0,0.0,0.0,<h3>Can&#39;t connect to EC2 instance via Putty</h3><p>I am doing Team Project about Web Server and streaming.</p><br><p>One of my team member has created Instance; and I want to use that instance.<br>He and I are both IAM user.</p><br><p>But when I tried to use SSH connect by PUTTY; there was an error:</p><br><pre><code>no supported authentication methods available (server sent publickey)<br></code></pre><br><p><img src="https://i.stack.imgur.com/7AJj8.jpg" alt="no supported authentication methods available (server sent publickey)" /></p><br><p>Is it because trying to access with my IAM account; but the owner of that instance is my team member? How can I connect his AWS EC2 instance with my account (using PUTTY)?</p><br><p>Thank you.</p><br>
0.0,0.0,0.0,0.0,0.0,0.3333333333333333,1.0,<h3>How can I send image as an attachment to the email in amazon ses sdk?</h3><p>I have used amazon ses sdk in my iOS app to send emails. I have successfully sent the email using the below code but now I have to send 2 images with the email as attachments and I have searched a lot on the internet regarding this but not able to find any solution. Can anyone help me to solve this issue.</p><br><br><pre><code>let subject : AWSSESContent = AWSSESContent()<br>subject.data = "Test email from demo app"<br><br>let messageBody : AWSSESContent = AWSSESContent()<br>messageBody.data = data<br><br>let emailBody : AWSSESBody = AWSSESBody()<br>emailBody.text = messageBody<br><br>let message : AWSSESMessage = AWSSESMessage()<br>message.subject = subject<br>message.body = emailBody<br><br>let destination : AWSSESDestination = AWSSESDestination()<br>destination.toAddresses = ["xxxx.com"]<br><br>let sendRequest : AWSSESSendEmailRequest = AWSSESSendEmailRequest()<br>sendRequest.source = "yyyy.com"<br>sendRequest.destination = destination<br>sendRequest.message = message<br><br>AWSSES<br>    .default()<br>    .sendEmail(sendRequest)<br>    .continueOnSuccessWith { (task) -&gt; Any? in<br>        print(task.result as AnyObject)<br>    }<br>    .continueWith { (task) -&gt; Any? in<br>        if task.error != nil {<br>            print("Error sending email")<br>           let type : String? = (task.error! as NSError).userInfo["Type"] as? String<br>            print("Type   : \(type!)")<br><br>            let message : String? = (task.error! as NSError).userInfo["Message"] as? String<br>            print("Message: \(message!)")<br><br>            var code : String? = (task.error! as NSError).userInfo["Code"] as? String<br>            code = (code != nil ? code : "Unknown")<br>            print("Code   : \(code!)")<br>        }<br>        return nil<br>    }<br></code></pre><br>
0.0,0.0,0.6666666666666666,0.0,0.3333333333333333,0.6666666666666666,0.0,<h3>Issue comparing datetime.now and EC2 image creation_date</h3><p>I'm working on a Lambda function to delete AMIs (images) older than a set number of days. I'm comparing datetime.now with the image Creation_date. I can see that these values are returned in different formats.</p><br><p>datetime.now format - 2019-11-15 20:34:53.057320+00:00<br /><br>image creation_date format - 2010-10-16T21::31:46.000Z</p><br><p>When I test this I get the error &quot;'&gt;' not supported between instances of 'datetime.datetime' and 'str'&quot;;</p><br><p>My code is below. I believe the issue is due to the different date formats.</p><br><pre><code>from datetime import datetime; timedelta; timezone<br><br>import boto3<br>import collections<br>import sys<br>from botocore.exceptions import ClientError<br><br>region ='us-east-1'<br><br>aws_account_numbers = {&quot;MassIT-Engineering-Sandbox&quot;:&quot;xxxxxxxxx&quot;}<br><br>def lambda_handler(event; context):<br><br>    delete_time = datetime.now(tz=timezone.utc) - timedelta(days=320)<br>    print (delete_time)<br>    <br>    ec2 = boto3.resource('ec2'; 'us-east-1')<br>    images = ec2.images.filter(Owners=[&quot;self&quot;])<br>    <br>    for name; acctnum in aws_account_numbers.items():<br>        roleArn = &quot;arn:aws:iam::%s:role/EOTSS-Snapshot-Cleanup-120days&quot; % acctnum<br>        stsClient = boto3.client('sts')<br>        sts_response = stsClient.assume_role(RoleArn=roleArn;RoleSessionName='AssumeCrossAccountRole'; DurationSeconds=1800)<br>        ec2 = boto3.resource(service_name='ec2';region_name=region;aws_access_key_id = sts_response['Credentials']['AccessKeyId'];<br>                aws_secret_access_key = sts_response['Credentials']['SecretAccessKey']; aws_session_token = sts_response['Credentials']['SessionToken'])<br><br>        for Image in images:<br>                try:<br>                        if delete_time &gt; image.creation_date:<br>                                Image.delete()<br>                                print('AMI with Id = {} is deleted '.format(image.image_id))<br>                           <br>                except ClientError as e:<br>                        if e.response['Error']['Code'] == 'InvalidSnapshot.InUse':<br>                                print(&quot;Snapshot in use&quot;)<br>                                continue<br>                        else:<br>                                print(&quot;Unexpected error: %s&quot; % e)<br>                                continue<br>                        <br>        return 'Execution Complete'<br></code></pre><br>
0.0,0.0,1.0,0.0,0.6666666666666666,0.0,0.0,<h3>Simple AD - change domain membership</h3><p>I have one Simple AD configured in a <strong>primary</strong> AWS account and a few Windows EC2 instances are joined to this Simple AD. I want to migrate my <strong>primary</strong> account Windows instances which are joined to the Simple AD of the <strong>primary</strong> AWS account to my <strong>secondary</strong> AWS account and need to join those instances with the Simple AD instance in the <strong>secondary</strong> account.</p><br><br><p>To do that I have created AMIs of the servers in the <strong>primary</strong> AWS account and shared the <em>AMIs</em> of the servers to my <strong>secondary</strong> AWS account.</p><br><br><p>After creating an instance from the shared AMI in the <strong>secondary account</strong> I am not able to login into the instance and not able to unjoin the instance from my <strong>primary</strong> Simple AD domain controller.</p><br><br><p>Please suggest me how to unjoin the instance from the primary domain controller in my <strong>secondary</strong> AWS account.</p><br><br><p>N.B I am not able to unjoin the instance in my primary AWS account those are production servers.<br>Please suggest me is there any way I can unjoin the instance in secondary AWS account and joined the instance with the new domain controller.</p><br>
0.0,0.3333333333333333,0.6666666666666666,0.0,0.0,0.0,0.3333333333333333,<h3>AWS ACM first time usage</h3><p>I am intending to use ACM for the first time for Proof of Concept purposes. I read that there is a 30 day free trial with no charges for deploying and provisioning ACM. Is this still valid? BTW; my 12 month free trial subscription is over.</p><br><br><p>Regards;<br>Ochen</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.3333333333333333,<h3>When trying to delete a user I&#39;m getting &quot;You do not have the permission required to perform this operation&quot; on my AWS Educate Account?</h3><p>I created an AWS Educate account with my university email.</p><br><p>I tried creating an IAM user and it said user created with errors. I did not document the errors.</p><br><p>Then when I tried to delete the created user; I get this error.</p><br><pre><code>You need permissions<br>You do not have the permission required to perform this operation. Ask your administrator to add permissions. Learn more<br>User: arn:aws:sts::&lt;id&gt;:assumed-role/vocstartsoft/user271512=&lt;university_email_address&gt; is not authorized to perform: iam:DeleteLoginProfile on resource: user &lt;user_name&gt; with an explicit deny<br></code></pre><br><p>What should I do?</p><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.0,<h3>How can I increase the disk space in AWS Fargate container?</h3><p>I am deploying containers via AWS Fargate but I am running into <code>"No Space left on device"</code>. Is there a way I can specify the volume size in task_definitions:</p><br><br><pre><code>task_size:<br>    mem_limit: 2GB<br>    cpu_limit: 256<br></code></pre><br>
0.3333333333333333,0.0,0.0,1.0,0.0,0.0,0.0,<h3>Attempting to Cache s3 files</h3><p>I have two pipelines that I run. The first pipeline reads files from s3 does some processing and updates the files. The second pipeline runs multiple jobs and for each job i download files from s3 and produce some output. I feel i am wasting a lot of time on my second pipeline by doing multiple downloads as i currently do not cache these files when i am using them for multiple jobs. So in that light i am attempting to cache s3 files locally.</p><br><p>I did some research and figured out that <a href="https://s3fs.readthedocs.io/en/latest/" rel="nofollow noreferrer">s3fs</a> or <a href="https://pypi.org/project/fsspec/" rel="nofollow noreferrer">fsspec</a> can be used. So far i am able to download and open a file from s3 using s3fs but I am not sure how to cache it locally.</p><br><pre><code>import s3fs<br>import pandas as pd<br><br>FS = s3fs.S3FileSystem()<br><br>file = FS.open('s3://my-datasets/something/foo.csv')<br># of = fsspec.open(&quot;filecache::s3://bucket/key&quot;; s3={'anon': True}; filecache={'cache_storage'='/tmp/files'})<br>df = pd.read_csv(file; sep='|'; header=None)<br>print(df)<br></code></pre><br><p>As you can see in the code above i am opening a file from s3 and then reading it to a dataframe. Now i am wondering if there is an argument or something i can pass so that this file gets cached.</p><br><p>The alternative approach of course is i can check if the file exists in some path and if it does then use that and if not then download it but i feel like there must be a better way to do caching. I am open to any and all suggestions.</p><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.0,<h3>Create a AWS Lambda layer using Docker</h3><p>I am trying to follow the instructions on this page:<br><a href="https://aws.amazon.com/premiumsupport/knowledge-center/lambda-layer-simulated-docker/" rel="nofollow noreferrer">How do I create a Lambda layer using a simulated Lambda environment with Docker?</a><br>on a Windows 7 environment.</p><br><p>I followed all of these steps:</p><br><ul><br><li>installed Docker Toolbox</li><br><li>created a local folder 'C:/Users/Myname/mylayer' containing requirements.txt and python 3.8 folder structure</li><br><li>run the following commands in docker toolbox:<br /><br><code>cd c:/users/myname/mylayer</code><br /><br><code>docker run -v &quot;$PWD&quot;:/var/task &quot;lambci/lambda:build-python3.8&quot; /bin/sh -c &quot;pip install -r requirements.txt -t python/lib/python3.8/site-packages/; exit&quot;</code></li><br></ul><br><p>It returns the following error:<br /><br><code>ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'</code></p><br><p>I don't understand what I am doing wrong. Maybe something obvious (I'm a beginner) but I spent the whole day trying to figure it out and it is getting quite frustrating. Appreciate the help!</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>How to get the user login duration from CloudWatch Logs Insights</h3><p>How I need to get the user login duration by writing query?</p><br><br><pre><code>2020-04-24T17:41:21.216+05:30<br>END RequestId: 8caf16f6-b1ef-434b-a318-c8e3e545a737<br>10<br>2020-04-24T17:41:21.216+05:30<br>REPORT RequestId: 8caf16f6-b1ef-434b-a318-c8e3e545a737 Duration: 368.89 ms Billed Duration: 400 ms Memory Size: 448 MB Max Memory Used: 121 MB Init Duration: 1037.35 ms<br>11<br>2020-04-24T17:41:21.215+05:30<br>Login successfull<br>12<br>2020-04-24T17:41:21.215+05:30<br>((1; 'Admin'; 'krish'; 'kittu'; '6;7;8;9'; 1; 'PRA0001))');)<br>13<br>2020-04-24T17:41:20.846+05:30<br>START RequestId: 8caf16f6-b1ef-434b-a318-c8e3e545a737 Version: $LATEST<br></code></pre><br>
0.0,0.0,0.0,0.0,0.6666666666666666,0.0,0.6666666666666666,<h3>AWS Windows GPU instance opengl</h3><p><strong>Description</strong>: I set up an AWS EC2 Windows Server 2019 instance. The instance I used is <code>g4dn</code> GPU instance class</p><br><p><strong>Purpose</strong>: I want to use this instance for 3d modeling</p><br><p><strong>Problem</strong>: When I open the 3d software. It says <code>opengl 2.0 not supported by your system</code></p><br><p><strong>My failed &quot;solutions&quot;</strong>:</p><br><ol><br><li>Install Nvidia driver. As I initially thought it's because GPU is not present</li><br><li>Use teamviewer. Based on online tutorial. They say connecting to Windows instance via RDP; the default opengl is 1.1; and teamviewer will resolve that. However; when I connect using teamviewer and check; the opengl is still 1.1</li><br><li>I tried AWS AMI Windows server 2019 with driver installed. However; it has the same problem as the instance I manually setup</li><br></ol><br>
0.0,0.3333333333333333,0.3333333333333333,0.0,0.6666666666666666,0.6666666666666666,0.0,<h3>Is there a way to connect a new AWS ApiGateway to existing lambda funtion using AWS-CDK? (TypeScript)</h3><p>I am new to AWS CDK. I am trying to create a API gateway; and connect it to the AWS lambda. But I want these 2 modules to be separate code structures.</p><br><p>I created the lambda functions and the API gateway in 2 different projects. And my AWS console has both API and lambda init. But I am not sure how to connect my API gateway to these lambdas.</p><br><p>The only way I could find was</p><br><pre><code>    new lambda.Function(this; 'createPr'; {<br>      code: lambda.Code.asset(&quot;resources/lambdas&quot;);<br>      ....<br>      ....<br>    });<br></code></pre><br><p>But this required the directory structure pointing towards lambda code.</p><br><p>Any help would be much appreciated!</p><br>
0.0,1.0,0.3333333333333333,1.0,0.0,0.0,0.0,<h3>Download zip file from S3 using API Gateway directly</h3><p>Is it possible to download Zip file present in S3 using API Gateway alone.</p><br><br><p>For the configurations:</p><br><br><p>Integration type : AWS Service (S3)</p><br><br><p>endpoint : GET</p><br><br><p><strong>Content-Type: application/zip (or) application/octet-stream</strong></p><br><br><p>A corrupted zip file is getting downloaded.</p><br><br><p>I could able to do a workaround using <strong>S3 presigned url</strong> and don't want to make the bucket public.</p><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>assuming role instance profile boto3 python</h3><p>I have multiple AWS accounts and I am trying to run script against all accounts from central account / instance profile.</p><br><br><p>I am having trouble assuming role / targeting the other accounts with role that has been created and allowed to an instance profile in python boto3 script.</p><br><br><p>I have tried 2 functions below but the script always runs against local account where instance resides. How should I tell the python script to run against target account ? </p><br><br><pre class="lang-py prettyprint-override"><code>def access_sub_account(access_key; secret_key; account_id; role_arn):<br>    client = boto3.client('sts'; aws_access_key_id=access_key; aws_secret_access_key=secret_key)<br>    response = client.assume_role(RoleArn=arn:aws:iam::12345678910:role/Role; RoleSessionName='CTOpsLambdaAuditRole')<br><br>    return response['Credentials']['AccessKeyId']; \<br>              response['Credentials']['SecretAccessKey']; \<br>              response['Credentials']['SessionToken']<br><br>    arn_role='arn:aws:iam::12345678910:role/Role'<br>arn_session='Role'<br><br>def assume_role(arn; session_name):<br>    """aws sts assume-role --role-arn arn:aws:iam::00000000000000:role/example-role --role-session-name example-role"""<br><br>    client = boto3.client('sts')<br>    assume_account_id = client.get_caller_identity()["Account"]<br>    print("Current Account ID";assume_account_id)<br><br>    response = client.assume_role(RoleArn=arn; RoleSessionName=session_name)<br><br>    session = Session(aws_access_key_id=response['Credentials']['AccessKeyId'];<br>                      aws_secret_access_key=response['Credentials']['SecretAccessKey'];<br>                      aws_session_token=response['Credentials']['SessionToken'])<br><br>    client = session.client('sts')<br>    assume_account_id = client.get_caller_identity()["Account"]<br>    print('Account ID Assumed:';assume_account_id)<br></code></pre><br><br><p>I can see the role is correct if I set this but script runs against local account where instance profile is</p><br><br><pre><code>assume_role(arn_role; arn_session)<br>account_id = boto3.client('sts').get_caller_identity()['Account']<br></code></pre><br><br><p>updated: when print the response it give the correct role and keys. script continues to use local profile from instance on local account; do I need to export the credentials to ~/.aws/credentials ?</p><br><br><pre><code>Response is: {'Credentials': {'AccessKeyId': 'ABCDEGHJJJJJJJSSSSSS'; 'SecretAccessKey': '123456789876543212344567898765653322'; 'SessionToken': 'sessiontokenblahblah=='; 'Expiration': datetime.datetime(2020; 1; 23; 16; 44; 32; tzinfo=tzlocal())}; 'AssumedRoleUser': {'AssumedRoleId': 'AccesskeyId:Role'; 'Arn': 'arn:aws:sts::123456789:assumed-role/Role/Role'}; 'ResponseMetadata': {'RequestId': '3f8839b4-3df7-11ea-85e2-316d5eca0a34'; 'HTTPStatusCode': 200; 'HTTPHeaders': {'x-amzn-requestid': '3f8839b4-3df7-11ea-85e2-316d5eca0a34'; 'content-type': 'text/xml'; 'content-length': '1095'; 'date': 'Thu; 23 Jan 2020 15:44:31 GMT'}; 'RetryAttempts': 0}}<br></code></pre><br><br><p>I was able to get it to assume updating the .aws/config and credentials and adding to code below; issue now is run against both accounts</p><br><br><pre><code>boto3.setup_default_session(profile_name='Role')<br>    session = boto3.session.Session()<br>    temp_credentials = session.get_credentials().get_frozen_credentials()<br></code></pre><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>AWS CLI get credentials/config from root</h3><p>how can I run aws cli command from a user by pointing the profile in root credentials/config file?</p><br><p>I have set the following:</p><br><pre><code>export AWS_CONFIG_FILE=/root/.aws/config<br>export AWS_SHARED_CREDENTIALS_FILE=/root/.aws/credentials<br></code></pre><br><p>my credentials and config file as follow:</p><br><pre><code># /root/.aws/config<br><br>[profile xaxa]<br>aws_access_key_id=xxx<br>aws_secret_access_key=xxx<br>region=ap-southeast-2<br><br># /root/.aws/credentials<br><br>[profile xaxa]<br>aws_access_key_id=xxx<br>aws_secret_access_key=xxx<br></code></pre><br><p>then; i run the following command:</p><br><pre><code>sudo aws --profile xaxa ecr get-login-password --debug <br></code></pre><br><p>but its returning me:</p><br><blockquote><br><p>The config profile (xaxa) could not be found</p><br></blockquote><br><p>How can i get another user to use root credentials/config file?</p><br>
0.0,1.0,0.0,0.0,0.6666666666666666,0.0,0.0,<h3>AWS CloudFormation: Target Group for Application Load Balancer is not working for multiple EC2 instances</h3><p>I am deploying my infrastructure to AWS using CloudFormation template. My infrastructure has an application load balancer that is pointing to a target group. The target group will have multiple EC2 instances in it.</p><br><p>The following is my target group and the EC2 instances in the template.</p><br><pre><code>Resources:<br>  PublicSubnet1:<br>    Type: AWS::EC2::Subnet<br>    Properties:<br>      VpcId: !Ref Vpc<br>      CidrBlock: !Select [ 0; !Cidr [ !Ref VpcCidr; 12; 8 ] ]<br>      MapPublicIpOnLaunch: True<br>      AvailabilityZone: !Select<br>        - 0<br>        - Fn::GetAZs: !Ref AWS::Region<br>  PublicSubnet2:<br>    Type: AWS::EC2::Subnet<br>    Properties:<br>      VpcId: !Ref Vpc<br>      CidrBlock: !Select [ 1; !Cidr [ !Ref VpcCidr; 12; 8 ] ]<br>      MapPublicIpOnLaunch: True<br>      AvailabilityZone: !Select<br>        - 1<br>        - Fn::GetAZs: !Ref AWS::Region<br>  PublicSubnet3:<br>    Type: AWS::EC2::Subnet<br>    Properties:<br>      VpcId: !Ref Vpc<br>      CidrBlock: !Select [ 2; !Cidr [ !Ref VpcCidr; 12; 8 ] ]<br>      MapPublicIpOnLaunch: True<br>      AvailabilityZone: !Select<br>        - 2<br>        - Fn::GetAZs: !Ref AWS::Region<br>  InternetGateway:<br>    Type: AWS::EC2::InternetGateway<br>  AttachGateway:<br>    Type: AWS::EC2::VPCGatewayAttachment<br>    Properties:<br>      VpcId: !Ref Vpc<br>      InternetGatewayId: !Ref InternetGateway<br>  RouteTable:<br>    Type: AWS::EC2::RouteTable<br>    Properties:<br>      VpcId: !Ref Vpc<br>  Route:<br>    Type: AWS::EC2::Route<br>    DependsOn: InternetGateway<br>    Properties:<br>      RouteTableId: !Ref RouteTable<br>      DestinationCidrBlock: 0.0.0.0/0<br>      GatewayId: !Ref InternetGateway<br>  SubnetRouteTableAssociation:<br>    Type: AWS::EC2::SubnetRouteTableAssociation<br>    Properties:<br>      SubnetId: !Ref PublicSubnet1<br>      RouteTableId: !Ref RouteTable<br>  LoadBalancerSecurityGroup:<br>    Type: AWS::EC2::SecurityGroup<br>    Properties:<br>      GroupDescription: Application Load Balancer Security Group<br>      SecurityGroupIngress:<br>        - IpProtocol: tcp<br>          FromPort: '80'<br>          ToPort: '80'<br>          CidrIp: 0.0.0.0/0<br>        - IpProtocol: tcp<br>          FromPort: '443'<br>          ToPort: '443'<br>          CidrIp: 0.0.0.0/0<br>      VpcId: !Ref Vpc<br>  LoadBalancer:<br>    Type: AWS::ElasticLoadBalancingV2::LoadBalancer<br>    Properties:<br>      Name: ApplicationLoadBalancer<br>      Subnets:<br>        - !Ref PublicSubnet1<br>        - !Ref PublicSubnet2<br>        - !Ref PublicSubnet3<br>      SecurityGroups:<br>        - !Ref LoadBalancerSecurityGroup<br>  LoadBalancerListener:<br>    Type: AWS::ElasticLoadBalancingV2::Listener<br>    Properties:<br>      LoadBalancerArn: !Ref LoadBalancer<br>      Port: 80<br>      Protocol: HTTP<br>      DefaultActions:<br>        - Type: forward<br>          TargetGroupArn: !Ref ApplicationTargetGroup<br>  ApplicationTargetGroup:<br>    Type: AWS::ElasticLoadBalancingV2::TargetGroup<br>    Properties:<br>      HealthCheckIntervalSeconds: 30<br>      HealthCheckProtocol: HTTP<br>      HealthCheckTimeoutSeconds: 15<br>      HealthyThresholdCount: 5<br>      UnhealthyThresholdCount: 3<br>      Matcher:<br>        HttpCode: '200'<br>      Name: ApplicationTargetGroup<br>      VpcId: !Ref Vpc<br>      Port: 80<br>      Protocol: HTTP<br>      TargetGroupAttributes:<br>        - Key: deregistration_delay.timeout_seconds<br>          Value: '20'<br>      Targets:<br>        - Id: !Ref WebServerInstance1<br>          Port: 80<br>        - Id: !Ref WebServerInstance2<br>            Port: 80<br>  WebServerInstance1:<br>    Type: AWS::EC2::Instance<br>    Properties:<br>      InstanceType: !Ref InstanceType<br>      KeyName: !Ref KeyName<br>      SubnetId: !Ref PublicSubnet1<br>      SecurityGroupIds:<br>        - !Ref WebServerSecurityGroup<br>      ImageId:<br>        Fn::FindInMap:<br>          - AWSRegionArch2AMI<br>          - Ref: AWS::Region<br>          - Fn::FindInMap:<br>              - AWSInstanceType2Arch<br>              - Ref: InstanceType<br>              - Arch<br>      AvailabilityZone: !Select<br>        - 0<br>        - Fn::GetAZs: !Ref AWS::Region<br>      UserData:<br>        Fn::Base64: !Sub |<br>          #!/bin/bash -xe<br>          cd /tmp<br>          yum update -y<br>          yum install -y httpd24<br>          echo &quot;Welcome from the instance 1&quot; &gt; /var/www/html/index.html<br>          sudo -u root service httpd start<br>  WebServerInstance2:<br>    Type: AWS::EC2::Instance<br>    Properties:<br>      InstanceType: !Ref InstanceType<br>      KeyName: !Ref KeyName<br>      SubnetId: !Ref PublicSubnet1<br>      SecurityGroupIds:<br>        - !Ref WebServerSecurityGroup<br>      ImageId:<br>        Fn::FindInMap:<br>          - AWSRegionArch2AMI<br>          - Ref: AWS::Region<br>          - Fn::FindInMap:<br>              - AWSInstanceType2Arch<br>              - Ref: InstanceType<br>              - Arch<br>      AvailabilityZone: !Select<br>        - 0<br>        - Fn::GetAZs: !Ref AWS::Region<br>      UserData:<br>        Fn::Base64: !Sub |<br>          #!/bin/bash -xe<br>          cd /tmp<br>          yum update -y<br>          yum install -y httpd24<br>          echo &quot;Welcome from the instance 2&quot; &gt; /var/www/html/index.html<br>          sudo -u root service httpd start<br>  WebServerSecurityGroup:<br>    Type: AWS::EC2::SecurityGroup<br>    Properties:<br>      GroupDescription: Enable HTTP access via port 80<br>      SecurityGroupIngress:<br>        - IpProtocol: tcp<br>          FromPort: '80'<br>          ToPort: '80'<br>          CidrIp: 0.0.0.0/0<br>        - IpProtocol: tcp<br>          FromPort: '22'<br>          ToPort: '22'<br>          CidrIp:<br>            Ref: SSHLocation<br>      VpcId: !Ref Vpc<br></code></pre><br><p>As you can see my target group has two targets. But when I deployed and open the load balancer DNS in the browser; it just keeps loading; loading and loading. But if I assign only one EC2 instance to the target group; it is working as expected. It is just not working for multiple instances. What is wrong with my template and how can I fix it?</p><br>
0.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.0,<h3>Class not found on AWS Elastic beanstalk environment</h3><p>Im getting the follow error from my elastic beanstalk application:</p><br><br><blockquote><br>  <p>Fatal error: Uncaught Error: Class 'App\PDO' not found in<br>  /var/app/current/crawler/app/SQLConnection.php:32 Stack trace: #0<br>  /var/app/current/crawler/init.php(18): App\SQLConnection->connect() #1<br>  {main} thrown in /var/app/current/crawler/app/SQLConnection.php on<br>  line 32</p><br></blockquote><br><br><p>The app runs perfectly locally (as is usually the case with errors). <br>All libraries and dependencies are included in the app I uploaded.</p><br><br><p>Can anyone see where I went wrong please?</p><br><br><p>The initialzation script to create DB tables:</p><br><br><pre><code>&lt;?php <br>require 'vendor/autoload.php'; <br>require (__DIR__.'/app/SQLConnection.php');<br>require (__DIR__.'/app/SQLCreateTable.php');<br>require (__DIR__.'/app/SQLInsert.php');<br>require (__DIR__.'/app/SQLRetrieve.php');<br>require (__DIR__.'/app/SQLDelete.php');<br>use App\SQLConnection;<br>use App\SQLCreateTable;<br>use App\SQLInsert;<br>use App\SQLRetrieve;<br>use App\SQLDelete;<br><br>try {<br><br>    //connect to DB<br>    $sql = new SQLCreateTable((new SQLConnection())-&gt;connect());<br>    //get list of tables<br>    $tables = $sql-&gt;getTableList();<br>    //check if required tables excist<br>    if(in_array('results';$tables) &amp;&amp; in_array('logs';$tables)){<br>        echo "Tables already created. You will be redirected to the home page"; <br>        header( "refresh:5;url=/index.php" );<br>    }else {<br>        //create tables<br>        try {<br>            $sql-&gt;createTables();<br>            echo "Tables succesfully created! You will be redirected to the home page";<br>            header( "refresh:5;url=/index.php" );<br>        } catch (\PDOException $e) {<br>            echo "Failed to create tables: " . $e-&gt;getMessage();<br>        }<br><br>    }<br><br>} catch (\PDOException $e) {<br>    echo "Connection failed: " . $e-&gt;getMessage();<br>}<br></code></pre><br><br><p>Then the SQL connection script:</p><br><br><pre><code>&lt;?php<br>namespace App;<br><br>/**<br> * SQL connnection<br> */<br>class SQLConnection {<br>    /**<br>     * PDO instance<br>     * @var type <br>     */<br>    private $pdo;<br><br>    /**<br>     * return in instance of the PDO object that connects to the SQLite database<br>     * @return \PDO<br>     */<br>    public function connect() {<br><br>        $dbhost = $_SERVER['RDS_HOSTNAME'];<br>        $dbport = $_SERVER['RDS_PORT'];<br>        $dbname = $_SERVER['RDS_DB_NAME'];<br>        $charset = 'utf8' ;<br><br>        $dsn = "mysql:host={$dbhost};port={$dbport};dbname={$dbname};charset={$charset}";<br>        $username = $_SERVER['RDS_USERNAME'];<br>        $password = $_SERVER['RDS_PASSWORD'];<br><br>        if ($this-&gt;pdo == null) {<br>            try {<br>                // $this-&gt;pdo = new \PDO("mysql:host=localhost;dbname=crawler"; "root"; "");<br>                $this-&gt;pdo = new PDO($dsn; $username; $password);<br>                // set the PDO error mode to exception<br>                $this-&gt;pdo-&gt;setAttribute(\PDO::ATTR_ERRMODE; \PDO::ERRMODE_EXCEPTION);<br>            } catch (\PDOException $e) {<br>               echo "Connection failed: " . $e-&gt;getMessage();<br>            }<br>        }<br>        return $this-&gt;pdo;<br>    }<br><br><br>}<br></code></pre><br><br><p>The 'Init' script is at root level and the SQLconnection script is in a folder called "app".</p><br><br><p>Thanks guys!</p><br>
0.0,0.0,0.0,0.6666666666666666,0.3333333333333333,0.6666666666666666,0.3333333333333333,<h3>Using Jest.fn() to see if s3.upload function was called...what am I doing wrong?</h3><p>I am a bit new to testing and I have been stuck on this issue for quite some time. So I am trying to test a s3.upload() function to see if it called; not to see if it actually uploads the object. The only constraint is that I cannot use any npm packages to mock out the functionality of the s3 bucket. </p><br><br><p>I was trying to follow this tutorial (<a href="https://stackoverflow.com/questions/42135426/how-to-mock-a-function-inside-another-function-which-i-am-testing-using-sinon">How to mock a function inside another function (which I am testing) using sinon?</a>) that uses sinon as a stub; but instead use jest instead. Any help or guidance with issue is appreciated.</p><br><br><pre><code>// function.js<br>const uploadToS3 = (params) =&gt; {<br>    const response = s3.upload(params).promise();<br>    return response;<br>}<br><br>// functions.test.js<br>describe("Lambda Handler Function"; () =&gt; {<br>    test('To test to see if the uploadToS3 function was called'; () =&gt; {<br>        const sampleParam = {<br>            Bucket:  'BucketName';<br>            Key: 'BucketKey.zip';<br>            Body: 'SGVsbG8sIFdvcmxk'<br>        }<br>        expect(uploadToS3(sampleParam).response).toBeCalled()<br>    })<br>})  <br></code></pre><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>Converting JSON to Parquet and Categorizing Objects Into Folder</h3><p>I have 0 experience with Snowflake so please bear with me. Currently; we have a system where we collect gyroscope and accelerometer data in form of JSON from iWatch using AWS Kinesis stored in S3 bucket (lets call it bucket A); then we use AWS Glue to convert those JSON files into parquet files and divide the data based on its respective sensors and store the data in 2 different folders(accelerometer and gyroscope folders). Those transformed data are stored in a new bucket (lets call it bucket B). Now; is it possible to have Snowflake to do exactly what AWS Glue is doing also storing the converted and transformed data in Snowflake (removing bucket B)? Thanks </p><br>
0.0,0.0,0.6666666666666666,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>InvalidChiperException for one lambda; but not the other</h3><pre><code>db_password = boto3.client('kms').decrypt(CiphertextBlob=b64decode(os.environ['DB_PASSWORD']))['Plaintext']<br><br>services_region = 'us-east-1'    <br>db_password = boto3.client('kms'; region_name=services_region).decrypt(CiphertextBlob=b64decode(os.environ['DB_PASSWORD']))['Plaintext']<br></code></pre><br><p>I have tried both of the above code; I have verified that both lambda functions are using the same VPC; private subnets; and that the security groups for both are added to the kms endpoint.  They both use the same exact role.  They both use the same S3 zip package and they both use the same line in the code for the .decrypt (I copied and pasted the code; it is the second one above).</p><br><p>One of them works without issue; the other is throwing the following:</p><br><pre><code>An error occurred (InvalidCiphertextException) when calling the Decrypt operation: : InvalidCiphertextException<br>Traceback (most recent call last):<br>  File &quot;/var/task/awsfinbi_workdocs_api_pull/bin/api_pull.py&quot;; line 143; in lambda_handler<br>    get_data()<br>  File &quot;/var/task/awsfinbi_workdocs_api_pull/bin/api_pull.py&quot;; line 100; in get_data<br>    db_load(df; s)<br>  File &quot;/var/task/awsfinbi_workdocs_api_pull/bin/api_pull.py&quot;; line 122; in db_load<br>    raise e<br>  File &quot;/var/task/awsfinbi_workdocs_api_pull/bin/api_pull.py&quot;; line 112; in db_load<br>    db_password = boto3.client('kms';region_name=services_region).decrypt(CiphertextBlob=b64decode(os.environ['DB_PASSWORD']))['Plaintext']<br>  File &quot;/var/runtime/botocore/client.py&quot;; line 316; in _api_call<br>    return self._make_api_call(operation_name; kwargs)<br>  File &quot;/var/runtime/botocore/client.py&quot;; line 635; in _make_api_call<br>    raise error_class(parsed_response; operation_name)<br>botocore.errorfactory.InvalidCiphertextException: An error occurred (InvalidCiphertextException) when calling the Decrypt operation:<br></code></pre><br><p>The only difference between the two is the passwords are different.  However; I tested it with my password which is different to the programmatic user and it seems to make no difference.</p><br><p>the permissions tab on the lambda shows <code>kms:*  Allow: All resources</code></p><br><p>I have tried creating a key and using it; I have tried using the all the other keys as well.  All throw this error.</p><br><p>Anything I might have missed to figure out this issue?</p><br>
0.0,0.0,0.0,1.0,0.0,1.0,0.0,<h3>How to use AWS AppSync in one account access DynamoDB in another account?</h3><p>I am a beginner of AWS service AppSync and DynamoDB.<br>I want to ask whether it is possible to use AWS AppSync in one account access DynamoDB in another account?<br>I follow this tutorial: <a href="https://docs.aws.amazon.com/appsync/latest/devguide/tutorial-dynamodb-resolvers.html" rel="nofollow noreferrer">AppSync Tutorial</a>; but when I choose the data source; I don't know how to choose the DynamoDB in my other AWS account.</p><br><p>Could anyone give me some suggestions about the most simple method to implement cross-account access with AppSync and DynamoDB? Thanks in advance!</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>How to set welcome message for messanger chat bot build using AWS Lex</h3><p>I built a chat bot for my project using AWS Lex and then I integrated it with Facebook Messenger. After integrating with Facebook Messenger my chatbot interface looked like this.</p><br><p><a href="https://i.stack.imgur.com/9Envg.png" rel="nofollow noreferrer">In the image you can see when some open the messenger he will get suggestions like &quot;Where are you located?&quot; or &quot;What are your hours?&quot; etc. by default.</a></p><br><p>Expected output: I want instead of these default suggestions messenger should display customized message like &quot;Hello welcome&quot;.</p><br>
0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.3333333333333333,0.3333333333333333,<h3>Why website is not auditable or readable for google?</h3><p>Have created a staging website on Herokuapp and production on AWS but whenever trying to audit or speed testing it's giving an attached error </p><br><br><ol><br><li><a href="http://findmy-web.herokuapp.com/" rel="nofollow noreferrer">http://findmy-web.herokuapp.com/</a> staging -> hosted on heroku</li><br><li><a href="https://www.gocatchy.com/" rel="nofollow noreferrer">https://www.gocatchy.com/</a> live -> hosted on AWS</li><br></ol><br><br><p>Technology used = node and react </p><br><br><p><a href="https://i.stack.imgur.com/5NGV4.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/5NGV4.png" alt="enter image description here"></a><br><a href="https://i.stack.imgur.com/Qi6ar.jpg" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/Qi6ar.jpg" alt="enter image description here"></a><br><a href="https://i.stack.imgur.com/0bWtB.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/0bWtB.png" alt="enter image description here"></a></p><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.3333333333333333,<h3>Page faults in Linux; how to resolve?</h3><p>I get a error and I dont have ideia what mean and what to do now. I try this code in AWS ubuntu:<code>sudo time docker checkpoint create --checkpoint-dir=/home/ubuntu/exec/checkpoint/ --leave-running=true container_2 checkpoint0</code>; but the output was:</p><br><blockquote><br><p>0:00.07elapsed 15%CPU (0avgtext+0avgdata 15024maxresident)k 0inputs+0outputs (0major+1410minor)pagefaults 0swaps</p><br></blockquote><br><p>but the correct output was:</p><br><blockquote><br><p>real   0m0.071s<br /><br>user    0m0.010s<br /><br>sys     0m0.000s</p><br></blockquote><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.0,0.0,<h3>VueJS build and Elastic Beanstalk environment variables</h3><p>I'm quite new to programming in AWS so I will do my best to explain my problem as thoroughly as possible. </p><br><br><p>My goal is to store an API (private) key as an environment variable on Elastic Beanstalk; for security purposes. My stack is NodeJS + Express + VueJS</p><br><br><hr><br><br><p><strong>Local setup</strong></p><br><br><p>Locally I've created a <strong>.env.local</strong> file in the root folder with a <code>VUE_APP_TEST=Test is working locally</code> parameter. The VUE_APP_ part is needed as I want to call on this parameter from my vue app.<br><em>After running a build</em>; I can call my parameter with NodeJS using the following code:</p><br><br><p><code>var testparam = process.env.VUE_APP_TEST || "Test is not working"</code></p><br><br><p>Which makes <code>testparam</code> equal to  <code>"Test is working locally"</code> in string format.<br>So far; so good!</p><br><br><hr><br><br><p><strong>Elastic Beanstalk setup</strong></p><br><br><p>Following <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environments-cfg-softwaresettings.html" rel="nofollow noreferrer">this information from AWS's docs</a> (under Configuring environment properties) I set the same variable as follows: <a href="https://i.stack.imgur.com/sB0zc.png" rel="nofollow noreferrer">aws environment variable declaration</a>. As explained in the  AWS docs; the code for running the parameter is the same:</p><br><br><p><code>var testparam = process.env.VUE_APP_TEST || "Test is not working"</code></p><br><br><p>Only this time I expect the result to be <code>"Test is working on aws"</code>. However; the same <code>"Test is working locally"</code> appears. </p><br><br><p>I cannot get it to work and it looks to me like a really trivial problem. Can anyone help me? I've spent an entire day on this. Thank you in advance!</p><br>
0.0,0.0,0.0,1.0,0.0,0.3333333333333333,0.0,<h3>Keep on recieving error: AWS HTTP error: cURL error 6: when trying to upload file</h3><p>Everytime I try to upload a file to my s3 bucket with sdk; I keep on getting the error (AWS HTTP error: cURL error 6). I have no idea why this is happening and I have no idea on how to fix it. Is there anything else I need to add to my code or specify? Is the problem just with the bucket or could it be with the user I made (IAM). I don't need a definitive answer I just need something that can narrow down the area I need to look in for the cause of the error. I'd gladly appreciate any response. Thanks.</p><br><br><p>This is my code: </p><br><br><pre><code>&lt;?php<br>require 'vendor/autoload.php';<br><br>use Aws\S3\S3Client;<br><br>// Instantiate an Amazon S3 client.<br>$s3 = new S3Client([<br>    'version' =&gt; 'latest';<br>    'region'  =&gt; 'US-West'; <br>    'credentials' =&gt; [<br>        'key'    =&gt; 'garbage';<br>        'secret' =&gt; 'garbage'<br>    ]<br>]);<br><br><br>$bucketName = 'garbage';<br>$file_Path = __DIR__ . '/my-image.png';<br>$key = basename($file_Path);<br><br>// Upload a publicly accessible file. The file size and type are determined by the SDK.<br>try {<br>    $result = $s3-&gt;putObject([<br>        'Bucket' =&gt; $bucketName;<br>        'Key'    =&gt; 'videouploads/' . $key;<br>        'Body'   =&gt; fopen($file_Path; 'r');<br>        'ACL'    =&gt; 'public-read';<br>    ]);<br>    echo $result-&gt;get('ObjectURL');<br>} catch (Aws\S3\Exception\S3Exception $e) {<br>    echo "There was an error uploading the file.\n";<br>    echo $e-&gt;getMessage();<br>}<br><br><br>?&gt;<br></code></pre><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>AWS:::sam local invoke &quot;HelloWorldFunction&quot;</h3><p>I have successfully executed sam init and sam build. and my folder structure is:</p><br><p><a href="https://i.stack.imgur.com/bqe3w.png" rel="nofollow noreferrer">enter image description here</a></p><br><p>While issuing sam local invoke &quot;HelloWorldFunction&quot; I am getting below error:::handle = win32file.CreateFile(<br>pywintypes.error: (2; 'CreateFile'; 'The system cannot find the file specified.')</p><br><p>Any solution please?</p><br>
0.0,0.3333333333333333,0.0,0.0,1.0,0.0,0.3333333333333333,<h3>Aws wordpress EC2 with 503 service unavailable</h3><p>This is my first time using wordpress on EC2 (I have 0 knowledge about it. I just follow youtube tutorial...). It was fine when I created in July until last week. I ran into 503 error </p><br><br><p>Service Unavailable<br>The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later.Additionally; a 503 Service Unavailable error was encountered while trying to use an ErrorDocument to handle the request.</p><br><br><p>and saw articles saying the instance load balancer needs to set to a target group(I follow AWS documentation but it shows unused). Can anyone help me? My site is been down for one week already :( Thank you.</p><br><br><p>Edit:<br>I ssh into the instance and saw my port 80 and 443 was block. So I rerun it make it active and my site is back again. For the targets I had 301 error code and I change the health checks success codes to 301 and its now healthy. </p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>AWS: Add Connection to glue job with boto3</h3><p>I want to add connection my glue job using lambda; so I did so:</p><br><pre><code>response = glue.create_job(Name=&quot;redshift&quot;; Role=&quot;XXXXXXX&quot;;Command={                                <br>            'Name':'glueetl';                      <br>            'PythonVersion':'3';       <br>            'ScriptLocation':path_redshift_job;    <br>        };ExecutionProperty={'MaxConcurrentRuns':1}; Connections={<br>        'Connections': ['RedshiftClusterConnection']})<br></code></pre><br><p>But as you see in the image connection no connection is added to the job; what can I do to solve this issue ?</p><br><p><img src="https://i.stack.imgur.com/wUenb.png" alt="connections image" /></p><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.0,<h3>Executing AWS Lambda Function not working</h3><p>I am trying to create a simple Lambda function <code>ChildFunction</code> with the following Code:</p><br><pre><code>    import json<br>    import uuid<br> <br>    def lambda_handler(event; context):<br>        <br>        productName = event['ProductName']<br>        quantity    = event['Quantity']<br>        unitPrice   = event['UnitPrice']<br> <br>        transactionId   = str(uuid.uuid1())<br> <br>        amount      = quantity * unitPrice<br> <br>        return {<br>            'TransactionID' :   transactionId;<br>            'ProductName'   :   productName;<br>            'Amount'        :   amount<br>        }<br></code></pre><br><p>I am creating a <code>Test Event</code> with the following Test parameters:</p><br><pre><code>{<br><br>  &quot;ProductName&quot;: &quot;iPhone SE&quot;;<br>  &quot;Quantity&quot;: &quot;2&quot;;<br>  &quot;UnitPrice&quot;: &quot;499&quot;<br>}<br></code></pre><br><p>When I execute the <code>Test Event</code>; I am getting the following output:</p><br><pre><code>Response:<br>{<br>  &quot;statusCode&quot;: 200;<br>  &quot;body&quot;: &quot;\&quot;Hello from Lambda!\&quot;&quot;<br>}<br><br>Request ID:<br>&quot;9c68e0d8-3781-4046-ac26-127c45321d71&quot;<br><br>Function logs:<br>START RequestId: 9c68e0d8-3781-4046-ac26-127c45321d71 Version: $LATEST<br>END RequestId: 9c68e0d8-3781-4046-ac26-127c45321d71<br>REPORT RequestId: 9c68e0d8-3781-4046-ac26-127c45321d71  Duration: 1.19 ms   Billed Duration: 100 ms Memory Size: 128 MB Max Memory Used: 52 MB<br></code></pre><br><p>I don't know why the Lambda Function is not executing. What am I doing wrong here?</p><br>
0.3333333333333333,0.0,0.0,0.6666666666666666,0.0,0.0,0.6666666666666666,<h3>BigQuery Transfer Service does not copy rows from S3</h3><p>I have created a BigQuery transfer from AWS S3 to Google BigQuery. It fails with the following error;</p><br><br><blockquote><br>  <p>No new files found matching "gs://bqdts-amazon_s3-prod-eu-w5jetqct8ohvcjih85apf7gvkbibvbkcj9o6l67/test/files"</p><br></blockquote><br><br><p>But; the data is successfully moved from S3 to Google Cloud</p><br><br><blockquote><br>  <p>Moving data from Amazon S3 to Google Cloud complete: Moved 10<br>  object(s).</p><br></blockquote><br><br><p>I have created a table in the BigQuery dataset also</p><br><br><p>Kindly help me resolve this issue</p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.0,0.0,<h3>Failing to create s3 buckets in specific regions</h3><p>I'm trying to create an s3 bucket in every region in AWS with boto3 in python but I'm failing to create a bucket in 4 regions (af-south-1; eu-south-1; ap-east-1 &amp; me-south-1)</p><br><br><p>My python code:</p><br><br><pre><code>def create_bucket(name; region):<br>    s3 = boto3.client('s3')<br>    s3.create_bucket(Bucket=name; CreateBucketConfiguration={'LocationConstraint': region})<br></code></pre><br><br><p>and the exception I get:</p><br><br><pre><code>botocore.exceptions.ClientError: An error occurred (InvalidLocationConstraint) when calling the CreateBucket operation: The specified location-constraint is not valid<br></code></pre><br><br><p>I can create buckets in these regions from the aws website but it is not good for me; so I tried to do create it directly from the rest API without boto3.</p><br><br><p>url: <strong>bucket-name</strong>.s3.amazonaws.com</p><br><br><p>body:</p><br><br><pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;<br>&lt;CreateBucketConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;<br>   &lt;LocationConstraint&gt;eu-south-1&lt;/LocationConstraint&gt;<br>&lt;/CreateBucketConfiguration&gt;<br></code></pre><br><br><p>but the response was similar to the exception:</p><br><br><pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;<br>&lt;Error&gt;<br>    &lt;Code&gt;InvalidLocationConstraint&lt;/Code&gt;<br>    &lt;Message&gt;The specified location-constraint is not valid&lt;/Message&gt;<br>    &lt;LocationConstraint&gt;eu-south-1&lt;/LocationConstraint&gt;<br>    &lt;RequestId&gt;********&lt;/RequestId&gt;<br>    &lt;HostId&gt;**************&lt;/HostId&gt;<br>&lt;/Error&gt;<br></code></pre><br><br><p>Does anyone have an idea why I can do it manually from the site but not from python?</p><br>
0.3333333333333333,0.0,0.0,0.0,0.6666666666666666,0.0,0.0,<h3>What package format do I need for my AWS Server (debian or Linux)</h3><p>I want to download Elasticsearch on an AWS Server. The AWS Server has the following configuration:</p><br><pre><code>Plattform: Ubuntu (Inferred)<br>Plattformdetails: Linux/UNIX<br></code></pre><br><p>The following packages exist:</p><br><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html" rel="nofollow noreferrer">https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html</a></p><br><p>It says:</p><br><pre><code>The tar.gz archives are available for installation on any Linux distribution and MacOS.<br><br>The deb package is suitable for Debian; Ubuntu; and other Debian-based systems. Debian packages may be downloaded from the Elasticsearch website or from our Debian repository.<br></code></pre><br><p>So should I download the <code>tar.gz</code> or the <code>deb package</code>?</p><br>
0.0,1.0,0.3333333333333333,0.0,0.3333333333333333,0.0,0.0,<h3>How to properly create security groups for instance classes in AWS?</h3><p>So right now I have 4 subnets per availability zone: The internet facing &quot;entrypoint&quot; subnet (associated with a load balancer); the generic &quot;service&quot; subnet for internal computation; the &quot;database&quot; subnet for all things data related; and the &quot;external request&quot; subnet for making requests out to the internet. This defines essentially 4 classes of EC2 instances.</p><br><p>I am supposed to now create security groups for these 4 classes of EC2 instances. What I'm wondering is how to do that correctly (I am using terraform).</p><br><p>Can I create 1 security group for &quot;ingress&quot; (incoming) traffic; and a 2nd security group for &quot;egress&quot; (outgoing) traffic; for each class; for each connection type?</p><br><p>So basically; I want this. I want the internet entrypoint to talk to the service. The service can only respond to requests from the internet; it doesn't make any external internet requests itself. The service can talk to the database and the external requesting class. The database can only talk to the service; and the external request can only respond back to the service. The entrypoint can come in as HTTP or HTTPS (or websockets; is that just HTTPS?). It comes in on port 443. This is the load balancer. It then converts the request to HTTP and connects to the compute with port 3000. Should I have a separate port for each different connection type? (So the service layer would have 1 port for the database to respond to like 4000; 1 port for the external request layer to respond to like 5000; etc.). Or does that part matter? Lets say we have the ports thing though.</p><br><ul><br><li>sg1 (security group 1): ingress 443 -&gt; 3000 (load balancer -&gt; service)</li><br><li>sg2: egress 3000 -&gt; internet? is that 0.0.0.0/0? I don't want it to make free requests out; only to connected clients.</li><br><li>sg3: ingress 3000 -&gt; 4000 (service -&gt; database); specifying the database subnet</li><br><li>sg3: egress 4000 -&gt; 3000 (database -&gt; service); specifying the service subnet; etc.</li><br></ul><br><p>Am I on the right track? I am new to this and trying to figure it out. Any guidance would be much appreciated; I've been reading the AWS docs for the past week but there's little in terms of best practices and architecture.</p><br>
0.0,0.0,0.0,0.0,1.0,1.0,0.0,<h3>How to filter Lambda responses using attributes in SNS from destination?</h3><p>Now that AWS Lambda supports sending responses directly to SNS without using the AWS API using the <a href="https://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/" rel="nofollow noreferrer">'Destinations' feature</a>; is it possible to include message attributes in the response? In particular in order to do message filtering (e.g. via a subscription filter policy). Something like this: </p><br><br><p><code>{ "Message" : "Hello World"; "Accepted" : "true" }</code></p><br><br><p>We want SNS to look at the <code>Accepted</code> field and forward the message to the next step if <code>true</code>. It looks like this should be possible but I can't find any docs on exactly how to specific the message attributes on a response payload when using a lambda destination rather than the AWS API.</p><br><br><p>How do we format the lambda response payload so that an SNS attribute filter can operate on it; for a Lambda Destination -> SNS message?</p><br>
0.0,0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,1.0,<h3>Jenkins On-prem vs Cloud(AWS)</h3><p>we have Jenkins currently setup on-prem planning to migrate onto AWS; what are advantages and disadvantages running on AWS and on-prem?</p><br>
0.6666666666666666,0.0,0.0,0.0,0.0,0.3333333333333333,0.0,<h3>Quicksight: Datedifference in the same field</h3><p>I've a table below:</p><br><div class="s-table-container"><br><table class="s-table"><br><thead><br><tr><br><th style="text-align: left;">Task #</th><br><th style="text-align: left;">Status</th><br><th style="text-align: left;">Date</th><br></tr><br></thead><br><tbody><br><tr><br><td style="text-align: left;">1</td><br><td style="text-align: left;">On going</td><br><td style="text-align: left;">08/12/2021</td><br></tr><br><tr><br><td style="text-align: left;">1</td><br><td style="text-align: left;">Done</td><br><td style="text-align: left;">08/13/2021</td><br></tr><br><tr><br><td style="text-align: left;">2</td><br><td style="text-align: left;">On going</td><br><td style="text-align: left;">08/12/2021</td><br></tr><br><tr><br><td style="text-align: left;">3</td><br><td style="text-align: left;">On going</td><br><td style="text-align: left;">08/13/2021</td><br></tr><br><tr><br><td style="text-align: left;">2</td><br><td style="text-align: left;">Done</td><br><td style="text-align: left;">08/15/2021</td><br></tr><br><tr><br><td style="text-align: left;">3</td><br><td style="text-align: left;">Done</td><br><td style="text-align: left;">08/14/2021</td><br></tr><br></tbody><br></table><br></div><br><p>what I would need to do is to find the date difference in Date column between On going and Done status depending on their task #.</p><br><p>I've tried doing a datedifference and ifelse (for the condition) but to no avail. Thanks in advance.</p><br>
0.0,0.0,0.3333333333333333,0.0,0.3333333333333333,0.6666666666666666,0.0,<h3>CodeDeploy Deployment failed to stop Application</h3><p><strong>The overall deployment failed because too many individual instances failed deployment; too few healthy instances are available for deployment; or some instances in your deployment group are experiencing problems.</strong></p><br><p><em>Above Error is on AWS Code Deploy By using Ec2 instance; Deployment is going to fail.</em></p><br><p>I have tried with Code Deploy Agent method; then also i am getting same Error; Is there any other option to resolve this issue; please let me know anyone knonw's the answer.</p><br><p>In Events Option; it was showing like this</p><br><pre><code>Event             Duration    Status   Errorcode    Start time                  End time<br>ApplicationStop   0 seconds   Failed                Aug 12; 2021 11:07 AM    Aug 12; 2021 11:07 AM <br>                                                         (UTC+5:30)                  (UTC+5:30)<br>DownloadBundle                Skipped<br>BeforeInstall                 Skipped<br>Install                       Skipped<br>AfterInstall                  Skipped<br>ApplicationStart              Skipped<br>ValidateService               Skipped.<br></code></pre><br>
0.0,0.3333333333333333,0.0,0.6666666666666666,1.0,0.0,0.0,<h3>Multiple aws ec2 instances results in poor communication with s3 buckets</h3><p>I have an overarching Bash script where there are 3 main processes that are executed within the script:</p><br><ol><br><li>Spin up an ec2 instance (lets say ec2-1) which will pull data from a private s3 bucket (in the same region: us-east-1) and run some programs.</li><br><li>Spin up an ec2 instance (lets say ec2-2) which will pull data from a public amazon s3 bucket (in the same region: us-east-1) and run some programs.</li><br><li>Spin up an ec2 instance (lets say ec2-3) which will pull data from a private s3 bucket (separate from 1); but still in region: us-east-1) and run some programs.</li><br></ol><br><p>To ensure that each; individual process worked; I ran them all separately. For example; in my bash script; I would run only process 1) and ensure it completes from start-to-finish. After that completes; I would test 2); wait for this to run through completely; and then test 3) to ensure that runs through completely. Everything works fine; and have it all working well. Download speeds are in excess of 25-30 MB/s; which is perfect since a lot of data is being moved to/from s3 buckets.</p><br><p>Now I am at the stage where I attempt to run 1; 2; and 3 together all within the same Bash script. Note: all three ec2 instances SHOULD be independent from one another as they all have their own unique instance-id but are all in the same region (us-east-1). However; when I run all 3 at once; there is something that causes download speeds to/from the s3 buckets to become VERY slow - from ~ 25MB/s to 1 kB/s; and sometimes even completely stopping. It is interesting because 1) and 3) are pulling data from a private bucket; whereas 2) is pulling data from Amazon's public s3 bucket; yet ALL THREE instances have slow/stopped download speeds. I have even increased all of the three ec2 instances to m5dn.24xlarge; and the download speeds are still abysmal.</p><br><p>I also tried to run two separate instances of 1); 2); or 3); and they perform slower as well. For example; if I run 1) for two separate dates (with two separate instance-id's); the speed is lower compared to if I just run one instance of 1).</p><br><p>My question is: how/why would this be happening? Any feedback / info would be very helpful.</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.3333333333333333,<h3>.gitlab-ci.yaml throws &quot;Cleaning up file based variables 00:01 ERROR: Job failed: exit code 1&quot; at the end after successfully run the job</h3><p>When I am trying to commit changes to gitlab for continuous integrations i am facing this error even though all my steps pass successfully;  Gitlab CI shows this</p><br><ul><br><li><em>Cleaning up file based variables 00:01 ERROR: Job failed: exit code 1</em></li><br></ul><br><p>I am running 1 stages &quot;deploy&quot; at the moment here is my script for deploy:</p><br><pre><code>image: python:3.8<br><br>stages:<br>  - deploy<br><br>default:<br> before_script:<br>    - wget https://golang.org/dl/go1.16.5.linux-amd64.tar.gz<br>    - rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.16.5.linux-amd64.tar.gz<br>    - export PATH=$PATH:/usr/local/go/bin<br>    - source ~/.bashrc<br>    - pip3 install awscli --upgrade<br>    - pip3 install aws-sam-cli --upgrade<br><br>deploy-development:<br> only:<br>  - feature/backend/ci/cd<br> stage: deploy<br> script:<br>    - sam build -p<br>    - yes | sam deploy<br></code></pre><br>
0.0,0.0,0.0,1.0,0.0,0.3333333333333333,0.0,<h3>Looping through a file to delete files from s3 - boto3?</h3><p>This is probably very basic but I'm new to this so please bear with me!</p><br><p>I've been given a list of objects stored in s3 that need to be permanently deleted. This list is just a .txt file that I have no control over; but it does conveniently contain all of the specific keys that need to be removed e.g:</p><br><pre><code>folder1/file1.xyz <br>folder1/file2.xyz<br>folder2/file1.xyz<br></code></pre><br><p>This is what I am trying which does not seem to actually delete the objects:</p><br><pre><code>import boto3<br>s3 = boto3.client('s3')<br>s3bucket = &quot;my bucket&quot;<br>trn = &quot;list.txt&quot;<br><br>with open(trn) as trn_list:<br>    for line in trn_list:<br>         s3.delete_object(Bucket=s3bucket; Key=line)<br></code></pre><br><p>If I specify a key to delete instead of looping through the file; it works and the object is removed; for example:</p><br><pre><code>s3.delete_object(Bucket=s3bucket; Key=folder1/file1.xyz)<br></code></pre><br><p>but when I try to include the delete into the loop it never seems to delete anything. If I run the loop to just <em>print(line)</em> then it correctly prints out each key in the .txt file.</p><br><p>Is there a different way I need to be doing this? Any advice is appreciated.</p><br>
0.0,0.0,0.0,0.0,0.0,0.0,1.0,<h3>AWS Free account renewal or closing after an year of free subscription</h3><p>Just have a question around; AWS Free tier accoount; Will they be charging after one year?<br>Should I be closing it after an year?<br>The account that I am using is for my perssonal practice.</p><br>
0.0,0.0,0.3333333333333333,0.6666666666666666,1.0,0.0,0.0,<h3>EC2 User Data copying outdated version of a file in S3?</h3><p>So; I'm trying to setup a Jenkins server in a EC2 instance and use CasC with it. I created an S3 bucket to store the CasC configuration and setup a user data script to copy that configuration into the specific path.<br>Everything goes well in the first run of the user data script and the file gets copied to the correct path. However; when I upload a new version of the file and reboot the EC2 instance; it seems that the aws cli is copying the old version; no matter how many times I reboot the instance after that.<br>Here's my user data script:</p><br><pre><code>#! /bin/bash<br>sudo su -<br>yum update -y<br>wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo<br>rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key<br>yum install jenkins java-1.8.0-openjdk-devel -y<br>systemctl daemon-reload<br><br>systemctl stop jenkins<br><br>aws s3 cp s3://jenkins-casc/jenkins.yaml /var/lib/jenkins/<br><br>systemctl start jenkins<br></code></pre><br><p>And here's the cloud-init output for the copy command:</p><br><pre><code>download: s3://jenkins-casc/jenkins.yaml to var/lib/jenkins/jenkins.yaml<br></code></pre><br>
0.0,0.0,1.0,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>AWS Boto3 - Datapoint within client.get_metric_statistics displays on one file but not on the other</h3><p>I have this function that records a Cloudwatch metric called <code>client.get_metric_statistics</code>; when I execute it the Datapoint doesn't show but I extracted the metric function to another file by itself and when executed it displayed the datapoint with no issues.</p><br><p>The only thing different is that I had a InstanceId on the one that displayed fine and I had a AMIID as you see on my main script which has to be automated so I am not sure if AMIID is allowed to be used but I dont see why it shouldnt or what the issue is so i'm looking for some feedback.</p><br><pre class="lang-py prettyprint-override"><code>import sys<br>import boto3<br>import time<br>ec2 = boto3.resource('ec2')<br>s3_resource = boto3.resource('s3'; region_name='eu-west-1')<br>s3 = boto3.resource('s3')<br><br><br>instance = ec2.create_instances(<br>    ImageId='ami-02ifd1b532b22l6h3';<br>    MinCount=1;<br>    MaxCount=1;<br>    InstanceType='t2.nano';<br>    KeyName = 'key1.pem';<br>    SecurityGroupIds=[sg.group_id];<br>    UserData = user_data;<br>)<br></code></pre><br><pre class="lang-py prettyprint-override"><code>from datetime import datetime; timedelta<br><br>time.sleep(300)<br>client = boto3.client(&quot;cloudwatch&quot;)<br>response = client.get_metric_statistics(<br>    Namespace=&quot;AWS/EC2&quot;;<br>    MetricName=&quot;CPUUtilization&quot;;<br>    Dimensions=[{&quot;Name&quot;: &quot;AMIID&quot;; &quot;Value&quot;: &quot;ami-13add1h575a25e4d6&quot;}];<br>    StartTime=datetime.utcnow() - timedelta(seconds=200);<br>    EndTime=datetime.utcnow();<br>    Period=300;<br>    Statistics=[&quot;Average&quot;];<br>    Unit=&quot;Percent&quot;;<br>)<br>print(response)<br><br>for cpu in response[&quot;Datapoints&quot;]:<br>    print(cpu)<br></code></pre><br><pre><code>s3.Bucket(name='buket2')<br>ec2.SecurityGroup(id='sg-06b84927ae5rd3ad1')<br>{'Label': 'CPUUtilization'; 'Datapoints': []; 'ResponseMetadata': {'RequestId': ''; 'HTTPStatusCode': 200; 'HTTPHeaders': {'x-amzn-requestid': ''; 'content-type': 'text/xml'; 'content-length': '357'; 'date': 'Sun; 18 Jul 2021 00:26:57 GMT'}; 'RetryAttempts': 0}}<br>sg-06b84927ae5rd3ad1<br></code></pre><br>
0.0,1.0,0.0,0.0,0.0,0.0,0.0,<h3>Sync Multple DNS Zones in Route 53?</h3><p>We're currently moving some dns zones from our on prem bind servers to route 53.</p><br><p>In the on prem bind servers we have multiple zones that all point to the same zone file. So if I have two zones; zone1.com and zone2.com; I can add 1 record to that single zone file (record1 A 45.0.0.1) and both record1.zone1.com and record1.zone2.com will resolve to 45.0.0.1.</p><br><p>So if we move these zones to route 53; we'll have 2 different zones; and would need to enter records into the both zones. Is there a way to be able to add a record into zone1.com and have it replicated to zone2.com I wanted to see if anyone knew of a way to do this natively in aws. I asked AWS and they said no; but I'm not sure they are understanding what I needed. My other thought would be to create a cloudwatch event that would trigger a lambda anytime zone1.com was to updated to then update zone2.com to keep them in sync.</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>TorchServe example model container always timeout</h3><p>So I just follow the tutorial right here<br><a href="https://aws.amazon.com/blogs/machine-learning/deploying-pytorch-models-for-inference-at-scale-using-torchserve/" rel="nofollow noreferrer">https://aws.amazon.com/blogs/machine-learning/deploying-pytorch-models-for-inference-at-scale-using-torchserve/</a></p><br><p>everything went well until the very last step: testing out endpoint invocation. I always got this error</p><br><p>&quot;ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message &quot;Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch; resolve the issue; and try again.&quot;</p><br><p>Checking out CloudWatch; my biggest suspicion is just this one line<br>ModuleNotFoundError: No module named 'image_classifier'</p><br><p>but I am not yet sure how to really make sure if it's really the cause of the timeout</p><br><p>from <a href="https://forums.aws.amazon.com/thread.jspa?messageID=907728" rel="nofollow noreferrer">https://forums.aws.amazon.com/thread.jspa?messageID=907728</a>; the answer says: &quot;if it's a custom container you wrote you'll need to debug it; if it's a built-in container you should reach out to AWS support for assistance.&quot;</p><br><p>I don't make any custom container; just totally using the default TorchServe docker container shown on the tutorial. What happened really here to that default torchserve tutorial docker container? am I unknowingly missing a configuration step or something?</p><br>
0.0,0.0,0.6666666666666666,0.0,1.0,0.0,0.0,<h3>How should I pass secret environment variables to Docker application on Elastic Beanstalk?</h3><p>I'm currently running a Node server deployed on a Docker application on AWS Elastic Beanstalk; and I have several env variables that should be kept hidden; like the database URL and the JWT secret. Passing them thru the Elastic Beanstalk application config would be optimal; but it doesn't work because I'm trying to access them within a Docker container; which doesn't receive those env variables.</p><br><p>I've seen a lot of answers to this but it seems to me that they all involve putting the actual variable values in places like the <code>Dockerrun.aws.json</code> or the <code>Dockerfile</code>; which would both add the secret values to the repo; exposing them to the public GitHub repo that I deploy from thru CodePipeline. So; how should I pass these secret environment variables to the Docker container? Is there a way to reference the variables in my <code>Dockerfile</code> or <code>docker-compose.yml</code> files and pass them down? Is there some other Elastic Beanstalk config I can use? Any suggestions would be greatly appreciated.</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>AWS QuickSight: Counting values in a column</h3><p>I have the following entries in a dataset:</p><br><pre><code>a<br>b<br>c<br>d<br>a<br>b<br>c<br>a<br>b<br>a<br></code></pre><br><p>How can I count the number of occurrences for each value?</p><br><p>In Excel; if this list were on column A; I would use the formula</p><br><pre><code>=COUNTIF(A:A;A1)<br>=COUNTIF(A:A;A2)<br>=COUNTIF(A:A;A3)<br>...<br></code></pre><br><p>and I would get the following:</p><br><pre><code>a: 4<br>b: 3<br>c: 2<br>d: 1<br>a: 4<br>b: 3<br>c: 2<br>a: 4<br>b: 3<br>a: 4<br></code></pre><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.3333333333333333,0.0,<h3>How to schedule a lambda function for multiple time in a month</h3><p>I want to schedule my lambda function to run every 3rd; 9th; and 10th of the month. I am thinking of using multiple schedules in the events section but I didn't get any example anywhere.<br>Thanks</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>The autoVerifyEmail on PreSignUp trigger does not verify the new user&#39;s email</h3><p>I trigger the PreSignUp lambda using admin_create_user:</p><br><pre><code>        self.cognito.admin_create_user(<br>            UserPoolId=self.user_pool_id;<br>            Username=user.username;<br>            UserAttributes=[<br>                {<br>                    'Name': 'name';<br>                    'Value': user.username<br>                };<br>                {<br>                    'Name': 'email';<br>                    'Value': user.username<br>                };<br>            ];<br>            MessageAction='SUPPRESS')<br></code></pre><br><p>Which triggers the lambda with this handler:</p><br><pre><code>def pre_signup(event; context):<br><br>    event['response']['autoConfirmUser'] = True<br>    event['response']['autoVerifyEmail'] = True<br>    <br>    print(f&quot;EVENT: {event}&quot;)<br>    return event<br></code></pre><br><p>And then on Cloudwatch the event is (Sentitive data has been obscured):</p><br><pre><code>EVENT: {'version': '1'; 'region': 'us-east-X'; 'userPoolId': 'us-east-X_XXXXXXXX'; 'userName': 'XXXXXXXX-bd2d-4fec-b421-d2c4b89f3274'; 'callerContext': {'awsSdkVersion': 'aws-sdk-unknown-unknown'; 'clientId': 'CLIENT_ID_NOT_APPLICABLE'}; 'triggerSource': 'PreSignUp_AdminCreateUser'; 'request': {'userAttributes': {'name': 'XXXXXXXXXX@XXXXXXXX.com'; 'email': 'XXXXXXXXXX@XXXXXXXX.com'}; 'validationData': None}; 'response': {'autoConfirmUser': True; 'autoVerifyEmail': True; 'autoVerifyPhone': False}}<br></code></pre><br><p>As you can see; the <code>autoVerifyEmail</code> option is set to True; <a href="https://docs.amazonaws.cn/en_us/cognito/latest/developerguide/user-pool-lambda-pre-sign-up.html" rel="nofollow noreferrer">just as the docs state</a>. Problem is; this thing is broken; it just doesn't work.</p><br><p>Even calling <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/cognito-idp.html#CognitoIdentityProvider.Client.admin_update_user_attributes" rel="nofollow noreferrer">admin_update_user_attributes</a> manually doesn't work.</p><br><pre><code>    self.cognito.admin_update_user_attributes(<br>            UserPoolId=self.user_pool_id;<br>            Username=user.username;<br>            UserAttributes=[<br>                {<br>                    'Name': 'email_verified';<br>                    'Value': True<br>                };<br>            ]<br>        )<br></code></pre><br><p><a href="https://forums.aws.amazon.com/thread.jspa?threadID=285941" rel="nofollow noreferrer">Even in the AWS forums there are no answers</a>. So; is this thing just completely broken or am I doing something wrong? I've even tried setting to a string 'true' just because of the quotes on the docs ('boolean'). I don't know what to do anymore; the user does not get created with a verified email attribute.</p><br>
0.0,1.0,0.0,0.3333333333333333,0.3333333333333333,0.3333333333333333,0.0,<h3>How can I avoid my Express API being blocked by corporate firewalls</h3><p>I have a production built React App being served from S3; via CloudFront; with DNS being done on Route 53 - and the homepage is rendered well from within my corporate firewall.</p><br><p>My issue is that it is unable to fetch data from the Express API (which connects to a MySQL DB) to populate the drop downs.</p><br><p>The express API is hosted on an AWS server on LightSail; (its glorified EC2; as it is accessed through http via IP:port); but is also accessed via Route 53 which points to S3; and eventually to the IP port combo.</p><br><p>I know that my corporate network also blocks :80; but I havent lost hope in somehow getting it to be accessible because the App is able to be served from S3 with no issues; so perhaps there is a workaround here.</p><br><p>It works perfectly from outside my corporate firewall; but I want/need it to be reachable from other companies; and I fear the IP is the turnoff for the firewalls here. Does anyone have any ideas how to make the API more corporate friendly; or a different way to architect the API so that this data can be retrieved in much the same way that the main code can be retrieved from S3 currently?</p><br><p>Thanks;</p><br><p>Adam</p><br>
0.0,0.0,0.3333333333333333,0.3333333333333333,0.0,0.6666666666666666,0.0,<h3>AWS SAM Package don&#39;t send to S3 Bucket</h3><p>I'm trying to send a simple hello world using net core 3.1 lambda with AWS SAM; so this is my template.yml:</p><br><pre><code>AWSTemplateFormatVersion: '2010-09-09'<br>Transform: AWS::Serverless-2016-10-31<br>Description: Net Core 3.1 Lambda<br>Resources:<br>  netcore31lambdafunc:<br>    Type: AWS::Serveless::Function<br>    Properties:<br>      FunctionName: Netcore31Test<br>      CodeUri: src/LambdaEmptyServerless/bin/Release/netcoreapp3.1/publish<br>      Handler: LambdaEmptyServerless::LambdaEmptyServerless.Functions::Get<br>      Runtime: dotnetcore3.1<br>      Events:<br>        netcore31ApiEvent:<br>          Type: Api<br>          Properties:<br>            Path: /<br>            Method: GET<br></code></pre><br><p>That is located in this level:</p><br><p><a href="https://i.stack.imgur.com/OzJo2.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/OzJo2.png" alt="enter image description here" /></a></p><br><p>Now I'm trying to package this to S3 bucket; so I'm executing the following:</p><br><pre><code>sam package --template-file template.yml --output-template-file out-template.yml  --s3-bucket demo-sam-001<br></code></pre><br><p>I got the following return:</p><br><pre><code>Successfully packaged artifacts and wrote output template to file out-template.yml.<br>Execute the following command to deploy the packaged template<br>sam deploy --template-file C:\Users\rlanh\Workspace\Pessoal\awssam-netcore31\LambdaEmptyServerless\out-template.yml --stack-name &lt;YOUR STACK NAME&gt;<br></code></pre><br><p>But nothing is uploaded to my S3 bucket.</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>EFS return empty file sometimes</h3><p>I has a EFS mount folder and a script to write a html file to there every 10 seconds. The html file size is around 1kb - 10kb. This folder is also service to web via Apache.<br>During high load; sometimes the request to html file return empty content or failed. I have no problem when I write that file to local storage instead of EFS. I guess the EFS network latency cause the problem.</p><br><p>I tried to mount EFS with recommend options form AWS but it not solve my problem.</p><br><p>Is there any way to solve this?</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>A PolicyStatement used in an identity-based policy cannot specify any IAM principals error</h3><pre><code>    let servicePrincipal: any = new iam.ServicePrincipal(&quot;lambda.amazonaws.com&quot;);<br><br>    let policyDoc = new iam.PolicyDocument({<br>      statements: [<br>        new iam.PolicyStatement({<br>          actions: [&quot;sts:AssumeRole&quot;];<br>          principals: [servicePrincipal];<br>          effect: iam.Effect.ALLOW;<br>          resources: [&quot;arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole&quot;];<br>          sid: &quot;&quot;<br>        })<br>      ];<br>    });<br><br>    let accessRole: any = new iam.Role(this; 'git-access-role'; {<br>      assumedBy: servicePrincipal;<br>      inlinePolicies: { policyDoc }<br>    });<br></code></pre><br><p>I'm creating a cdk lambda with a role that has AWSLambdaBasicExecutionRole but I get an error saying</p><br><pre><code>A PolicyStatement used in an identity-based policy cannot specify any IAM principals<br></code></pre><br><p>not quite sure...what does it mean and what should I do?</p><br>
0.0,0.0,0.6666666666666666,0.6666666666666666,0.6666666666666666,0.0,0.0,<h3>&quot;An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied&quot; when using batch jobs</h3><ul><br><li>I have a compute environment with 'ecsInstanceRole'. It contains the policies 'AmazonS3FullAccess' and 'AmazonEC2ContainerServiceforEC2Role'</li><br><li>Since I am using the AmazonS3FullAccess policy; I assume the batch job has permission to list; copy; put etc.<br>-The image I am using is a custom docker image that has a startup script which uses &quot;aws s3 ls &lt;S3_bucket_URL&gt;&quot;</li><br><li>When I start this image on an EC2 instance; it runs fine and lists the contents of the bucket</li><br><li>when I do the same as a batch job; I get the access denied error seen above.</li><br></ul><br><p>I dont understand how this is happening.</p><br><p>Things I have tried so far:</p><br><ul><br><li>having the bucket policy as</li><br></ul><br><p>.</p><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Id&quot;: &quot;Policy1546414123454&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Sid&quot;: &quot;Stmt1546414471931&quot;;<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Principal&quot;: {<br>                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;Account Id&gt;:root&quot;<br>            };<br>            &quot;Action&quot;: [<br>                &quot;s3:ListBucket&quot;;<br>                &quot;s3:ListBucketVersions&quot;<br>            ];<br>            &quot;Resource&quot;: [<br>                &quot;arn:aws:s3:::&quot;bucketname&quot;;<br>                &quot;arn:aws:s3:::bucketname/*&quot;<br>            ]<br>        }<br>    ]<br>}<br></code></pre><br><ul><br><li>Granted public access to the bucket</li><br></ul><br>
0.0,0.0,1.0,0.0,0.3333333333333333,0.0,0.0,<h3>AWSElasticBeanstalkFullAccess (Provides full access...) equivalent?</h3><p>I'm following a course on AWS Beanstalk that might be out of date. In IAM Management Console a user is added and one of the existing policies attached directly is <code>AWSElasticBeanstalkFullAccess</code>; which I cannot find while filtering policies myself.</p><br><p>Has this changed name? How can I find it or an equivalent?</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>Problem using ODBC for PostgeSql in Goldengate Console with dblogin</h3><p>Hello my name is manuel I have a problem using the ODBC for PostgreSql in Goldengate; I have connection using isql -v DSN but in the goldengate console I can not access using the dblogin; I have seen a similar question for which they say that the DriverUnicodeType=1 is placed to solve the problem and if it solves the previous problem but now I get another error<br>which is the following<br>ERROR OGG-03039 Database character set U is not supported.<br /><br>It is worth mentioning that my architecture is in AWS having GG in an EC2 instance and PostgreSql in RDS in the same VPC.</p><br><p>here is my ODBC.ini</p><br><pre><code>[test]<br>Description=PostgreSQL<br>Driver=/usr/lib/x86_64-linux-gnu/odbc/psqlodbcw.so<br>Trace=No<br>TraceFile=/tmp/psqlodbc.log<br>Database=postgres<br>Servername=xxxx.xxxxxxxxx.us-east-2.rds.amazonaws.com<br>UserName=postgres<br>Password=password<br>Port=5432<br>ReadOnly=Yes<br>RowVersioning=No<br>ShowSystemTables=No<br>ShowOidColumn=No<br>FakeOidIndex=No<br>ConnSettings=No<br>DriverUnicodeType=1<br>TransactionErrorBehavior=2<br></code></pre><br><p>I hope you can help me. Thanks</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.6666666666666666,<h3>How to show all images from a AWS S3 bucket?</h3><p>I'm using <a href="https://www.npmjs.com/package/aws-s3" rel="nofollow noreferrer">https://www.npmjs.com/package/aws-s3</a> and <a href="https://www.npmjs.com/package/filepond" rel="nofollow noreferrer">https://www.npmjs.com/package/filepond</a> to upload images to my AWS S3 bucket. I've got it running; but I'm wondering if there's an easy way to show all images in the AWS S3 bucket. I don't want to save each link to a image in a database and then run through that. Any suggestions?</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>Sqs metric numberofmessagesreceived growing</h3><p>Number Of Messages Received is larger in SQS as compared to Number of messages sent.<br>Almost from a month my count for both the above metric that is number of messages received and sent are equal but suddenly the number of messages received count started increasing as compared to number of messages sent.</p><br><p>As per my understanding number of messages sent is the count of messages added to the SQS and number of messages received is the count of number of messages received by the consumer. So how the number of received messages can be larger than sent messages.<br>Also the approximate number of visible messages are growing.<br>Can anyone please help by explaining how this can happen.</p><br><p>Thanks in advance.</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>Kinesis Analytics Group By Across Shards</h3><p>I have a setup where data from the same tenant arrives at multiple shards in a kinesis stream . So some thing that looks like :</p><br><pre><code>tenant_id           product             shard_id<br>    XYZ               car                   1<br>    XYZ               car                   2<br>    XYZ               bike                  1<br>    XYZ               bike                  2<br>    XYZ               car                   1<br></code></pre><br><p>I want the result to look like :</p><br><pre><code>tenant_id           product               frequency<br>    XYZ                car                  3<br>    XYZ                bike                 2<br></code></pre><br><p>The SQL I currently have produces the following output which seems to do the aggregation on a per shard basis :</p><br><pre><code> tenant_id          product                frequency<br>    XYZ                car                   2<br>    XYZ                car                   1<br>    XYZ                bike                  1<br>    XYZ                bike                  1<br></code></pre><br><p>My current solution :</p><br><pre><code>CREATE OR REPLACE STREAM &quot;DESTINATION_SQL_STREAM&quot; (<br>    &quot;tenant_id&quot; varchar(128);<br>    &quot;product&quot; varchar(128);<br>    &quot;frequency&quot; DOUBLE<br>);<br><br>CREATE OR REPLACE PUMP &quot;STREAM_PUMP&quot; AS<br>INSERT INTO &quot;DESTINATION_SQL_STREAM&quot; <br>    SELECT STREAM<br>        &quot;tenant_id&quot;;<br>        &quot;product&quot;;<br>        count(&quot;product&quot;) as &quot;frequency&quot;<br>    FROM &quot;SOURCE_SQL_STREAM_001&quot;<br>    GROUP BY &quot;tenant_id&quot;; &quot;product&quot;;<br></code></pre><br>
0.0,0.6666666666666666,0.3333333333333333,0.0,0.0,0.6666666666666666,0.0,<h3>AWS single ApiGateway with multiple services</h3><p>I am creating a design for my eCommerce application. It will have multiple services backed by AWS Lambdas. Orderservice; InventoryService; PaymentService; LoggingService; dashboardService are some of those services. I cannot give the exact number of services but it will surely be more than 15. As per this link <a href="https://microservices.io/patterns/apigateway.html" rel="nofollow noreferrer">microservices </a>; a good microservices architecture should have one gateway that will route to the corresponding services. The code for my AWS ApiGateway with order Lambda function looks like below. <br/> My question is that each of Orderservice; inventoryservice; paymentservice etc can have multiple routes for get; post; delete; put. Most of them will have nested resources. In this situation should I include api routes and lambda functions for all these services within the same SAM template. If yes; wouldn't it be a monolith Template. If I need to change in any service; I have to deploy the whole template and breaks the microservice principles. Ideally; I want to deploy all these services independently and yet share the ApiGateway. Is this possible ?. If not; should I create separate ApiGateway for each service in different SAM template so they can be deployed separately. This will cause authentication; authorisation; monitoring be repeated in all gateways which again doesn't sound like way to go. <br/><br>Please suggest what is the right way to do this.</p><br><pre class="lang-yaml prettyprint-override"><code>AWSTemplateFormatVersion: &quot;2010-09-09&quot;<br>Transform: &quot;AWS::Serverless-2016-10-31&quot;<br>Description: ApiGateway for the Ecommerce <br><br>Resources:<br>  EcomApi:<br>    Type: AWS::Serverless::Api<br>    Properties:<br>      DefinitionBody:<br>        swagger: &quot;2.0&quot;<br>        info:<br>          title: Ecommerce Api<br>        schemes:<br>          - &quot;https&quot;<br>        x-amazon-apigateway-request-validator: Validate body and params<br>        paths:<br>          /order:<br>            get:<br>              summary: Get the orders<br>              consumes:<br>                - &quot;application/json&quot;<br>              produces:<br>                - &quot;application/json&quot;<br>              responses:<br>                &quot;200&quot;:<br>                  description: &quot;200 response&quot;<br>                  schema:<br>                    $ref: &quot;#/definitions/Empty&quot;<br>                  headers:<br>                    Access-Control-Allow-Origin:<br>                      type: &quot;string&quot;<br>              security:<br>                - Auth: []<br>              x-amazon-apigateway-integration:<br>                type: &quot;AWS_PROXY&quot;<br>                httpMethod: &quot;POST&quot;<br>                uri: !Sub arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${GetOrders.Arn}/invocations<br>                responses:<br>                  default:<br>                    statusCode: &quot;200&quot;<br>                    responseParameters:<br>                      method.response.header.Access-Control-Allow-Origin: &quot;'*'&quot;<br>                passthroughBehavior: &quot;when_no_match&quot;<br>                contentHandling: &quot;CONVERT_TO_TEXT&quot;<br><br><br>  GetOrders:<br>    Type: AWS::Serverless::Function<br>    Properties:<br>      CodeUri: .<br>      Handler: src/handlers/getOrders.handler<br>      Role:<br>        Fn::GetAtt:<br>          - TestRole<br>          - Arn<br>      Events:<br>        List:<br>          Type: Api<br>          Properties:<br>            Path: /orders<br>            Method: get<br>            RestApiId: !Ref EcomApi<br><br></code></pre><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>AWS DynamoDB querying with values in an array</h3><p>With the following (TableName: &quot;Table&quot;):</p><br><pre class="lang-json prettyprint-override"><code>[<br> {<br>  name: &quot;name1&quot;;<br>  values: [&quot;string1&quot;; &quot;string2&quot;]<br> };<br> {<br>  name: &quot;name2&quot;;<br>  values: [&quot;string1&quot;; &quot;string2&quot;; &quot;string3&quot;]<br> }<br>]<br></code></pre><br><p>My partition key would be <code>name</code>; without any sort key. I am trying to query all the items with the same <code>value</code> field. The following is what I have tried:</p><br><pre class="lang-js prettyprint-override"><code><br>docClient.query({<br>      TableName: &quot;Table&quot;;<br>      KeyConditionExpression: &quot;name = :name&quot;;<br>      FilterExpression: &quot;contains(values; :value)&quot;;<br>      ExpressionAttributeValues: {<br>        &quot;:name&quot;: &quot;certain_name&quot;;<br>        &quot;:value&quot;: &quot;string1&quot;;<br>      };<br>    });<br><br></code></pre><br><p>Suppose I want to query all the items with the <code>value</code> field of &quot;string1&quot;. However; AWS DynamoDB requires the <code>partition key</code> which is unique for all my items. Are there any ways to query all the items with the same <code>value</code> field; without bothering about the <code>partition key</code>?</p><br><p>Or would a better approach be to just get all the items from DynamoDB; and just query with my own methods?</p><br><p>Thank you everyone!</p><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.0,<h3>How can I maintain the current state of my EC2 instance after stopping it?</h3><p>At the time of working with AWS; each time when I want to stop an EC2 instance; I lose all data. For example; when I try to ssh from one instance to another one; each time I have to copy the public key from the source instance to the target one. In particular; after stopping an instance; this data is delted in my target instance. I would like to know; is there any way to keep this data after stopping my instances?</p><br>
0.0,1.0,0.0,0.0,0.0,0.0,0.0,<h3>[AWS Network Firewall]Why can&#39;t i communicate with different request and response routes?</h3><p>I have a question about network.</p><br><p>I am testing a AWS Network Firewall in AWS with private Subnet and NAT Gateway.</p><br><p>So; My Connection Flow with Internet is:</p><br><p>Request:<br><code>EC2 in private subnet -&gt; NAT gateway -&gt; FW -&gt; IGW</code></p><br><p>Response:<br><code>Server from Internet -&gt; FW -&gt; NAT -&gt; EC2 in private subnet</code></p><br><p>Request and Response flows are exactly reversed. but; I want to filter only Egress traffic; so I changed Response Route with changing Ingress Route table which applied to IGW. Because AWS Network Firewall is transparent; so I thought that it would be okay.</p><br><p>New Response route:<br><code>Server from Internet -&gt; NAT -&gt; EC2 in private subnet</code></p><br><p>What i expected is just only checking Egress traffic; but it didn't work. I mean; they couldn't connect with Internet.</p><br><p>So What i want to know is:</p><br><ol><br><li>Is there any problem to communicate with different route? isn't possible anyway?</li><br><li>if possible; what should i do?</li><br></ol><br><p>Thank you for reading this question.</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>AWS Cognito Credentials Set Up</h3><p>I am using the AWS SDK on a Xamarin Forms application. Currently I have a user pool; identity pool and a user with a username and password. I am able to sign in to this user within my application using the provided AWS configuration. Currently; sign in only works when using <code>AnonymousAWSCredentials</code>. When trying to use <code>CognitoAWSCredentials</code>; I get <code>Unauthenticated access is not supported for this identity pool</code>. There are several people who have asked this question on GitHub and SO; but all of the suggestions are to enable unauthenticated identity access.</p><br><p>What I am confused about is the difference between unauthenticated identities and authenticated identities. I don't really want to use a third party identity (Facebook; Google etc.) but I still want to make sure my user authenticated with AWS using their username and password. The tutorials on unauthenticated identities give the impression that users do not need to authenticate; is this actually true? My use case is requiring users to sign in to the application; then be able to use their credentials to call an API Gateway I have set up. Any advice/tips would be appreciated.</p><br>
0.0,0.0,0.6666666666666666,0.0,0.3333333333333333,0.0,0.6666666666666666,<h3>Auto unseal Vault in AWS ASG failed</h3><p>I have faced with strange behavior during testing auto unseal.<br>On a first try Ive launched 2 EC2 instances with Vault onboard with Raft storage and AWS KMS unseal type. After logging on the first server and running vault operator init; both servers were unsealed and Raft was launched - I was able to see two nodes; active and standby after running <code>vault operator raft list-peers</code>. It ok.</p><br><p>But I thought that it would be better if I placed Vault in ASG; so in case of failure of some node the new one will be launched. So I made launch configuration; pretty similar to configuration of 2 servers above and ASG based on it.</p><br><p>But after running <code>vault operator init</code> on the first node; I see that only the first node is unsealed; and the second node acts as a Raft primary (its only one listed in <code>vault operator raft list-peers</code>).</p><br><p>Could you; pls; help me to figure out why I can use auto unseal with 2 separate severs and cant use the same approach with ASG?</p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.0,0.0,<h3>Amazon S3 cp command can&#39;t bring the deleted file on a version enabled S3 bucket</h3><p>I have 2 Amazon S3 buckets A and B. Version enabled on both the bucket.</p><br><p>There is file X present on bucket A and B bucket is empty. Now I ran</p><br><pre><code>aws s3 sync s3://A &quot;s3://B&quot;<br></code></pre><br><p>The above command copied file X from bucket A to bucket B. Then I delete file X from bucket A.</p><br><p>I ran this command again but this time copying from B to A</p><br><pre><code>aws s3 sync s3://B &quot;s3://A&quot;<br></code></pre><br><p>In log I can file is getting copied from bucket B to Bucket A but when I checked bucket A; I don't see file X. Only when I click show version I can see the file marked as <code>Delete marker</code>.</p><br><p>How I can make file X available on bucket A. I tried <code>cp</code> command as well.</p><br>
0.0,0.0,0.0,0.0,0.0,0.6666666666666666,0.6666666666666666,<h3>Amazon SNS is not delivering to Saudi numbers?</h3><p>I can't send SMS to Saudi numbers.<br>My app is sandboxed so all the numbers are registered (as subscriptions).<br>According to the delivery reports produced by Amazon; all the messages have been delivered successfully (status: &quot;Message has been accepted by phone&quot;); but we received none of them.</p><br><p>SNS works fine when we send SMS to other countries; like Hungary or India.</p><br><p>Even the test message sent from SNS console doesn't get delivered to Saudi Arabia.</p><br><p>Any ideas? Please help!</p><br>
0.0,0.0,0.3333333333333333,0.0,0.6666666666666666,1.0,0.0,<h3>Sending Notification after completion of ECS Task (Success/Failure)</h3><p>I am new to AWS so please be gentle.</p><br><p>We have an ECS task which is executed when user submit a request thru UI. ECS task updates few tables on RDS and ends. This ECS task is triggered using a Lambda function which Runs the ECS task and shuts down.</p><br><p>We need to send a success notification if DB update was successful and a failure notification if DB update failed. I was thinking of using SNS service to send notification based on ECS task status. But thats not helping as ECS task status does not change based on whats happening inside the task. I can not use Lambda function which runs the task as lambda does not wait for task to complete.</p><br><p>Is there a better way of using SNS &amp; Cloudwatch rather than hardcoding the the notification process in the code?</p><br><p>Thanks In advance...</p><br>
0.0,0.0,1.0,1.0,0.0,0.3333333333333333,0.0,<h3>Invalid template property or properties [ElasticacheCluster]</h3><p>I'm getting the error mentioned in the title and I'm out of ideas on how to fix it.<br><strong>The Error is in the ElasticacheCluster part.</strong><br>I tried to modify it in a lot of ways; that's why there's some commented line of code but I did not remove maybe they might help in the troubleshooting.<br>This is my code below:</p><br><pre><code>    #### Creating Elasticache ####<br><br>  ElasticacheSecurityGroup:<br>    Type: 'AWS::EC2::SecurityGroup'<br>    Properties:<br>      GroupDescription: Elasticache Security Group<br>      VpcId: !Ref PubPrivateVPC<br>      SecurityGroupIngress:<br>        - IpProtocol: tcp<br>          FromPort: '11211'<br>          ToPort: '11211'<br>          CidrIp: 0.0.0.0/0<br>      Tags:<br>        -<br>          Key: &quot;Name&quot;<br>          Value: !Join [_; [!Ref 'AWS::StackName';ElasiCache-SG]]<br>#          SourceSecurityGroupName: !Ref InstanceSecurityGroup<br><br>  CacheSubnetGroup:<br>    Type: 'AWS::ElastiCache::SubnetGroup'<br>    Properties:<br>      Description: cache<br>      SubnetIds:<br>        - !Ref PrivateSubnet1<br>        - !Ref PrivateSubnet2<br>        - !Ref PrivateSubnet3<br><br>ElasticacheCluster:<br>  Type: AWS::ElastiCache::CacheCluster<br>  Properties:    <br>    Engine: memcached<br>    EngineVersion: 1.6.6<br>    CacheNodeType: cache.t2.micro<br>    CacheSubnetGroupName: !Ref CacheSubnetGroup<br>    NumCacheNodes: '1'<br>#    VpcId: !Ref PubPrivateVPC<br>    VpcSecurityGroupIds: !Ref ElasticacheSecurityGroup<br>#      - !GetAtt <br>#        - ElasticacheSecurityGroup<br>#        - GroupId<br></code></pre><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>AWS Ansible iam_policy: The security token included in the request is invalid</h3><p>The below list of tasks; I am trying to execute</p><br><pre><code>- hosts: localhost<br>  connection: local<br>  tasks:<br>    - name: List s3 buckets<br>      shell: &quot;aws s3 ls&quot;<br><br>    - name: Gather SNI router instance facts<br>      ec2_instance_info:     <br>        region: &quot;us-east-1&quot;<br>        filters:<br>          &quot;tag:name&quot;: &quot;selva&quot;<br>      register: instances<br><br>    - name: Create IAM role<br>      iam_role:<br>        name: vpc_dns<br>        description: Allow Lambda to access AWS services<br>        region: &quot;us-east-1&quot;<br>        assume_role_policy_document:<br>          Version: '2012-10-17'<br>          Statement:<br>          - Action: sts:AssumeRole<br>            Effect: Allow<br>            Principal:<br>              Service: lambda.amazonaws.com<br><br>    - name: Create IAM policy for DNS Lambda<br>      iam_policy:<br>        region: &quot;us-east-1&quot;<br>        iam_name: vpc_dns<br>        iam_type: role<br>        state: present<br>        policy_name: vpc_dns<br>        policy_json:<br>          Version: '2012-10-17'<br>          Statement:<br>          - Action: ec2:Describe*<br>            Effect: Allow<br>            Resource: &quot;*&quot;<br>      register: vpc_dns_role<br></code></pre><br><p>I have got the temporary credentials using sts get-session-token and stored the secrets in the default config using below</p><br><pre><code>aws configure set default.region us-east-1 ; aws configure set aws_access_key_id XXXX ; aws configure set aws_secret_access_key XXXX ; aws configure set aws_session_token XXXX ;<br></code></pre><br><p>From the above list of task first 3 got executed successfully; whereas the last one; iam_policy task failed and throws an error as below</p><br><pre><code>File &quot;/tmp/ansible_iam_policy_payload_ldb3fV/ansible_iam_policy_payload.zip/ansible/modules/cloud/amazon/iam_policy.py&quot;; line 176; in role_action<br>  File &quot;/home/ubuntu/.local/lib/python2.7/site-packages/boto/iam/connection.py&quot;; line 1314; in list_role_policies<br>    list_marker='PolicyNames')<br>  File &quot;/home/ubuntu/.local/lib/python2.7/site-packages/boto/iam/connection.py&quot;; line 102; in get_response<br>    raise self.ResponseError(response.status; response.reason; body)<br>fatal: [localhost]: FAILED! =&gt; {<br>    &quot;changed&quot;: false;<br>    &quot;invocation&quot;: {<br>        &quot;module_args&quot;: {<br>            &quot;aws_access_key&quot;: null;<br>            &quot;aws_secret_key&quot;: null;<br>            &quot;debug_botocore_endpoint_logs&quot;: false;<br>            &quot;ec2_url&quot;: null;<br>            &quot;iam_name&quot;: &quot;vpc_dns&quot;;<br>            &quot;iam_type&quot;: &quot;role&quot;;<br>            &quot;policy_document&quot;: null;<br>            &quot;policy_json&quot;: &quot;{\&quot;Version\&quot;: \&quot;2012-10-17\&quot;; \&quot;Statement\&quot;: [{\&quot;Action\&quot;: \&quot;ec2:Describe*\&quot;; \&quot;Resource\&quot;: \&quot;*\&quot;; \&quot;Effect\&quot;: \&quot;Allow\&quot;}]}&quot;;<br>            &quot;policy_name&quot;: &quot;vpc_dns&quot;;<br>            &quot;profile&quot;: null;<br>            &quot;region&quot;: &quot;us-east-1&quot;;<br>            &quot;security_token&quot;: null;<br>            &quot;skip_duplicates&quot;: true;<br>            &quot;state&quot;: &quot;present&quot;;<br>            &quot;validate_certs&quot;: true<br>        }<br>    };<br>    &quot;msg&quot;: &quot;The security token included in the request is invalid.&quot;<br>}<br></code></pre><br>
1.0,0.0,0.0,0.3333333333333333,0.0,0.3333333333333333,0.0,<h3>Write in Spark a fabricated autoincremented Key to Redshift</h3><p>I'm writing to a Redshift schema using Spark. In particular I'm writing to multiple tables (in a one-to-many relationship) that I'd like to then join (at analysis time) using a unique / auto incremental value.</p><br><p>I appreciate Spark provides the method <code>monotonically_increasing_id()</code> but because this is a scheduled ingestion; this method per se doesn't guarantee uniqueness on different runs.</p><br><p>How to guarantee an auto-incremental fabricated ID in this scenario? <strong>Ideally</strong> using already existing libraries as I don't want to reinvent something already existing</p><br>
0.0,0.0,0.0,0.6666666666666666,0.6666666666666666,0.6666666666666666,0.0,<h3>How to parse multiple items from DynamoDB in JSON format using PHP?</h3><p>I'm using PHP &amp; cURL to make an API call that returns all items for a given primary key(e.g. userId); which include string data for a job application such as &quot;Company&quot;:&quot;Apple&quot;; &quot;JobLocation&quot;:&quot;New York&quot;; etc...</p><br><p>The AWS Lambda function which the API calls simply uses the AWS SDK and returns the items in a format as below.</p><br><pre><code>[{&quot;Location&quot;: &quot;San Diego&quot;; &quot;Title&quot;: &quot;Developer&quot;; &quot;Company&quot;: &quot;NASA&quot;; &quot;Progress&quot;: &quot;draft&quot;}; {&quot;Location&quot;: &quot;Irvine&quot;; &quot;Title&quot;: &quot;Developer&quot;; &quot;Company&quot;: &quot;Google&quot;; &quot;Progress&quot;: &quot;submitted&quot;}; {&quot;Location&quot;: &quot;Hawthorne&quot;; &quot;Title&quot;: &quot;Developer&quot;; &quot;Company&quot;: &quot;SpaceX&quot;; &quot;Progress&quot;: &quot;interviewed&quot;}]<br></code></pre><br><p>If I use <code>json_decode($data; true)</code> all I get is an array of length 1 that contains the entire string.<br>I also tried doing <code>substr($data; 1; -1)</code> to get rid of the open and close brackets; then did <code>$newData = json_encode($data)</code> and once again <code>$userData = json_decode($newData; true)</code> but it returns an array of length 1 and I cannot reference any of the values based on key; for example <code>$company = $userData['Company']</code>. My goal is to display certain application data in a summary view such as Company; Title; Location; and Progress.</p><br><p>Does the Lambda function need to have a key for each application; in order for it to work properly with <code>json_decode($data)</code>? As in</p><br><pre><code>{&quot;application&quot;: [{&quot;Location&quot;: &quot;San Diego&quot;; &quot;Title&quot;: &quot;Developer&quot;; &quot;Company&quot;: &quot;NASA&quot;; &quot;Progress&quot;: &quot;draft&quot;}; {&quot;Location&quot;: &quot;Irvine&quot;; &quot;Title&quot;: &quot;Developer&quot;; &quot;Company&quot;: &quot;Google&quot;; &quot;Progress&quot;: &quot;submitted&quot;}; {&quot;Location&quot;: &quot;Hawthorne&quot;; &quot;Title&quot;: &quot;Developer&quot;; &quot;Company&quot;: &quot;SpaceX&quot;; &quot;Progress&quot;: &quot;interviewed&quot;}]}<br></code></pre><br><p>Looking for any advice on whether this is possible with PHP or if I need to explore other options. Thanks!</p><br>
1.0,0.0,0.0,0.0,0.6666666666666666,0.6666666666666666,0.0,<h3>Strategies reading large data from Microsoft CSV and writing to Microsoft Excel in AWS</h3><p>Kindly assist in deciding the strategy to do Extraction; Transformation and Loading (ETL) work flow with Amazon AWS offerings. I am a newbie in Amazon Cloud. My use case is to read thousands of record rows from Microsoft CSV file. My intention is to write out this file into Microsoft Excel document. I want to store this excel file in single piece within S3 bucket as an object.<br>Currently; I am doing this proof of concept with AWS Lambda. My issue is /tmp size is exceeding beyond 512 MB even if I select 6 GB RAM size in lambda specification. I do not do any disk operation. I read all of S3 csv file content once; in RAM and write out to excel; in RAM.</p><br>
0.0,0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,0.0,<h3>Getting error while creating CI pipeline for gitlab with aws</h3><p>I am creating a ci pipeline in gitlab with aws using below code to work for ci. I have to execute only 3 stages for it and I have posted for code for coverage only as I am getting error init</p><br><pre><code>coverage:<br>stage: test<br>tags:<br>    - aws-runner<br>only: <br>    - Feature/CI            <br>image: python:3.7-slim<br>coverage: '/TOTAL\s+\d+\s+\d+\s+(\d+%)/'<br>script:<br>    - pip install coverage moto mock pytest<br>    - MIN_COVERAGE=85<br>    - pip3 install --upgrade pip setuptools wheel<br>    - pip3 install -r requirements.txt<br>    - coverage run --omit 'utility/*' -m pytest tests/<br>    - coverage report -m<br>    - coverage html<br>    - COVERAGE=$(coverage report -m | grep -i &quot;TOTAL&quot; | awk '{print $4}' | sed 's/%//')<br>    - if [ &quot;$COVERAGE&quot; -lt &quot;$MIN_COVERAGE&quot; ]; then exit 1; else exit 0; fi<br>artifacts:<br>    expire_in: 7 days<br>    paths:<br>       - htmlcov/<br><br>    <br></code></pre><br><hr /><br><p>This is the error for above code</p><br><pre><code> $ coverage run --omit 'utility/*' -m pytest tests/<br> ============================= test session starts==============================<br> platform linux -- Python 3.7.11; pytest-6.2.4; py-1.10.0; pluggy-0.13.1 rootdir: /builds/test-backend <br> collected 0 items<br> ============================ no tests ran in 0.00s =============================<br> ERROR: file or directory not found: tests/<br> Coverage.py warning: No data was collected. (no-data-collected)<br> Cleaning up file based variables<br> 00:02<br> ERROR: Job failed: exit code 1<br></code></pre><br><p>As I am new to this; Any suggestion?<br>How can I solve this?</p><br>
0.0,0.3333333333333333,0.3333333333333333,0.6666666666666666,0.3333333333333333,0.6666666666666666,0.0,<h3>Django AWS S3 Buckets: CKEditor gives SignatureDoesNotMatch error</h3><p>I have put all of my static files in an S3 bucket on AWS. The images; javascript and CSS works fine on the site but the CKEditor rich text editor won't show up and is giving me <strong>the following errors:</strong></p><br><pre><code>SignatureDoesNotMatch. The request signature we calculated does not match the signature you provided. Check your key and signing method<br><br>TypeError: c[a] is undefined // I think that this error is only because of the first one<br><br>There is also a script that didn't load in the console for the CKEditor which I think  is also due to the signature error<br></code></pre><br><p><strong>Here are my settings</strong></p><br><pre><code>AWS_ACCESS_KEY_ID = id<br>AWS_SECRET_ACCESS_KEY = secret_id<br>AWS_STORAGE_BUCKET_NAME = bucket_name<br>AWS_S3_FILE_OVERWRITE = False<br>AWS_DEFAULT_ACL = None<br>DEFAULT_FILE_STORAGE = 'storages.backends.s3boto3.S3Boto3Storage'<br>STATICFILES_STORAGE = 'storages.backends.s3boto3.S3Boto3Storage'<br>AWS_S3_REGION_NAME = 'eu-west-2'<br>AWS_S3_ADDRESSING_STYLE = &quot;virtual&quot;<br>TEXT_CKEDITOR_BASE_PATH = 'https://bucket_name.s3.amazonaws.com/ckeditor/ckeditor/'<br></code></pre><br><p><strong>Note</strong><br>This error is happening also in the admin page so it's not the javascript that puts in the CKEditor that is causing the error so I haven't shared that javascript here as it is irrelevant</p><br>
0.0,0.0,0.6666666666666666,1.0,0.0,0.0,0.0,<h3>bucket_properties.acceleration_configuration.error_put.denied.header</h3><p>I'm new to AWS and S3. I have created a bucket and trying to enable transfer acceleration  there. While doing so - I'm getting below error:</p><br><pre><code>    bucket_properties.acceleration_configuration.error_put.denied.header<br>    After you or your AWS administrator have updated your permissions to allow the <br>    s3:PutAccelerateConfiguration action; choose Save changes. <br></code></pre><br><p>Attaching screenshot for more clarification:</p><br><p><a href="https://i.stack.imgur.com/QLlMI.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/QLlMI.png" alt="enter image description here" /></a></p><br><p>Any suggestion about how I can fix this error? Also ; I'm logged in with root user.</p><br><p>Also; I have checked - I have write access to this bucket:</p><br><p><a href="https://i.stack.imgur.com/YwFpO.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/YwFpO.png" alt="enter image description here" /></a></p><br>
0.0,0.0,1.0,0.0,0.6666666666666666,0.0,0.0,<h3>Packer + Terraform + EC2 Windows Server: can&#39;t make winrm remote-exec provisioner to work</h3><p>I'm trying to set up a Windows Server on EC2 using Packer to create the AMI and Terraform to deploy it; but I can't make the Terraform <code>remote-exec</code> provisioner to work with the <em>auto-generated Administrator password</em>; while it works with a fixed passord.</p><br><p><strong>Packer file</strong></p><br><pre><code>packer {<br>  required_plugins {<br>    amazon = {<br>      version = &quot;&gt;= 0.0.1&quot;<br>      source  = &quot;github.com/hashicorp/amazon&quot;<br>    }<br>  }<br>}<br><br>variable &quot;image_name&quot; {<br>  type = string<br>}<br><br>variable &quot;password&quot; {<br>  type = string<br>}<br><br>source &quot;amazon-ebs&quot; &quot;windows&quot; {<br>  ami_name       = var.image_name<br>  communicator   = &quot;winrm&quot;<br>  instance_type  = &quot;t2.micro&quot;<br>  winrm_insecure = true<br>  #winrm_password = var.password<br>  winrm_port     = 5986<br>  winrm_use_ssl  = true<br>  winrm_username = &quot;Administrator&quot;<br><br>  source_ami_filter {<br>    filters = {<br>      #name                = &quot;Windows_Server-2019*English-Full-Base*&quot;<br>      name                = &quot;Windows_Server-2016-English-Full-Base-*&quot;<br>      root-device-type    = &quot;ebs&quot;<br>      virtualization-type = &quot;hvm&quot;<br>    }<br>    most_recent = true<br>    owners      = [&quot;amazon&quot;]<br>  }<br><br>  user_data = &lt;&lt;EOUD<br>&lt;powershell&gt;<br># Set administrator password<br>#net user Administrator ${var.password}<br>#wmic useraccount where &quot;name='Administrator'&quot; set PasswordExpires=FALSE<br><br>Set-ExecutionPolicy Unrestricted -Scope LocalMachine -Force -ErrorAction Ignore<br><br># Don't set this before Set-ExecutionPolicy as it throws an error<br>$ErrorActionPreference = &quot;stop&quot;<br><br># Remove HTTP listener<br>Remove-Item -Path WSMan:\Localhost\listener\listener* -Recurse<br><br># Create a self-signed certificate to let ssl work<br>$Cert = New-SelfSignedCertificate -CertstoreLocation Cert:\LocalMachine\My -DnsName &quot;packer&quot;<br>New-Item -Path WSMan:\LocalHost\Listener -Transport HTTPS -Address * -CertificateThumbPrint $Cert.Thumbprint -Force<br><br># WinRM<br>cmd.exe /c winrm quickconfig -q<br>cmd.exe /c winrm set &quot;winrm/config&quot; '@{MaxTimeoutms=&quot;1800000&quot;}'<br>cmd.exe /c winrm set &quot;winrm/config/winrs&quot; '@{MaxMemoryPerShellMB=&quot;1024&quot;}'<br>cmd.exe /c winrm set &quot;winrm/config/service&quot; '@{AllowUnencrypted=&quot;true&quot;}'<br>cmd.exe /c winrm set &quot;winrm/config/client&quot; '@{AllowUnencrypted=&quot;true&quot;}'<br>cmd.exe /c winrm set &quot;winrm/config/service/auth&quot; '@{Basic=&quot;true&quot;}'<br>cmd.exe /c winrm set &quot;winrm/config/client/auth&quot; '@{Basic=&quot;true&quot;}'<br>cmd.exe /c winrm set &quot;winrm/config/service/auth&quot; '@{CredSSP=&quot;true&quot;}'<br>cmd.exe /c winrm set &quot;winrm/config/listener?Address=*+Transport=HTTPS&quot; &quot;@{Port=`&quot;5986`&quot;;Hostname=`&quot;packer`&quot;;CertificateThumbprint=`&quot;$($Cert.Thumbprint)`&quot;}&quot;<br>cmd.exe /c netsh advfirewall firewall set rule group=&quot;remote administration&quot; new enable=yes<br>cmd.exe /c netsh firewall add portopening TCP 5986 &quot;Port 5986&quot;<br>cmd.exe /c net stop winrm<br>cmd.exe /c sc config winrm start= auto<br>cmd.exe /c net start winrm<br>&lt;/powershell&gt;<br>EOUD<br>}<br><br>build {<br>  name    = &quot;build-win&quot;<br>  sources = [&quot;source.amazon-ebs.windows&quot;]<br><br>  provisioner &quot;file&quot; {<br>    destination = &quot;C:\\&quot;<br>    source      = &quot;./data&quot;<br>  }<br><br>  provisioner &quot;powershell&quot; {<br>    script  = &quot;./setup.ps1&quot;<br>    timeout = &quot;10m&quot;<br>  }<br><br>  provisioner &quot;powershell&quot; {<br>    inline = [<br>      &quot;C:\\ProgramData\\Amazon\\EC2-Windows\\Launch\\Scripts\\InitializeInstance.ps1 -Schedule&quot;;<br>      &quot;C:\\ProgramData\\Amazon\\EC2-Windows\\Launch\\Scripts\\SysprepInstance.ps1 -NoShutdown&quot;<br>    ]<br>  }<br><br>  post-processor &quot;manifest&quot; {<br>    output     = &quot;manifest.json&quot;<br>    strip_path = true<br>  }<br>}<br></code></pre><br><p><strong>Setup.ps1</strong></p><br><pre><code>try {<br>    # Set display file extension<br>    reg add HKCU\Software\Microsoft\Windows\CurrentVersion\Explorer\Advanced /v HideFileExt /t REG_DWORD /d 0 /f<br><br>    # Enable multiple Remote Desktop connections<br>    Set-ItemProperty -Path &quot;HKLM:\SYSTEM\CurrentControlSet\Control\Terminal Server&quot; -Name &quot;fdenyTSConnections&quot; -Value 0<br>    Set-ItemProperty -Path &quot;HKLM:\SYSTEM\CurrentControlSet\Control\Terminal Server&quot; -Name &quot;fSingleSessionPerUser&quot; -Value 0<br><br>    # Install chocolatey<br>    Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; Invoke-Expression ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))<br><br>    # Set non interactive chocolatey<br>    choco feature enable -n allowGlobalConfirmation<br>}<br>catch {<br>    Exit 1<br>}<br></code></pre><br><p>Packer works fine: <code>winrm</code> connects using the generated password.</p><br><p><strong>Terraform file</strong></p><br><pre><code>resource &quot;aws_instance&quot; &quot;it&quot; {<br>  ami                    = var.ami<br>  get_password_data      = true<br>  instance_type          = &quot;t3.micro&quot;<br>  key_name               = &quot;my_key&quot;<br>  subnet_id              = var.subnet_id<br>  vpc_security_group_ids = var.security_groups<br><br>  connection {<br>    agent    = false<br>    insecure = true<br>    host     = self.public_ip<br>    https    = true<br>    #password = var.password<br>    password = rsadecrypt(self.password_data; file(&quot;my_key/id_rsa&quot;))<br>    port     = 5986<br>    type     = &quot;winrm&quot;<br>    user     = &quot;Administrator&quot;<br>  }<br><br>  provisioner &quot;remote-exec&quot; {<br>    inline     = var.inline<br>    on_failure = continue<br>  }<br>}<br><br>resource &quot;local_file&quot; &quot;foo&quot; {<br>  content  = rsadecrypt(aws_instance.it.password_data; file(&quot;my_key/id_rsa&quot;))<br>  filename = &quot;${path.module}/foo.bar&quot;<br>}<br></code></pre><br><p>... but the <code>remote-exec</code> does not connect! :(</p><br><p>Please note that I had to add the <code>on_failure = continue</code> to create the instance regardless from the <code>remote-exec</code> error; and through the <code>resource &quot;local_file&quot; &quot;foo&quot;</code> I verify that the Administrator passowrd I get is the correct one! (I can connect to the instance through Remote Desktop using it).</p><br><p>If I uncomment the lines to use a fixed password... everything works perfectly! :(</p><br><p>To be more specific; these are the lines I uncomment to use a fixed passwoprd.</p><br><p><strong>Packer file</strong></p><br><pre><code>...<br><br>source &quot;amazon-ebs&quot; &quot;windows&quot; {<br>  ...<br>  #winrm_password = var.password<br>  ...<br><br>  user_data = &lt;&lt;EOUD<br>&lt;powershell&gt;<br>...<br>#net user Administrator ${var.password}<br>#wmic useraccount where &quot;name='Administrator'&quot; set PasswordExpires=FALSE<br><br>...<br>&lt;/powershell&gt;<br>EOUD<br>}<br><br>...<br></code></pre><br><p><strong>Terraform file</strong></p><br><pre><code>resource &quot;aws_instance&quot; &quot;it&quot; {<br>  ...<br><br>  connection {<br>    ...<br>    #password = var.password<br>    ...<br>  }<br><br>  ...<br>}<br></code></pre><br><p><strong>Can you see something wrong in my code to use the <em>auto-generated Administrator password</em>?</strong></p><br><p>Thank you!</p><br>
0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,1.0,0.0,<h3>How can i use Aws amplify and s3 storage to store items privately; but allow admin to access all files</h3><p>I am using AWS Amplify for my web application with Cognito for auth &amp; also roles and s3 to store content per user.</p><br><p>Going through the documentation and <a href="https://docs.amplify.aws/lib/storage/upload/q/platform/js/" rel="nofollow noreferrer">amplify storage</a> docs. it was easy to configure private storage per user and was able to retrieve the data for that specific user as well.</p><br><p>How can I achieve getting files for all users if I am an &quot;admin&quot; user ( based on the Cognito role)</p><br>
0.0,0.0,0.3333333333333333,0.6666666666666666,0.0,0.0,1.0,<h3>List GrandChildern and GrandParents of a Object in AWS Cloud Directory</h3><p>Give a directory structure like below; how can I list all the parents nodes and all of the child nodes?</p><br><pre><code>-/ - root<br> -ITStaff - Node<br>  - ITAdmin - Node<br>     -John - LeafNode<br></code></pre><br><p>Currently when I use listObjectParents for John it only return ITAdmin not include ITStaff</p><br><p>when I use listObjectChildren on ITStaff it only retun ITAdmin and not include John.</p><br><p>I am using <code>Java</code> SDK for get <code>AmazonCloudDirectory</code> client.</p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.0,0.0,<h3>AWS dynamodb; table is getting deleted automatically</h3><p>Created a table in AWS Dynamodb using AWS console in us-west-2 region. Table is getting automatically delete with no trace at all. To debug I enabled BAckup point in time recovery mode. I can see that there are backups of table which got deleted automatically by the system with $deletedTableBackup as a suffix.<br>Each time I create the table; I can pump data using access<br>_key and secrets.</p><br><p>Any help what's going on; what exactly is causing the issue. I am using a corporate account and I have the access to create/delete/modify table.</p><br>
0.0,0.0,0.0,0.0,1.0,0.6666666666666666,0.0,<h3>No ecs task definition (or empty definition file) found in environment</h3><h2>System</h2><br><p>CI/CD using <code>AWS CodePipeline</code>:</p><br><ul><br><li><code>AWS CodeBuild</code> connect Github 2</li><br><li><code>AWS Beanstalk</code> using images from <code>ECR</code>; Multi-container docker platform</li><br></ul><br><h2>Error</h2><br><pre><code>Deployment completed; but with errors: Failed to deploy application. No ecs task definition (or empty definition file) found in environment<br></code></pre><br><h2>Elastic Beanstalk Upload Application</h2><br><pre><code>app.zip<br>  |<br>  --Dockerrun.aws.json<br>  |<br>  --nginx<br>     |<br>     --default.conf<br></code></pre><br><h3><code>Dockerrun.aws.json</code></h3><br><p>Content <a href="https://pastebin.com/yXy71rS4" rel="nofollow noreferrer">here</a>.</p><br><p>I tried to deploy only Elastic Beanstalk using images after building and file <code>app.zip</code>; and surprisingly it worked.</p><br><p>Thanks in advance!</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.0,<h3>AWS completeNewPasswordChallenge switch from on_success to on_failure</h3><p>I have a post request inside of my on_success function but it sometimes fails and in that case I want to switch the function from on_success to on_failure.</p><br><p>Any help is appreciated.</p><br><p>And I can't put the post request outside of completeNewPasswordChallenge because it needs signInUserSession.</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>Is there a way to add timer between messages in AWS SQS?</h3><p>I am developing an email client that uses SMTP server from my customers. I stock the credentials in an AWS RDS database. Customers are able to create mail campaigns; and email have to be sent asynchronously after the campagin creation.</p><br><p>I want to configure a SQS queue to make messages available with a delay of 2 minutes between each message. The purpose is to send the message to a Lambda function able to send the message through SMTP (using the credentials in the database). Thanks to this delay between messages; I can optimize the delivery of the emails. The order of the delivery is not really important.</p><br><p>Unfortunately; I do not find a way to do that. The timers available in SQS are for the entire Queue; or have to be specific for each message. In my case; if i put 100 messages in the SQS queue; I need the first one to be sent immediately; the second 2 minutes after and so on.</p><br><p>Does someone has solutions with this issue ? If SQS is not the right service to manage this need; is there another one available on the AWS platform ?</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.0,<h3>How to search only in selective log streams in AWS?</h3><p>If my search pattern is <em>search_pattern</em> and the log group contains many log streams; but I am only interested in the ones with a certain prefix; is there a way to filter to log streams to search from instead of searching the entire log stream using <code>Search Log Stream</code> option?<br>Illustration:<br>Say my log group contains:<br>student-logs-A<br>student-logs-B<br>student-logs-C<br>teacher-logs-A<br>teacher-logs-B<br>teacher-logs-C<br>management-logs-A<br>management-logs-B<br>management-logs-C</p><br><p>I want to search for pattern <em>search_pattern</em> only in log streams starting from 'student-logs'? AWS cli or Console; any solution is okay.</p><br>
0.0,0.0,0.0,0.3333333333333333,0.3333333333333333,1.0,0.0,<h3>should I create Public api based on the current internal api</h3><p>My project is having public website and Content Management System(CMS).<br><br>I am using Lambda and API Gateway for the api.<br><br>The CMS currently has an api GET request to get ALL the data from the table below.<br></p><br><p>Banner Table<br></p><br><p>attribute:<br><br>-id: string(primary key/partition key)<br><br>-title: string<br><br>-isActive: boolean<br><br>...</p><br><pre><code>-----------------------------------------------<br>Id    isActive    title<br>1    true        title1<br>2    false        title2<br>3    true        title3<br>4    true        title4<br>----------------------------------------------<br></code></pre><br><p>This is My lambda function: <br><br>getBanner.js<br></p><br><pre><code>'use strict'<br>const AWS = require('aws-sdk');<br><br>exports.handler = async function (event; context; callback) {<br>    const documentClient = new AWS.DynamoDB.DocumentClient();<br><br>    let responseBody = &quot;&quot;;<br>    let statusCode = 0;<br><br>    const params = {<br>        TableName : &quot;Banner&quot;;<br>    };<br><br>    try{<br>        const data = await documentClient.scan(params).promise();<br>        responseBody = JSON.stringify(data.Items);<br>        statusCode = 200<br>    }catch(err){<br>        statusCode = 403<br>    }<br><br>    const response = {<br>        statusCode: statusCode;<br>        body: responseBody<br>    }<br><br>    return response<br>}<br><br></code></pre><br><p>I need an api to get all banners with isActive = true.<br></p><br><p>There are 2 approaches I can think of<br></p><br><p>1.Modify the existing lambda function<br></p><br><p>I can add something like below:<br></p><br><pre><code><br>if(event.queryStringParameters.isActive === true){<br>     // add filter or query to get all result<br>}<br>...<br></code></pre><br><p>But everyone is able to get all the data(including the results with isActive = false) if they do not use the queryStringParameters;<br> which is what I want to avoid since the data with isActive = false should not be seen by the public.<br></p><br><p>2.Create new lambda function<br></p><br><p>This is probably the best way to protect the data.<br><br>But I have a lot of API encountering the same situation(having inActive attribute); it means I need to create a lot of public api.<br><br>Which method should I use?<br></p><br>
0.0,1.0,0.0,0.6666666666666666,0.3333333333333333,1.0,0.0,<h3>Can API Gateway use values from DynamoDB as input to request mapping template?</h3><p><strong>Use Case</strong>: I need to perform an API request mapping that requires data from DynamoDB.</p><br><p><strong>Desired Solution</strong>: I'd like to do this using API Gateway features if possible; which would look something like this:</p><br><ol><br><li>An external REST API request is received by API Gateway</li><br><li>A <code>Proxy Resource</code> extracts a parameter; say <strong>accountId</strong>; from the HTTP path</li><br><li>A Service Integration (<code>GetItem</code>) reads a set of values from DynamoDB using the <strong>accountId</strong> key</li><br><li>The values read from the DB are input to a <code>Request Mapper</code> VTL template</li><br><li>The transformed API request is then sent to an <code>HTTP Integration</code> endpoint</li><br></ol><br><p><strong>Questions</strong>:</p><br><ul><br><li>Is that possible to do using API Gateway out-of-the-box or is that sequence too advanced?</li><br><li>If it's not possible; then is a lambda the best option to do most of this work (read DB; transform request; route HTTP)?</li><br></ul><br><p>Thanks for your help!</p><br>
0.0,0.0,0.0,1.0,0.3333333333333333,0.3333333333333333,0.0,<h3>How to delete item based on its existence in dynamodb using nodejs?</h3><p>I am trying to delete an item in dynamodb ( if it exist otherwise no); tried <a href="https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DeleteItem.html#API_DeleteItem_Examples" rel="nofollow noreferrer">following docs</a> but i am looking whether the object exist or not.</p><br><pre><code>let modify = function () {<br><br>    const params = {<br>        TableName: 'Notes';<br>        Key: {<br>          &quot;id&quot;: &quot;1&quot;;<br>        };<br>        UpdateExpression: &quot;set id = :y&quot;;<br>        ConditionExpression: &quot;attribute_exists(:y)&quot;;<br>        ExpressionAttributeValues:{<br>            &quot;:y&quot;:&quot;1&quot;  //static for now otherwise req.params.id for api<br>        };<br>        ReturnValues: 'ALL_OLD'<br>      };<br>    <br>    <br>    docClient.delete(params; function (err; data) {<br><br>        if (err) {<br>            console.log(&quot;users::update::error - &quot; + JSON.stringify(err; null; 2));<br>        } else {<br>            console.log(&quot;users::update::success &quot;+JSON.stringify(data.Attributes) );<br>        }<br>    });<br>}<br><br>modify();<br></code></pre><br><p>the item in table has two attributes name and id; id being primary key.<br>On running this code i get error <code>&quot;message&quot;: &quot;Invalid ConditionExpression: Operator or function requires a document path; operator or function: attribute_exists&quot;</code>; since this will be a part of API i cannot hold static values in json file.<br>any workaround for this?</p><br>
0.0,0.0,1.0,0.0,0.0,0.6666666666666666,0.3333333333333333,<h3>Can the mq server/client TLS keystore stash file reside elsewhere</h3><p>I have a scenario where the Security administrator is concerned with having the stash file residing on ec2/AWS disk with the keystore files and asks if there are any configuration options in v9.2 to retrieve KDB password dynamically from a password store and pass it to the Queue Manager for authenticating TLS connections. He understands the stash file is encrypted but concerned with having the stash fileon ec2/AWS.</p><br>
1.0,0.0,0.0,0.3333333333333333,0.3333333333333333,0.0,0.0,<h3>java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found</h3><p>I am running Spark on EC2(ubuntu) to write data to AWS S3 Bucket. But it didn't work. I downloaded the hadoop-aws-2.7.4.jar Jar to the Jars folder in Spark</p><br><p>Spark version : 3.1.2</p><br><p>I'm getting an error :</p><br><pre><code>java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found<br>    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)<br>    at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)<br>    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)<br>    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)<br>    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)<br>    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)<br>    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)<br>    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)<br>    at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:470)<br>    at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:572)<br>    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)<br>    at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)<br>    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)<br>    at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)<br>    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>    at java.base/java.lang.reflect.Method.invoke(Method.java:566)<br>    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)<br>    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)<br>    at py4j.Gateway.invoke(Gateway.java:282)<br>    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)<br>    at py4j.commands.CallCommand.execute(CallCommand.java:79)<br>    at py4j.GatewayConnection.run(GatewayConnection.java:238)<br>    at java.base/java.lang.Thread.run(Thread.java:829)<br>Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found<br>    at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)<br>    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)<br>    ... 24 more<br></code></pre><br><p>What can i do?</p><br>
0.0,0.0,0.0,0.0,0.6666666666666666,0.6666666666666666,0.0,<h3>Lambda S3 unzipping problem returns &quot;Error: null Forbidden: null&quot;</h3><p>I have this <code>index.js</code> which uses <a href="https://www.npmjs.com/package/unzipper" rel="nofollow noreferrer">this <strong>unzipper</strong> module</a></p><br><pre><code>console.log('Loading function');<br><br>const aws = require('aws-sdk');<br><br>const s3Client = new aws.S3({ apiVersion: '2006-03-01' });<br><br>const unzipper = require(&quot;unzipper&quot;);<br><br>exports.handler = async (event) =&gt; {<br>    const bucket = 'bucket_name';<br>    const filename = JSON.stringify(event.Records[0].s3.object.key.replace(/\+/g; ' '); null; 2);<br>    const filepath = filename.substring(0; filename.lastIndexOf(&quot;/&quot;) + 1);<br>    <br>    console.log(&quot;Filename: &quot; + filename);<br>    <br>    console.log(&quot;Filepath: &quot; + filepath);<br>    try {<br>        // the lines of code below is referenced from a tutorial inside the link below this code<br>        const directory = await unzipper.Open.s3(s3Client; { Bucket: bucket; Key: filename });<br>        return new Promise((resolve; reject) =&gt; {<br>            directory.files[0]<br>                .stream()<br>                .pipe(s3Client.createWriteStream(filepath))<br>                .on('error'; reject)<br>                .on('finish'; resolve)<br>        });<br>    } catch (error) {<br>        console.log(&quot;Error: &quot;; error.message; error.stack);<br>    }<br><br>};<br></code></pre><br><p>Which is referenced from <a href="https://www.npmjs.com/package/unzipper#opens3aws-sdk-params-options" rel="nofollow noreferrer">this</a> tutorial.</p><br><p>But I'm getting this error in my Cloudwatch:</p><br><pre><code>2021-04-15T04:58:12.844Z    undefined   INFO    Loading function<br>START RequestId: fa89f24c-d6e1-4452-ae1d-ef7246ad0edf Version: $LATEST<br>2021-04-15T04:58:13.332Z    fa89f24c-d6e1-4452-ae1d-ef7246ad0edf    INFO    Filename: &quot;New folder/New folder/events.zip&quot;<br>2021-04-15T04:58:13.341Z    fa89f24c-d6e1-4452-ae1d-ef7246ad0edf    INFO    Filepath: &quot;New folder/New folder/<br>2021-04-15T04:58:14.084Z    fa89f24c-d6e1-4452-ae1d-ef7246ad0edf    INFO    Error:  null Forbidden: null<br>    at Request.extractError (/opt/nodejs/node_modules/aws-sdk/lib/services/s3.js:698:35)<br>    at Request.callListeners (/opt/nodejs/node_modules/aws-sdk/lib/sequential_executor.js:106:20)<br>    at Request.emit (/opt/nodejs/node_modules/aws-sdk/lib/sequential_executor.js:78:10)<br>    at Request.emit (/opt/nodejs/node_modules/aws-sdk/lib/request.js:688:14)<br>    at Request.transition (/opt/nodejs/node_modules/aws-sdk/lib/request.js:22:10)<br>    at AcceptorStateMachine.runTo (/opt/nodejs/node_modules/aws-sdk/lib/state_machine.js:14:12)<br>    at /opt/nodejs/node_modules/aws-sdk/lib/state_machine.js:26:10<br>    at Request.&lt;anonymous&gt; (/opt/nodejs/node_modules/aws-sdk/lib/request.js:38:9)<br>    at Request.&lt;anonymous&gt; (/opt/nodejs/node_modules/aws-sdk/lib/request.js:690:12)<br>    at Request.callListeners (/opt/nodejs/node_modules/aws-sdk/lib/sequential_executor.js:116:18)<br>    at Request.emit (/opt/nodejs/node_modules/aws-sdk/lib/sequential_executor.js:78:10)<br>    at Request.emit (/opt/nodejs/node_modules/aws-sdk/lib/request.js:688:14)<br>    at Request.transition (/opt/nodejs/node_modules/aws-sdk/lib/request.js:22:10)<br>    at AcceptorStateMachine.runTo (/opt/nodejs/node_modules/aws-sdk/lib/state_machine.js:14:12)<br>    at /opt/nodejs/node_modules/aws-sdk/lib/state_machine.js:26:10<br>    at Request.&lt;anonymous&gt; (/opt/nodejs/node_modules/aws-sdk/lib/request.js:38:9)<br>END RequestId: fa89f24c-d6e1-4452-ae1d-ef7246ad0edf<br></code></pre><br><p>I do not understand if what exactly this error means. For my current level of understanding the AWS methods; it seems insufficient (I'm a newbie at AWS).</p><br>
1.0,0.0,0.0,0.6666666666666666,0.0,0.0,0.0,<h3>Parquet data to AWS Redshift slow</h3><p>I want to insert data from S3 parquet files to Redshift.</p><br><p>Files in parquet comes from a process that reads <code>JSON</code> files; flatten them out; and store as parquet. To do it we use <code>pandas dataframes</code>.</p><br><p>To do so; I tried two different things. The first one:</p><br><pre><code>COPY schema.table<br>FROM 's3://parquet/provider/A/2020/11/10/11/'<br>IAM_ROLE 'arn:aws:iam::XXXX'<br>FORMAT AS PARQUET;<br></code></pre><br><p>It returned:</p><br><pre><code>Invalid operation: Spectrum Scan Error<br>error:  Spectrum Scan Error<br>code:      15001<br>context:   Unmatched number of columns between table and file. Table columns: 54; Data columns: 41<br></code></pre><br><p>I understand the error but I don't have an easy option to fix it.<br>If we have to do a reload from 2 months ago the file will only have for example 40 columns; because on that given data we needed just this data but table already increased to 50 columns.<br>So we need something automatically; or that we can specify the columns at least.</p><br><p>Then I applied another option which is to do a <code>SELECT</code> with <code>AWS Redshift Spectrum</code>. We know how many columns the table have using system tables; and we now the structure of the file loading again to a <code>Pandas dataframe</code>. Then I can combine both to have the same identical structure and do the insert.</p><br><p>It works fine but it is slow.</p><br><p>The select looks like:</p><br><pre><code>SELECT fields<br>FROM schema.table<br>WHERE partition_0 = 'A'<br>  AND partition_1 = '2020'<br>  AND partition_2 = '11'<br>  AND partition_3 = '10'<br>  AND partition_4 = '11';<br></code></pre><br><p>The partitions are already added as I checked using:</p><br><pre><code>select *<br>from SVV_EXTERNAL_PARTITIONS<br>where tablename = 'table'<br>  and schemaname = 'schema'<br>  and values = '[&quot;A&quot;;&quot;2020&quot;;&quot;11&quot;;&quot;10&quot;;&quot;11&quot;]'<br>limit 1;<br></code></pre><br><p>I have around 170 files per hour; both in json and parquet file. The process list all files in <code>S3 json path</code>; and process them and store in <code>S3 parquet path</code>.</p><br><p>I don't know how to improve execution time; as the <code>INSERT</code> from <code>parquet</code> takes 2 minutes per each <code>partition_0</code> value. I tried the <code>select</code> alone to ensure its not an <code>INSERT</code> issue; and it takes 1:50 minutes. So the issue is to read data from <code>S3</code>.</p><br><p>If I try to select a non existent value for <code>partition_0</code> it takes again around 2 minutes; so there is some kind of problem to access data. I don't know if <code>partition_0</code> naming and others are considered as Hive partitioning format.</p><br><p><strong>Edit:</strong><br>AWS Glue Crawler table specification<br><a href="https://i.stack.imgur.com/pRuc6.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/pRuc6.png" alt="enter image description here" /></a></p><br><p>Edit: Add SVL_S3QUERY_SUMMARY results</p><br><pre><code>step:1<br>starttime: 2020-12-13 07:13:16.267437<br>endtime: 2020-12-13 07:13:19.644975<br>elapsed: 3377538<br>aborted: 0<br>external_table_name: S3 Scan schema_table<br>file_format: Parquet         <br>is_partitioned: t<br>is_rrscan: f<br>is_nested: f<br>s3_scanned_rows: 1132<br>s3_scanned_bytes: 4131968<br>s3query_returned_rows: 1132<br>s3query_returned_bytes: 346923<br>files: 169<br>files_max: 34<br>files_avg: 28<br>splits: 169<br>splits_max: 34<br>splits_avg: 28<br>total_split_size: 3181587<br>max_split_size: 30811<br>avg_split_size: 18825<br>total_retries:0<br>max_retries:0<br>max_request_duration: 360496<br>avg_request_duration: 172371<br>max_request_parallelism: 10<br>avg_request_parallelism: 8.4<br>total_slowdown_count: 0<br>max_slowdown_count: 0<br></code></pre><br><p><strong>Add query checks</strong></p><br><p>Query: 37005074 (SELECT in localhost using pycharm)<br>Query: 37005081 (INSERT in AIRFLOW AWS ECS service)</p><br><p><strong>STL_QUERY</strong> Shows that both queries takes around 2 min</p><br><pre><code>select * from STL_QUERY where query=37005081 OR query=37005074 order by query asc;<br><br>Query: 37005074 2020-12-14 07:44:57.164336;2020-12-14 07:46:36.094645;0;0;24<br>Query: 37005081 2020-12-14 07:45:04.551428;2020-12-14 07:46:44.834257;0;0;3<br></code></pre><br><p><strong>STL_WLM_QUERY</strong> Shows that no queue time; all in exec time</p><br><pre><code>select * from STL_WLM_QUERY where query=37005081 OR query=37005074;<br><br>Query: 37005074 Queue time 0 Exec time: 98924036 est_peak_mem:0<br>Query: 37005081 Queue time 0 Exec time: 100279214 est_peak_mem:2097152<br></code></pre><br><p><strong>SVL_S3QUERY_SUMMARY</strong> Shows that query takes 3-4 seconds in s3</p><br><pre><code>select * from SVL_S3QUERY_SUMMARY where query=37005081 OR query=37005074 order by endtime desc;<br><br>Query: 37005074 2020-12-14 07:46:33.179352;2020-12-14 07:46:36.091295<br>Query: 37005081 2020-12-14 07:46:41.869487;2020-12-14 07:46:44.807106<br></code></pre><br><p><strong>stl_return</strong> Comparing min start for to max end for each query. 3-4 seconds as says <code>SVL_S3QUERY_SUMMARY</code></p><br><pre><code>select * from stl_return where query=37005081 OR query=37005074 order by query asc;<br><br>Query:37005074  2020-12-14 07:46:33.175320 2020-12-14 07:46:36.091295<br>Query:37005081  2020-12-14 07:46:44.817680 2020-12-14 07:46:44.832649<br></code></pre><br><p>I dont understand why <code>SVL_S3QUERY_SUMMARY</code> shows just 3-4 seconds to run query in spectrum; but then <code>STL_WLM_QUERY</code> says the excution time is around 2 minutes as i see in my localhost and production environtments... Neither how to improve it; because <code>stl_return</code> shows that query returns few data.</p><br><p><strong>EXPLAIN</strong></p><br><pre><code>XN Partition Loop  (cost=0.00..400000022.50 rows=10000000000 width=19608)<br>  -&gt;  XN Seq Scan PartitionInfo of parquet.table  (cost=0.00..22.50 rows=1 width=0)<br>        Filter: (((partition_0)::text = 'A'::text) AND ((partition_1)::text = '2020'::text) AND ((partition_2)::text = '12'::text) AND ((partition_3)::text = '10'::text) AND ((partition_4)::text = '12'::text))<br>  -&gt;  XN S3 Query Scan parquet  (cost=0.00..200000000.00 rows=10000000000 width=19608)<br>&quot;        -&gt;  S3 Seq Scan parquet.table location:&quot;&quot;s3://parquet&quot;&quot; format:PARQUET  (cost=0.00..100000000.00 rows=10000000000 width=19608)&quot;<br></code></pre><br><p><strong>svl_query_report</strong></p><br><pre><code>select * from svl_query_report where query=37005074 order by segment; step; elapsed_time; rows;<br></code></pre><br><p><a href="https://i.stack.imgur.com/1GXvd.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/1GXvd.png" alt="enter image description here" /></a></p><br>
0.0,1.0,0.0,0.6666666666666666,0.0,0.0,0.6666666666666666,<h3>Reducing transfer fees from S3 buckets (multiple ones) to our datacenter</h3><p>I'm trying to lower egress fees from multiple S3 buckets in one AWS account (several Terabytes per month) to our US datacenter.</p><br><p>I thought of setting up a VPC on our AWS account; and using a <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html" rel="nofollow noreferrer"><code>Gateway Endpoint for S3</code></a> and then having a <a href="https://aws.amazon.com/directconnect/pricing/" rel="nofollow noreferrer"><code>Direct Connect</code></a> always active from our datacenter to our AWS VPC (and pay the usual hourly + GB transferred reduced fees).</p><br><p>After reading the documentation it is my understanding that I will <strong>not</strong> pay any traffic to use the <code>Gateway Endpoint for S3</code> since traffic never leaves AWS until it reaches our AWS VPC. To transfer it to our datacenter the usual hourly + GB transferred reduced fees for the <code>Direct Connect</code> is then billed.</p><br><p>Is this correct? Will we still be able to initiate a GET from our datacenter applications (through the <code>Direct Connect</code> to this VPC <code>Gateway Endpoint for S3</code> so that we can pull S3 files from the 3rd party AWS account that endpoint is linked to? (requests would <em>only</em> originate from our datacenter servers to get or sometimes put S3 files).</p><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>Why do I get a &quot;No Export Named&quot; error when using nested stacks in CloudFormation?</h3><p>I'm defining an export in a CloudFormation template to be used in another.</p><br><p>I can see the export is being created in the AWS console however; the second stack fails to find it.</p><br><p>The error:</p><br><pre><code>UPDATE_ROLLBACK_IN_PROGRESS with reason: No export named sandbox06-ODM-KinesisStreamArn found<br></code></pre><br><p><code>template.yml</code></p><br><pre><code>Resources:<br>  KinesisStream:<br>    Type: AWS::Kinesis::Stream<br>    Properties:<br>      ShardCount: 1<br>      RetentionPeriodHours: 24<br>      Name: !Sub ${Environment}-${Application}<br>Outputs:<br>  Topic:<br>    Value: !Ref Topic<br>  KinesisStreamArn:<br>    Value: !GetAtt KinesisStream.Arn<br>    Export:<br>      Name: !Sub ${Environment}-${Domain}-KinesisStreamArn<br></code></pre><br><p><code>firehose.yml</code></p><br><pre><code>KinesisFirehoseRole:<br>    Type: AWS::IAM::Role<br>    Properties:<br>      AssumeRolePolicyDocument:<br>        Version: 2012-10-17<br>        Statement:<br>          - Effect: Allow<br>            Principal:<br>              Service: firehose.amazonaws.com<br>            Action: sts:AssumeRole<br>      Path: /<br>      Policies:<br>        - PolicyName: KinesisFirehosePolicy<br>          PolicyDocument:<br>            Version: 2012-10-17<br>            Statement:<br>              - Effect: Allow<br>                Action:<br>                  - kinesis:*<br>                  - s3:*<br>                  - s3-object-lambda:*<br>                Resource:<br>                  - !Sub &quot;${Bucket.Arn}/*&quot;<br>                  - Fn::ImportValue: !Sub &quot;${Environment}-${Domain}-KinesisStreamArn&quot;<br></code></pre><br><p><code>nested_template.yml</code></p><br><pre><code>  OperationalData:<br>    Type: AWS::Serverless::Application<br>    Properties:<br>      Parameters:<br>        Environment: !Ref Environment<br>        Domain: OperationalData<br>        Application: odm<br>        BucketPrefix: pie<br>        WhiteListCidr: !Ref WhiteListCidr<br>        VpcId:<br>          Fn::ImportValue: !Sub vpc-${Environment}-VPCID<br>        VpcCidr:<br>          Fn::ImportValue: !Sub vpc-${Environment}-VPCCIDR<br>        Subnets:<br>          Fn::ImportValue: !Sub vpc-${Environment}-PrivateSubnets<br>      Location: ./data/odm/template.yml<br>      Tags:<br>        Environment: !Ref Environment<br>        Domain: odm<br>        Application: !Ref Application<br>        Developer: !Ref Developer<br>        DevOpsAdmin: !Ref DevOpsAdmin<br>        Repository: !Ref Repository<br>        Team: !Ref Team<br><br>  DataEngineeringData:<br>    Type: AWS::Serverless::Application<br>    Properties:<br>      Parameters:<br>        Environment: !Ref Environment<br>        Domain: DataEngineeringData<br>        Application: data-engineering<br>      Location: ./data/data-engineering/template.yml<br>      Tags:<br>        Environment: !Ref Environment<br>        Domain: DataEngineeringData<br>        Application: data-engineering<br>        Developer: !Ref Developer<br>        DevOpsAdmin: !Ref DevOpsAdmin<br>        Repository: !Ref Repository<br>        Team: !Ref Team<br></code></pre><br><p>What is the issue?</p><br>
1.0,0.0,0.3333333333333333,0.6666666666666666,0.0,0.0,0.3333333333333333,<h3>Different permissions same S3 bucket; parquet files</h3><p><strong>Problem</strong><br>I have multiple files in the same S3 bucket. When I try to load one file into Snowflake; I get a &quot;access denied&quot; error. When I try a different file (in the same bucket); I can successfully load into Snowflake.</p><br><p>The file highlighted does not load into Snowflake.</p><br><p><a href="https://i.stack.imgur.com/6daP1.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/6daP1.png" alt="File does not load" /></a></p><br><p>This is the error</p><br><p><a href="https://i.stack.imgur.com/vEQrL.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/vEQrL.png" alt="Error" /></a></p><br><hr /><br><p>Using a different file but in the same bucket; I can successfully load into Snowflake.</p><br><p><a href="https://i.stack.imgur.com/9udBt.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/9udBt.png" alt="this file loads" /></a></p><br><p><strong>Known Difference:</strong> The file that does not work was generated by AWS. The file that can be loaded into Snowflake was generated by AWS; saved to my local then reuploaded to the bucket.</p><br><p>The only difference is I brought it down to my local machine.</p><br><p><strong>Question:</strong> Is there a known file permission on parquet files? Why does this behavior go away when I download and upload to the same bucket.</p><br><p>It cannot be an S3 bucket issue. It has to be some encoding on the parquet file.</p><br>
0.0,0.3333333333333333,0.0,0.0,0.0,0.0,1.0,<h3>General questions about AWS Ethereum network</h3><p>I followed these steps and I believe I have an ethereum network running successfully:<br><a href="https://docs.aws.amazon.com/blockchain-templates/latest/developerguide/blockchain-templates-getting-started.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/blockchain-templates/latest/developerguide/blockchain-templates-getting-started.html</a></p><br><p>After starting the network; I have a bastion host to create a SSH tunnel; and I added the FoxyProxy extension to Chrome; and then I am able to connect to the explorer URL and stats URL; shown below.</p><br><p>Here are some general questions I am struggling with:</p><br><ol><br><li><p>The explorer URL shows blocks; but the stats URL looks empty; why is that?</p><br></li><br><li><p>There is a EthJsonRPCURL with the description &quot;Use this URL to access the Geth JSON RPC of your Ethereum Clients; or input it into Metamask&quot; and I am trying to use that to deploy a smart contract. If I want to do this from Visual Studio; how would I do it? When I try; I get a timeout error. To connect to the other URL's I am using the proxy through the browser; but what would I do in Visual Studio to connect to the network using the json rpc URL to deploy a smart contract?</p><br></li><br></ol><br><p><a href="https://i.stack.imgur.com/C3GN4.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/C3GN4.png" alt="enter image description here" /></a></p><br><p><a href="https://i.stack.imgur.com/eiFai.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/eiFai.png" alt="enter image description here" /></a></p><br>
0.0,0.0,0.3333333333333333,0.3333333333333333,1.0,0.3333333333333333,0.0,<h3>AWS ECS Fargate copy local file to container</h3><p>there is a simple way to copy files from local host to AWS ECS Fargate container?</p><br><p>Like Docker cp<br>docker cp :/file/path/in/container/file /host/local/path/file ?</p><br><p>I've found some workarounds like:</p><br><ul><br><li>pass through a S3 Bucket (I have to install AWS cli on container and give him credentials inside container...not a beaeutiful thing!)</li><br><li>mount an EFS shared folder in EC2 instance (I've to mount EFS folder in instance and upload the file to it from my host...2 steps and shared folder)</li><br></ul><br><p>Can someone help me please?</p><br><p>Thanks</p><br>
0.0,0.3333333333333333,0.0,0.0,0.3333333333333333,0.6666666666666666,0.0,<h3>AWS - How to connect to a specific port using Socket.io?</h3><p>I have multiple Node.js servers running on an <strong>Amazon Web Service</strong> application. One of the servers is being ran on port 8080. How do I connect to this port?</p><br><p>Using <code>var socket = io.connect('example.region-1.elasticbeanstalk.com')</code> connects me to the primary server. What I need is something like <code>var socket = io.connect('example.region-1.elasticbeanstalk.com:8080')</code>.</p><br>
0.0,0.0,0.3333333333333333,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>Docker Image can&#39;t push to ECR via Jenkins Pipeline &amp; docker.withRegistry</h3><p>I am running jenkins on a aws server<br>Working on a pipeline for building docker images and push to ECR on the same aws account</p><br><pre><code>def aws_account = &quot;https://xxxxxx.ecr.us-west-2.amazonaws.com/&quot;<br>def ecr_credentials = &quot;iam-role-arn for ecr&quot;<br><br><br>    docker.withRegistry(aws_account + &quot;${ecr_repository_name}&quot;; &quot;ecr:us-west-2:${ecr_credentials}&quot;) {<br>        docker.image(customImage).push()<br>    }<br></code></pre><br><p>While pushing I am getting below error<br>tcp:lookup is pointing to the account number itself</p><br><p><strong>Error:</strong><br><strong>docker.service</strong><br>Mar 04 10:46:13 ip-x-xxx-x-xxx dockerd[921]: time=&quot;2021-03-04T10:46:13.989576275Z&quot; level=error msg=&quot;Handler for POST /v1.41/auth returned error: Get <a href="https://yyyyyyy.ecr.us-west-2.amazonaws.com/v2/" rel="nofollow noreferrer">https://yyyyyyy.ecr.us-west-2.amazonaws.com/v2/</a>: dial tcp: lookup yyyyyyy.ecr.us-west-2.amazonaws.com: no such host&quot;</p><br>
0.0,0.0,0.0,1.0,0.0,1.0,0.3333333333333333,<h3>android cross promotion architecture</h3><p>I have an architecture question. I have 10 android apps and I want to create a cross promotion system in those apps - which means; every time a user opens one of those apps - they will see an interstitial ad that promotes another app.</p><br><p>In a very basic architecture; all I did was creating an AWS database which contains the URLs of the other apps and the ad in an mp4 format.<br>Then; when a user opens an app; I have a class that randomly chooses from the AWS db an ad and shows it to the user -&gt; it loads the mp4 video and displays it to the user using android class video <code>SurfaceView</code>.</p><br><p>I'm currently facing to major issues:</p><br><ol><br><li>the buffer until the ad loads is very long - unlike other ads I see on other apps that loads a video ad in seconds.</li><br><li>the bandwidth that being used from AWS is very high - because every time the user open an app the video is loading.</li><br></ol><br><p>anyone has suggestions how can I improve my architecture and solve my main two problems?</p><br>
0.0,0.0,0.0,1.0,0.0,0.6666666666666666,0.0,<h3>S3 event notification body content</h3><p>I've configured an event notification on an AWS s3 bucket; putting a message on an SQS queue.</p><br><p>The body of that event contains an array of records.<br>I would like to understand in which conditions there are multiple records in the body.</p><br><p>Is it when we upload files immediately after each other?<br>Or only when uploading multiple files at once?</p><br><p>So is this generated on a time basis; collecting all the requests in X amount of time and sending a message to SQS; or is it a separate event for each request to the bucket?</p><br>
0.6666666666666666,0.0,0.6666666666666666,0.3333333333333333,0.6666666666666666,0.0,0.0,<h3>Send AWS EC2 metrics to AWS Elasticsearch Service Domain for monitoring in Kibana</h3><p>I am stuck on one point I have created one EC2 Linux based instance in Aws.<br>Now I want to send the EC2 metrics data to the managed Elasticsearch domain for monitoring purposes in Kiban; I go through the cloud watch console and check the metric is present of instance but didn't get how to connect with the Elasticsearch domain that I have created.</p><br><p>Can anyone please help me with this situation?</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>AWS RDS Aurora MySQL Cluster; Reader Replica shows no connections</h3><p>I added a reader replica to my RDS Aurora MySQL cluster. The instance is running with minor cpu usage but it does not show connections on the monitoring page. I have enabled detailed monitoring. Access groups are the same as the writer instance.</p><br><p>How to I ensure that traffic is going to my reader instance?</p><br>
0.0,0.0,0.6666666666666666,0.0,1.0,0.6666666666666666,0.0,<h3>Issues with subsequent requests from same user going to different AWS lambda versions when traffic being shifted using AWS CodeDeploy</h3><p>I trying to see if I can use AWS Lambda code deploy traffic shifting for blue/green deployments. My concern is during traffic shifting If first request from user A goes to old lambda version and second request from user A goes to new lambda version then there is possibility that user A might see discrepancies in response since requests from user A might go to old or new lambda version.</p><br><p>I want to understand how others are handling this.</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>Sync S3 bucket to local machine but delete local objects that have been moved to glacier storage</h3><p>I have a script that I run on a cron to sync an S3 bucket to my local machine:</p><br><pre><code>LOCAL_DATA_PATH=~/Data/my/local/data/path<br>S3_BUCKET_PATH=s3://my-data-bucket/and/path<br><br>aws s3 sync --delete --exclude &quot;*_ignore-me_*&quot; $S3_BUCKET_PATH $LOCAL_DATA_PATH<br></code></pre><br><p>This is great except that my hard drive is filling up. In AWS; I have machinery that automatically glaciates certain files once they become outdated. How can I modify my <code>sync</code> command to automatically remove those glaciated files from my local machine? It appears that the files; though stored in glacier; still show up in the metadata and therefore aren't removed via the <code>--delete</code> option. I do notice; however; that on a fresh run to an empty local directory; the glacier-stored files are ignored.</p><br><p>Alternatively; is there another command that can clear out any glacier-stored files from my local directory? I think this could work too because; as I say; sync doesn't download glacier-stored files.</p><br>
0.0,0.0,1.0,0.3333333333333333,0.0,0.0,0.0,<h3>Using AWS Terraform How to enable s3 backend authentication with assumed role MFA credentials</h3><p>I provision AWS resources using Terraform using a python script that call terraform via shell</p><br><pre><code>os.system('terraform apply')<br></code></pre><br><p>The only way I found to enable terraform authentication; after enabling MFA and assuming a role; is to publish these environment variables:</p><br><pre><code>os.system('export ASSUMED_ROLE=&quot;&lt;&gt;:botocore-session-123&quot;;<br>export AWS_ACCESS_KEY_ID=&quot;vfdgdsfg&quot;;<br>export AWS_SECRET_ACCESS_KEY=&quot;fgbdzf&quot;;<br>export AWS_SESSION_TOKEN=&quot;fsrfserfgs&quot;;<br>export AWS_SECURITY_TOKEN=&quot;fsrfserfgs&quot;; terraform apply')<br></code></pre><br><p>This worked OK until I configured s3 as backend; terraform action is performed but before the state can be stored in the bucket I get the standard (very confusing) exception:</p><br><pre><code>Error: error configuring S3 Backend: Error creating AWS session: AssumeRoleTokenProviderNotSetError: assume role with MFA enabled; but AssumeRoleTokenProvider session option not set.<br></code></pre><br><p>I read this <a href="https://stackoverflow.com/a/51946463/1447071">excellent answer</a> explaining that for security and other reasons backend configuration is separate.</p><br><p>Since I don't want to add actual secret keys to source code (as suggested by the post) I tried adding a reference to the profile and when it failed I added the actual keys just to see if it would work; which it didn't.</p><br><p>My working theory is that behind the scenes terraform starts another process which doesn't access or inherit the credential e environment variables.</p><br><p>How do I use s3 backend; with an MFA assumed role?</p><br>
0.0,0.0,0.0,1.0,0.3333333333333333,0.0,0.0,<h3>Facing redis problem: unknown command `config`; with args beginning with: `set` on amazon linux</h3><p>I am trying to get around this problem since yesterday but no matter what I do; I just cannot make redis to execute for my project. Here are the details in steps:</p><br><p>Step 1: I created new setup for redis using Elasticache in aws console; then I copied the url mentioned in primary endpoint e.g. my-redisxxxxx.cache.amazonaws.com. Thereafter; I assigned custom parameter group to this cluster from aws console where I have also configured the &quot;notify-keyspace-events&quot; in following way:</p><br><p><a href="https://i.stack.imgur.com/6oom5.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/6oom5.png" alt="enter image description here" /></a></p><br><p>It still breaks even if I select default parameter group and don't assign this key.</p><br><p>Step 2: I made ssh connection to my nodejs EC2 instance and went to the environment variables file and assigned this url in this way:</p><br><pre><code>REDIS_URI=&quot;my-redisxxxxx.cache.amazonaws.com&quot;<br>REDIS_HOST=&quot;my-redisxxxxx.cache.amazonaws.com&quot;<br>REDIS_PORT=6379<br>REDIS_INDEX=14<br></code></pre><br><p>Now all other configurations are working fine in this manner; even redis works if I replace above url with one another existing cluster url. Only this particular new cluster that I made using elasticache seems to be having some problem.</p><br><p>As soon as I run node project using pm2; it hits the wall and throws this:</p><br><pre><code> subscribed to channel ===&gt; __keyevent@14__:expired<br>0|npm  | events.js:377<br>0|npm  |       throw er; // Unhandled 'error' event<br>0|npm  |       ^<br>0|npm  | ReplyError: ERR unknown command `config`; with args beginning with: `set`; `notify-keyspace-events`; `AKE`;<br>0|npm  |     at parseError (/usr/share/nginx/project-directory/node_modules/redis-parser/lib/parser.js:193:12)<br>0|npm  |     at parseType (/usr/share/nginx/project-directory/node_modules/redis-parser/lib/parser.js:303:14)<br>0|npm  | Emitted 'error' event on RedisClient instance at:<br>0|npm  |     at Object.callbackOrEmit [as callback_or_emit] (/usr/share/nginx/project-directory/node_modules/redis/lib/utils.js:91:14)<br>0|npm  |     at RedisClient.return_error (/usr/share/nginx/project-directory/node_modules/redis/index.js:706:11)<br>0|npm  |     at JavascriptRedisParser.returnError (/usr/share/nginx/project-directory/node_modules/redis/index.js:196:18)<br>0|npm  |     at JavascriptRedisParser.execute (/usr/share/nginx/project-directory/node_modules/redis-parser/lib/parser.js:572:12)<br>0|npm  |     at Socket.&lt;anonymous&gt; (/usr/share/nginx/project-directory/node_modules/redis/index.js:274:27)<br>0|npm  |     at Socket.emit (events.js:400:28)<br>0|npm  |     at Socket.emit (domain.js:475:12)<br>0|npm  |     at addChunk (internal/streams/readable.js:293:12)<br>0|npm  |     at readableAddChunk (internal/streams/readable.js:267:9)<br>0|npm  |     at Socket.Readable.push (internal/streams/readable.js:206:10)<br>0|npm  |     at TCP.onStreamRead (internal/stream_base_commons.js:188:23) {<br>0|npm  |   command: 'CONFIG';<br>0|npm  |   args: [ 'set'; 'notify-keyspace-events'; 'AKE' ];<br>0|npm  |   code: 'ERR'<br>0|npm  | }<br>0|npm  | npm<br>0|npm  |  ERR! code ELIFECYCLE<br>0|npm  | npm<br>0|npm  |  ERR! errno 1<br>0|npm  | npm ERR!<br></code></pre><br><p>I spent countless hours to understand this problem by reading documentation; only to find out that config command is <a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/RestrictedCommands.html" rel="nofollow noreferrer">restricted</a> in aws elasticache. Also I found this answer <a href="https://stackoverflow.com/questions/58166382/redis-config-set-with-node-js">here</a> however not able to adapt it in my current code as I am creating redis connection like this (also I am not sure whether this is good solution):</p><br><pre><code>const sub = redis.createClient(options);<br><br>export const subscribe = async (channel: string) =&gt; {<br>    try {<br>        sub.subscribe(channel);<br>        console.log(`subscribed to channel ===&gt; ${channel}`);<br>        return {};<br>    }<br>    catch (error) {<br>        console.log(&quot;Error while subscribing to a channel&quot;; error);<br>        return {}<br>    }<br>}<br></code></pre><br><p>I am completely lost now as not able to think over what can solve this.</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,1.0,0.0,<h3>How i can embed aws CloudWatch metric with AWS amplify application</h3><p>I am working on a <em>serverless react.js</em> application. I have deployed the application on <em>AWS</em> amplify. I have some metrics in <em>AWS CloudWatch</em> and I would like to embed that metric in <em>aws</em> amplify framework?</p><br><p>How I can achieve this ?</p><br>
0.0,0.6666666666666666,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,<h3>Cloud File Storage with Bandwidth Limits</h3><p>I want to develop an app for a friend's small business that will store/serve media files. However I'm afraid of having a piece of media goes viral; or getting DDoS'd. The bill could go up quite easily with a service like S3 and I really want to avoid surprise expenses like that. Ideally I'd like some kind of max-bandwidth limit.</p><br><p>Now; the solutions for S3 this has been <a href="https://stackoverflow.com/questions/39473419/limiting-amazon-s3-downloads-service?noredirect=1&amp;lq=1">posted here</a><br>But it does require quite a few steps. So I'm wondering if there is a cloud storage solution that makes this simpler I.e. where I don't need to create a custom microservice. I've talked to the support on Digital Ocean and they also <a href="https://ideas.digitalocean.com/ideas/CDNX-I-9" rel="nofollow noreferrer">don't support this</a></p><br><p>So in the interest of saving time; and perhaps for anyone else who finds themselves in a similar dilemma; I want to ask this question here; I hope that's okay.</p><br><p>Thanks!</p><br>
0.6666666666666666,0.3333333333333333,1.0,0.0,0.0,0.0,0.0,<h3>Access Policy fix for anonymous user error in AWS?</h3><p>My IP address changes when I reconnect to the internet so when I visit Kibana from Elastic Search I get an error saying <code>User: anonymous is not authorized to perform: es:ESHttp</code></p><br><p>This is my Access Policy</p><br><pre><code>{<br>  &quot;Version&quot;: &quot;2012-10-17&quot;;<br>  &quot;Statement&quot;: [<br>    {<br>      &quot;Effect&quot;: &quot;Allow&quot;;<br>      &quot;Principal&quot;: {<br>        &quot;AWS&quot;: &quot;*&quot;<br>      };<br>      &quot;Action&quot;: &quot;es:ESHttp*&quot;;<br>      &quot;Resource&quot;: &quot;arn:aws:es:xxxx:xxxxxxxx:domain/xxxxxxxx/*&quot;;<br>      &quot;Condition&quot;: {<br>        &quot;IpAddress&quot;: {<br>          &quot;aws:SourceIp&quot;: &quot;45.75.245.8&quot;<br>        }<br>      }<br>    }<br>  ]<br>}<br><br></code></pre><br><p>Whenever I visit kibana I have to modify this access policy with my current IP address. If I do something like</p><br><pre><code>&quot;IpAddress&quot;: {<br>          &quot;aws:SourceIp&quot;: &quot;45.*&quot;<br>        }<br></code></pre><br><p>Or to fully just &quot;*&quot; it give me the same error &quot;anonymous user is not authorized&quot;. how can I fix this? I always login through the same root account.</p><br>
0.0,0.0,0.3333333333333333,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>Fluxcd ImageRepository authentication with AWS Elastic Container Registry Not working</h3><p><code>Fluxcd</code> <code>ImageRepository</code> authentication with <code>AWS Elastic Container Registry</code> Not working on <code>ARM64 graviton</code> node.</p><br><p>After debugging I found that the image used in the <code>init container</code> to get cred credentials is not supporting <code>Arm64</code> instances.</p><br><pre><code>image name:-bitnami/kubectl<br></code></pre><br><p><strong>doc link:-https://fluxcd.io/docs/guides/image-update/</strong></p><br>
0.0,0.6666666666666666,0.0,0.0,0.0,1.0,0.0,<h3>AWS CDK event bridge and api gateway AWS example does not work</h3><p>I am following the instructions here to setup an event bridge: <a href="https://eventbus-cdk.workshop.aws/en/04-api-gateway-service-integrations/01-rest-api/rest-apis.html" rel="nofollow noreferrer">https://eventbus-cdk.workshop.aws/en/04-api-gateway-service-integrations/01-rest-api/rest-apis.html</a></p><br><p>Based on the error message; the error is coming from this line of code: languageResource.addMethod(&quot;POST&quot;; new apigw.Integration({</p><br><p>I am not sure what is causing this issue because this is an example given by AWS and should work; but it does not.</p><br><p>I can build it but it fails with the following error on cdk deploy:</p><br><pre><code>CREATE_FAILED        | AWS::ApiGateway::Method           | MyRestAPI/Default/{language}/POST (MyRestAPIlanguagePOSTB787D51A) Invalid Resource identifier specified (Service: AmazonApiGateway; Status Code: 404; Error Code: NotFoundException;<br></code></pre><br><p>The code is below:</p><br><pre><code>    const myLambda = new lambda.Function(this; &quot;MyEventProcessor&quot;; {<br>  code: new lambda.InlineCode(&quot;def main(event; context):\n\tprint(event)\n\treturn {'statusCode': 200; 'body': 'Hello; World'}&quot;);<br>  handler: &quot;index.main&quot;;<br>  runtime: lambda.Runtime.PYTHON_3_7<br>})<br><br>  <br>const bus = new events.EventBus(this; `pwm-${this.stage}-MdpEventBus`)<br>new cdk.CfnOutput(this; &quot;PwmMdpEventBus&quot;; {value: bus.eventBusName})<br><br>new events.Rule(this; `PwmMdpEventBusRule`; {<br>  eventBus: bus;<br>  eventPattern: {source: [`com.amazon.alexa.english`]};<br>  targets: [new targets.LambdaFunction(myLambda)]<br>})<br><br>const apigwRole = new iam.Role(this; &quot;MYAPIGWRole&quot;; {<br>  assumedBy: new iam.ServicePrincipal(&quot;apigateway&quot;);<br>  inlinePolicies: {<br>    &quot;putEvents&quot;: new iam.PolicyDocument({<br>      statements: [new iam.PolicyStatement({<br>        actions: [&quot;events:PutEvents&quot;];<br>        resources: [bus.eventBusArn]<br>      })]<br>    })<br>  }<br>});<br><br>const options = {<br>  credentialsRole: apigwRole;<br>  requestParameters: {<br>    &quot;integration.request.header.X-Amz-Target&quot;: &quot;'AWSEvents.PutEvents'&quot;;<br>    &quot;integration.request.header.Content-Type&quot;: &quot;'application/x-amz-json-1.1'&quot;<br>  };<br>  requestTemplates: {<br>    &quot;application/json&quot;: `#set($language=$input.params('language'))\n{&quot;Entries&quot;: [{&quot;Source&quot;: &quot;com.amazon.alexa.$language&quot;; &quot;Detail&quot;: &quot;$util.escapeJavaScript($input.body)&quot;; &quot;Resources&quot;: [&quot;resource1&quot;; &quot;resource2&quot;]; &quot;DetailType&quot;: &quot;myDetailType&quot;; &quot;EventBusName&quot;: &quot;${bus.eventBusName}&quot;}]}`<br>  };<br>  integrationResponses: [{<br>    statusCode: &quot;200&quot;;<br>    responseTemplates: {<br>      &quot;application/json&quot;: &quot;&quot;<br>    }<br>  }]<br>}<br><br>const myRestAPI = new apigw.RestApi(this; &quot;MyRestAPI&quot;);<br><br>const languageResource = myRestAPI.root.addResource(&quot;{language}&quot;);<br><br>languageResource.addMethod(&quot;POST&quot;; new apigw.Integration({<br>  type: apigw.IntegrationType.AWS;<br>  uri: `arn:aws:apigateway:${cdk.Aws.REGION}:events:path//`;<br>  integrationHttpMethod: &quot;POST&quot;;<br>  options: options;<br>});<br>{<br>  methodResponses: [{<br>      statusCode: &quot;200&quot;<br>    }];<br>  requestModels: {&quot;application/json&quot;: model.getModel(this; myRestAPI) };<br>  requestValidator: new apigw.RequestValidator(this; &quot;myValidator&quot;; {<br>    restApi: myRestAPI;<br>    validateRequestBody: true<br>  })<br>})<br></code></pre><br>
0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,1.0,0.0,<h3>Resource with id [ApiGatewayLambdaS3Event] is invalid. property BucketName not defined for resource of type S3</h3><p>I am adding an S3 event in my sam.yaml file to trigger the lambda function when a file is added inside the bucket.</p><br><p>The trigger for these functions that I shall be making will be for existing S3 buckets. Would I be able to create the triggers for existing buckets using SAM or would I need to create the triggers manually?</p><br><p>However; I am getting the following error</p><br><pre><code>Resource with id [ApiGatewayLambdaS3Event] is invalid. property BucketName not defined for resource of type S3<br></code></pre><br><p>Here is my template.yaml file that I have created with all the lambda functions and triggers I am trying to create.</p><br><pre><code>AWSTemplateFormatVersion: '2010-09-09'<br>Transform: 'AWS::Serverless-2016-10-31'<br>Description: CD Demo Lambda<br>Resources:<br>  CDDemoLambda:<br>    Type: 'AWS::Serverless::Function'<br>    Properties:<br>      Handler: lambda_function.lambda_handler<br>      Runtime: python3.6<br>      CodeUri: ./FunctionOne<br>      FunctionName: CDDemoLambda<br>      Description: 'Lambda function for CD Demo Test'<br>      MemorySize: 128<br>      Timeout: 30<br>      Events:<br>        getAZsAPI:<br>          Type: Api<br>          Properties:<br>            Path: /getazs<br>            Method: get<br>            <br>  HelloWorld:<br>    Type: 'AWS::Serverless::Function'<br>    Properties:<br>      AutoPublishAlias: qaTest<br>      Handler: qa-hello-world.lambda_handler<br>      Runtime: python3.6<br>      CodeUri: ./FunctionTwo<br>      FunctionName: HelloWorld<br>      Description: 'Hello WOrld'<br>      <br>  ApiGatewayLambda:<br>    Type: 'AWS::Serverless::Function'<br>    Properties:<br>      AutoPublishAlias: apigateway<br>      Handler: lambdafunctionthree.lambda_handler<br>      Runtime: python3.6<br>      FunctionName: ApiGatewayLambda<br>      CodeUri: ./FunctionThree<br>      Description: 'Hello WOrld'<br></code></pre><br><p>What changes do I need to make to add an S3 event trigger to be created along with my lambda function.</p><br>
0.0,0.0,0.6666666666666666,1.0,0.0,0.0,0.0,<h3>Can we have Fine-Grained Access Control (table level) in Amazon Aurora?</h3><p>I am looking in Amazon Aurora to have Fine-Grained Access Control using IAM policies. eg - If user doesn't have permission to access few attributes of Aurora table ; should not be able to access/view those.</p><br>
0.0,0.6666666666666666,0.0,0.0,1.0,0.0,0.0,<h3>Adonis - 502 Bad Gateway on Elastic BeanStalk</h3><p>I am trying to deploy an AdonisJS app on Elastic Beanstalk. I have followed several tutorials to do so but I am getting 502 Bad Gateway :(</p><br><p>I have set <code>PORT</code> env variable to 8081 from the Configuration portal as suggested in all the tutorials out there.</p><br><pre><code>----------------------------------------<br>/var/log/nginx/error.log<br>----------------------------------------<br>2020/12/28 19:54:52 [error] 5218#0: *266 connect() failed (111: Connection refused) while connecting to upstream; client: 172.31.xxx.xx; server: ; request: &quot;GET / HTTP/1.1&quot;; upstream: &quot;http://127.0.0.1:8081/&quot;; host: &quot;172.xx.xx.x&quot;<br></code></pre><br><pre><code>----------------------------------------<br>/var/log/web.stdout.log<br>----------------------------------------<br>Dec 28 20:06:36 ip-xxx-31-30-5 web: &gt; adonis-api-app@4.1.0 start /var/app/current<br>Dec 28 20:06:36 ip-xxx-31-30-5 web: &gt; node server.js<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: #033[32minfo#033[39m: serving app on http://xxxxx-api-staging.us-west-2.elasticbeanstalk.com:8081<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: events.js:291<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: throw er; // Unhandled 'error' event<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: ^<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: Error: listen EADDRNOTAVAIL: address not available 44.242.xx.xx:8081<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: at Server.setupListenHandle [as _listen2] (net.js:1300:21)<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: at listenInCluster (net.js:1365:12)<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: at GetAddrInfoReqWrap.doListen [as callback] (net.js:1502:7)<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: at GetAddrInfoReqWrap.onlookup [as oncomplete] (dns.js:68:8)<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: Emitted 'error' event on Server instance at:<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: at emitErrorNT (net.js:1344:8)<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: at processTicksAndRejections (internal/process/task_queues.js:84:21) {<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: code: 'EADDRNOTAVAIL';<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: errno: 'EADDRNOTAVAIL';<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: syscall: 'listen';<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: address: '44.242.xx.xx';<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: port: 8081<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: }<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: npm ERR! code ELIFECYCLE<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: npm ERR! errno 1<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: npm ERR! adonis-api-app@4.1.0 start: `node server.js`<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: npm ERR! Exit status 1<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: npm ERR!<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: npm ERR! Failed at the adonis-api-app@4.1.0 start script.<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: npm ERR! This is probably not a problem with npm. There is likely additional logging output above.<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: npm ERR! A complete log of this run can be found in:<br>Dec 28 20:06:37 ip-xxx-31-30-5 web: npm ERR!     /home/webapp/.npm/_logs/2020-12-28T20_06_37_082Z-debug.log<br></code></pre><br><p>Please help as its getting really frustrating now :(</p><br><p>Thank you</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>How to get entries based on date?</h3><p>Using DynamoDB GSI Index; I like to return entries with the following condition: <code>exit_at</code> date that is passed after 30 mins of the current time. How do I archive this using query?</p><br><p>GSI example: <code>sent</code> is a PK and and <code>exit_at</code> is a Sort Key.</p><br><p>For example in the dynamodb:</p><br><pre><code>{<br>  &quot;exit_at&quot;: &quot;2021-02-12T09:30:01Z&quot;;<br>  &quot;sent&quot;: 0;<br>  &quot;uuid&quot;: &quot;ee1b0230-bfb7-1084-5020-8fef7gg6a86b&quot;;<br>}<br><br>{<br>  &quot;exit_at&quot;: &quot;2021-02-12T09:25:01Z&quot;;<br>  &quot;sent&quot;: 0;<br>  &quot;uuid&quot;: &quot;gg2b0245-fgb2-5552-1454-g3gf7he6f4g&quot;;<br>}<br></code></pre><br><p>Let say for example current time is <code>2021-02-12T10:00:01Z</code> only 1 entry should return:</p><br><pre><code>{<br>  &quot;exit_at&quot;: &quot;2021-02-12T09:30:01Z&quot;;<br>  &quot;sent&quot;: 0;<br>  &quot;uuid&quot;: &quot;ee1b0230-bfb7-1084-5020-8fef7gg6a86b&quot;;<br>}<br></code></pre><br><p>I would run a cron every 5 minutes to get the entries to send the messages.</p><br><p>If this is not possible; what is another approach to change the structure design or solution in AWS infrastructure?</p><br>
0.6666666666666666,0.0,0.0,0.6666666666666666,0.0,0.0,0.0,<h3>Store pandas df in s3 and be able to retrieve single column</h3><p>I try to save pandas df-like object in AWS S3 and retrieve it in Jupyter Notebook. In my solution I have to be able to retrieve single column from it; so .csv is not good solution. I thought about using .h5 files; saved dataframe and stored it as that kind of file in my S3; but when i try to call:</p><br><pre><code>pd.read_hdf('https://mytests3h5storage-123.s3.amazonaws.com/iris.h5')<br></code></pre><br><p>i got:</p><br><p><em>FileNotFoundError: File <a href="https://mytests3h5storage-123.s3.amazonaws.com/iris.h5" rel="nofollow noreferrer">https://mytests3h5storage-123.s3.amazonaws.com/iris.h5</a> does not exist</em></p><br><p>and I am 100% sure that file exist.</p><br><p>I find info that other people have same problems; so may be it is not implemented. Is there any good way to solve it? I need to retrieve single column (or few of them) in Lambda and use it there.</p><br>
0.0,0.3333333333333333,0.0,0.0,0.3333333333333333,0.3333333333333333,0.6666666666666666,<h3>API resolved without sending a response for /api/campaign/dynamicid; this may result in stalled requests. in next js</h3><p>I am getting an error while running amazon ses code in next js. don't know where am wrong; please try to fix my error. If you have any question please free feel to ask.</p><br><blockquote><br><p>sendmail.js</p><br></blockquote><br><p>This is the sendmail.js file where i got error. here I am using amazon ses for sending mail.</p><br><pre><code>var AWS = require('aws-sdk');<br><br>AWS.config.update({ region: process.env.AWS_REGION });<br><br>// var mail = '';<br><br>function sendMail(Email) {<br><br>    var result;<br>    // Create sendEmail params <br>    var params = {<br>        Destination: { /* required */<br>            CcAddresses: [<br>                Email;<br>                /* more items */<br>            ];<br>            ToAddresses: [<br>                Email;<br>                /* more items */<br>            ]<br>        };<br>        Message: { /* required */<br>            Body: { /* required */<br>                Html: {<br>                    Charset: &quot;UTF-8&quot;;<br>                    Data: &quot;HTML_FORMAT_BODY&quot;<br>                };<br>                Text: {<br>                    Charset: &quot;UTF-8&quot;;<br>                    Data: &quot;TEXT_FORMAT_BODY&quot;<br>                }<br>            };<br>            Subject: {<br>                Charset: 'UTF-8';<br>                Data: 'Test email'<br>            }<br>        };<br>        Source: 'abc@gmail.com'; /* required */<br>        ReplyToAddresses: [<br>            'abc12@gmail.com';<br>            /* more items */<br>        ];<br>    };<br><br>    // Create the promise and SES service object<br>    var sendPromise = new AWS.SES({ apiVersion: '2010-12-01' }).sendEmail(params).promise();<br><br>    // Handle promise's fulfilled/rejected states<br>    sendPromise.then(<br>        function (data) {<br>            result = 'Success';<br>        }).catch(<br>            function (err) {<br>                result = 'Failed';<br>            });<br>}<br><br>export default sendMail;<br></code></pre><br><blockquote><br><p>dynamicid.js</p><br></blockquote><br><p>This is the dynamic id .js file where i wrote my endpoint code</p><br><pre><code>import { getDataFromSheets } from '../../../libs/sheets';<br>import sendmail from '../../../libs/ses/sendmail';<br><br>export default function handler(req; res) {<br>  var data;<br>  getDataFromSheets()<br>    .then(sheet =&gt; {<br>      data = sheet.length<br>      for (var i = 1; i &lt; data; i++) {<br>        sendmail(sheet[i].Email)<br>      }<br>    })<br>    .catch(err =&gt; console.log(err))<br><br>}<br></code></pre><br>
0.0,0.3333333333333333,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>Terraform - AWS Network Interface get public IP</h3><p>I want to get the Public Ip in a Network Interface via Terraform.<br>the output of the NI show me the following json:</p><br><pre><code>+ app_lb_ips = [<br>      + {<br>          + association       = [<br>              + {<br>                  + allocation_id     = &quot;&quot;<br>                  + association_id    = &quot;&quot;<br>                  + carrier_ip        = &quot;&quot;<br>                  + customer_owned_ip = &quot;&quot;<br>                  + ip_owner_id       = &quot;amazon-elb&quot;<br>                  + public_dns_name   = &quot;ec2-3cxxxxxxx.compute.amazonaws.com&quot;<br>                  + public_ip         = &quot;3.xx.xx.50&quot;<br>                };<br>            ]<br>        }<br>    ]<br></code></pre><br><p>I tryied data.aws_network_interface.my_ni.*.association..public_ip but I get a error.</p><br><pre><code>Error: Unsupported attribute<br> <br>   on modules/folder/outputs.tf line 33; in output &quot;app_lb_ips&quot;:<br>   33:   value = data.aws_network_interface.my_lb.*.association[0].public_dns_name<br>     <br>      data.aws_network_interface.my_lb is tuple with 2 elements<br> <br> This value does not have any attributes.<br></code></pre><br><p>Any ideas or suggestions?<br>Thanks in advance</p><br>
0.0,0.0,0.0,1.0,0.6666666666666666,0.6666666666666666,0.0,<h3>How to write a dataframe to dynamodb using AWS Lambda</h3><p>I'M having a Lambda function set up in AWS Cloudformation. The runtime is python3.8.<br>The purpose is to pull some weather data from an API and write it to DynamoDB once a day.<br>So far the Lambda Test on AWS checks out; all green ...but the function doesnt write any values to the dynamodb.<br>Is there an error in indenting maybe?</p><br><p>Here is the code:</p><br><pre><code>import boto3<br>import pyowm<br>import time<br>import json<br>import requests<br>from datetime import datetime; date; timedelta; timezone<br>import pandas as pd<br>from geopy.geocoders import Nominatim<br><br>def lambda_handler(event; context):<br>   api_key = &quot;xxxxxxx&quot;    #Enter your own API Key<br>   owm = pyowm.OWM(api_key)<br>   city = 'Berlin; DE'<br><br>   geolocator = Nominatim(user_agent='aerieous@myserver.com')<br>   location = geolocator.geocode(city)<br>   lat = location.latitude<br>   lon = location.longitude<br><br>   # set the date to pull the data from to yesterday<br>   # format = '2021-09-09 00:00:00'<br>   x = (datetime.now() - timedelta(days = 1 ))<br>   d = x.isoformat(' '; 'seconds')<br><br>   # convert time to epoch<br>   p = '%Y-%m-%d %H:%M:%S'<br>   dt = int(time.mktime(time.strptime(d;p)))<br><br>   url = &quot;https://api.openweathermap.org/data/2.5/onecall/timemachine?lat=%s&amp;lon=%s&amp;   dt=%s&amp;appid=%s&amp;units=metric&quot; % (lat; lon; dt; api_key)<br>   response = requests.get(url)<br>   data_history = json.loads(response.text)<br><br>   # here we flatten only the nested list &quot;hourly&quot; <br>   df_history2 = pd.json_normalize(data_history; record_path='hourly'; meta=['lat'; 'lon'; 'timezone'];<br>                                errors='ignore')<br>   # convert epoch to timestamp<br>   df_history2['dt'] = pd.to_datetime(df_history2['dt'];unit='s').dt.strftime(&quot;%m/%d/%Y %H:%M:%S&quot;)<br>   # replace the column header<br>   df_history2 = df_history2.rename(columns={'dt': 'timestamp'})<br>   df_history2['uuid'] = df_history2[['timestamp';'timezone']].agg('-'.join; axis=1)<br>   df_select_hist2 = df_history2[['uuid';'lat';'lon'; 'timezone'; 'timestamp'; 'temp'; 'feels_like'; 'humidity'; 'pressure']]<br><br>   df_select_hist2 = df_select_hist2.astype(str)<br>   df_select_hist2<br><br>   content = df_select_hist2.to_dict('records')<br>   return content<br><br>   dynamodb = boto3.resource(<br>      'dynamodb';<br>      aws_access_key_id='xx';<br>      aws_secret_access_key='xx';<br>      region_name='eu-west-1')<br>   table = dynamodb.Table(&quot;Dev_Weather&quot;)<br><br>   for item in content:<br>     uuid = item['uuid']<br>     timezone = item['timezone']<br>     timestamp = item['timestamp']<br>     lat = item['lat']<br>     lon = item['lon']<br>     temp = item['temp']<br>     feels_like = item['feels_like']<br>     humidity = item['humidity']<br>     pressure = item['pressure']<br><br>     table.put_item(<br>        Item={<br>            'pk_id': uuid;<br>            'sk': timestamp;<br>            'gsi_1_pk': lat;<br>            'gsi_1_sk': lon;<br>            'gsi_2_pk': temp;<br>            'gsi_2_sk': feels_like;<br>            'humidity': humidity;<br>            'pressure': pressure;<br>            'timezone': timezone<br>         }<br>     )<br></code></pre><br><p>Thank you for any help in advance.</p><br><p>A</p><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.6666666666666666,0.0,<h3>Setting provisioned concurrency for lambda conditionally using serverless</h3><p>The lambda function is created using serverless(2.22) and is deployed using codebuild (standard5.0). I want to conditionally set the provisioned concurrency only for UAT and Prod env and not for dev and test env.</p><br><p>These are the things I tried:</p><br><p><em>serverless.yml:</em></p><br><pre class="lang-yaml prettyprint-override"><code>    ...<br>    alarms:<br>      - functionErrors<br>    Conditions:<br>      isProdOrUAT: {&quot;Fn::Or&quot;: [{&quot;${opt:stage}&quot;: &quot;prod&quot;}; {&quot;${opt:stage}&quot;: &quot;uat&quot;}]} <br>functions:<br>  TestApi:<br>    handler: test.api.StreamLambdaHandler::handleRequest<br>    description: Lambda function for the Test API<br>    timeout: 20<br>    custom: !If [isProdOrUAT; &quot;${file(./provisionedconcurrency.yml)}&quot;; !Ref &quot;AWS::NoValue&quot;]<br></code></pre><br><p><em>provisionedconcurrency.yml</em></p><br><pre><code>provisionedConcurrency: &quot;${ssm:/${opt:stage}/TestsServiceProvisionedConcurrency}&quot;<br></code></pre><br><p>In this case what happens is; it imports the file successfully; build also passes. But there is no provisioned concurrency attached in the lambda configuration for UAT and Prod.</p><br><p><em>serverless.yml</em></p><br><pre class="lang-yaml prettyprint-override"><code> ...<br>    alarms:<br>      - functionErrors<br>    Conditions:<br>      isProdOrUAT: {&quot;Fn::Or&quot;: [{&quot;${opt:stage}&quot;: &quot;prod&quot;}; {&quot;${opt:stage}&quot;: &quot;uat&quot;}]} <br>functions:<br>  TestApi:<br>    handler: test.api.StreamLambdaHandler::handleRequest<br>    description: Lambda function for the Test API<br>    timeout: 20<br>    provisionedConcurrency: !If [isProdOrUAT; &quot;${ssm:/${opt:stage}/TestsServiceProvisionedConcurrency}&quot;; !Ref &quot;AWS::NoValue&quot;]<br></code></pre><br><p>In this case getting the following error</p><br><pre><code>Error --------------------------------------------------<br>  Error: The CloudFormation template is invalid: [/Resources/TestApiProvConcLambdaAlias/Type/ProvisionedConcurrencyConfig/ProvisionedConcurrentExecutions] 'null' values are not allowed in templates <br></code></pre><br><p>Checked the <code>serverless-state.json</code>; found this to be there.</p><br><pre class="lang-json prettyprint-override"><code>    &quot;Name&quot;: &quot;provisioned&quot;;<br>      &quot;ProvisionedConcurrencyConfig&quot;: {<br>        &quot;ProvisionedConcurrentExecutions&quot;: null<br>      }<br></code></pre><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.0,0.0,<h3>Need help understanding why my s3 bucket always has 4xx errors?</h3><p>So I'm montioring and s3 bucket that is accessed through an SFTP server on AWS. I'm monitoring the 4xx error request metric and have set up an alarm to sum up all 4xx errors in a 24 hour period and check if its below 20. The issue I'm having is it seems that even when the s3 bucket isn't being used I have a couple of 4xx errors on a daily basis. And I'm not sure why. If anyone has any idea why this might be any help would be highly appreciated. Thanks!</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>ERROR :MalformedCertificate: Unable to parse certificate. Please ensure the certificate is in PEM format</h3><p>Hey so I import my  ssl certificate to IAM<br>I opened .key and .crt files in notepad which displayed RSA format which is similar to .pem yet I am getting the above mentioned error. Kindly let me know how to do the process</p><br>
1.0,1.0,0.0,0.0,0.0,0.0,0.0,<h3>Amazon Elastic Search Custom Endpoint</h3><p>Im probably not understanding something here and wanted to ask; if somebody could explain.</p><br><p>I set up an AWS ES domain and created a custom endpoint for it. I provided a ACM certificate which I uploaded upfront and created the domain with it.</p><br><p>Lets say; the custom endpoint should be a subdomain of my existing ROUTE 53 Domain (eg. logging.my-r53-hosted.domain)<br>I didnt do anything at Route 53; yet; just provide an imaginary subdomain as endpoint(example above).</p><br><p>Then what ? .....<br>When I create this subdomain on Route 53 as CNAME which points to the autocreated endpoint of AWS ES (eg. vpc-logging-72354762589234579.es.amazonaws.com) and curl the custom endpoint (logging.my-r53-hosted.domain) Im getting just the aws certificate back; but not the one i provided before. That gives me a self signed cert warning. I can circumvent this with --insecure; but thats obviously not what I want.</p><br><p>Can anybody explain how to do it right ?</p><br><p>Thanks all and again thanks for your patience :-)</p><br><p>Greetings!</p><br>
0.0,0.0,0.0,0.0,1.0,0.6666666666666666,0.0,<h3>Django app deployment in AWS Beanstalk - Error after deployment</h3><p>I have a django app deployed to AWS beanstalk. The first version was fine after following the <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-django.html" rel="nofollow noreferrer">official documentation</a>.</p><br><p>when I deploy; I get this error on the terminal and my eb status health is Red.</p><br><p>I follow all the instructions but don't know what is the problem.</p><br><p>and the 502 Bad Gateway occure in my page.</p><br><blockquote><br><p>2020/12/17 15:03:40.884211 [ERROR] An error occurred during execution of command [app-deploy] - [StageApplication]. Stop running the command. Error: chown /var/app/staging/unifolio-src/bin/python: no such file or directory</p><br></blockquote><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>Fn::ImpotValue not woking in custom ressource</h3><p><strong>Problem:</strong> I'm experiencing a bizarre issue with a <code>cloudformation</code> intrinsic function within a  custom resource. When I use <code>!ImportValue</code> my template passes and deploys successfully; but if I switch to using <code>Fn::ImportValue</code> I get the following error message:</p><br><pre><code>Template format error: YAML not well-formed.<br></code></pre><br><p>For completeness the this is the resource in question:</p><br><pre><code># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#<br># Custom Ressources                                                    #<br># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#<br>LambdaEmptyArtifactBucket:<br>  DependsOn:<br>  - ArtifactsBucket<br>  Type: AWS::CloudFormation::CustomResource<br>  Properties: <br>    ServiceToken: !ImportValue util-s3-object-remover-lambda:us-east-1:Lambda:Arn ### &lt;&lt;&lt;--- WORKS<br>    # ServiceToken: Fn::ImportValue: util-s3-object-remover-lambda:us-east-1:Lambda:Arn ### &lt;&lt;&lt;--- DOES NOT WORKS<br>    BucketName: !Ref ArtifactsBucket<br></code></pre><br><p><strong>Question:</strong><br>Is there any way of using the long form of <code>ImportValue</code> within a custom resource?</p><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.3333333333333333,0.0,<h3>Trouble Deploying to Beanstalk</h3><p>On the beanstalk webpage I have it connected to a aws pipeline; and that is working okay. But when I deploy it; I am getting a severe health waring and this error.<br><a href="https://i.stack.imgur.com/TexF2.png" rel="nofollow noreferrer">enter image description here</a></p><br><p>Is there any way to fix this?</p><br><p>This is the package.js file.</p><br><pre><code>{<br>  &quot;name&quot;: &quot;food4allkids&quot;;<br>  &quot;version&quot;: &quot;1.0.0&quot;;<br>  &quot;description&quot;: &quot;&quot;;<br>  &quot;main&quot;: &quot;index.js&quot;;<br>  &quot;scripts&quot;: {<br>    &quot;test&quot;: &quot;echo \&quot;Error: no test specified\&quot; &amp;&amp; exit 1&quot;<br>  };<br>  &quot;keywords&quot;: [];<br>  &quot;author&quot;: &quot;&quot;;<br>  &quot;license&quot;: &quot;ISC&quot;;<br>  &quot;dependencies&quot;: {<br>    &quot;express&quot;: &quot;^4.17.1&quot;;<br>    &quot;fs&quot;: &quot;0.0.1-security&quot;;<br>    &quot;mysql&quot;: &quot;^2.18.1&quot;;<br>    &quot;nodemon&quot;: &quot;^2.0.12&quot;;<br>    &quot;path&quot;: &quot;^0.12.7&quot;<br>  };<br>  &quot;start&quot;:&quot;node server.js&quot;;<br>  &quot;engines&quot;: {<br>    &quot;node&quot;: &quot;&gt;=10.0.0&quot;;<br>    &quot;npm&quot;: &quot;&gt;=6.0.0&quot;<br>  }<br>  <br>}<br></code></pre><br><p>This is the server.js file</p><br><pre><code>const express = require('express')<br>const app = express();<br><br>app.get('/'; (req; res) =&gt; {<br>    res.send(&quot;Welcome to the Hone Page&quot;)<br>});<br><br>const port = process.env.port || 3000;<br>app.listen(port; () =&gt; {<br>    console.log(&quot;Hello&quot;)<br>});<br></code></pre><br><p>This is the log that I am getting<br><a href="https://pastebin.com/U8XjT235" rel="nofollow noreferrer">https://pastebin.com/U8XjT235</a></p><br>
0.0,0.0,0.0,0.0,0.6666666666666666,1.0,0.0,<h3>AWS Step Function: The JSONPath &#39;$.S3_BUCKET&#39; specified for the field &#39;Value.$&#39; could not be found</h3><p>Starting Step function with the following input</p><br><pre><code>[{\&quot;name\&quot;:\&quot;S3_BUCKET\&quot;;\&quot;value\&quot;:\&quot;test-bucket\&quot;};{\&quot;name\&quot;:\&quot;S3_KEY\&quot;;\&quot;value\&quot;:\&quot;key-name.txt\&quot;}]'&quot;<br></code></pre><br><p>What is the correct way to pass this to ECS's container? here is what I have so far under the step functions parameters</p><br><pre><code>      &quot;Overrides&quot;: {<br>            &quot;ContainerOverrides&quot;: [<br>                {<br>                  &quot;Name&quot;: &quot;test&quot;;<br>                  &quot;Environment&quot;: [<br>                  { &quot;Name&quot;: &quot;S3_BUCKET&quot;; &quot;Value.$&quot;: &quot;$.S3_BUCKET&quot;}<br>                   ]<br>                }<br>              ]<br>           }<br>      };<br></code></pre><br><p>Here is the error message I am getting:</p><br><blockquote><br><p>The JSONPath '$.S3_BUCKET' specified for the field 'Value.$' could not be found</p><br></blockquote><br>
0.0,0.0,0.6666666666666666,0.3333333333333333,0.6666666666666666,0.0,0.0,<h3>Name of device for additional volumes in EC2 are always xvdX?</h3><p>today I start to play around with ansible; amazon-ec2 and ebs. After the automation for the provisioning of a new ec2-instance with ubuntu works I try attach an additional volume to an instance by extend my command as follow:</p><br><pre><code>- name: Launch the new EC2 Instance<br>  ec2:<br>    aws_access_key: &quot;AAAAAAAAAAAAAAAABBBBBBBBBBBBBBBCCCCCCCCCCCCCC&quot;<br>    aws_secret_key: &quot;DDDDDDDDDDDDDDDDEEEEEEEEEEEEEEFFFFFFFFFFFFFF&quot;<br>    group: &quot;webserver&quot;<br>    instance_type: &quot;t2.micro&quot;<br>    image: &quot;ami: ami-0767046d1677be5a0&quot;<br>    wait: true <br>    region: &quot;eu-central-1&quot;<br>    keypair: &quot;my_keypair&quot;<br>    volumes:<br>      - delete_on_termination: yes<br>        device_name: &quot;/dev/sdd&quot;<br>        volume_size: 10<br>        volume_type: &quot;gp2&quot;<br>    count: &quot;1&quot;<br></code></pre><br><p>and expect an new volume with the device <code>/dev/sdd</code> but I get in the instance the device <code>/dev/xvdd</code><br>I can live with that but I would like to understand why? Because every documentation I see so far is that there should be a link from xvdd sdd but this link don't exists.<br>Maybe someone here can me explain why ansible (or aws?) ignored the device name is this depending on the kind of storage (or ami?)<br>best regards<br>Dan</p><br>
0.0,0.3333333333333333,0.0,0.0,0.0,0.3333333333333333,1.0,<h3>AWS Server is not sending email to outside the domain</h3><p>In our Magento website; it is not sending email to the outside domain. For example; a website is <strong>ads.com</strong>; and if a customer order a product &amp; customer mail id is test@gmail.com; but he didn't get email. But I am getting a copy of the email [myemail@ads.com]</p><br><p>If the customer's mail id is new@ads.com then the customer is getting an email. That means the only @ads.com only getting the email. Why this happens. Our website is hosted on an AWS server.</p><br><p>I just create a sample PHP file in domain root and write the mail function. But still; no email is going outside ads.com. Then I added</p><br><p><strong>&quot;-f &quot;.$from</strong> in the code then <strong>the mail is sending to any email id without any issue</strong>.</p><br><pre><code>$from='no-reply@ads.com';<br>$headers .= 'From: &lt;no-reply@ads.com&gt;' . &quot;\r\n&quot;;<br>mail($to;$subject;$message;$headers; &quot;-f &quot;.$from);<br></code></pre><br><p>Please help to solve the issue.</p><br><p>When I am sending emails to another email example <strong>gmail.com  or yahoo.com or anyotherdomain.com</strong> I am <strong>getting the following error</strong>.</p><br><blockquote><br><p>Jan 29 11:50:20 ip-232-11-99-372 sendmail[19122]: 10TBoIjU019120:<br>to=mygamil@gmail.com;<br>ctladdr=ec2-user@ip-232-11-99-372.ap-north-1.compute.internal<br>(500/500); delay=00:00:02; xdelay=00:00:02; mailer=esmtp; pri=171765;<br>relay=gmail-smtp-in.l.google.com. [74.125.24.27]; dsn=5.0.0;<br>stat=Service unavailable</p><br></blockquote><br>
0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.6666666666666666,0.0,<h3>Hosting several API&#39;s on a single EC2 instance and accessing them through a subdomain with HTTPS</h3><p>I am trying to host my entire portfolio which consists of 5 React + Node.js apps (including the portfolio itself) on AWS. For each project I am hosting the frontend on S3 and fronting them with a CloudFront distribution. I have set up the portfolio to be served from a Route53 custom domain I'll call <code>mydomain.com</code> which has HTTPS through an ACM SSL certificate. The other apps can use the website endpoint from their bucket. I want the frontends of all the apps to interact with the EC2 instance via calling <code>https://api.mydomain.com:${APP_PORT}/${ROUTE}</code>; where <code>APP_PORT</code> is whatever port the desired API is running at. I have installed Node.js on my instance; cloned 2 repos; and started the apps with PM2. Each app listens to a get request at <code>/</code> and returns <code>${APP_NAME} API working properly</code>. One is running on the port 5000 and the other on 5001. I have the following inbound rules in the security group attached to the instance:</p><br><div class="s-table-container"><br><table class="s-table"><br><thead><br><tr><br><th>Type</th><br><th>Protocol</th><br><th>Port range</th><br><th>Source</th><br></tr><br></thead><br><tbody><br><tr><br><td>HTTP</td><br><td>TCP</td><br><td>80</td><br><td>0.0.0.0/0</td><br></tr><br><tr><br><td>HTTP</td><br><td>TCP</td><br><td>80</td><br><td>::/0</td><br></tr><br><tr><br><td>SSH</td><br><td>TCP</td><br><td>22</td><br><td>0.0.0.0/0</td><br></tr><br><tr><br><td>SSH</td><br><td>TCP</td><br><td>22</td><br><td>::/0</td><br></tr><br><tr><br><td>Custom TCP</td><br><td>TCP</td><br><td>5000</td><br><td>0.0.0.0/0</td><br></tr><br><tr><br><td>Custom TCP</td><br><td>TCP</td><br><td>5000</td><br><td>::/0</td><br></tr><br><tr><br><td>HTTPS</td><br><td>TCP</td><br><td>443</td><br><td>0.0.0.0/0</td><br></tr><br><tr><br><td>HTTPS</td><br><td>TCP</td><br><td>443</td><br><td>::/0</td><br></tr><br><tr><br><td>Custom TCP</td><br><td>TCP</td><br><td>5001</td><br><td>0.0.0.0/0</td><br></tr><br><tr><br><td>Custom TCP</td><br><td>TCP</td><br><td>5001</td><br><td>::/0</td><br></tr><br></tbody><br></table><br></div><br><p>Currently; I can call each API through the public IPv4 DNS of the instance and the port; so <code>http://ec2-012-34-56-789.compute-1.amazonaws.com:5000/</code> and <code>http://ec2-012-34-56-789.compute-1.amazonaws.com:5001/</code> work. I want to make it work with <code>https://api.mydomain.com...</code> instead. From what I've seen I should use an application load balancer and then create an A record on my Route53 hosted zone.</p><br><p>Here's where I'm stuck.</p><br><p>I create an application load balancer with an internal-facing scheme; an IPv4 IP address type; and the following listeners:</p><br><div class="s-table-container"><br><table class="s-table"><br><thead><br><tr><br><th>Load balancer protocol</th><br><th>Load balancer port</th><br></tr><br></thead><br><tbody><br><tr><br><td>HTTP</td><br><td>80</td><br></tr><br><tr><br><td>HTTPS</td><br><td>443</td><br></tr><br><tr><br><td>HTTPS</td><br><td>5000</td><br></tr><br><tr><br><td>HTTPS</td><br><td>5001</td><br></tr><br></tbody><br></table><br></div><br><p>I leave the default VPC and check all the availability zones. In security settings I choose my ACM certificate; which covers <code>mydomain.com</code>; <code>www.mydomain.com</code>; and <code>*.mydomain.com</code>. Default security policy (<code>ELBSecurityPolicy-2016-08</code>). Next; the security group is the same I am using for my instance. Here's where I think I'm making a mistake: I create a new target groupof type instance; protocol HTTP and version HTTP1; and port 80. The health checks are performed through HTTP at <code>/</code>. I register my instance and clicking create.</p><br><p>I create an A record for <code>api.mydomain.com</code> on Route53 with as an alias that points to the application load balancer. The target group finished the health check and shows unhealthy. HTTP get requests to the load balancer DNS name or <code>api.mydomain.com</code> both followed by either port show <code>400 The plain HTTP request was sent to HTTPS port</code>. HTTPS requests show <code>Error: Hostname/IP does not match certificate's altnames...</code> and <code>502 Bad Gateway</code> respectively.</p><br>
1.0,0.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>How to make connection from Aws Glue Catalog tables to custom python shell script?</h3><p>I have some tables in aws glue data catalog which have been created by crawling the data from S3 buckets.I am writing my own python shell script to perform some data trasformations for data in those tables.But how can I make the connection to those tables in data catalog via python script?</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>Best Practice to load single fact table from multiple sources in Redshift</h3><p>Is there a best practice to load the fact data from multiple sources in redshift? Redshift doesn't have a partitioning concept; it does however have the distribution concept. With distribution; one would need to use the DELETE operation which is costly; and also would need to VACUUM the tables to reclaim the space. The requirement is that the fact tables need to be truncated and loaded for a source. In oracle; we would have created a partitioned table with source and truncated partitioned before loading the data. Looking for the best practices for the DW.</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>High throughput data storage</h3><p>I'm currently looking for the best solution for the following scenario:</p><br><ul><br><li>~100k - 1M writes/minute</li><br><li>~10k reads minute</li><br><li>~16kb write/read item size (JSON)</li><br></ul><br><p>I would love to use this to store time series data but would be satisfied with only storing the latest value for each item.</p><br><p>I have used the cost estimator for AWS serverless options such as dynamo and document store but the monthly fees were outrageous. I'm thinking RDS with Redis may be up to the task throughput-wise but not sure how time-series would work there. Any suggestions?</p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.0,0.0,<h3>Getting Access Denied when trying to copy file from one path to another within same bucket on AWS</h3><p>Trying to copy the file to a different location within same bucket</p><br><p>$ <strong>aws s3 cp s3://bucket1/files/qa/test/ac.png s3://bucket1/qa/temp/ac.png</strong></p><br><p><strong>copy failed</strong>: s3://bucket1/files/qa/test/ac.png to s3://bucket1/qa/temp/ac.png An error occurred (AccessDenied) when calling the CopyObject operation: <strong>Access Denied</strong></p><br>
0.0,0.0,0.6666666666666666,0.0,0.3333333333333333,0.6666666666666666,0.0,<h3>How to terminate instances automatically if the code deployment fails using aws</h3><p>I am using Blue Green Code Deployment to deploy instances to the application load balancer using auto-scaling group launch template. Is there anyway to terminate instances when the code deployment fails and doesn't get added to the target group specified in the code deployment configuration?</p><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>How to use aws java sdk to retrieve accessKeyId; secretAccesskey and sessionToken?</h3><p>our company have a Jenkins job; which need to input <strong>accessKeyId</strong>; <strong>secretAccesskey</strong> and <strong>sessionToken</strong> before build to production environment. However; developers need to follow a long long guideline to setup the aws client.. (and we cannot pull docker image from public repo)</p><br><p>do we have any method to build a spingboot web service; which just need developers input <strong>email address</strong> &amp; <strong>password</strong> then return <strong>accessKeyId</strong>; <strong>secretAccesskey</strong> and <strong>sessionToken</strong>?</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.6666666666666666,<h3>AWS Cloudtrail: Unable to find StartWorkspaces api in Cloudtrail</h3><p>I'm unable to find the StartWorkspaces api call in cloudtrail.</p><br><p>My goal is when a user starts a WorkSpace the server(ec2) should also get start at the same time. The approach I am taking is to create a custom event in cloudwatch and use Event type as AWS API call via cloud trail. Under specific operation use StartWorkspaces api to get the logon status.</p><br><p>Any pointer would be helpful.</p><br>
0.0,0.0,0.6666666666666666,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>Ansible and Amazon Linux 2: How can I use yum module with Python3?</h3><p>I use Ansible 2.9 to create EC2 instances with Amazon Linux 2. For some purposes I need Python3 on EC2.</p><br><p>So I use option <code>ansible_python_interpreter: &quot;/usr/bin/python3&quot;</code></p><br><p>But with this option module <strong>yum</strong> return error <code>pkg_mgr: yum msg: The Python 2 bindings for rpm are needed for this module. If you require Python 3 support use the `dnf` Ansible module instead.</code></p><br><p>But Amazon Lunux 2 doesn't work with <strong>dnf</strong>.</p><br><p>The same issue is described here <a href="https://stackoverflow.com/questions/62100869/ansible-error-the-python-2-bindings-for-rpm-are-needed-for-this-module">Ansible error: &quot;The Python 2 bindings for rpm are needed for this module&quot;</a>; and in other forums. Everywhere suggested solution is Python2.</p><br><p>Is there any way to use Python3 and yum? Or the only way is to use <strong>shell</strong> module instead?</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>how to decrypt private key with master key using aws kms</h3><p>anyone knows how to decrypt private key with master key using aws kms. I have tried  using aws kms decrypt but it has an error saying  security token invalid. And also is aws secret access key and access key id in aws configure a must?<br>this is my what i have tried</p><br><pre><code>aws kms decrypt  --region xxxxx --ciphertext-blob xxxx --output text &gt; xxxxxx<br></code></pre><br>
0.6666666666666666,0.0,0.0,0.6666666666666666,0.0,0.3333333333333333,0.0,<h3>What should I do to fix Sqoop if I am getting a java.lang.NoClassDefFoundError exception during export?</h3><p>I am attempting to export a Hive database table into a MySQL database table on an Amazon AWS cluster using the command:</p><br><pre><code>sqoop export --connect jdbc:mysql://database_hostname/universities --table 19_20 --username admin -P --export-dir '/final/hive/19_20' <br></code></pre><br><p>I am trying to export from the folder '/final/hive/19_20' which is the Hive output directory into a MySQL database 'universities'; table '19_20'.<br>In response I get:</p><br><pre><code>Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.<br>Please set $ACCUMULO_HOME to the root of your Accumulo installation.<br>SLF4J: Class path contains multiple SLF4J bindings.<br>SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]<br>SLF4J: Found binding in [jar:file:/usr/share/aws/redshift/jdbc/redshift-jdbc42-1.2.37.1061.jar!/org/slf4j/impl/StaticLoggerBinder.class]<br>SLF4J: Found binding in [jar:file:/usr/lib/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]<br>SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.<br>SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]<br>21/04/11 01:42:13 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7<br>Enter password:<br>21/04/11 01:42:18 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.<br>21/04/11 01:42:18 INFO tool.CodeGenTool: Beginning code generation<br>21/04/11 01:42:19 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `19_20` AS t LIMIT 1<br>21/04/11 01:42:19 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `19_20` AS t LIMIT 1<br>21/04/11 01:42:19 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce<br>/tmp/sqoop-hadoop/compile/8aac2b94e7d11dc02d064c8213465c05/_19_20.java:37: warning: Can't initialize javac processor due to (most likely) a class loader problem: java.lang.NoClassDefFoundError: com/sun/tools/javac/processing/JavacProcessingEnvironment<br>public class _19_20 extends SqoopRecord  implements DBWritable; Writable {<br>       ^<br>        at lombok.javac.apt.LombokProcessor.getJavacProcessingEnvironment(LombokProcessor.java:411)<br>        at lombok.javac.apt.LombokProcessor.init(LombokProcessor.java:91)<br>        at lombok.core.AnnotationProcessor$JavacDescriptor.want(AnnotationProcessor.java:124)<br>        at lombok.core.AnnotationProcessor.init(AnnotationProcessor.java:177)<br>        at lombok.launch.AnnotationProcessorHider$AnnotationProcessor.init(AnnotationProcessor.java:73)<br>        at com.sun.tools.javac.processing.JavacProcessingEnvironment$ProcessorState.&lt;init&gt;(JavacProcessingEnvironment.java:508)<br>        at com.sun.tools.javac.processing.JavacProcessingEnvironment$DiscoveredProcessors$ProcessorStateIterator.next(JavacProcessingEnvironment.java:605)<br>        at com.sun.tools.javac.processing.JavacProcessingEnvironment.discoverAndRunProcs(JavacProcessingEnvironment.java:698)<br>        at com.sun.tools.javac.processing.JavacProcessingEnvironment.access$1800(JavacProcessingEnvironment.java:91)<br>        at com.sun.tools.javac.processing.JavacProcessingEnvironment$Round.run(JavacProcessingEnvironment.java:1043)<br>        at com.sun.tools.javac.processing.JavacProcessingEnvironment.doProcessing(JavacProcessingEnvironment.java:1184)<br>        at com.sun.tools.javac.main.JavaCompiler.processAnnotations(JavaCompiler.java:1170)<br>        at com.sun.tools.javac.main.JavaCompiler.compile(JavaCompiler.java:856)<br>        at com.sun.tools.javac.main.Main.compile(Main.java:523)<br>        at com.sun.tools.javac.api.JavacTaskImpl.doCall(JavacTaskImpl.java:129)<br>        at com.sun.tools.javac.api.JavacTaskImpl.call(JavacTaskImpl.java:138)<br>        at org.apache.sqoop.orm.CompilationManager.compile(CompilationManager.java:224)<br>        at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)<br>        at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:63)<br>        at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)<br>        at org.apache.sqoop.Sqoop.run(Sqoop.java:147)<br>        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)<br>        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)<br>        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)<br>        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)<br>        at org.apache.sqoop.Sqoop.main(Sqoop.java:252)<br>  Caused by: java.lang.ClassNotFoundException: com.sun.tools.javac.processing.JavacProcessingEnvironment<br>        at java.lang.ClassLoader.findClass(ClassLoader.java:523)<br>        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)<br>        at lombok.launch.ShadowClassLoader.loadClass(ShadowClassLoader.java:530)<br>        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)<br>        ... 26 more<br>Note: /tmp/sqoop-hadoop/compile/8aac2b94e7d11dc02d064c8213465c05/_19_20.java uses or overrides a deprecated API.<br>Note: Recompile with -Xlint:deprecation for details.<br>1 warning<br>21/04/11 01:42:24 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/8aac2b94e7d11dc02d064c8213465c05/19_20.jar<br>21/04/11 01:42:24 INFO mapreduce.ExportJobBase: Beginning export of 19_20<br>21/04/11 01:42:24 INFO Configuration.deprecation: mapred.jar is deprecated. Instead; use mapreduce.job.jar<br>21/04/11 01:42:26 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead; use mapreduce.reduce.speculative<br>21/04/11 01:42:26 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead; use mapreduce.map.speculative<br>21/04/11 01:42:26 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead; use mapreduce.job.maps<br>21/04/11 01:42:26 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-6-179.ec2.internal/172.31.6.179:8032<br>21/04/11 01:42:26 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-6-179.ec2.internal/172.31.6.179:10200<br>21/04/11 01:42:28 INFO input.FileInputFormat: Total input files to process : 1<br>21/04/11 01:42:29 INFO input.FileInputFormat: Total input files to process : 1<br>21/04/11 01:42:29 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library<br>21/04/11 01:42:29 INFO lzo.LzoCodec: Successfully loaded &amp; initialized native-lzo library [hadoop-lzo rev 3fb854bbfdabadafad1fa2cca072658fa097fd67]<br>21/04/11 01:42:29 INFO mapreduce.JobSubmitter: number of splits:4<br>21/04/11 01:42:29 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead; use mapreduce.map.speculative<br>21/04/11 01:42:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1618090360850_0017<br>21/04/11 01:42:29 INFO conf.Configuration: resource-types.xml not found<br>21/04/11 01:42:29 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.<br>21/04/11 01:42:29 INFO resource.ResourceUtils: Adding resource type - name = memory-mb; units = Mi; type = COUNTABLE<br>21/04/11 01:42:29 INFO resource.ResourceUtils: Adding resource type - name = vcores; units = ; type = COUNTABLE<br>21/04/11 01:42:29 INFO impl.YarnClientImpl: Submitted application application_1618090360850_0017<br>21/04/11 01:42:29 INFO mapreduce.Job: The url to track the job: http://ip-172-31-6-179.ec2.internal:20888/proxy/application_1618090360850_0017/<br>21/04/11 01:42:29 INFO mapreduce.Job: Running job: job_1618090360850_0017<br>21/04/11 01:42:37 INFO mapreduce.Job: Job job_1618090360850_0017 running in uber mode : false<br>21/04/11 01:42:37 INFO mapreduce.Job:  map 0% reduce 0%<br>21/04/11 01:43:00 INFO mapreduce.Job:  map 100% reduce 0%<br>21/04/11 01:43:01 INFO mapreduce.Job: Job job_1618090360850_0017 failed with state FAILED due to: Task failed task_1618090360850_0017_m_000002<br>Job failed as tasks failed. failedMaps:1 failedReduces:0<br><br>21/04/11 01:43:01 INFO mapreduce.Job: Counters: 12<br>        Job Counters<br>                Failed map tasks=3<br>                Killed map tasks=1<br>                Launched map tasks=4<br>                Data-local map tasks=4<br>                Total time spent by all maps in occupied slots (ms)=3779136<br>                Total time spent by all reduces in occupied slots (ms)=0<br>                Total time spent by all map tasks (ms)=78732<br>                Total vcore-milliseconds taken by all map tasks=78732<br>                Total megabyte-milliseconds taken by all map tasks=120932352<br>        Map-Reduce Framework<br>                CPU time spent (ms)=0<br>                Physical memory (bytes) snapshot=0<br>                Virtual memory (bytes) snapshot=0<br>21/04/11 01:43:01 WARN mapreduce.Counters: Group FileSystemCounters is deprecated. Use org.apache.hadoop.mapreduce.FileSystemCounter instead<br>21/04/11 01:43:01 INFO mapreduce.ExportJobBase: Transferred 0 bytes in 34.8867 seconds (0 bytes/sec)<br>21/04/11 01:43:01 INFO mapreduce.ExportJobBase: Exported 0 records.<br>21/04/11 01:43:01 ERROR mapreduce.ExportJobBase: Export job failed!<br>21/04/11 01:43:01 ERROR tool.ExportTool: Error during export:<br>Export job failed!<br>        at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:445)<br>        at org.apache.sqoop.manager.SqlManager.exportTable(SqlManager.java:931)<br>        at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:80)<br>        at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99)<br>        at org.apache.sqoop.Sqoop.run(Sqoop.java:147)<br>        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)<br>        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)<br>        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)<br>        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)<br>        at org.apache.sqoop.Sqoop.main(Sqoop.java:252)<br></code></pre><br><p>Please let me know if this can be fixed and what to do to fix it.</p><br>
0.0,0.0,0.0,0.0,0.6666666666666666,0.0,1.0,<h3>What is the reason for these concepts in AWS (Billing panel)</h3><p>Normally my monthly consumptions go from 25 to 30 dollars but these last two months it has doubled and reviewing the concepts I can see the following; do you know the reason for this if no change has been made?</p><br><p><img src="https://i.stack.imgur.com/SOldc.png" alt="Image 1" /></p><br><p><img src="https://i.stack.imgur.com/qZEdn.png" alt="Image 2" /></p><br><p><img src="https://i.stack.imgur.com/nGEXl.png" alt="Image 3" /></p><br><p><img src="https://i.stack.imgur.com/W4EMR.png" alt="Image 4" /></p><br>
0.0,0.0,0.6666666666666666,0.0,1.0,0.0,0.0,<h3>AWS errors LimitExceededException and EntityAlreadyExistsException</h3><p>I have been having problem deploying my python project unto aws<br>It has been giving me the error <code>NotAuthorizedError: Operation Denied. Access Denied</code><br>But on getting into amazon I saw these errors. These are just the summary</p><br><pre><code>{&quot;Records&quot;:[<br>{&quot;eventName&quot;:&quot;AddRoleToInstanceProfile&quot;;<br>&quot;errorCode&quot;:&quot;LimitExceededException&quot;;<br>&quot;errorMessage&quot;:&quot;Cannot exceed quota for InstanceSessionsPerInstanceProfile: 1&quot;;<br>&quot;requestParameters&quot;:{<br>    &quot;instanceProfileName&quot;:&quot;aws-elasticbeanstalk-ec2-role&quot;;<br>    &quot;roleName&quot;:&quot;aws-elasticbeanstalk-ec2-role&quot;}};<br>{&quot;eventName&quot;:&quot;CreateRole&quot;;<br>&quot;errorCode&quot;:&quot;EntityAlreadyExistsException&quot;;<br>&quot;errorMessage&quot;:&quot;Role with name aws-elasticbeanstalk-ec2-role already exists.&quot;;<br>&quot;requestParameters&quot;:{&quot;roleName&quot;:&quot;aws-elasticbeanstalk-ec2-role&quot;;}};<br>{&quot;eventName&quot;:&quot;CreateInstanceProfile&quot;;<br>&quot;errorCode&quot;:&quot;EntityAlreadyExistsException&quot;;<br>&quot;errorMessage&quot;:&quot;Instance Profile aws-elasticbeanstalk-ec2-role already exists.&quot;}]}<br></code></pre><br><p>Please how can i overcome this problems since I am doing it programmatically through ebcli and I am not creating it manually<br>Any help would be appreciated</p><br>
0.0,0.0,1.0,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>Terraform data aws_lambda_function.existing throws Error: error getting Lambda Function (MyLambda): ResourceNotFoundException: Function not found</h3><p>We want to check whether a Lambda exists or not. So we can provide it as a parameter for a count to do some other stuff. However; when we are running the <code>terraform plan</code> and the lambda doesn't exist we get a fatal exception 404 ResourceNotFoundException.</p><br><pre><code>locals { <br>  #Try 1 fails<br>  myLambda_exist         = data.aws_lambda_function.existing != null<br>  #Try 2 fails<br>  myLambda_exist         = try(data.aws_lambda_function.existing; false)<br>  #Try 3 fails<br>  myLambda_exist         = can(data.aws_lambda_function.existing)<br>}<br><br>data &quot;aws_lambda_function&quot; &quot;existing&quot; {<br>  function_name = &quot;MyLambda&quot;<br>}<br><br></code></pre><br><p>Exception</p><br><pre><code>Error: error getting Lambda Function (MyLambda): ResourceNotFoundException: Function not found: arn:aws:lambda:region:XXXXXX:function:MyLambda<br>{<br>  RespMetadata: {<br>    StatusCode: 404;<br>    RequestID: &quot;12345&quot;<br>  };<br>  Message_: &quot;Function not found: arn:aws:lambda:region:XXXXXX:function:MyLambda&quot;;<br>  Type: &quot;User&quot;<br>}<br><br></code></pre><br><p>In this case; it's ok not having the lambda created yet!</p><br><p>Versions:</p><br><pre><code>terraform {<br>  required_version = &quot;=0.14.5&quot;<br><br>  required_providers {<br>    aws = {<br>      source  = &quot;hashicorp/aws&quot;<br>      version = &quot;=3.11&quot;<br>    }<br>  }<br>}<br><br></code></pre><br>
0.0,0.0,0.0,0.0,0.0,0.3333333333333333,1.0,<h3>AWS-IoT-SDK-JS-v2 connection problem - AWS CRT binary not present in any of the following locations</h3><p>I'm trying to connect to AWS IoT Core via aws-iot-sdk-js-v2 and receiving the following error when I'm running the <a href="https://github.com/aws/aws-iot-device-sdk-js-v2/tree/main/samples#nodepub_sub" rel="nofollow noreferrer">PubSub example</a> as described below:</p><br><pre><code>/home/pi/aws-iot-device-sdk-js-v2/node_modules/aws-crt/dist/native/binding.js:60<br>    throw new Error(&quot;AWS CRT binary not present in any of the following locations:\n\t&quot; + search_paths.join('\n\t'));<br>    ^<br><br>Error: AWS CRT binary not present in any of the following locations:<br>        /home/pi/aws-iot-device-sdk-js-v2/node_modules/aws-crt/dist/bin/native/aws-crt-nodejs<br>        /home/pi/aws-iot-device-sdk-js-v2/node_modules/aws-crt/dist/bin/linux-arm/aws-crt-nodejs<br>    at Object.&lt;anonymous&gt; (/home/pi/aws-iot-device-sdk-js-v2/node_modules/aws-crt/dist/native/binding.js:60:11)<br>    at Module._compile (internal/modules/cjs/loader.js:1063:30)<br>    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1092:10)<br>    at Module.load (internal/modules/cjs/loader.js:928:32)<br>    at Function.Module._load (internal/modules/cjs/loader.js:769:14)<br>    at Module.require (internal/modules/cjs/loader.js:952:19)<br>    at require (internal/modules/cjs/helpers.js:88:18)<br>    at Object.&lt;anonymous&gt; (/home/pi/aws-iot-device-sdk-js-v2/node_modules/aws-crt/dist/native/crt.js:22:35)<br>    at Module._compile (internal/modules/cjs/loader.js:1063:30)<br>    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1092:10)<br></code></pre><br><p><strong>I have a Raspberry Pi 3B+</strong></p><br><pre><code>PRETTY_NAME=&quot;Raspbian GNU/Linux 10 (buster)&quot;<br>NAME=&quot;Raspbian GNU/Linux&quot;<br>VERSION_ID=&quot;10&quot;<br>VERSION=&quot;10 (buster)&quot;<br>VERSION_CODENAME=buster<br>ID=raspbian<br>ID_LIKE=debian<br>HOME_URL=&quot;http://www.raspbian.org/&quot;<br>SUPPORT_URL=&quot;http://www.raspbian.org/RaspbianForums&quot;<br>BUG_REPORT_URL=&quot;http://www.raspbian.org/RaspbianBugs&quot;<br></code></pre><br><ul><br><li>Node: 14.15.3</li><br><li>aws-iot-device-sdk-v2: 1.5.2</li><br><li>aws-crt: 1.8.1</li><br></ul><br><p><strong>I tried the solutions here <a href="https://github.com/aws/aws-iot-device-sdk-js-v2/issues/119" rel="nofollow noreferrer">aws-iot-device-sdk-js-v2 #119</a> but nothing helped.</strong></p><br><p><strong>I followed <a href="https://docs.aws.amazon.com/iot/latest/developerguide/connecting-to-existing-device.html#gs-device-prereqs" rel="nofollow noreferrer">onnecting-to-existing-device</a> as described in AWS IoT but still; this error occurs.</strong></p><br><p><strong>The commands I ran (in order after removing all was services on my raspi)</strong> after trying the installed and updated in the following order:</p><br><pre><code>sudo apt-get update<br>sudo apt-get upgrade<br>sudo apt-get install cmake<br>sudo apt-get install libssl-dev<br>sudo apt-get install -y nodejs<br>restarted by sudo shutdown -r 0<br>cd ~<br>npm install aws-crt<br>npm install aws-iot-device-sdk-v2<br>cd ~<br>git clone https://github.com/aws/aws-iot-device-sdk-js-v2.git<br>cd ~/aws-iot-device-sdk-js-v2<br>npm install<br>cd ~<br>mkdir certs<br>(I copied the certs and changed the name accordingly...)<br>cd ~/aws-iot-device-sdk-js-v2/samples/node/pub_sub<br>npm install<br>node dist/index.js --topic topic_1 --root-ca ~/certs/Amazon-root-CA-1.pem --cert ~/certs/device.pem.crt --key ~/certs/private.pem.key --endpoint &lt;endpoint&gt;<br> (I swap the &lt;endpoint&gt; with my own....)<br></code></pre><br><p>by the way <a href="https://github.com/aws/aws-iot-device-sdk-python-v2/tree/main/samples" rel="nofollow noreferrer">the aws-iot-device-sdk-python-v2</a>  works; but I prefer using the node.js SDK...</p><br><p>I opened a <a href="https://github.com/aws/aws-iot-device-sdk-js-v2/issues/167" rel="nofollow noreferrer">new issue</a> on the aws-iot-device-sdk-js-v2 but I'm still waiting for an answer.</p><br><p>Please help.</p><br>
0.3333333333333333,0.3333333333333333,1.0,0.0,0.0,0.0,0.3333333333333333,<h3>Access CloudSearch from API Gateway only</h3><p>I would like to access CloudSearch only from API Gateway; because I don't like the idea of having public access to my CloudSearch endpoint. I tried adding an access policy:</p><br><pre><code>{<br>  &quot;Version&quot;: &quot;2012-10-17&quot;;<br>  &quot;Statement&quot;: [<br>    {<br>      &quot;Effect&quot;: &quot;Allow&quot;;<br>      &quot;Principal&quot;: {<br>        &quot;AWS&quot;: &quot;arn:aws:iam::*********:user/admin&quot;<br>      };<br>      &quot;Action&quot;: [<br>        &quot;cloudsearch:search&quot;;<br>        &quot;cloudsearch:suggest&quot;<br>      ]<br>    }<br>  ]<br>}<br></code></pre><br><p>When I try to access the CloudSearch endpoint from my browser I get <code>User: anonymous is not authorized to perform: cloudsearch:search</code>.</p><br><p>API Gateway gets <code>&quot;Request forbidden by administrative rules&quot;</code>.</p><br><p>My API Gateway endpoint is HTTP GET and the URI is set to my cloudsearch endpoint. Am I doing things correctly? How do people set this up usually; it's my first time using both services. I'm using CloudSearch for an autocomplete input field on a website.</p><br>
0.0,0.0,0.6666666666666666,0.0,0.3333333333333333,0.0,0.6666666666666666,<h3>How to install new relic on amazon linux</h3><p>I have have unzipped newrelic files onto amazon linux and have both the installer.sh and config_defaults.sh</p><br><p>I have the license key in the parameter store which I am able to call</p><br><p>I install newrelic with the following command</p><br><pre><code>sudo ./installer.sh GENERATE_CONFIG=true LICENSE_KEY=$APIKEY<br>sudo systemctl start newrelic-infra # to start the service<br></code></pre><br><p>Where APPIKEY comes from the parameter store.<br>However when I do<br>sudo systemctl start newrelic-infra</p><br><p>I get error message</p><br><pre><code>level=error msg=&quot;can't load configuration file&quot; component=&quot;New Relic Infrastructure Agent&quot; error=&quot;no license key; please add it to agent's config file or NRIA_LICENSE_KEY environment variable&quot;<br></code></pre><br><p>How can I make the agent recognize the license key?</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>aws ssm session - how do I set &quot;runAsDefaultuser&quot; to be a interactive parameter when starting a session</h3><p>I'm trying to create a document that I call and specific my IAM user as the user to connect with. This document will be a shared document that has a parameter &quot;runAsDefaultuser&quot;. It will default to a user that doesn't exist. This will force me to provide an argument like;<br>--parameters '{&quot;runAsDefaultUser&quot;: [&quot;joeschmo&quot;]}'.</p><br><p>My end goal is to allow users to login in as their IAM user via CLI through SSM. I've tried using tag's in the IAM user account. That only works when using a SSM session over the Web UI in the AWS Session Manager page. Doesn't work vi SSM CLI. The SSM documents override this.</p><br><p>I can't set this as a parameter. It will only accept it as a hardcoded value. Same with &quot;runAsEnabled&quot;.</p><br><p>I get this error when I try to set it as a parameter using the &quot;aws ssm update-document or create-document command.</p><br><pre><code>An error occurred (InvalidDocumentContent) when calling the UpdateDocument operation: DefaultUser: {{runAsDefaultUser}} is invalid<br></code></pre><br><p>I'm using this page as a reference.<br><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/getting-started-create-preferences-cli.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/systems-manager/latest/userguide/getting-started-create-preferences-cli.html</a></p><br><p>Here is the example document that I am using.</p><br><pre><code>{<br>   &quot;schemaVersion&quot;:&quot;1.0&quot;;<br>   &quot;description&quot;:&quot;Session Document Parameter Example JSON Template&quot;;<br>   &quot;sessionType&quot;:&quot;Standard_Stream&quot;;<br>   &quot;parameters&quot;:{<br>      &quot;s3BucketName&quot;:{<br>         &quot;type&quot;:&quot;String&quot;;<br>         &quot;default&quot;:&quot;&quot;<br>      };<br>      &quot;s3KeyPrefix&quot;:{<br>         &quot;type&quot;:&quot;String&quot;;<br>         &quot;default&quot;:&quot;&quot;<br>      };<br>      &quot;s3EncryptionEnabled&quot;:{<br>         &quot;type&quot;:&quot;String&quot;;<br>         &quot;default&quot;:&quot;false&quot;<br>      };<br>      &quot;cloudWatchLogGroupName&quot;:{<br>         &quot;type&quot;:&quot;String&quot;;<br>         &quot;default&quot;:&quot;&quot;<br>      };<br>      &quot;cloudWatchEncryptionEnabled&quot;:{<br>         &quot;type&quot;:&quot;String&quot;;<br>         &quot;default&quot;:&quot;false&quot;<br>      };<br>      &quot;runAsDefaultUser&quot;:{<br>         &quot;type&quot;:&quot;String&quot;;<br>         &quot;default&quot;:&quot;nobody&quot;<br>      }<br>   };<br>   &quot;inputs&quot;:{<br>      &quot;s3BucketName&quot;:&quot;{{s3BucketName}}&quot;;<br>      &quot;s3KeyPrefix&quot;:&quot;{{s3KeyPrefix}}&quot;;<br>      &quot;s3EncryptionEnabled&quot;:&quot;{{s3EncryptionEnabled}}&quot;;<br>      &quot;cloudWatchLogGroupName&quot;:&quot;{{cloudWatchLogGroupName}}&quot;;<br>      &quot;cloudWatchEncryptionEnabled&quot;:&quot;{{cloudWatchEncryptionEnabled}}&quot;;<br>      &quot;kmsKeyId&quot;:&quot;&quot;;<br>      &quot;runAsEnabled&quot;: true<br>      &quot;runAsDefaultUser&quot;:&quot;{{runAsDefaultUser}}&quot;;<br>      &quot;shellProfile&quot;: {<br>        &quot;windows&quot;: &quot;&quot;;<br>        &quot;linux&quot;: &quot;bash&quot;<br>      }<br>   }<br>}<br></code></pre><br><p>The command that I want to use:</p><br><pre><code>aws ssm start-session --target i-ThisIsObviouslyMadeUp \<br>--document-name Custom-SessionManagerRunShell \<br>--parameters '{&quot;runAsDefaultUser&quot;: [&quot;joeschmo&quot;]}'<br></code></pre><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>AWS EMR Python process is abnormally exited error</h3><p>When I trying to run the following code in AWS EMR Zeppelin with Spark:</p><br><pre><code>%spark.pyspark<br>knn_imputer = KNNImputer()<br>pre_stage_1.iloc[:; :] = knn_imputer.fit_transform(pre_stage_1.toPandas())<br></code></pre><br><p>I got the following error:</p><br><pre><code>Python process is abnormally exited; please check your code and log.<br></code></pre><br><p>pre_stage_1 is a dataset with about 1000000 rows.</p><br><p>How can I fix it?</p><br>
0.0,1.0,0.6666666666666666,0.0,0.0,0.0,0.0,<h3>Got an error after associating EIP to my NAT Gateway in Terraform with modules</h3><p>I'm trying to create an infrastructure in AWS using Terraform with modules. So I got 2 modules; one for my Nat Gateway and one for the EIP Creation and Association. In my main file; to associate the EIP to the NAT Gateway I use 2 output attributes of my NAT GW and EIP which are the ID of the NAT and the EIP ID; like this:</p><br><pre><code>module &quot;eip_assoc&quot; {<br>  source = &quot;../Modules/eip_assoc&quot;<br>  nat_gw_qa = module.NAT_GW.aws_nat_gateway_id_output<br>  nat_gw_association = module.EIP.eip_one_id_output<br>}<br></code></pre><br><p>Everything is ok with the rest of the infrastructure;  when I apply the main file it creates the resources in AWS and do the association of the EIP to the NAT GW but at the end of the consoles it shows an error like this:</p><br><p>&quot;Error: Error associating EIP: InvalidNetworkInterfaceId.Malformed: Invalid id: &quot;nat-0c15axxxxxxxxxx&quot; (expecting &quot;eni-...&quot;)&quot;</p><br><p>So ok; it says I need the eni-... output from the NAT GW instead of the Allocation_id; I change the output to be the network_interface_id which returns the ENI ID of the NAT GW; but after that I got another different error:</p><br><p>Error: Error associating EIP: AuthFailure: You do not have permission to access the specified resource.</p><br><p>In this case; same as before; it creates the infrastructure in AWS and it does the association of the EIP to the NAT GW with no problem at all; but after looking for that error message in Terraform Documentation it says &quot;Do not use network_interface to associate the EIP to aws_lb or aws_nat_gateway resources. Instead use the allocation_id available in those resources to allow AWS to manage the association; otherwise you will see AuthFailure errors.&quot;</p><br><p>So if I use the allocation ID it shows the error of expecting ENI ID; if use the ENI ID it says AuthFailure error; in both scenarios deploys the whole resources but got those error messages.</p><br>
0.0,0.3333333333333333,0.0,0.0,0.3333333333333333,0.3333333333333333,0.3333333333333333,<h3>Why updated features on my website not visible when i open in browser?</h3><p>I have hosted my node.js (server side) website on aws and using load balancer with that domain</p><br><p>I update the code on my local machine and use winscp to transfer files to aws server</p><br><p>But the updated changes in code are only reflected on the website if i open it in incognito mode of any browser</p><br><p>Changes are not visible if i open it in normal mode?</p><br><p>What could be going wrong?</p><br>
0.0,0.0,0.3333333333333333,0.0,0.3333333333333333,0.6666666666666666,0.0,<h3>Is there way to remove cloud resources ONLY using AWS Amplify?</h3><pre><code>amplify delete<br></code></pre><br><p>deletes both</p><br><ol><br><li>the provisioned cloud resources and</li><br><li>the local \amplify folder; including any development and work you might have done on graphql schema; lambdas etc.</li><br></ol><br><p>Is there a way to only deprovision the could resources in bulk from the command line that would cleanly preserve the local files. One way is to remove cloud resources one-by-one; but that's suboptimal and record of them being provisioned removes locally. Many thanks.</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Prevent users from signing up on their own with federated identity providers (FIP) but allow sign in with a FIP if added by an administrator</h3><p>I've set up a user pool in Amazon Cognito for my web application. The application is not meant to be public and only specific users are allowed to sign in. The policies of that user pool in the Amazon Console allow only administrators to create new users.</p><br><p>I've implemented sign in through Facebook and Google. Cognito does indeed let users sign into the application with these federated identity providers; which is great. However; it seems that anybody with a Facebook or Google account can sign themselves up now.</p><br><p>So; on one hand; people can not create their own user with regular Cognito credentials but; on the other hand; they can create a new user in Cognito if they use a federated identity provider.</p><br><p><strong>Is there a way to restrict signing into my application with Facebook or Google to only users that already exist in the user pool?</strong> That way; administrators would still be able to control who exactly can access the application. I would like to use the email shared by the federated identity provider to check if they are allowed to sign in.</p><br><p>The application is set up with CloudFront. I've written a Lambda that intercepts origin requests to check for tokens in cookies and authorize access based on the validity of the access token.</p><br><p>I would like to avoid writing additional code to prevent users to sign themselves up with Facebook or Google but if there is no other way; I'll update the Lambda.</p><br>
1.0,0.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>How To perform ETL operation on column level security in AWS glue job?</h3><p>I have table in one database which has column level security applied to it; now I am performing some ETL operation on this table using glue job and writing this new table into another database after running glue job I am getting below error<br>Py4JjavaError:An error occurred while calling 065.getDynamicFrame.java.lang.reflect.Invocation Target Exception</p><br><p>so my question is it is possible to perform ETL operation on column level security if yes than how can I do that plz inform me and give some reference.</p><br><p>Advance Thanks &amp; regards</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>Problems when writing parquet with timestamps prior to 1900 in AWS Glue 3.0</h3><p>When switching from Glue 2.0 to 3.0; which means also switching from Spark 2.4 to 3.1.1;<br>my jobs start to fail when processing timestamps prior to 1900 with this error:</p><br><pre><code>An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.<br>You may get a different result due to the upgrading of Spark 3.0: reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from Parquet INT96 files can be ambiguous; <br>as the files may be written by Spark 2.x or legacy versions of Hive; which uses a legacy hybrid calendar that is different from Spark 3.0+s Proleptic Gregorian calendar.<br>See more details in SPARK-31404.<br>You can set spark.sql.legacy.parquet.int96RebaseModeInRead to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during reading. <br>Or set spark.sql.legacy.parquet.int96RebaseModeInRead to 'CORRECTED' to read the datetime values as it is.<br></code></pre><br><p>I tried everything to set the <code>int96RebaseModeInRead</code> config in Glue; even contacted the Support; but it seems that currently Glue is overwriting that flag and you can not set it yourself.</p><br><p>If anyone knows a workaround; that would be great. Otherwise I will continue with Glue 2.0. and wait for the Glue dev team to fix this.</p><br>
0.0,0.3333333333333333,0.0,1.0,1.0,0.3333333333333333,0.3333333333333333,<h3>Local cache for each web server in different AZ - cache replication/eviction - Redis or other?</h3><p>I am running 3 Kubernetes worker nodes spread in different availability zones. Each worker node has own sets of pods which heavily relies on Redis cache.<br>At the moment I've got only single Redis cache instance which is hosted on one of the Kubernetes nodes.</p><br><p>That means that cache is local only to pods hosted on the single node where Redis resides; whereas the remaining ~2/3 cache traffic needs to be sent to different AZ which incurs small latency and huge bill for: <code>$0.010 per GB - regional data transfer - in/out/between EC2 AZs or using elastic IPs or ELB</code>. If I decide to increase Kubernetes workers to 5; than this would become even worse ~4/5.</p><br><p>I could set up separate Redis in each AZ; that would make Redis local for each worker node.<br>I am fine if cache is building up separately (no need to replicate cache sets); the problem is the cache eviction. If one AZ decides to delete key or replace it with newer one; that means that the remaining AZ works with stale cache.</p><br><p>That means that I need some simple cache replication / eviction capabilities. I don't have high consistency requirements. As long as cache gets eventually deleted or replaced with newer version; I am fine.</p><br><p>From what I understand this is not Redis replication or Redis cluster provides.<br>For instance with Redis Cluster you have the concept of <code>slots</code> and you receive the redirects:</p><br><blockquote><br><p>127.0.0.1:6379&gt; set key t1<br /><br>(error) MOVED 12539 172.23.0.4:6379</p><br></blockquote><br><p>This doesn't solve my traffic problem anyway.</p><br><p>What solution I could apply? I am huge fan of simplicity.</p><br><p>I've seen Dynomite which supports Redis protocol; which maybe would solve this issue.</p><br><p>Any other candidates?</p><br>
0.0,1.0,0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>aws_alb_target_group not dropping aws_alb_listener</h3><p>When I do a change in my <code>aws_alb_target_group</code> and terraform says will drop it; it fails because there is a listener:</p><br><pre><code>Target group '' is currently in use by a listener or a rule<br></code></pre><br><p>Listener is:</p><br><pre><code>resource &quot;aws_alb_listener&quot; &quot;https&quot; {<br>  depends_on        = [aws_alb_target_group.target-group]<br>  load_balancer_arn = aws_lb.tableau-lb.arn<br><br>  protocol = &quot;HTTPS&quot;<br>  port     = &quot;443&quot;<br><br>  ssl_policy      = &quot;&quot;<br>  certificate_arn = &quot;&quot;<br><br>  default_action {<br>    target_group_arn = aws_alb_target_group.target-group.arn<br>    type             = &quot;forward&quot;<br>  }<br></code></pre><br><p>And <code>aws_alb_target_group</code>:</p><br><pre><code>resource &quot;aws_alb_target_group&quot; &quot;target-group&quot; {<br>  name = &quot;${var.namespace}-group&quot;<br><br>  port        = 80<br>  protocol    = &quot;HTTP&quot;<br>  vpc_id      = var.vpc_id<br>  target_type = &quot;instance&quot;<br><br>  health_check {<br>    healthy_threshold   = var.health_check_healthy_threshold<br>    unhealthy_threshold = var.health_check_unhealthy_threshold<br>    timeout             = var.health_check_timeout<br>    interval            = var.health_check_interval<br>    path                = var.path<br>  }<br><br>  tags = {<br>    Name = var.namespace<br>  }<br><br>  lifecycle {<br>    create_before_destroy = false<br>  }<br><br>  depends_on = [aws_lb.tableau-lb]<br>}<br><br>  lifecycle {<br>    ignore_changes = [<br>      default_action.0.target_group_arn;<br>    ]<br>  }<br>}<br></code></pre><br><p>Is there a way to destroy fisrt the listener and then the target_group?<br>I though that with the depens_on should work but doesnt seems to have effect.</p><br><pre><code>An execution plan has been generated and is shown below.<br>Resource actions are indicated with the following symbols:<br>  + create<br>-/+ destroy and then create replacement<br><br>Terraform will perform the following actions:<br><br>  # module.ec2.aws_alb_target_group.target-group must be replaced<br>-/+ resource &quot;aws_alb_target_group&quot; &quot;target-group&quot; {<br>      ~ arn                                = &quot;&quot; -&gt; (known after apply)<br>      ~ arn_suffix                         = &quot;&quot; -&gt; (known after apply)<br>      ~ id                                 = &quot;&quot; -&gt; (known after apply)<br>      ~ load_balancing_algorithm_type      = &quot;round_robin&quot; -&gt; (known after apply)<br>        name                               = &quot;pro-tableau-group&quot;<br>      + preserve_client_ip                 = (known after apply)<br>      ~ protocol_version                   = &quot;HTTP1&quot; -&gt; (known after apply)<br>        tags                               = {<br>            &quot;Name&quot; = &quot;&quot;<br>        }<br>      ~ target_type                        = &quot;ip&quot; -&gt; &quot;instance&quot; # forces replacement<br>        # (8 unchanged attributes hidden)<br><br>      ~ health_check {<br>          ~ matcher             = &quot;200&quot; -&gt; (known after apply)<br>            # (8 unchanged attributes hidden)<br>        }<br><br>      ~ stickiness {<br>          ~ cookie_duration = 86400 -&gt; (known after apply)<br>          ~ enabled         = false -&gt; (known after apply)<br>          ~ type            = &quot;lb_cookie&quot; -&gt; (known after apply)<br>        }<br>    }<br><br>  # module.ec2.aws_lb_target_group_attachment.tableau-attachment will be created<br>  + resource &quot;aws_lb_target_group_attachment&quot; &quot;tableau-attachment&quot; {<br>      + id               = (known after apply)<br>      + port             = 80<br>      + target_group_arn = (known after apply)<br>      + target_id        = &quot;&quot;<br>    }<br><br>Plan: 2 to add; 0 to change; 1 to destroy.<br></code></pre><br>
0.3333333333333333,0.0,0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,<h3>get data of how results have been found in Amazon Cloudsearch</h3><p>as we use cloudsearch to find our documents and data; we have this issue that some of the data that are returned back to us; we have to know that how they've been found.</p><br><p>I know that we can specify which fields to search for them; but is there any way that amazon gives us a hint or some information that how the returned data has been found. on which fields it exists?</p><br><p>this can be really really useful information for us and affects the way we show data to our users.</p><br><p>I know amazon provides highlighting service;  but highlights change results; we don't want to change results or values; we just want to use this knowledge for backend purposes.</p><br>
0.0,0.3333333333333333,0.0,0.0,1.0,0.0,0.0,<h3>Assigning an AWS Elastic IP to an EC2 Instance in Lambda</h3><p>Is there a way to assign an elastic ip to a ec2 instance that was just made using cloud formation scripts in amazon aws? I'm not able to find any simple examples of how to &quot;get an elastic ip&quot; by it's tag; or any api references about whether or not this is even possible. I need to first get the elastic ip by it's tag; and then assign it to an existing instance in lambda.</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>Why is the option group not showing up in the AWS Backup &#39;Restore&#39; feature?</h3><p>I have created a simple on-demand backup of an Oracle RDS instance with AWS Backup service in the AWS dashboard. The backup shows in the UI and is presumably valid. When I click on the radio button next to the recovery point ID; and click on the 'restore' button; it takes me into the restore UI; as expected. Everything seems fine with the Instance Specifications; Settings; and Network and Security. When I get down to the 'Database Options' portion of the Restore dialog; I have no option group listed and the dropdown is blank (shown below and circled in red).</p><br><p><a href="https://i.stack.imgur.com/J0P2K.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/J0P2K.png" alt="enter image description here" /></a></p><br><p>There are four option groups available in this account and region; yet none show in the dropdown. I feel like I'm missing something fundamental to this puzzle.</p><br><p><strong>UPDATE</strong>: I successfully restored from the CLI (where you explicitly supply the option group in the metadata). Having posed this question to AWS and having heard nothing back yet; I can only conclude that this feature isn't yet working in the UI; or there is a bug in the UI.</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Is it necessary to use triggers in CUSTOM_AUTH of AWS cognito?</h3><p>I want to create OTP based authentication using AWS cognito (CUSTOM_AUTH); all resources I saw were having 3 triggers(defineauth; createauth etc). Can I implement custom auth without using them ?</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>S3 Bucket objects starting at a certain date</h3><p>I need to get S3 objects from a bucket from a certain date on; otherwise i would just reread all the objects again and again. Is this option available without actually reading the object.<br>Any other suggesting; how to avoid rereading of objects?</p><br>
0.0,0.0,1.0,0.6666666666666666,0.0,0.0,0.0,<h3>Which permissions are required for a technical Terraform user in AWS?</h3><p>I like to run my terraform scripts as a technical user in the AWS-cloud - but <code>terraform init</code> returns a <code>Error refreshing state: AccessDenied</code>.</p><br><p>Here is what I did: I created a user in AWS (IAM) and gave it the &quot;AdministratorAccess&quot; and &quot;AmazonS3FullAccess&quot; privilege; saved the <code>ACCESS_KEY</code> and <code>SECRET_ACCESS_KEY</code>and put those as env-valiables in my local console</p><br><pre><code>   set AWS_ACCESS_KEY_ID=..<br>   set AWS_SECRET_ACCESS_KEY=...<br></code></pre><br><p>For more infos I set the <code>DEBUG</code> flag in terraform; too</p><br><pre><code>    set TF_LOG=DEBUG<br></code></pre><br><p>then</p><br><pre><code>    terraform init<br></code></pre><br><p>results in a successful first <code>POST</code> to <code>sts/GetCallerIdentity</code>:</p><br><pre><code><br>&lt;SNIP&gt;<br>2021-07-26T09:14:10.232+0200 [DEBUG] [aws-sdk-go] DEBUG: Request sts/GetCallerIdentity Details:<br>---[ REQUEST POST-SIGN ]-----------------------------<br>POST / HTTP/1.1<br>Host: sts.amazonaws.com<br>User-Agent: aws-sdk-go/1.37.0 (go1.16.4; windows; amd64) APN/1.0 HashiCorp/1.0 Terraform/1.0.3<br>Content-Length: 43<br>Authorization: AWS4-HMAC-SHA256 Credential=&lt;SNIP&gt;/20210726/us-east-1/sts/aws4_request; SignedHeaders=content-length;content-type;host;x-amz-date; Signature=&lt;SNIP&gt;<br>Content-Type: application/x-www-form-urlencoded; charset=utf-8<br>X-Amz-Date: 20210726T071410Z<br>Accept-Encoding: gzip<br><br>Action=GetCallerIdentity&amp;Version=2011-06-15<br>-----------------------------------------------------<br>2021-07-26T09:14:10.971+0200 [DEBUG] [aws-sdk-go] DEBUG: Response sts/GetCallerIdentity Details:<br>---[ RESPONSE ]--------------------------------------<br>HTTP/1.1 200 OK<br>Connection: close<br>Content-Length: 416<br>Content-Type: text/xml<br>Date: Mon; 26 Jul 2021 07:14:10 GMT<br>X-Amzn-Requestid: &lt;SNIP&gt;<br></code></pre><br><p>But the next <code>GET</code> to <code>s3/ListObjects</code> is <code>403 - Forbidden</code></p><br><pre><code>---[ REQUEST POST-SIGN ]-----------------------------<br>GET /?max-keys=1000&amp;prefix=env%3A%2F HTTP/1.1<br>Host: &lt;SNIP&gt;.s3.eu-central-1.amazonaws.com<br>User-Agent: aws-sdk-go/1.37.0 (go1.16.4; windows; amd64) APN/1.0 HashiCorp/1.0 Terraform/1.0.3<br>Authorization: AWS4-HMAC-SHA256 Credential=&lt;SNIP&gt;/20210726/eu-central-1/s3/aws4_request; SignedHeaders=host;x-amz-content-sha256;x-amz-date; Signature=&lt;SNIP&gt;<br>X-Amz-Content-Sha256: &lt;SNIP&gt;<br>X-Amz-Date: 20210726T071410Z<br>Accept-Encoding: gzip<br><br><br>-----------------------------------------------------<br>2021-07-26T09:14:11.054+0200 [DEBUG] [aws-sdk-go] DEBUG: Response s3/ListObjects Details:<br>---[ RESPONSE ]--------------------------------------<br>HTTP/1.1 403 Forbidden<br>Connection: close<br>Transfer-Encoding: chunked<br>Content-Type: application/xml<br>Date: Mon; 26 Jul 2021 07:14:10 GMT<br>Server: AmazonS3<br>X-Amz-Bucket-Region: eu-central-1<br>X-Amz-Id-2: &lt;SNIP&gt;<br>X-Amz-Request-Id: &lt;SNIP&gt;<br></code></pre><br><p>I explain that as suficcient access rights for log-in but not for more. Strange thing: the &quot;AmazonS3FullAccess&quot; explicitly allows <code>ListObjects</code>...</p><br><p>I found no info about the required rights in the docs - can you help?</p><br><p><strong>EDIT</strong></p><br><p>Thanks for your suggestions; you pointed me in the right direction:<br>Turned out I was trying to access a S3 bucket that was created under a different AWS account than the IAM tech user. Unfortunately you cannot create the same S3 bucket name again in a different accout. So I had to create a different one with another name. Next I had to change the <code>bucket</code> var in my script depending on target system. I learned that you cannot pass variables in the <code>terraform / backend</code> section of your <code>.tf</code> files... Took me some time to find out that you can change the S3 bucket via a CLI parameter:</p><br><pre><code>terraform init -backend-config=&quot;bucket=...&quot;<br></code></pre><br><p>Hope someone finds this useful - cheers!</p><br>
0.0,0.3333333333333333,0.0,0.0,1.0,0.0,0.3333333333333333,<h3>Frequently getting - There was a problem connecting to your instance (Not first time setup)</h3><p>I'm frequently receiving the error <strong>There was a problem connecting to your instance</strong> when connecting via AWS console. I have a database instance running there and the application connecting to this server for DB service goes down!</p><br><p>This is neither a new instance having a network config error nor this has been purposefully isolated. Considering a day; I am able to connect to this instance for about 10hours; and for the rest; I get this error! So this is definitely not a configuration error.<br>Any advice/suggestion to debug this further for a permanent fix please? TIA.</p><br><p><a href="https://i.stack.imgur.com/lNQBA.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/lNQBA.png" alt="enter image description here" /></a></p><br>
0.0,0.0,0.0,0.6666666666666666,1.0,0.0,0.3333333333333333,<h3>AWS Kubernetes Cluster - WordPress Error establishing a database connection</h3><p>I am novice person to  Kubernetes and trying to deploying WordPress and MySQL using the Kubernetes pod containers but its throwing the error &quot;Error establishing a database connection&quot; while running the Kubernetes .</p><br><p><a href="https://i.stack.imgur.com/jn7W4.png" rel="nofollow noreferrer">Wordpress Error</a></p><br><p><a href="https://i.stack.imgur.com/jn7W4.png" rel="nofollow noreferrer">Overall Kubect status</a></p><br><p><a href="https://i.stack.imgur.com/Ht7YZ.png" rel="nofollow noreferrer">AWS Inbound Rules</a></p><br><p><strong>mysql-deployment.yaml</strong></p><br><pre><code>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: mysql-deployment<br>  labels:<br>    app: mysql<br>spec:<br>  replicas: 1<br>  selector:<br>    matchLabels:<br>      app: mysql<br>  template:<br>    metadata:<br>      labels:<br>        app: mysql<br>    spec:<br>      containers:<br>      - name: mysql<br>        image: mysql:5.7<br>        ports:<br>        - containerPort: 80<br>        env:<br>        - name: MYSQL_ROOT_PASSWORD<br>          value: DEVOPS1<br>        - name: MYSQL_USER<br>          value: wpuser<br>        - name: MYSQL_PASSWORD<br>          value: DEVOPS12345<br>        - name: MYSQL_DATABASE<br>          value: wpdb<br></code></pre><br><p><strong>mysql-service.yaml</strong></p><br><pre><code>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: mysql-service<br>spec:<br>  selector:<br>    app: mysql<br>  ports:<br>    - protocol: TCP<br>      port: 3306<br>      targetPort: 3306<br></code></pre><br><p><strong>wordpress-deployment.yaml</strong></p><br><pre><code>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: wordpress-deployment<br>  labels:<br>    app: wordpress<br>spec:<br>  replicas: 3<br>  selector:<br>    matchLabels:<br>      app: wordpress<br>  template:<br>    metadata:<br>      labels:<br>        app: wordpress<br>    spec:<br>      containers:<br>      - name: wordpress<br>        image: wordpress<br>        ports:<br>        - containerPort: 80<br>        env:<br>        - name: WORDPRESS_DB_HOST<br>          value: mysql-service<br>        - name: WORDPRESS_DB_USER<br>          value: wpuser<br>        - name: WORDPRESS_DB_PASSWORD<br>          value: wpdb<br>        - name: WORDPRESS_DEBUG<br>          value: &quot;1&quot;<br></code></pre><br><p><strong>wp-service.yaml</strong></p><br><pre><code>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: wordpress-service<br>spec:<br>  type: NodePort<br>  selector:<br>    app: wordpress<br>  ports:<br>    - protocol: TCP<br>      port: 80<br>      targetPort: 80<br></code></pre><br><p>Also I opened same thread on Kubernetes forum .  <a href="https://discuss.kubernetes.io/t/error-establishing-a-database-connection-error-kubernates-wordpress-mysql/15407/15" rel="nofollow noreferrer">Click Here</a> to view it</p><br>
0.0,0.6666666666666666,0.0,0.0,0.0,1.0,0.0,<h3>Getting cache issue with service worker file and cloud-front as service worker is not initiating on new build release</h3><p>I am looking for a suitable solution where my web app auto reloads on every update of new build release. i.e user should not have to manually reloads window for new changes to be reflected.</p><br><p>I am using react with service workers &amp; using S3 for deployment and cloud-front for distribution.</p><br><p>Below is the script from package.json where I have called sw-build file on npm run build command.</p><br><pre><code>&quot;scripts&quot;: {<br>    &quot;analyze&quot;: &quot;source-map-explorer 'build/static/js/*.js'&quot;;<br>    &quot;start&quot;: &quot;react-app-rewired start&quot;;<br>    &quot;build&quot;: &quot;GENERATE_SOURCEMAP=false react-app-rewired build &amp;&amp; npm run build-sw&quot;;<br>    &quot;build-sw&quot;: &quot;node --max_old_space_size=4096 ./src/sw-build.js&quot;;<br>    &quot;deploy&quot;: &quot;npm run build &amp;&amp; firebase deploy&quot;;<br>    &quot;test&quot;: &quot;react-app-rewired test&quot;;<br>    &quot;eject&quot;: &quot;react-scripts eject&quot;<br>  };<br></code></pre><br><p>sw-build.js file -</p><br><pre><code>const workboxBuild = require('workbox-build')<br>// NOTE: This should be run *AFTER* all your assets are built<br>const buildSW = () =&gt;<br>  // This will return a Promise<br>  workboxBuild.injectManifest({<br>    swSrc: 'src/sw-template.js'; // this is your sw template file<br>    swDest: 'build/sw.js'; // this will be created in the build step<br>    globIgnores: [<br>        &quot;node_modules/**/*&quot;;<br>        &quot;**/sw-build.js&quot;;<br>        &quot;**/sw-template.js&quot;<br>    ];<br>    globDirectory: 'build';<br>    globPatterns: [<br>      '**\/*.{js;css;html;png;jpeg;jpg;svg}'<br>    ];<br>    maximumFileSizeToCacheInBytes: 8388608<br>  }).then(({ count; size; warnings }) =&gt; {<br>    // Optionally; log any warnings and details.<br>    warnings.forEach(console.warn)<br>    console.log(`${count} files will be precached; totaling ${size} bytes.`)<br>  })<br><br>buildSW()<br></code></pre><br><p>Below is code snippet of service worker file</p><br><pre><code>function registerValidSW(swUrl; config) {<br>  navigator.serviceWorker<br>    .register(swUrl)<br>    .then((registration) =&gt; {<br>      registration.onupdatefound = () =&gt; {<br>       const installingWorker = registration.installing<br>        if (installingWorker == null) {<br>          return<br>        }<br>        installingWorker.onstatechange = () =&gt; {<br>          if (installingWorker.state === 'installed') {<br>            if (navigator.serviceWorker.controller) {<br>              if (config &amp;&amp; config.onUpdate) {<br>                config.onUpdate(registration)<br>              }else{<br>                handleCacheClear(registration)<br>              }<br>            } else if (config &amp;&amp; config.onSuccess) {<br>              config.onSuccess(registration)<br>            }<br>          }else{<br>            // reload once when the new Service Worker starts redundant<br>            if (installingWorker.state === 'redundant') {<br>              handleCacheClear(registration)<br>            }<br>          }<br>        }<br>      }<br>    })<br>    .catch((error) =&gt; {<br>      console.error('Error during service worker registration:'; error)<br>    })<br>}<br><br>function handleCacheClear(registration) {<br>  if (registration &amp;&amp; registration.waiting) {<br>    registration.waiting.postMessage('skipWaiting')<br>    navigator.serviceWorker.ready.then((registrationSW) =&gt; {<br>      registrationSW.unregister().then(() =&gt; {<br>        caches.keys().then(function (names) {<br>          // console.log(names; 'names of caches')<br>          // delete the available cache for<br>          caches.delete('workbox-precache')<br>          caches.delete('images')<br>          caches.delete('api-cache')<br>        })<br>        window.location.reload()<br>      })<br>    })<br>  }<br>}<br></code></pre><br><p>I have deployed same code on Firebase and Netlify and it's updating on every new build release but in AWS cloud-front; new service worker is not initiating.a</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>Can Regex Serde in AWS Athena handle new line characters?</h3><p>I have a table which has a few columns that contain line breaks within the data. I have tried using Glue Crawler to create the tables in Athena but the values are overflowing into the wrong columns due to line breaks. The source data is a CSV in S3 bucket</p><br><p>Now I am trying to achieve the same but by creating a table using the Regex SerDe. My problem is that I am unable to handle the new line characters within the data. I read this post: <a href="https://stackoverflow.com/questions/49080491/how-to-handle-new-line-characters-in-hive">How to handle new line characters in hive?</a> and wondering if the below is still the case:</p><br><blockquote><br><p>You cannot put records into Hive that are separated by newlines which contain newlines within the data itself. At least; not as plain text. You need to use sqoop otherwise for the correct columns to be parsed and loaded  OneCricketeer Mar 4 '18 at 7:24</p><br></blockquote><br><p><strong>Simple example of source data with no line breaks; that works:</strong><br>&quot;This is a sentence; with a comma; it is great&quot;;&quot;Kind regards; John&quot;</p><br><p>&quot;This is another sentence with a comma; its not great&quot;;Hello</p><br><p><strong>Two columns; two rows; below is the regex that works:</strong></p><br><pre><code>CREATE EXTERNAL TABLE `commas_regex`(<br>  `col1` string COMMENT ''; <br>  `col2` string COMMENT '')<br>ROW FORMAT SERDE <br>  'org.apache.hadoop.hive.serde2.RegexSerDe' <br>WITH SERDEPROPERTIES ( <br>  'input.regex'='(\&quot;*[\\w|;|\\s|\\n|\\r]+\&quot;*);(\&quot;*[\\w|;|\\s\\n|\\r]+\&quot;*)$[\\r\\n]*') <br>STORED AS INPUTFORMAT <br>  'org.apache.hadoop.mapred.TextInputFormat' <br>OUTPUTFORMAT <br>  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'<br>LOCATION<br>  's3://....' /* removed for security */ <br>TBLPROPERTIES (<br>  'has_encrypted_data'='false'; <br>  'transient_lastDdlTime'='1623409617')<br></code></pre><br><p><strong>Now change source data to include line breaks and it stops working:</strong></p><br><p>&quot;This is a sentence; with a comma; it is great&quot;;&quot;Kind regards;</p><br><p>John&quot;</p><br><p>&quot;This is another sentence with a comma; its not great&quot;;Hello<br><a href="https://i.stack.imgur.com/i79W7.png" rel="nofollow noreferrer">See picture of the results in Athena here</a></p><br>
0.0,0.0,1.0,1.0,0.0,0.0,0.0,<h3>Amazon S3 Bucket Policy Public</h3><p>I'm trying to make a signedUrl PUT in my Javascript code; the URL was already signed by my Node.js API.</p><br><p>If I leave my Bucket Policy empty; I receive a Acess Denied message when performing the PUT request; but; when I set this policy:</p><br><pre><code>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Id&quot;: &quot;Policy1610714112327&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Sid&quot;: &quot;Stmt1610714107917&quot;;<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Principal&quot;: &quot;*&quot;;<br>            &quot;Action&quot;: [<br>                &quot;s3:PutObject&quot;;<br>                &quot;s3:PutObjectAcl&quot;<br>            ];<br>            &quot;Resource&quot;: &quot;arn:aws:s3:::&lt;bucketname&gt;/*&quot;<br>        }<br>    ]<br>}<br></code></pre><br><p>I can successfully upload the file.</p><br><p>My question is: with this policy; anyone can PUT files into my bucket? Or just when they have a signed URL?</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.0,1.0,<h3>AWS Greengrass Quickstart Docs - Invalid Token Error</h3><p>Following <a href="https://docs.aws.amazon.com/greengrass/v2/developerguide/getting-started.html" rel="nofollow noreferrer">the AWS Greengrass Quickstart Docs</a><br><code>Install the AWS IoT Greengrass Core software</code> step 5 leads to the error :</p><br><p><code>The security token included in the request is invalid</code></p><br><p>I have tried:</p><br><ul><br><li>with both my normal creds; temporary creds (as recommended);</li><br><li>created new secret keys and retried normal and temporary creds;</li><br><li>tried in both v1 and v2 of Greengrass</li><br><li>I have tested the &quot;export AWS_...&quot; variables set to make sure they are properly set.</li><br></ul><br><p>I just dont understand what is happening in GreengrassCore to try to debug if this is something with my user credentials or if something is breaking in the script itself. If it hadn't been for running into so many AWS tutorials and &quot;Quickstarts&quot; that were broken out of the box; outdated; and wrong I would just assume I'm doing something wrong but at this point I was wondering if anyone has been able to get this to work or if they could give me any additional insight of the <code>security token...is invalid</code> message when I have tried using credentials from the &quot;My Security Credentials&quot; and from <code>aws sts get-session-token --duration-seconds 129600</code> <em>repeatedly</em>.</p><br><p>Thanks for any and all insights.</p><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.3333333333333333,0.0,<h3>How to get actual ECS Task created date&amp;time</h3><p>Who knows how can I retrieve correct createAt data&amp;time of ECS task?</p><br><p>I'm getting info about ECS tasks in the cluster with boto3.<br>I need to know how much time passed since the ECS task created.</p><br><p>This means I need to do smth if the task is waiting in the ECS tasks queue for more than a few hours. But I cannot get the correct time because it's always updating...</p><br><p>I do:</p><br><pre><code>prod = boto3.Session(profile_name=profile_name)<br><br>ecs = prod.client('ecs'; region_name=region_name)<br>  tasks = ecs.list_tasks(<br>  cluster=ecs_cluster<br>)<br><br>etasks = ecs.describe_tasks(<br>  cluster=ecs_cluster;<br>  tasks=tasks.get('taskArns')<br>)<br></code></pre><br><p>Finally; I get:</p><br><ul><br><li>task ARN</li><br><li>last status</li><br><li>created date &amp; time</li><br></ul><br><p>Problem: This data &amp; time are always changing... and I cannot get the correct time</p><br><p>What I get:</p><br><pre><code> --- Tasks Data ---<br>-&gt; Last Status: PROVISIONING<br>-&gt; Created At: 2021-06-30 14:29:54.803000+03:00<br>-&gt; Task ARN: arn:aws:ecs:&lt;region&gt;:&lt;account&gt;:task/use1-dev-Flexo-Dragen-38/00dfa050a288.....92e6e4f805bfd6<br><br>-&gt; Last Status: PROVISIONING<br>-&gt; Created At: 2021-06-30 14:10:29.617000+03:00<br>-&gt; Task ARN: arn:aws:ecs:&lt;region&gt;:&lt;account&gt;:task/use1-dev-Flexo-Dragen-38/2ab03b9d8a7.....3eea3332283fb8<br><br>-&gt; Last Status: PROVISIONING<br>-&gt; Created At: 2021-06-30 14:21:50.035000+03:00<br>-&gt; Task ARN: arn:aws:ecs:&lt;region&gt;:&lt;account&gt;:task/use1-dev-Flexo-Dragen-38/83d8607......1f47ae24<br></code></pre><br>
0.0,1.0,0.3333333333333333,0.0,0.0,0.0,0.0,<h3>AWS security group between ALB and EC2 instances</h3><p>I'm using Terraform to configure an ALB on AWS with a target group consisting of EC2 instances. I try to create the following security groups using Terraform:</p><br><pre><code>1) sg-alb (SG associated to the ALB):<br>   ----------------------------------<br>   Inbound:<br>      HTTP with source 0.0.0.0/0<br>      HTTPS with source 0.0.0.0/0<br><br>   Outbound: <br>      All traffic with destination 0.0.0.0/0 <br><br>  2) sg-http-alb (SG associated to the EC2 instances and should only receive traffic from the ALB):<br>     ----------------------------------------------------------------------------------------------<br>   Inbound:<br>      HTTP with source sg-alb<br>      HTTPS with source sg-alb<br><br>   Outbound: <br>      All traffic with destination 0.0.0.0/0 <br></code></pre><br><p>I read <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html" rel="nofollow noreferrer">here</a> that it's a best practice to limit the outbound traffic to the <code>instance security group</code> destination on the listener port.</p><br><p>So I changed the configuration as follows:</p><br><pre><code>1) sg-alb (SG associated to the ALB):<br>   ----------------------------------<br>   Inbound:<br>      HTTP with source 0.0.0.0/0<br>      HTTPS with source 0.0.0.0/0<br><br>   Outbound: <br>      HTTP with destination sg-http-alb (&lt;---- this line changed)<br><br>  2) sg-http-alb (SG associated to the EC2 instances and should only receive traffic from the ALB):<br>     ----------------------------------------------------------------------------------------------<br>   Inbound:<br>      HTTP with source sg-alb<br>      HTTPS with source sg-alb<br><br>   Outbound: <br>      All traffic with destination 0.0.0.0/0 <br></code></pre><br><p>When I configure this in Terraform I get <code>Error: Cycle:</code> which seem to indicates there is loop. Indeed I'm specifying from security group <code>sg-alb</code> to the instances <code>sg-http-alb</code> and from security group <code>sg-http-alb</code> I'm using security group <code>sg-alb</code> as a source. Both EC2 and ALB are in the same public subnet (there's reasons for that).</p><br><p>However using the console this is allowed. Also when I specify the internal IP address (using /32) of my EC2 instances as the outbound destination; it works but not sure if this is a proper way.</p><br>
0.6666666666666666,1.0,0.0,0.3333333333333333,0.0,0.3333333333333333,0.0,<h3>AWS Network Load Balancer Redirect Port</h3><p>I am trying to deploy a Kafka cluster on AWS (using CloudFormation). My advertised listeners are (using a private DNS namespace to resolve the internal IP):</p><br><p>INTERNAL://kafka-${id}.local:9092<br /><br>EXTERNAL://&lt;public-ip&gt;:9092</p><br><p>However; Kafka complains that two listeners cannot share the same port. The problem is I'm using a load balancer for external traffic; and I'm not sure if there's a way to redirect that traffic to a different port.</p><br><p>My desired configuration would be:</p><br><p>INTERNAL://kafka-${id}:9092<br /><br>EXTERNAL://&lt;public-ip&gt;:19092</p><br><p>But the load balancer takes the incoming request and passes it to the internal IP at the same port. Ultimately I'd like to have the load balancer take connections on port 19092 and pass them to 9092; but I don't see any way to configure that.</p><br><p>If there are any recommendations on alternative ways to do this; I'm happy to hear them. Currently; I need services that are on other VPCs to be able to communicate with these brokers; and I'd prefer to use a load balancer to handle these requests.</p><br>
0.0,1.0,0.0,0.0,0.0,0.0,0.0,<h3>Cloudfront proxy requests to subdomain</h3><p>I have set up <a href="http://www.xyz.com" rel="nofollow noreferrer">www.xyz.com</a> to a Cloudfront distribution (frontend).</p><br><p>I have my API behind a load balancer at api.xyz.com and a public DNS record pointing to it.</p><br><p>The frontend is requesting API data from <a href="http://www.xyz.com/api/" rel="nofollow noreferrer">www.xyz.com/api/</a>* to avoid CORS.</p><br><p>I need to proxy requests from <strong><a href="http://www.xyz.com/api/" rel="nofollow noreferrer">www.xyz.com/api/</a></strong>* to <strong>api.xyz.com/</strong>*</p><br><p><strong>What I have tried:</strong></p><br><p>Set up another custom origin for the distribution with Origin Domain Name: api.xyx.com and no Path.</p><br><p>Added another behavior with path pattern: /api* pointing to the origin above.</p><br><p>Shouldn't those requests get proxied correctly under the hood?</p><br><p><strong>What I get back:</strong></p><br><pre><code>403 Http Error. <br>This distribution is not configured to allow the HTTP request method that was used for this request. The distribution supports only cachable requests. We can't connect to the server for this app or website at this time. There might be too much traffic or a configuration error. Try again later; or contact the app or website owner.<br>If you provide content to customers through CloudFront; you can find steps to troubleshoot and help prevent this error by reviewing the CloudFront documentation.<br></code></pre><br>
0.0,0.6666666666666666,1.0,0.0,0.0,0.0,0.0,<h3>How can I link cognito token and API authentication header automatically?</h3><p>I deployed a REST API gateway and Cognito user pool in AWS. Then configured API gateway to use cognito as authorizer method. In API gateway authroizer configuration page; it asks me to configure the token header name and validation method. Like below screenshot:</p><br><p><a href="https://i.stack.imgur.com/T4gkr.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/T4gkr.png" alt="enter image description here" /></a></p><br><p>In Cognito App client setting; it asks me to set the callback url after sign in successfully. Now; my question is how can I link this two setting automatically? I'd like to forward users' browser to the API endpoint after they sign in cognito. But the forward fails with this error message <code>{&quot;message&quot;:&quot;Unauthorized&quot;}</code>. I know the reason is the token is not set correctly open open the API gateway endpoint. How can I make them work together without setting up a different backend service?<br>Is there a way for me to configure cognito callback url header?</p><br>
0.6666666666666666,0.0,0.0,0.6666666666666666,0.0,0.0,0.0,<h3>Daily incremental copying from Amazon S3 data into Amazon Redshift</h3><p>I have a RDS database whose snapshot is taken everyday and is kept in a S3 bucket. I copy the RDS snapshot data from S3 to Amazon Redshift database daily. I can use <code>copy</code> to copy the tables but instead of copying the whole table; I want to copy only the rows which were added since the last snapshot was taken(Incremental copying).</p><br><p>For example; in RDS; there is a table name &quot;user&quot; which looks like this at 25-05-2021</p><br><pre><code>id | username<br>1  | john<br>2  | cathy<br></code></pre><br><p>When I will run the data loader for first time on 26-05-2021; it will copy these two rows into the Redshift table with the same name.</p><br><p>Now on 26-05-2021; the table in RDS looks like this:</p><br><pre><code>id | username<br>1  | john<br>2  | cathy<br>3  | ola<br>4  | mike<br></code></pre><br><p>When I will run the data loader on 27-05-2021; instead of copying all three rows; I want to copy/take only the rows which has been newly added(<code>id = 3 and id = 4</code>) as I already have the other rows.</p><br><p>What should be the best way of doing this incremental loading?</p><br>
0.0,1.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>Some Cloudfront Distributions are not appearing on AWS Cloudfront Console</h3><p>I can see this Cloudfront distribution (d1ee8khuaj1n9e.cloudfront.net) in Route 53 (see 2nd screenshot -- <a href="https://i.stack.imgur.com/C81eF.png" rel="nofollow noreferrer">https://i.stack.imgur.com/C81eF.png</a>). But it seems I can't see it on the Cloudfront Console. My account is now &quot;Global&quot; (see 1st screenshot -- <a href="https://i.stack.imgur.com/4ksPl.png" rel="nofollow noreferrer">https://i.stack.imgur.com/4ksPl.png</a>)</p><br><p>Is there lacking with my access or am I looking at the wrong place? Thanks for all the help!</p><br>
0.0,0.3333333333333333,0.3333333333333333,0.0,0.6666666666666666,1.0,0.0,<h3>AWS Codebuild - Error while executing command: python -m pip install --upgrade --force pip. Reason: exit status 1</h3><p>I'm trying to run a build after creating a stack in the AWS cloudFormation but unfortunately; the build has failed with an error message:</p><br><blockquote><br><p>Phase context status code: COMMAND_EXECUTION_ERROR Message: Error<br>while executing command: python -m pip install --upgrade --force pip.<br>Reason: exit status 1</p><br></blockquote><br><p>here is the log for the build and why it failed:</p><br><pre><code>[Container] 2021/10/15 13:01:39 Waiting for agent ping<br>[Container] 2021/10/15 13:01:40 Waiting for DOWNLOAD_SOURCE<br>[Container] 2021/10/15 13:01:41 Phase is DOWNLOAD_SOURCE<br>[Container] 2021/10/15 13:01:41 CODEBUILD_SRC_DIR=/codebuild/output/src061758247/src<br>[Container] 2021/10/15 13:01:41 YAML location is /codebuild/output/src061758247/src/buildspec.yml<br>[Container] 2021/10/15 13:01:41 Processing environment variables<br>[Container] 2021/10/15 13:01:41 Decrypting parameter store environment variables<br>[Container] 2021/10/15 13:01:41 [WARN] Skipping install of runtimes. Runtime version selection is not supported by this build image.<br>[Container] 2021/10/15 13:01:43 Moving to directory /codebuild/output/src061758247/src<br>[Container] 2021/10/15 13:01:43 Registering with agent<br>[Container] 2021/10/15 13:01:43 Phases found in YAML: 4<br>[Container] 2021/10/15 13:01:43  POST_BUILD: 10 commands<br>[Container] 2021/10/15 13:01:43  INSTALL: 10 commands<br>[Container] 2021/10/15 13:01:43  PRE_BUILD: 6 commands<br>[Container] 2021/10/15 13:01:43  BUILD: 1 commands<br>[Container] 2021/10/15 13:01:43 Phase complete: DOWNLOAD_SOURCE State: SUCCEEDED<br>[Container] 2021/10/15 13:01:43 Phase context status code:  Message: <br>[Container] 2021/10/15 13:01:43 Entering phase INSTALL<br>[Container] 2021/10/15 13:01:43 Running command echo 'about to call dockerd'<br>about to call dockerd<br><br>[Container] 2021/10/15 13:01:43 Running command nohup /usr/local/bin/dockerd --host=unix:///var/run/docker.sock --host=tcp://127.0.0.1:2375 --storage-driver=overlay2&amp;<br><br>[Container] 2021/10/15 13:01:43 Running command timeout 15 sh -c &quot;until docker info; do echo .; sleep 1; done&quot;<br>Error starting daemon: pid file found; ensure docker is not running or delete /var/run/docker.pid<br>Containers: 0<br> Running: 0<br> Paused: 0<br> Stopped: 0<br>Images: 0<br>Server Version: 17.09.0-ce<br>Storage Driver: overlay<br> Backing Filesystem: extfs<br> Supports d_type: true<br>Logging Driver: json-file<br>Cgroup Driver: cgroupfs<br>Plugins:<br> Volume: local<br> Network: bridge host macvlan null overlay<br> Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog<br>Swarm: inactive<br>Runtimes: runc<br>Default Runtime: runc<br>Init Binary: docker-init<br>containerd version: 06b9cb35161009dcb7123345749fef02f7cea8e0<br>runc version: 3f2f8b84a77f73d38244dd690525642a72156c64<br>init version: 949e6fa<br>Security Options:<br> seccomp<br>  Profile: default<br>Kernel Version: 4.14.243-185.433.amzn2.x86_64<br>Operating System: Ubuntu 14.04.5 LTS (containerized)<br>OSType: linux<br>Architecture: x86_64<br>CPUs: 2<br>Total Memory: 3.645GiB<br>Name: 9d1ea8d456c4<br>ID: GA3S:TOF2:A43S:WTEP:JIFT:RNGG:X3XM:5N6S:7JMU:5IE3:HV2Z:AFGS<br>Docker Root Dir: /var/lib/docker<br>Debug Mode (client): false<br>Debug Mode (server): false<br>Registry: https://index.docker.io/v1/<br>Experimental: false<br>Insecure Registries:<br> 127.0.0.0/8<br>Live Restore Enabled: false<br><br>WARNING: bridge-nf-call-iptables is disabled<br>WARNING: bridge-nf-call-ip6tables is disabled<br><br>[Container] 2021/10/15 13:01:43 Running command curl -sS -o aws-iam-authenticator https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/linux/amd64/aws-iam-authenticator<br><br>[Container] 2021/10/15 13:01:44 Running command curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl<br>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current<br>                                 Dload  Upload   Total   Spent    Left  Speed<br><br>  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0<br>100 44.7M  100 44.7M    0     0  60.0M      0 --:--:-- --:--:-- --:--:-- 60.1M<br><br>[Container] 2021/10/15 13:01:45 Running command chmod +x ./kubectl ./aws-iam-authenticator<br><br>[Container] 2021/10/15 13:01:45 Running command echo `kubectl version`<br>/codebuild/output/tmp/script.sh: 1: /codebuild/output/tmp/script.sh: kubectl: not found<br><br><br>[Container] 2021/10/15 13:01:45 Running command export PATH=$PWD/:$PATH<br><br>[Container] 2021/10/15 13:01:45 Running command python -m pip install --upgrade --force pip<br>Collecting pip<br>/usr/local/lib/python2.7/dist-packages/pip/_vendor/urllib3/util/ssl_.py:339: SNIMissingWarning: An HTTPS request has been made; but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate; which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information; see https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings<br>  SNIMissingWarning<br>/usr/local/lib/python2.7/dist-packages/pip/_vendor/urllib3/util/ssl_.py:137: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information; see https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings<br>  InsecurePlatformWarning<br>  Could not find a version that satisfies the requirement pip (from versions: )<br>No matching distribution found for pip<br>/usr/local/lib/python2.7/dist-packages/pip/_vendor/urllib3/util/ssl_.py:137: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information; see https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings<br>  InsecurePlatformWarning<br><br>[Container] 2021/10/15 13:01:45 Command did not exit successfully python -m pip install --upgrade --force pip exit status 1<br>[Container] 2021/10/15 13:01:45 Phase complete: INSTALL State: FAILED<br>[Container] 2021/10/15 13:01:45 Phase context status code: COMMAND_EXECUTION_ERROR Message: Error while executing command: python -m pip install --upgrade --force pip. Reason: exit status 1<br></code></pre><br><p>My buildspec.yaml file looks like this:</p><br><pre><code>---<br>version: 0.2<br><br><br>phases:<br>  install:<br>    commands:<br>      - curl -sS -o aws-iam-authenticator https://amazon-eks.s3-us-west-2.amazonaws.com/1.14.6/2019-08-22/bin/linux/amd64/aws-iam-authenticator<br>      - curl -sS -o kubectl https://amazon-eks.s3-us-west-2.amazonaws.com/1.14.6/2019-08-22/bin/linux/amd64/kubectl<br>      - chmod +x ./kubectl ./aws-iam-authenticator<br>      - export PATH=$PWD/:$PATH<br>      - apt-get update &amp;&amp; apt-get -y install jq python3-pip python3-dev &amp;&amp; pip3 install --upgrade awscli<br>  pre_build:<br>      commands:<br>        - TAG=&quot;$REPOSITORY_NAME.$REPOSITORY_BRANCH.$ENVIRONMENT_NAME.$(date +%Y-%m-%d.%H.%M.%S).$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | head -c 8)&quot;<br>        - sed -i 's@CONTAINER_IMAGE@'&quot;$REPOSITORY_URI:$TAG&quot;'@' simple_jwt_api.yml<br>        - $(aws ecr get-login --no-include-email)<br>        - export KUBECONFIG=$HOME/.kube/config<br>        - pip3 install -r requirements.txt<br>        - pytest<br>  build:<br>    commands:<br>      - docker build --tag $REPOSITORY_URI:$TAG .<br><br>  post_build:<br>    commands:<br>      - docker push $REPOSITORY_URI:$TAG<br>      - CREDENTIALS=$(aws sts assume-role --role-arn $EKS_KUBECTL_ROLE_ARN --role-session-name codebuild-kubectl --duration-seconds 900)<br>      - export AWS_ACCESS_KEY_ID=&quot;$(echo ${CREDENTIALS} | jq -r '.Credentials.AccessKeyId')&quot;<br>      - export AWS_SECRET_ACCESS_KEY=&quot;$(echo ${CREDENTIALS} | jq -r '.Credentials.SecretAccessKey')&quot;<br>      - export AWS_SESSION_TOKEN=&quot;$(echo ${CREDENTIALS} | jq -r '.Credentials.SessionToken')&quot;<br>      - export AWS_EXPIRATION=$(echo ${CREDENTIALS} | jq -r '.Credentials.Expiration')<br>      - aws eks update-kubeconfig --name $EKS_CLUSTER_NAME<br>      - kubectl apply -f simple_jwt_api.yml<br>      - printf '[{&quot;name&quot;:&quot;simple_jwt_api&quot;;&quot;imageUri&quot;:&quot;%s&quot;}]' $REPOSITORY_URI:$TAG &gt; build.json<br>      - pwd<br>      - ls<br>artifacts:<br>  files: build.json<br><br>env:<br>  parameter-store:<br>    JWT_SECRET: JWT_SECRET<br></code></pre><br><p>can anyone help me with this issue or guide me to a similar asked question?<br>thanks</p><br>
0.0,0.0,0.0,1.0,0.3333333333333333,0.3333333333333333,0.3333333333333333,<h3>Uploading files to an Amazon S3 bucket using flask Python</h3><p>I am trying to upload a file using the flask and boto3 module to one of my Amazon S3 buckets. My code does not just upload the file; but it also uploads the folder where that file is stored. Can somebody please help me with that. If I am already providing the path of the file in the code which you can see below. How does the upload thing work in the Html button?</p><br><pre><code>@app.route(&quot;/upload&quot;; methods=['POST'])<br>def upload():<br>if request.method == &quot;POST&quot;:<br>    f = request.files['file']<br>    f.save(os.path.join(UPLOAD_FOLDER; f.filename))<br>    upload_file(f&quot;readme/{f.filename}&quot;; BUCKET)<br><br>    return redirect(&quot;/storage&quot;)<br></code></pre><br>
0.0,0.6666666666666666,0.0,0.0,0.6666666666666666,0.0,0.0,<h3>Nginx Allow all port to be listened</h3><p>I am trying to run multiple Nodejs apps on EC2 server.</p><br><pre><code> rewrite ^(/\d+)$ $1/ redirect;<br>   rewrite ^/(?&lt;port&gt;\d+)(.+)$ $2;<br><br>        location / {<br>                proxy_pass http://127.0.0.1:$port;<br>                proxy_http_version 1.1;<br>                proxy_set_header Upgrade $http_upgrade;<br>                proxy_set_header Connection 'upgrade';<br>                proxy_set_header Host $host;<br>                proxy_cache_bypass $http_upgrade;<br>        }<br></code></pre><br><p>Suppose I am running app on 1234 port. When I request  127.0.0.1:1234. It is not getting passed.<br>I am trying to make it dynamic. It should pass request on any port.</p><br><p>I have tried for harded coded port. It is working fine.</p><br><pre><code> location /api {<br>                proxy_set_header  X-Real-IP  $remote_addr;<br>                proxy_set_header  X-Forwarded-For $proxy_add_x_forwarded_for;<br>                proxy_set_header  Host $http_host;<br>                proxy_redirect  off;<br>                proxy_pass http://127.0.0.1:3000;<br>                }<br><br>        }<br></code></pre><br><p>Is there any way where I can listen to any port dynamically.</p><br><p>Thanks</p><br>
0.0,0.0,0.0,1.0,0.3333333333333333,0.3333333333333333,0.0,<h3>Return Data After S3 Upload in Node</h3><p>I finally have file uploads working through Node and the AWS SDK...there's just one thing that's missing now that I haven't been able to crack yet; which is that I need to get the URL to the newly uploaded file on S3 and save it to my database.</p><br><pre><code>const s3 = new AWS.S3();<br><br>  const fileContent = Buffer.from(req.files.listPDF.data; 'binary');<br><br>  const params = {<br>    Bucket: 'my_bucket';<br>    Key: filename;<br>    Body: fileContent<br>  };<br><br>  s3.upload(params; function(err; data){<br>    if (err) {<br>      throw err;<br>    }<br>  });<br></code></pre><br><p>I'm guessing it's promise-related; but I haven't had success with &quot;await&quot; yet. The data parameter in the function has a &quot;Location&quot; attribute; which I need. Originally; I was trying to set a previously-declared var to it. However; it wasn't doing anything since the upload was not yet completed. If anyone's grappled with this and cracked the code; I'd really appreciate your thoughts!</p><br>
0.0,1.0,0.0,0.0,0.6666666666666666,0.0,0.0,<h3>Very slow first load time on calls to AWS Elastic Beanstalk server in VPC behind ELB</h3><p>I have an Elastic Beanstalk server behind an Application Load Balancer; all inside a VPC. The first call to the server after leaving it along for a while takes a very long time. It's almost as if the instance is being booted up right then! Instead of being already on...</p><br><p>This issue does not present locally; nor outside of a VPC; it only happens in the VPC on AWS so something in my configuration must be off.</p><br><p>The VPC has 3 public and 3 private subnets; in the same availability zones; and the public subnets all have auto-assign public IP on</p><br><p><a href="https://i.stack.imgur.com/CHwi0.png" rel="noreferrer"><img src="https://i.stack.imgur.com/CHwi0.png" alt="subnets" /></a><br><a href="https://i.stack.imgur.com/ZQNtR.png" rel="noreferrer"><img src="https://i.stack.imgur.com/ZQNtR.png" alt="availability zones" /></a><br><a href="https://i.stack.imgur.com/3j8sX.png" rel="noreferrer"><img src="https://i.stack.imgur.com/3j8sX.png" alt="auto assign public ips" /></a></p><br><p>I've assigned these to the network settings on my Elastic Beanstalk environment; assigning the public subnets to the public load balancer; and then the private subnets to the private instance.</p><br><p><a href="https://i.stack.imgur.com/6PO3X.png" rel="noreferrer"><img src="https://i.stack.imgur.com/6PO3X.png" alt="load balancer settings" /></a><br><a href="https://i.stack.imgur.com/b48uA.png" rel="noreferrer"><img src="https://i.stack.imgur.com/b48uA.png" alt="instance settings" /></a></p><br><p>I've set the auto scaling load balanced group with minimum of 3 instances; and confirmed they're running</p><br><p><a href="https://i.stack.imgur.com/zSDnf.png" rel="noreferrer"><img src="https://i.stack.imgur.com/zSDnf.png" alt="auto scaling group" /></a><br><a href="https://i.stack.imgur.com/H0rVD.png" rel="noreferrer"><img src="https://i.stack.imgur.com/H0rVD.png" alt="instances" /></a><br><a href="https://i.stack.imgur.com/1ANKc.png" rel="noreferrer"><img src="https://i.stack.imgur.com/1ANKc.png" alt="status" /></a></p><br><p>Despite this; after leaving the site alone for a while... the first new call to the server consistently takes over one minute; and then works great. I assume I'm just missing something small but cannot figure out what it is...</p><br><p>Thanks in advance!</p><br><p>I am convinced this is not an application issue because; on first load the call takes over one minute; but on subsequent loads it's near instant; and this behavior is constant across days. Locally; I never have this issue. Outside a VPC; I never have this issue.</p><br><p>first/slow load (after leaving the app alone overnight)<br><a href="https://i.stack.imgur.com/vzGc7.png" rel="noreferrer"><img src="https://i.stack.imgur.com/vzGc7.png" alt="slow load" /></a></p><br><p>second/fast load (refreshing right after the above)<br><a href="https://i.stack.imgur.com/2DPWi.png" rel="noreferrer"><img src="https://i.stack.imgur.com/2DPWi.png" alt="fast load" /></a></p><br><p><strong>UPDATE</strong></p><br><p>AWS support suggested I deassociate the subnets from my route tables. I did that and now all subnets public and private are showing current routing table Main. Now though; instead of taking a long time all calls to my server are failing!</p><br><p>I tried attaching the internet gateway in that VPC to the routing table via edge association but I'm getting the error that</p><br><blockquote><br><p>Route table contains unsupported route destination. The unsupported route destination is less specific than or non-overlapping with VPC local CIDR</p><br></blockquote><br><p>There is one public subnet with overlapping CIDRs with the internet gateway (10.1.0.0/24 on the subnet and 10.1.0.0/24 on the gateway). I tried manually associating that to the Main routing table but still get the same error</p><br>
0.0,0.0,0.6666666666666666,0.0,1.0,0.3333333333333333,0.0,<h3>How to provide tasks with different environment variables ECS Terraform</h3><p>I have an ECS service and within that ECS service; I want to boot up 3 tasks all from the same task definition. I need each of these tasks to be on a separate EC2 instance; this seems simple enough however I want to pass a different command to each one of the running tasks to specify where their config can be found and some other options via the CLI within my running application.</p><br><p>For example for task 1 I want to pass <code>run-node CONFIG_PATH=&quot;/tmp/nodes/node_0</code> and task 2 <code>run-node CONFIG_PATH=&quot;/tmp/nodes/node_1&quot; --bootnode true</code> and task 3 <code>run-node CONFIG_PATH=&quot;/tmp/nodes/node_0 --http true&quot;</code></p><br><p>I'm struggling to see how I can manage individual task instances like this within a single service using Terraform; it seems really easy to manage multiple instances that are all completely equal but I can't find a way to pass custom overrides to each task that are all running off the same task definition.</p><br><p>I am thinking this may be a job for a different dev-ops automation tool but would love to carry on doing it in Terraform if possible.</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>Get data based on sort index AWS dynamoDB</h3><p>I have a DynamoDB table like this:<a href="https://i.stack.imgur.com/IqcgW.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/IqcgW.png" alt="enter image description here" /></a></p><br><p>I want to list all posts irrespective of users; i-e by getting all data whose sort key is &quot;post&quot;. How can I achieve this?</p><br><p>I have heard about Global Secondary Index; but couldn't figure out how to use them.</p><br>
0.0,1.0,0.0,0.0,0.6666666666666666,0.0,0.0,<h3>AWS load balancer size limits</h3><p>The AWS ALB limits size to 100 MB.  Part of my API response is a rendered README.md from the Github API.  I don't control the rendering; but it turns out ALB will not return a response with text of this rendered REAME.md file from Github.  When I move response to directly hitting an Nginx LB from a VM; there is no issue.  I looked at content; and yes; the README content is fairly large it seems.  I don't know if/how exceeds 100 MB; but it seems to be he problem.  Regardless; I cannot control how Github controls the rendering.</p><br><p>Is there any workaround this 100 MB limit?  If not; can I use Nginx as a reverse load balancer for lambda functions?  Otherwise; I'll need to go back to regular VMs and do a weighted DNS among non-autoscaling VMs since I still can't use a load balancer for VMs; Fargate; or anything.</p><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.6666666666666666,0.0,<h3>Application Deployment for particular elastic beanstalk environment shows health degraded</h3><p>The elastic beanstalk environment is deployed when changes are made in code and <code>codepipeline</code> is triggered.<br>-&gt; the <code>codepipeline</code> for particular eb-instance is deployed successfully<br>-&gt; the <code>eb-instance</code> shows health degraded.</p><br><p>the logs show connect() failed (111: Connection refused) while connecting to upstream; client: ip; server: ; request: &quot;GET / HTTP/1.1&quot;; upstream: &quot;http://127.0.0.1:3030/ &quot;; host: &quot;ip&quot;</p><br><p>logging into the EC2-Instance shows that node service is not deployed ( on particular port: 3030 ) . The deployment for eb-instance is done through codepipeline ( the codebuild builds app and then deploy part takes &quot;Procfile&quot; from code and runs it )</p><br><p>The Procfile is added to the application root and contains your app run script; e.g.:</p><br><pre class="lang-sh prettyprint-override"><code>web: node server.js<br></code></pre><br><p>The <code>codepipeline</code> is successful but the instance does not show any node service running ? ( any help to debug this error would be helpful )</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>AWS CLI not automatically using multipart upload for s3</h3><p>According to AWS pages; if a file size is big enough it will automatically use multi part uploads so I can run this command</p><br><pre><code>aws s3 cp large_test_file s3://DOC-EXAMPLE-BUCKET/<br></code></pre><br><p>and the upload should be multipart.</p><br><p>However; I am uploading a 1gb file and it seems to be uploading it as a normal file.</p><br><p>Furthermore; my bucket policy has no permissions for multipart list or upload (just put object) so I don't understand how that is working.</p><br><p>Would like some clarity on this if it is</p><br><ul><br><li>Indeed using multipart upload (without informing me)</li><br><li>How is it able to do so without me granting it permissions?</li><br></ul><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>AWS Codebuild not starting when pushing commit and tag</h3><p>I followed this tutorial <a href="https://ruddra.com/aws-codebuild-use-git-tags/" rel="nofollow noreferrer">https://ruddra.com/aws-codebuild-use-git-tags/</a> to make my codebuild in aws only trigger when tag is pushed to my bitbucket project; and it works if I'm ONLY pusing tags.</p><br><p>It works here:</p><br><pre><code># make some code changes<br><br>git add .<br>git commit -m &quot;changes&quot;<br>git push origin HEAD # OK: codebuild not trigered as expected<br><br>git tag v1<br>git push origin HEAD --tags # SUCCESS: codebuild is triggered<br></code></pre><br><p>but if I tag with a commit:</p><br><pre><code># make some code changes<br><br>git add .<br>git commit -m &quot;changes&quot;<br>git tag v1<br>git push origin HEAD --tags # UNSUCCESS: codebuild is NOT triggered<br></code></pre><br><p>it has to do with the expression: <code>^refs/tags/.*</code> ?</p><br><p><strong>Bitbucket payload</strong></p><br><p>I was trying to figure out by checking the bitbucket payload; it seems when the tag hash doesn't match the current <code>$CODEBUILD_SOURCE_VERSION</code>; it won't check the tag (My guess so far)</p><br><p>Working payload (push tag)</p><br><pre><code>push.changes[0].type: &quot;tag&quot;<br>push.changes[0].target.hash: &quot;6f08xxx&quot;; &lt;== $CODEBUILD_SOURCE_VERSION<br></code></pre><br><p>Non Working payload (push commmit + tag)</p><br><pre><code>push.changes[0].type: &quot;branch&quot;<br>push.changes[0].target.hash: &quot;25b7xxx&quot;; &lt;== $CODEBUILD_SOURCE_VERSION<br><br>push.changes[0].type: &quot;tag&quot;<br>push.changes[0].target.hash: &quot;6f08xxx&quot;;<br></code></pre><br><p>you may noticed the <code>6f08xxx</code> repeated; it's because during my testing I tried the non-working steps first and then I just tag and push; that's why I'm guessing the hash has to do with the checking of the tags on codebuild</p><br><p>Any help to solve or debug this would be appretiated.</p><br><p>Thanks for your time in advance</p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.3333333333333333,0.0,<h3>Can you download a file from Amazon S3 bucket without having AWS login credentials</h3><p>I have a problem statement wherein I wish to download the file in using Express and NodeJS Framework.</p><br><p>Can I download the file from amazon/s3 bucket without having a login account in AWS ?</p><br>
0.0,1.0,0.0,0.0,1.0,0.0,0.0,<h3>Cant connect the created cluster that is exposed by using NodePort Service</h3><p>So; I have been trying to create a cluster on AWS EKS for couple of days. I managed to upload the docker image on ECR; created appropriate VPC but could not managed to connect it from http://&lt;ip:port. I am using NodePort service for exposing the project. The project is a basic .NET Core REST API that returns JSON.<br>I am using AWS CLI and kubectl for operations. I have already implemented the generated nodePort IP in the inbound rules of the worker nodes (EC2 Instances) security protocols.<br>Here are my yaml files;</p><br><p><strong>Cluster.yaml -&gt; yaml file for using pre-created VPC setup and defining node groups</strong></p><br><pre class="lang-yaml prettyprint-override"><code>apiVersion: eksctl.io/v1alpha5<br>kind: ClusterConfig<br>metadata:<br>  name: EKS-Demo-Cluster<br>  region: eu-central-1<br><br>vpc:<br>  id: vpc-056dbccebc402e9a8<br>  cidr: &quot;192.168.0.0/16&quot;<br>  subnets:<br>    public:<br>      eu-central-1a:<br>        id: subnet-04192a691f3c156a6<br>      eu-central-1b:<br>        id: subnet-0f89762f3d78ccb47<br>    private:<br>      eu-central-1a:<br>        id: subnet-07fe8b089287a16c4<br>      eu-central-1b:<br>        id: subnet-0ae524ea2c78b49a7<br><br>nodeGroups:<br>  - name: EKS-public-workers<br>    instanceType: t3.medium<br>    desiredCapacity: 2<br>  - name: EKS-private-workers<br>    instanceType: t3.medium<br>    desiredCapacity: 1<br>    privateNetworking: true<br></code></pre><br><p><strong>deployment.yaml</strong></p><br><pre class="lang-yaml prettyprint-override"><code>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: sample-app<br>spec:<br>  replicas: 2<br>  selector:<br>    matchLabels:<br>      app: demojsonapp<br>  template:<br>    metadata:<br>      labels:<br>        app: demojsonapp<br>    spec:<br>      containers:<br>        - name: back-end<br>          image: 921915718885.dkr.ecr.eu-central-1.amazonaws.com/sample_repo:latest<br>          ports:<br>            - name: http<br>              containerPort: 8080<br></code></pre><br><p><strong>service.yaml</strong></p><br><pre class="lang-yaml prettyprint-override"><code>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: backend-service<br><br>spec:<br>  type: NodePort<br>  selector:<br>    app: demojsonapp<br>  ports:<br>    - protocol: TCP<br>      port: 80<br>      targetPort: 80<br></code></pre><br><p>I dont understand where the problem is. If you could help me it is very much appreciated.</p><br><p><strong>Finally; here is the docker file that created the image which I uploaded to ECR for EKS cluster;</strong></p><br><pre><code>FROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base<br>WORKDIR /app<br>EXPOSE 3000<br>EXPOSE 443<br><br>FROM mcr.microsoft.com/dotnet/core/sdk:3.1-buster AS build<br>WORKDIR /src<br>COPY [&quot;WebApplication2/WebApplication2.csproj&quot;; &quot;WebApplication2/&quot;]<br>RUN dotnet restore &quot;WebApplication2/WebApplication2.csproj&quot;<br>COPY . .<br>WORKDIR &quot;/src/WebApplication2&quot;<br>RUN dotnet build &quot;WebApplication2.csproj&quot; -c Release -o /app/build<br><br>FROM build AS publish<br>RUN dotnet publish &quot;WebApplication2.csproj&quot; -c Release -o /app/publish<br><br>FROM base AS final<br>WORKDIR /app<br>COPY --from=publish /app/publish .<br>ENTRYPOINT [&quot;dotnet&quot;; &quot;WebApplication2.dll&quot;]<br></code></pre><br>
0.0,0.0,0.0,0.0,0.6666666666666666,1.0,0.0,<h3>SQS fifo trigger invoke Lambda Function (1 message - 1 invocation)</h3><p>I have a SQS FIFO queue triggering a Lambda function.<br>I sent 10 messages (all different) and the lambda was invoked just once.</p><br><p>Details:</p><br><ul><br><li><p>SQS</p><br><ul><br><li>Visibility timeout: 30 min</li><br><li>Delivery delay: 0 secs</li><br><li>Receive Message Wait Time: 0 secs</li><br></ul><br></li><br><li><p>Lambda:</p><br><ul><br><li>Batch size: 1</li><br><li>timeout: 3secs</li><br></ul><br></li><br></ul><br><p>I don't see any errors on Lambda invocations.</p><br><p>I don't want to touch the delivery delay; but if I increase; seems working.<br>The avg duration time is less than 1;5ms</p><br><p>Any ideas how I can achieve this?<br>Should I increase the delivery delay or time out?</p><br><p>The message is being sent from a ecs task with the following code:</p><br><pre class="lang-py prettyprint-override"><code>from flask import Flask; request; redirect; url_for; send_from_directory; jsonify<br>app = Flask(__name__)<br>from werkzeug.utils import secure_filename<br>import os<br>import random<br>import boto3<br><br>s3  = boto3.client('s3')<br>sqs = boto3.client('sqs';region_name='eu-west-1')<br><br>@app.route('/'; methods=['GET'])<br>def hello_world():<br>  return 'Hello World!'<br><br>@app.route('/upload'; methods=['POST'])<br>def upload():<br>  print (str(random.randint(0;9)))<br>  file = request.files['file']<br>  if file:<br>    filename = secure_filename(file.filename)<br>    file.save(filename)<br>    s3.upload_file(<br>        Bucket = os.environ['bucket'];<br>        Filename=filename;<br>        Key = filename<br>    )<br>    resp = sqs.send_message(<br>        QueueUrl=os.environ['queue'];<br>        MessageBody=filename;<br>        MessageGroupId=filename<br>    )<br>    return jsonify({<br>        'msg': &quot;OK&quot;<br>    })<br>else:<br>    return jsonify({<br>        'msg': &quot;NOT OK&quot;<br>    })<br></code></pre><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.0,<h3>Docker executable not found in PATH when using AWS batch/ECS</h3><p>I am trying to run a simple Dockerized Python script with AWS batch.</p><br><h3>Is there a problem with my Docker image?</h3><br><p>I have locally built the Docker image and it runs fine locally. I pushed the image to a AWS repository; and pulling this remote image to my local machine also runs correctly.</p><br><h3>Problem</h3><br><p>I have setup my compute env; job queue; and job definition; but I get this error</p><br><pre><code>CannotStartContainerError: Error response from daemon: <br>OCI runtime create failed: container_linux.go:370: <br>starting container process caused: <br>exec: &quot;docker&quot;: executable file not found in $PATH: unknown<br></code></pre><br><p>when I run</p><br><pre><code>[&quot;docker&quot;;&quot;run&quot;;&quot;-t&quot;;&quot;111111111111.dkr.ecr.us-region-X.amazonaws.com/myimage:latest&quot;;&quot;python3&quot;;&quot;hello_world.py&quot;;&quot;--MSG&quot;;&quot;ok&quot;]<br></code></pre><br><h3>Is Docker installed?</h3><br><p>I am using the <code>ECS_AL2</code> image type. When I start a EC2 with this AMI and ssh into it; I can see that Docker is already installed. <code>docker run</code> works fine for instance.</p><br><h3>Is there a (generic) problem with my compute env; job queue; or job def?</h3><br><p>When I instead try to run the command <code>echo hello</code> this works fine.</p><br><p>Appreciate any advice/help you can provide.</p><br><h3>UPDATE - ANSWER</h3><br><p>@samtoddler helped me to realize that I only needed</p><br><pre><code>[&quot;python3&quot;;&quot;hello_world.py&quot;;&quot;--MSG&quot;;&quot;ok&quot;]<br></code></pre><br><p>in the Command statement</p><br>
0.3333333333333333,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.0,<h3>Template contains errors.: Template format error: YAML not well-formed</h3><p>I am trying to create cloud formation stack from below template but i am getting Template contains errors.: Template format error: YAML not well-formed error</p><br><pre class="lang-yaml prettyprint-override"><code>AWSTemplateFormatVersion: 2010-09-09<br>Description: A sample template<br>Resources:<br> BFASchemaRegistry: <br>  Type: AWS::Glue::Registry<br>  Properties: <br>   Description: AWS Glue Schema Registry for BFA<br>   Name: BFASchemaRegistry1<br>   Tags: <br>    -key: band<br>     value: bfa<br></code></pre><br>
1.0,0.0,0.6666666666666666,0.0,0.0,0.6666666666666666,0.0,<h3>AWS Lex how to perform an action on every request to lex. i.e. for every CloudWatch log of lex</h3><p>Im trying to integrate with AWS lex. I need to perform an operation on every request to lex. For instance run a lambda on every cloudwatch conversation (text) log that pushes the log to an external service. What I want to capture is <a href="https://docs.aws.amazon.com/lex/latest/dg/conversation-logs-cw.html" rel="nofollow noreferrer">this event</a></p><br><p>I am struggling to work out what the event is called and am unable to make a rule that captures them because of this. What rule do I need to capture these conversation logs?</p><br><p>Is cloudwatch the best way to achieve this?</p><br><p>And is there a way to capture historical conversations text logs?</p><br><p>Thanks</p><br>
0.0,0.0,0.0,0.0,0.3333333333333333,1.0,0.0,<h3>How can show and stop the execution when several codebuild are completed in bash?</h3><p>I am creating a script that starts several builds in AWS Codebuild. In addition to running it; I would like that when each build completes (reaches the COMPLETED phase) and reads the string &quot;COMPLETED&quot;; it stops.</p><br><p>This is the script:</p><br><pre><code>#!/bin/bash<br># Deploy the Layers<br><br>BUILD_PROJECT=$1<br>ENVIRONMENT=$2<br><br>if [ -z &quot;$BUILD_PROJECT&quot; ]<br>then<br>    echo &quot;BUILD_PROJECT is empty; exiting....&quot;<br>    exit 1<br>fi<br><br>if [ -z &quot;$ENVIRONMENT&quot; ]<br>then<br>    echo &quot;ENVIRONMENT is empty; exiting....&quot;<br>    exit 1<br>fi<br><br># Takes the final phase of codebuild deployment; which is COMPLETED<br>function getStatus {<br>    for ids in $BUILD_PROJECT<br>    do            <br>        id=$(aws codebuild list-builds-for-project --project-name &quot;${ids}-${ENVIRONMENT}&quot; | jq -r '.ids[0]')<br>        aws codebuild batch-get-builds --ids &quot;$id&quot; | jq -r '.builds[].phases[] | select (.phaseType==&quot;COMPLETED&quot;) | .phaseType'<br>    done<br>}<br><br>for project in $BUILD_PROJECT<br>do<br>    echo &quot;----------------------------------------------------&quot;<br>    echo &quot;Deploying: ${project}-${ENVIRONMENT}&quot;<br>    echo &quot;----------------------------------------------------&quot;<br>    aws codebuild start-build --project-name &quot;${project}-${ENVIRONMENT}&quot;<br><br>    while [[ $(getStatus) != &quot;COMPLETED&quot; ]]<br>    do<br>        if [[ $(getStatus) == &quot;COMPLETED&quot; ]]<br>        then<br>            echo $(getStatus)<br>            exit 0<br>        else<br>            echo &quot;Deploying ${project}-${ENVIRONMENT}...&quot;<br>            getStatus<br>        fi<br>    done <br>done<br></code></pre><br><p>I would like to see as output something like:</p><br><pre><code>----------------------------------------------------<br>Deploying: lambda1-us2-stage<br>----------------------------------------------------<br>----------------------------------------------------<br>Deploying: lambda2-us2-stage<br>----------------------------------------------------<br>----------------------------------------------------<br>Deploying: lambda3-us2-stage<br>----------------------------------------------------<br>Deploying lambda1-us2-stage...<br>Deploying lambda2-us2-stage...<br>Deploying lambda3-us2-stage...<br>Deploying lambda1-us2-stage...<br>Deploying lambda2-us2-stage...<br>Deploying lambda3-us2-stage...<br>lambda1-us2-stage COMPLETED<br>lambda2-us2-stage COMPLETED<br>lambda3-us2-stage COMPLETED<br></code></pre><br><p>And after all 3 are finished; end the script.</p><br><p>The current output is:</p><br><pre><code>----------------------------------------------------<br>Deploying: lambda1-us2-stage<br>----------------------------------------------------<br>Deploying lambda1-us2-stage...<br>COMPLETED<br>COMPLETED<br>Deploying lambda1-us2-stage...<br>COMPLETED<br>COMPLETED<br>Deploying lambda1-us2-stage...<br>COMPLETED<br>COMPLETED<br>Deploying lambda1-us2-stage...<br>COMPLETED<br>COMPLETED<br></code></pre><br><p>And so it iterates to infinity and only 1 lambda is deployed; the others are not.</p><br>
0.0,0.0,0.0,0.6666666666666666,0.6666666666666666,0.0,0.0,<h3>FileSystem and mountPoint in df -h</h3><p>I have an AWS instance; and I issued the following command <code>df -h</code> ; the below is the output:</p><br><pre><code>$ df -h<br>Filesystem      Size  Used Avail Use% Mounted on<br>devtmpfs        3.7G     0  3.7G   0% /dev<br>tmpfs           3.7G     0  3.7G   0% /dev/shm<br>tmpfs           3.7G  484K  3.7G   1% /run<br>tmpfs           3.7G     0  3.7G   0% /sys/fs/cgroup<br>/dev/xvda9       22G  5.4G   16G  26% /<br>/dev/xvda3      985M  589M  345M  64% /usr<br>tmpfs           3.7G     0  3.7G   0% /tmp<br>tmpfs           3.7G     0  3.7G   0% /media<br>/dev/xvda1      128M   37M   92M  29% /boot<br>/dev/xvda6      108M   64K   99M   1% /usr/share/oem<br>/dev/xvdh       493G   86G  382G  19% /mnt/ebs0<br>/dev/xvdi       493G   73M  467G   1% /mnt/data<br>/dev/xvdj       296G  2.2G  278G   1% /mnt/state<br><br></code></pre><br><p>I am not clear on <code>FileSystem</code> and <code>Mounted on</code>. Does <code>Mounted on</code> mean the local directory where the external disk/volume is attached to?</p><br><p>Also; what does <code>FileSystem</code> mean? I have heard about <code>NTFS</code>; <code>NFS</code> etc. Is it the same in this context? For example; what does <code>/dev/xvdj</code> mean? Is it an external disk/volume attached and what's its filesystem? I tried to find info on online; didn't get satisfactory detail.</p><br>
0.0,0.6666666666666666,0.6666666666666666,0.0,0.6666666666666666,0.0,0.0,<h3>Are there any security issues if I open a port for public on my AWS EC2 for running slackbot</h3><p>I am making a Slackbot on my AWS EC2; and I need to open port 3000 for public to listen post requests from Slack whenever users do some actions because Slack doesn't provide their IP range.</p><br><p>I wonder if there are any security issues with my EC2 if I open a port publicly ? I also use this EC2 to run Airflow.</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.0,<h3>After code deployment; an updated aws cloudformation nested stack custom parameter from console gets its default value</h3><p>I am trying to update a custom parameter on CloudFormation nested stack from console. It is updated but after code deployment; it gets default value. I want to save last updated value after deployment. I used a bash script as a build command on buildspec.yml. It works if there is a stack but does not work for a stack + a nested stack. Does anyone have any idea about it? I want to use this for updating a parameter from console without code deployment and save it for further usage if it is not changed.</p><br><p><a href="https://i.stack.imgur.com/0v30t.png" rel="nofollow noreferrer">console</a></p><br><p>publish.sh</p><br><pre><code>mkdir -p build<br><br># SAM Package<br>aws cloudformation package --template template-build.yml --s3-bucket ${DEPLOYMENT_BUCKET} --output-template build/template.yml --debug<br><br># SAM Deploy<br>aws cloudformation deploy --template-file build/template.yml --stack-name ${STACK_NAME} --capabilities CAPABILITY_IAM CAPABILITY_AUTO_EXPAND --role-arn ${ROLE_ARN} \<br>        --s3-bucket $DEPLOYMENT_BUCKET \<br>        --parameter-overrides \<br>        DeploymentBucketName=${DEPLOYMENT_BUCKET} \<br>        NodeModulesZipFileName=${packageJsonHash}.zip<br></code></pre><br><p>buildspec.yml</p><br><pre><code>version: 0.2<br>phases:<br>  install:<br>    runtime-versions:<br>      python: 3.7<br>      nodejs: 10<br>    commands:<br>      - yum install python3-pip -y<br>      - whereis pip<br>      - pip3 install aws-sam-cli --upgrade --user<br>      - pip3 install awscli --upgrade --user<br>  pre_build:<br>    commands:<br>      - mkdir -p build/<br>  build:<br>    commands:<br>      - echo Build started on `date`<br>      - npm install --only=prod<br>      - chmod +x publish.sh<br>      - bash publish.sh<br>  post_build:<br>    commands:<br>      - echo &quot;no post build needed...&quot;<br>artifacts:<br>  type: zip<br>  files:<br>    - build/template.yml<br><br></code></pre><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>semantic content recommendation system with Amazon SageMaker; storing in S3</h3><p>I am fairly new to AWS and Sagemaker and have decided to follow some of the tutorials Amazon has to familiarize myself with it. I've been following this one (<a href="https://aws.amazon.com/getting-started/hands-on/semantic-content-recommendation-system-amazon-sagemaker/5/" rel="nofollow noreferrer">tutorial</a>) and I've realized that it's an older tutorial using Sagemaker v1. I've been able to look up and change whatever is needed for the tutorial to work in v2 but I became stuck at this part for storing the training data in a S3 bucket to deploy the model.</p><br><pre><code>import io<br>import sagemaker.amazon.common as smac<br><br>print('train_features shape = '; predictions.shape)<br>print('train_labels shape = '; labels.shape)<br>buf = io.BytesIO()<br>smac.write_numpy_to_dense_tensor(buf; predictions; labels)<br>buf.seek(0)<br><br>bucket = BUCKET<br>prefix = PREFIX<br>key = 'knn/train'<br>fname = os.path.join(prefix; key)<br>print(fname)<br>boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)<br>s3_train_data = 's3://{}/{}/{}'.format(bucket; prefix; key)<br>print('uploaded training data location: {}'.format(s3_train_data))<br></code></pre><br><p>It returns this error</p><br><pre><code>NameError Traceback (most recent call <br>last)<br>&lt;ipython-input-20-9e52dd949332&gt; in &lt;module&gt;<br> 3<br> 4<br>----&gt; 5 print('train_features shape = '; predictions.shape)<br> 6 print('train_labels shape = '; labels.shape)<br> 7 buf = io.BytesIO()<br>NameError: name 'predictions' is not defined<br></code></pre><br><p>I'm curious as to why this would have worked in Sagemaker v1 and not v2 if predictions is not defined and if anyone could point me in the right direction for correcting this.</p><br><p>Thanks.</p><br>
0.0,0.0,1.0,0.0,0.6666666666666666,0.6666666666666666,0.0,<h3>How to add Policies to AWS Lambda function using the .yaml file?</h3><p>I am developing a REST API with <code>AWS Lambda</code>; <code>API Gateway</code>; <code>RDS (MySQL)</code>. I am using the <code>aws-sam</code> tool to build; configure and publish my work to cloud.</p><br><p>Please check the below <code>template.yaml</code> file which I am using now.</p><br><pre><code>AWSTemplateFormatVersion: '2010-09-09'<br>Transform: AWS::Serverless-2016-10-31<br>Description: &gt;<br>  aaaa-restapi<br><br>  Sample SAM Template for aaaa-restapi<br><br># More info about Globals: https://github.com/awslabs/serverless-application-model/blob/master/docs/globals.rst<br>Globals:<br>  Function:<br>    Timeout: 100<br><br>Resources:<br>  GetAllAccountTypesLambda:<br>    Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction<br>    Properties:<br>      CodeUri: aaaa-restapi<br>      Handler: com.aaaa.dao.accountingtype.GetAllAccountTypesLambda::getAllAccountTypes<br>      Runtime: java11<br>      MemorySize: 1024<br>      Environment: # More info about Env Vars: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#environment-object<br>        Variables:<br>          PARAM1: VALUE<br>      Events:<br>        HelloWorld:<br>          Type: Api # More info about API Event Source: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#api<br>          Properties:<br>            Path: /accounttype<br>            Method: get<br></code></pre><br><p>However to enable my lambda function to find the database; I had to enable some policies from the AWS Web console. I followed this link - <a href="https://ao.ms/the-provided-execution-role-does-not-have-permissions-to-call-createnetworkinterface-on-ec2/" rel="nofollow noreferrer">https://ao.ms/the-provided-execution-role-does-not-have-permissions-to-call-createnetworkinterface-on-ec2/</a></p><br><p>Below is the policy I created for my Lambda function in AWS web console.</p><br><pre><code>{<br>  &quot;Version&quot;: &quot;2012-10-17&quot;;<br>  &quot;Statement&quot;: [<br>    {<br>      &quot;Effect&quot;: &quot;Allow&quot;;<br>      &quot;Action&quot;: [<br>        &quot;ec2:DescribeNetworkInterfaces&quot;;<br>        &quot;ec2:CreateNetworkInterface&quot;;<br>        &quot;ec2:DeleteNetworkInterface&quot;;<br>        &quot;ec2:DescribeInstances&quot;;<br>        &quot;ec2:AttachNetworkInterface&quot;<br>      ];<br>      &quot;Resource&quot;: &quot;*&quot;<br>    }<br>  ]<br>}<br></code></pre><br><p>However there is no way I can do this in web console; from function to function. I need to get this done in the <code>yaml</code> file.</p><br><p>With my <code>yaml</code> file provided above; how can I put these permissions to my Lambda function?</p><br><p><strong>------------UPDATE---------------</strong></p><br><p>Following Gaurauv's comment; I made the following changes to the <code>yaml</code> file.</p><br><pre><code>AWSTemplateFormatVersion: '2010-09-09'<br>Transform: AWS::Serverless-2016-10-31<br>Description: &gt;<br>  aaaa-restapi<br><br>  Sample SAM Template for aaaa-restapi<br><br># More info about Globals: https://github.com/awslabs/serverless-application-model/blob/master/docs/globals.rst<br>Globals:<br>  Function:<br>    Timeout: 100<br><br>Resources:<br>  GetAllAccountTypesLambda:<br>    Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction<br>    Properties:<br>      CodeUri: aaaa-restapi<br>      Handler: com.aaaa.dao.accountingtype.GetAllAccountTypesLambda::getAllAccountTypes<br>      Runtime: java11<br>      MemorySize: 1024<br>      Environment: # More info about Env Vars: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#environment-object<br>        Variables:<br>          PARAM1: VALUE<br>      Events:<br>        HelloWorld:<br>          Type: Api # More info about API Event Source: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#api<br>          Properties:<br>            Path: /accounttype<br>            Method: get<br>      Role: !GetAtt LambdaRole.Arn<br>  <br>  LambdaRole:<br>    Type: &quot;AWS::IAM::Role&quot;<br>    Properties:<br>      Path: &quot;/&quot;<br>      ManagedPolicyArns:<br>        - &quot;arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole&quot;<br>      Policies:<br>        - PolicyName: 'ec2-access-policy'<br>          PolicyDocument:<br>            Statement:<br>              - Effect: Allow<br>                Action:<br>                  - ec2:DescribeNetworkInterfaces<br>                  - ec2:CreateNetworkInterface<br>                  - ec2:DeleteNetworkInterface<br>                  - ec2:DescribeInstances<br>                  - ec2:AttachNetworkInterface<br>                Resource: '*'<br></code></pre><br><p>However it failed to deploy; following error was produced.</p><br><pre><code>CREATE_FAILED                           AWS::IAM::Role                          LambdaRole                              Property AssumeRolePolicyDocument<br>                                                                                                                        cannot be empty.<br></code></pre><br>
0.0,0.3333333333333333,0.3333333333333333,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>Handling a event return on AWS lambda</h3><p>I am currently using the lambda to send a API details to get a access token. I then want to save that access token to use later on. It seems it doesn't save the access key. Any tips?<br>Lambda:</p><br><pre><code>var axios = require('axios');<br>var qs = require('qs');<br><br>exports.handler = async (event) =&gt; {<br>var data = qs.stringify({<br>  'client_id': 'theID';<br>  'client_secret': 'theSecret';<br>  'scope': 'ski';<br>  'grant_type': 'client_credentials' <br>});<br>var config = {<br>  method: 'post';<br>  url: 'myendpoint';<br>  headers: { <br>    'Content-Type': 'application/x-www-form-urlencoded'<br>  };<br>  data : data;<br>};    <br><br>        const postData = await axios.post('myendpoint';data; config)<br>        console.log(postData);<br>        <br>        <br>    const response = {<br>        statusCode: 200;<br>        body: JSON.stringify(postData.response.data.access_token);<br>    };<br>    var mykey = data.access_token;<br>    return response;<br>};<br></code></pre><br><p>Here is my Error Report:</p><br><pre><code> Response<br>    {<br>      &quot;errorType&quot;: &quot;TypeError&quot;;<br>      &quot;errorMessage&quot;: &quot;Cannot read property 'data' of undefined&quot;;<br>      &quot;trace&quot;: [<br>        &quot;TypeError: Cannot read property 'data' of undefined&quot;;<br>        &quot;    at Runtime.exports.handler (/var/task/index.js:26:48)&quot;;<br>        &quot;    at processTicksAndRejections (internal/process/task_queues.js:93:5)&quot;<br>      ]<br>    }<br></code></pre><br><p>Here is my logs:</p><br><pre><code>    Function Logs<br>    gOutput];<br>        agent: Agent {<br>          _events: [Object: null prototype];<br>          _eventsCount: 2;<br>          _maxListeners: undefined;<br>          defaultPort: 443;<br>          protocol: 'https:';<br>          options: [Object];<br>          requests: {};<br>          sockets: [Object];<br>          freeSockets: {};<br>          keepAliveMsecs: 1000;<br>          keepAlive: false;<br>          maxSockets: Infinity;<br>          maxFreeSockets: 256;<br>          scheduling: 'fifo';<br>          maxTotalSockets: Infinity;<br>          totalSocketCount: 1;<br>          maxCachedSessions: 100;<br>          _sessionCache: [Object];<br>          [Symbol(kCapture)]: false<br>        };<br>        socketPath: undefined;<br>        method: 'POST';<br>        maxHeaderSize: undefined;<br>        insecureHTTPParser: undefined;<br>        path: 'API/connect/token';<br>        _ended: true;<br>        res: IncomingMessage {<br>          _readableState: [ReadableState];<br>          _events: [Object: null prototype];<br>          _eventsCount: 3;<br>          _maxListeners: undefined;<br>          socket: [TLSSocket];<br>          httpVersionMajor: 1;<br>          httpVersionMinor: 1;<br>          httpVersion: '1.1';<br>          complete: true;<br>          headers: [Object];<br>          rawHeaders: [Array];<br>          trailers: {};<br>          rawTrailers: [];<br>          aborted: false;<br>          upgrade: false;<br>          url: '';<br>          method: null;<br>          statusCode: 200;<br>          statusMessage: 'OK';<br>          client: [TLSSocket];<br>          _consuming: false;<br>          _dumped: false;<br>          req: [Circular *1];<br>          responseUrl: 'myendpoint';<br>          redirects: [];<br>          [Symbol(kCapture)]: false;<br>          [Symbol(RequestTimeout)]: undefined<br>        };<br>        aborted: false;<br>        timeoutCb: null;<br>        upgradeOrConnect: false;<br>        parser: null;<br>        maxHeadersCount: null;<br>        reusedSocket: false;<br>        host: 'myhost';<br>        protocol: 'https:';<br>        _redirectable: Writable {<br>          _writableState: [WritableState];<br>          _events: [Object: null prototype];<br>          _eventsCount: 2;<br>          _maxListeners: undefined;<br>          _options: [Object];<br>          _ended: true;<br>          _ending: true;<br>          _redirectCount: 0;<br>          _redirects: [];<br>          _requestBodyLength: 81;<br>          _requestBodyBuffers: [];<br>          _onNativeResponse: [Function (anonymous)];<br>          _currentRequest: [Circular *1];<br>          _currentUrl: 'myendpoint';<br>          [Symbol(kCapture)]: false<br>        };<br>        [Symbol(kCapture)]: false;<br>        [Symbol(kNeedDrain)]: false;<br>        [Symbol(corked)]: 0;<br>        [Symbol(kOutHeaders)]: [Object: null prototype] {<br>          accept: [Array];<br>          'content-type': [Array];<br>          'user-agent': [Array];<br>          'content-length': [Array];<br>          host: [Array]<br>        }<br>      };<br>      data: {<br>        access_token: 'ThisIsMyAccessKeyIGetBackAndWantToSave';<br>        expires_in: 3600;<br>        token_type: 'Bearer'<br>      }<br>    }<br>    2021-04-10T10:41:22.858Z    e5e3bc3a-f08d-4077-8897-709225abd5f8    ERROR   Invoke Error    {&quot;errorType&quot;:&quot;TypeError&quot;;&quot;errorMessage&quot;:&quot;Cannot read property 'data' of undefined&quot;;&quot;stack&quot;:[&quot;TypeError: Cannot read property 'data' of undefined&quot;;&quot;    at Runtime.exports.handler (/var/task/index.js:26:48)&quot;;&quot;    at processTicksAndRejections (internal/process/task_queues.js:93:5)&quot;]}<br>    END RequestId: e5e3bc3a-f08d-4077-8897-709225abd5f8<br>    REPORT RequestId: e5e3bc3a-f08d-4077-8897-709225abd5f8  Duration: 1631.22 ms    Billed Duration: 1632 ms    Memory Size: 128 MB Max Memory Used: 19 MB<br>    <br>    Request ID<br>    e5e3bc3a-f08d-4077-8897-709225abd5f8<br></code></pre><br><p>What I am doing is sending client details to this endpoint and it returns a access key that is valid for a certain period. It does give a massive body back that is not needed and I only need to save the info in Data: but it keeps giving me the mentioned error even though I do get the correct response.</p><br>
0.0,0.0,0.0,0.6666666666666666,0.3333333333333333,1.0,0.0,<h3>Java - convert word to pdf directly in AWS S3 bucket without having to download locally</h3><p>I am wondering if i can convert a word document to pdf document directly in the AWS S3 bucket meaning without having to download on my local drive...</p><br><p>I tried using the BytesArrayOutputStream without any luck..</p><br><p>thx</p><br>
0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,0.3333333333333333,0.3333333333333333,<h3>AWS instance change to https</h3><p>We now use AWS to set up our website; we're recently trying to set up a page that will allow our customers to send emails to us. We set up an EC2 instance to work as an email server; but it runs on HTTP. Since our website works on HTTPS; ajax can't send HTTP messages and we have to make the EC2 instance run on HTTPS; but I don't know how to do that.</p><br><pre><code>$.ajax({<br>    type: &quot;POST&quot;;<br>    url: &quot;https://ec2-*-*-*-*.*.compute.amazonaws.com/send&quot;;<br>    contentType: &quot;application/json; charset=utf-8&quot;;<br>    beforeSend: function(request) {<br>        request.setRequestHeader(&quot;Access-Control-Allow-Origin&quot;; &quot;*&quot;);<br>        request.setRequestHeader(&quot;Access-Control-Allow-Method&quot;; &quot;POST&quot;);<br>    };<br>    async: true;<br>    data: JSON.stringify({<br>        &quot;name&quot;: name;<br>        &quot;email&quot;: email;<br>        &quot;message&quot;: message<br>    });<br>    traditional: true;<br>    error: function(xhr; status; error) {<br>        var errorMessage = xhr.status + ': ' + xhr.statusText<br>        alert('Error - ' + errorMessage);<br>    };<br>    success: function(result) {<br>        alert(&quot; Good link&quot;);<br>    }<br>});<br></code></pre><br>
0.0,0.3333333333333333,1.0,0.0,0.0,0.0,0.0,<h3>How to block sql injection in web acl in aws?</h3><p>adding few rules in web ACL in aws; following documetation I have sample template; but not sure if i need to add anything additional to block any sql injection. any idea?</p><br><pre><code> WebAcl:<br>    Type: AWS::WAFv2::WebACL<br>    Properties:<br>      Scope: REGIONAL<br>      DefaultAction:<br>        Allow: {}<br>      Rules:<br>      - Name: Block-SQLinjection<br>        Priority: 1<br>        Action:<br>          Block: {}<br>        Statement:<br>          SqliMatchStatement:<br>            FieldToMatch:<br>              AllQueryArguments: {}<br>            TextTransformations:<br>            - Priority: 1<br>        Type: NONE<br></code></pre><br>
0.3333333333333333,0.0,0.0,0.0,0.0,0.0,1.0,<h3>Do I need an AWS Software Development Kit (SDK) just to run my qualtrics survey through MTurk?</h3><p>I am running a Qualtrics survey through Amazon Mechanical Turk. I went through the required steps laid out here: <a href="https://docs.aws.amazon.com/AWSMechTurk/latest/AWSMechanicalTurkRequester/SetUpMturk.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/AWSMechTurk/latest/AWSMechanicalTurkRequester/SetUpMturk.html</a></p><br><p>I am very confused by 5; however. I don't understand why an SDK is required for what I'm doing (just linking to a survey).</p><br><p>I have very little familiarity with programming languages; and would like to avoid this step if possible.</p><br><p>Any input would be greatly appreciated!</p><br><p>As requested; I have pasted the steps below. Step 5 is the one I'm confused about.</p><br><p><strong>Step 1: Create a Mechanical Turk account</strong><br>To create an Amazon Mechanical Turk account; go to the Amazon Mechanical Turk Requester website; choose Create an account; and follow the on-screen instructions.</p><br><p>Note that Mechanical Turk accounts use the same login credentials and profiles as Amazon retail websites such as Amazon.com. Changes in the name or address on your account; on either Amazon.com or Mechanical Turk; are reflected in both locations.</p><br><p>To use Mechanical Turk programmatically; you must have an AWS account. If you don't already have an account; you are prompted to create one when you sign up. You're not charged for any AWS services that you sign up for unless you use them.</p><br><p>To create an AWS account</p><br><p>Open <a href="https://portal.aws.amazon.com/billing/signup" rel="nofollow noreferrer">https://portal.aws.amazon.com/billing/signup</a>.</p><br><p>Follow the online instructions.</p><br><p>Part of the sign-up procedure involves receiving a phone call and entering a verification code on the phone keypad.</p><br><p>Note your AWS account ID. You need it for the next step.</p><br><p><strong>Step 2: Link your AWS account to your Mechanical Turk requester account</strong><br>You need to link your AWS account to your Mechanical Turk requester account. This operation grants permission to your AWS account to access your requester account using the Mechanical Turk APIs.</p><br><p>Go to <a href="https://requester.mturk.com/developer/" rel="nofollow noreferrer">https://requester.mturk.com/developer/</a>.</p><br><p>Choose Link your AWS Account and sign in with your AWS root user email address and password.</p><br><p><strong>Step 3: Select a payment option</strong><br>Before you can post HITs to the Mechanical Turk marketplace; you need to enable AWS Billing for your account to pay worker rewards and Mechanical Turk fees. These appear on the AWS Anniversary Bill for your linked AWS account.</p><br><p>Alternatively; you can prepay for the HITs you plan to create using a credit card payment.</p><br><p>To enable AWS Billing or prepay for HITs; go to the account section of the Requester website.</p><br><p><strong>Step 4: Get an AWS access key</strong><br>Before you can access Mechanical Turk programmatically; you must have an AWS access key. Access keys consist of an access key ID and secret access key; which are used to sign programmatic requests that you make to AWS. If you don't have access keys; you can create them from the AWS Management Console. As a best practice; do not use the AWS account root user access keys for any task where they are not required. Instead; create a new administrator IAM user with access keys for yourself. To learn how; see Creating your first IAM admin user and group in the IAM User Guide. If you do not wish to grant administrator access to this account; you can choose either the AmazonMechanicalTurkFullAccess or AmazonMechanicalTurkReadOnly policy rather than AdministratorAccess when you attach a policy to the user.</p><br><p>The only time that you can view or download the secret access key is when you create the keys. You cannot recover them later. However; you can create new access keys at any time. You must also have permissions to perform the required IAM actions. For more information; see Permissions Required to Access IAM Resources in the IAM User Guide.</p><br><p>To create access keys for an IAM user:</p><br><p>Sign in to the AWS Management Console and open the IAM console at <a href="https://console.aws.amazon.com/iam/" rel="nofollow noreferrer">https://console.aws.amazon.com/iam/</a> .</p><br><p>In the navigation pane; choose Users.</p><br><p>Choose the name of the user whose access keys you want to create; and then choose the Security credentials tab.</p><br><p>In the Access keys section; choose Create access key.</p><br><p>To view the new access key pair; choose Show. You will not have access to the secret access key again after this dialog box closes. Your credentials should resemble the following example:</p><br><p>Access key ID: AKIAIOSFODNN7EXAMPLE</p><br><p>Secret access key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</p><br><p>To download the key pair; choose Download .csv file. Store the keys in a secure location. You will not have access to the secret access key again after this dialog box closes.</p><br><p>Keep the keys confidential in order to protect your AWS account. Never email them. Do not share them outside your organization; even if an inquiry appears to come from AWS or Amazon.com. No one who legitimately represents Amazon will ever ask you for your secret key.</p><br><p>After you download the .csv file; choose Close. When you create an access key; the key pair is active by default; and you can use the pair right away.</p><br><p>Related topics</p><br><p>What Is IAM? in the IAM User Guide</p><br><p>AWS Security Credentials in AWS General Reference</p><br><p><strong>Step 5: Configure Your Credentials</strong><br>To access Mechanical Turk programmatically; you must configure your credentials to enable authorization for your applications.</p><br><p>There are several ways to do this. For example; you can manually create the credentials file to store your access key ID and secret access key. You also can use the aws configure command of the AWS CLI to automatically create the file. Alternatively; you can use environment variables. For more information about configuring your credentials; see the programming language-specific AWS SDK developer guide.</p><br><p>The Mechanical Turk API endpoint is only available in the us-east-1 Region so it is recommended that you configure your default Region as us-east-1. If you primarily work with a different default AWS Region; you can specify the us-east-1 Region and endpoint as part of your CLI or SDK requests to Mechanical Turk.</p><br><p>To install and configure the AWS CLI; see Installing; updating; and uninstalling the AWS CLI and Configuring the AWS CLI in the IAM User Guide; respectively.</p><br><p><strong>Step 6: Set up the developer sandbox</strong><br>You may wish to test your HITs in the Amazon Mechanical Turk sandbox testing environment to make sure they work as expected before publishing them in the Mechanical Turk marketplace. The sandbox is an environment where you can publish and work on HITs at no cost before publishing them in the production Mechanical Turk marketplace. The sandbox consists of a requester sandbox website and a worker sandbox website.</p><br><p>Create a requester account on the requester sandbox website; which is located at <a href="https://requestersandbox.mturk.com" rel="nofollow noreferrer">https://requestersandbox.mturk.com</a>. This follows the same procedure as creating a Mechanical Turk account described in Step 1: Create a Mechanical Turk account. You can use the same email address and account if you wish.</p><br><p>You also need to create a worker account on the worker sandbox website located at <a href="https://workersandbox.mturk.com" rel="nofollow noreferrer">https://workersandbox.mturk.com</a> to view your sandbox HITs as a worker. There is no charge for using the Mechanical Turk sandbox sites.</p><br><p>To create HITs in the sandbox using the Mechanical Turk APIs; you also need to link your AWS account to your sandbox requester account; as described in Step 2: Link your AWS account to your Mechanical Turk requester account; on the requester sandbox website.</p><br><p>To configure the AWS CLI or SDKs to access the sandbox instead of the production environment; you must set the API endpoint to be <a href="https://mturk-requester-sandbox.us-east-1.amazonaws.com" rel="nofollow noreferrer">https://mturk-requester-sandbox.us-east-1.amazonaws.com</a>. Refer to the AWS CLI Command Reference or SDK documentation for how best to do this.</p><br>
0.6666666666666666,0.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>aws elasticsearch least and greatest function error</h3><p>&quot;elasticsearch  endpoint  /_opendistro/_sql&quot;</p><br><p>Elasticsearch version:7.9</p><br><p>Reference link : <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-functions-conditional.html" rel="nofollow noreferrer">https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-functions-conditional.html</a></p><br><p>least function is n running properly with docker container  elastic search<br>but with aws elasticsearch as service it throwing below error.</p><br><p>{<br>&quot;error&quot;: {<br>&quot;reason&quot;: &quot;Invalid SQL query&quot;;<br>&quot;details&quot;: &quot;Function [LEAST] cannot be found or used here. Did you mean [CAST]?&quot;;<br>&quot;type&quot;: &quot;SemanticAnalysisException&quot;<br>};<br>&quot;status&quot;: 400</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.3333333333333333,<h3>How do retrieve the specific value of an AWS Secrets Manager secret?</h3><p>I have this java code that retrieves the values from a secret</p><br><pre><code>public class fetchSecrets {<br>    public static void main(String[] args) {<br>        String secretId = &quot;test&quot;;<br><br>        SecretsManagerClient secretsClient = SecretsManagerClient.builder()<br>                .region(Region.EU_WEST_1)<br>                .build();<br><br>        fetchPassword(secretsClient; secretId);<br>        secretsClient.close();<br><br>    }<br><br>    public static void fetchPassword(SecretsManagerClient secretsClient;String secretId){<br>        try {<br>            GetSecretValueRequest valueRequest = GetSecretValueRequest.builder()<br>                .secretId(secretId)<br>                .build();<br><br>            GetSecretValueResponse valueResponse = secretsClient.getSecretValue(valueRequest);<br>            String secret = valueResponse.secretString();<br>            System.out.println(secret);<br><br>        } catch (SecretsManagerException e) {<br>            System.err.println(e.awsErrorDetails().errorMessage());<br>            System.exit(1);<br>        }<br><br>    }<br><br>}<br><br></code></pre><br><p>when i run this; i get:</p><br><pre><code>{&quot;username&quot;:&quot;test&quot;;&quot;password&quot;:&quot;password123456&quot;}<br></code></pre><br><p>How do i output <strong>only</strong> the value of the password or the username keys?<br>so expected output is e.g. <code>password123456</code></p><br>
0.0,0.0,0.0,1.0,0.0,1.0,0.0,<h3>Filtering List Query By Another Table&#39;s Field (a.k.a Cross-Table or Nested Filtering) in AWS Amplify GraphQL DynamoDB</h3><p><strong>Which Category is your question related to?</strong><br>DynamoDB; AppSync(GraphQL)</p><br><p><strong>Amplify CLI Version</strong><br>4.50.2</p><br><p><strong>Provide additional details e.g. code snippets</strong></p><br><p>BACKGROUND:<br>I'm new in AWS serverless app systems and as a frontend dev; I'm quite enjoying it thanks to auto-generated APIs; tables; connections; resolvers etc. I'm using Angular/Ionic in frontend and S3; DynamoDB; AppSync; Cognito; Amplify-cli for the backend.</p><br><p>WHAT I HAVE:<br>Here is a part of my schema. I can easily use auto-generated APIs to List/Get Feedbacks with additional filters (i.e. score: { ge: 3 }). And thanks to the @connection I can see the User's details in the listed Feedback items.</p><br><pre><code>type User @model @auth(rules: [{ allow: owner }]) {<br>  id: ID!<br>  email: String!<br>  name: String!<br>  region: String!<br>  sector: String!<br>  companyType: String!<br>}<br><br>type Feedback @model @auth(rules: [{ allow: owner }]) {<br>  id: ID!<br>  user: User @connection<br>  score: Int!<br>  content: String<br>}<br></code></pre><br><p>WHAT I WANT:<br>I want to list Feedbacks based on several fields on User type; such as user's region (i.e. user.region: { contains: 'United States' }). Now I searched for a solution quite a lot like; <a href="https://github.com/aws-amplify/amplify-cli/issues/2311" rel="nofollow noreferrer">#2311</a> ; and I learned that amplify codegen only creates top-level filtering. In order to use cross-table filtering; I believe I need to modify resolvers; lambda functions; queries and inputs. Which; for a beginner; it looks quite complex.</p><br><p>WHAT I TRIED/CONSIDERED:</p><br><ol><br><li>I tried listing all Users and Feedbacks separately and filtering them in front-end. But then the client downloads all these unnecessary data. Also because of the pagination limit; user experience takes a hit as they see an empty list and repeatedly need to click Load More button.</li><br><li>Thanks to some suggestions; I also thought about duplicating the User details in Feedback table to be able to search/filter them. Then the problem is that if User updates his/her info; duplicated values will be out-of-date. Also there will be too many duplicated data; as I need this feature for other tables also.</li><br><li>I also heard about using ElasticSearch for this problem but someone mentioned for a simple filtering he got 30$ monthly cost; so I got cold feet.</li><br><li>I tried the resolver solution to add a custom filtering in it. But I found that quite complex for a beginner. Also I will need this cross-table filtering in many other tables as well; so I think would be hard to manage. If that is the best-practice; I'd appreciate it if someone can guide me through it.</li><br></ol><br><p>QUESTIONS:</p><br><ol><br><li>What would be the easiest/beginner-friendly solution for me to achieve this cross-table filtering? I am open to alternative solutions.</li><br><li>Is this cross-table filtering a bad approach for a no-SQL setup? Since I need some relationship between two tables. (I thought @connection would be enough). Should I switch to an SQL setup before it is too late?</li><br><li>Is it possible for Amplify to auto-generate a solution for this in the future? I feel like many people are experiencing the same issue.</li><br></ol><br><p>Thank you in advance.</p><br>
0.0,0.0,1.0,0.0,1.0,0.0,0.0,<h3>Sharing an AWS EC2 image encrypted with an AWS managed key across the accounts</h3><p>I like to share the image across accounts but the image is encrypted with an AWS managed key and I was wondering how I can transfer this image to another account.</p><br><p>I gather an image encrypted with custome keys is transferrable; and is it the same with the image with an AWS key?</p><br>
0.0,0.3333333333333333,0.0,0.3333333333333333,1.0,0.6666666666666666,0.0,<h3>Can AWS Lambda serve static .png icons to front end UI from its /tmp folder located in the lambda function&#39;s source code</h3><p>have a good day.<br>I'm new to AWS. I have created a web application using angular; a backend using NodeJS and Express framework. I copied the 'dist' folder(output of angular built) into the backend root and served the frontend from the app.js file of backend.<br>Deployed this backend to EC2 instance with elastic beanstalk. It was working great. The backend had a 'public' folder containing all the required '.png' icons to be served to the frontend UI on load in the browser.</p><br><p>But now the requirement is to deploy this solution in a serverless architecture since it is cost effective and zero server maintenance. I have deployed the angular dist folder to AWS amplify; working great. And deployed the NodeJS Express backend to AWS Lambda as a function using serverless framework available as a NodeJS library; got an endpoint link too that I already added in the angular frontend using proxy; so that all the requests for icons from browser will be redirected to the lambda based function endpoint.</p><br><p>The issue is that the front end is served to the browser from amplify but the icons are not getting loaded from the lambda based function backend. The icons are still in the 'public' folder of the backend which is deployed on the lambda using serverless framwork. The web app successfully connects with the backend in lambda; authorizes users successfully. The only thing it doesn't server those static images.</p><br><p>Is there any way that AWS lambda backend can serve the .png icons to the frontend in browser as the backend serves if deployed in AWS EC2 server.</p><br><p>Regards</p><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>Unity Cross Platform Third Party Login with AWS Cognito Hosted UI</h3><p>I'm trying to build a login scene for my app. Using AWS Cognito user pools I have managed to build a generic sample of code which handles the code exchange for login through Google and Facebook through a URL provided from the hosted UI. For Example:</p><br><p><a href="https://appname.auth.eu-central-1.amazoncognito.com/oauth2/authorize?response_type=CODE&amp;scope=openid%20email&amp;redirect_uri=...&amp;...identity_provider=Google/Facebook.." rel="nofollow noreferrer">https://appname.auth.eu-central-1.amazoncognito.com/oauth2/authorize?response_type=CODE&amp;scope=openid%20email&amp;redirect_uri=...&amp;...identity_provider=Google/Facebook..</a>.</p><br><p>All that is left to implement is how unity will prompt the user with this URL. I want some kind of dialog box to popup and not call Application.OpenURL which simply opens the browser. This is fairly common around android/IOS apps. I have tried looking into some embedded browser solutions for that but it seems overwhelmingly complicated.</p><br><p>My question is what is the simplest and lightest way to achieve the login dialog box; will I have to use embedded browsers like this open source repo: <a href="https://github.com/gree/unity-webview" rel="nofollow noreferrer">https://github.com/gree/unity-webview</a>. Am I missing something trying to achieve my goal?</p><br><p>I'm sorry that there are no code samples to show; since I still don't know what plugin/UI component to use.</p><br>
0.0,0.0,0.0,0.6666666666666666,0.0,0.6666666666666666,0.3333333333333333,<h3>Error while converting image file from AWS S3 Object to Base64</h3><p>I am using the below code for parsing S3 object into Base64 string.</p><br><pre><code>InputStream is = s3Object.getObjectContent().getDelegateStream();<br>byte[] bytes = IOUtils.toByteArray(is);<br>String base64Data = Base64.getEncoder().encodeToString(bytes); //Java Util lib.<br></code></pre><br><p>This code works fine for text and pdf file. But while converting image files I am getting error MIME type not supported.</p><br><p>I also tried to use AWS and Apache Commons Base64 lib. still it is not working.</p><br><pre><code>Base64.encodeAsString(bytes) //AWS<br>Base64.encodeBase64String(bytes) //Apache Commons<br></code></pre><br>
0.0,0.0,0.0,0.0,0.6666666666666666,1.0,0.0,<h3>sam package too large when deploying custom runtime lambda</h3><p>So I replicated <a href="https://github.com/aws-samples/sessions-with-aws-sam/tree/master/swift-custom-runtime" rel="nofollow noreferrer">this project</a> which uses <strong>swift</strong> as custom lambda runtime using a makefile as build method.</p><br><p>Now I created a AWS CodePipeline that packages my project using CodeBuild using <code>sam package</code> and finally deploys it via CloudFormation.</p><br><p>The <code>codeUri</code> of my lambda is set at the root folder like you see in the repo I linked above. I think that is how it should as I saw that as well in the sam documentation under the custom runtime section. The problem with that is that <code>sam package</code> packages my entire project and lambda is complaining at deploy time that the zip is too large.</p><br><p>How would I set up the <code>makefile</code> as well as the <code>template.yml</code> so that <code>sam package</code> only packages my lambdas?</p><br>
0.0,0.3333333333333333,0.0,0.6666666666666666,1.0,0.0,0.0,<h3>Cloning environment in elastic beanstalk</h3><p>I am wanting to update my Rails EB Linux to 2.12.2 from 1.11.8; so I cloned the environment and committed to it but I am getting this error:</p><br><pre><code>PG::ConnectionBad (could not connect to server: Connection timed out<br>    Is the server running on host &quot;example.ccexample.us-east-1.rds.amazonaws.com&quot; (111.11.21.22) and accepting<br>    TCP/IP connections on port 5432?<br></code></pre><br><p><strong>Another error -- likely the same issue?:</strong></p><br><pre><code>/opt/elastticbeanstalk/hooks/appdeploy/pre/12_db_migration.sh failed.<br></code></pre><br><p>My env variables are all correct so shouldn't the database just simply connect?</p><br><p>This is the error log:</p><br><pre><code>    [2020-12-31T22:05:28.834Z] INFO  [5012]  - [Application update app-example/AppDeployStage0/AppDeployPreHook/12_db_migration.sh] : Starting activity...<br>[2020-12-31T22:07:45.564Z] INFO  [5012]  - [Application update example/AppDeployStage0/AppDeployPreHook/12_db_migration.sh] : Activity execution failed; because: ++ /opt/elasticbeanstalk/bin/get-config container -k script_dir<br>  + EB_SCRIPT_DIR=/opt/elasticbeanstalk/support/scripts<br>  ++ /opt/elasticbeanstalk/bin/get-config container -k app_staging_dir<br>  + EB_APP_STAGING_DIR=/var/app/ondeck<br>  ++ /opt/elasticbeanstalk/bin/get-config container -k app_user<br>  + EB_APP_USER=webapp<br>  ++ /opt/elasticbeanstalk/bin/get-config container -k support_dir<br>  + EB_SUPPORT_DIR=/opt/elasticbeanstalk/support<br>  + . /opt/elasticbeanstalk/support/envvars-wrapper.sh<br>  +++ /opt/elasticbeanstalk/bin/get-config container -k support_dir<br>  ++ EB_SUPPORT_DIR=/opt/elasticbeanstalk/support<br>  ++ set +x<br>  + RAKE_TASK=db:migrate<br>  + . /opt/elasticbeanstalk/support/scripts/use-app-ruby.sh<br>  ++ . /usr/local/share/chruby/chruby.sh<br>  +++ CHRUBY_VERSION=0.3.9<br>  +++ RUBIES=()<br>  +++ for dir in '&quot;$PREFIX/opt/rubies&quot;' '&quot;$HOME/.rubies&quot;'<br>  +++ [[ -d /opt/rubies ]]<br>  ++++ ls -A /opt/rubies<br>  +++ [[ -n ruby-2.4.10<br>  ruby-2.5.8<br>  ruby-2.6.6<br>  ruby-current ]]<br>  +++ RUBIES+=(&quot;$dir&quot;/*)<br>  +++ for dir in '&quot;$PREFIX/opt/rubies&quot;' '&quot;$HOME/.rubies&quot;'<br>  +++ [[ -d /.rubies ]]<br>  +++ unset dir<br>  +++ cat /etc/elasticbeanstalk/.ruby_version<br>  ++ chruby 2.5.8<br>  ++ case &quot;$1&quot; in<br>  ++ local dir match<br>  ++ for dir in '&quot;${RUBIES[@]}&quot;'<br>  ++ dir=/opt/rubies/ruby-2.4.10<br>  ++ case &quot;${dir##*/}&quot; in<br>  ++ for dir in '&quot;${RUBIES[@]}&quot;'<br>  ++ dir=/opt/rubies/ruby-2.5.8<br>  ++ case &quot;${dir##*/}&quot; in<br>  ++ match=/opt/rubies/ruby-2.5.8<br>  ++ for dir in '&quot;${RUBIES[@]}&quot;'<br>  ++ dir=/opt/rubies/ruby-2.6.6<br>  ++ case &quot;${dir##*/}&quot; in<br>  ++ for dir in '&quot;${RUBIES[@]}&quot;'<br>  ++ dir=/opt/rubies/ruby-current<br>  ++ case &quot;${dir##*/}&quot; in<br>  ++ [[ -z /opt/rubies/ruby-2.5.8 ]]<br>  ++ shift<br>  ++ chruby_use /opt/rubies/ruby-2.5.8 ''<br>  ++ [[ ! -x /opt/rubies/ruby-2.5.8/bin/ruby ]]<br>  ++ [[ -n '' ]]<br>  ++ export RUBY_ROOT=/opt/rubies/ruby-2.5.8<br>  ++ RUBY_ROOT=/opt/rubies/ruby-2.5.8<br>  ++ export RUBYOPT=<br>  ++ RUBYOPT=<br>  ++ export PATH=/opt/rubies/ruby-2.5.8/bin:/opt/elasticbeanstalk/lib/ruby/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/sbin:/sbin:/bin<br>  ++ PATH=/opt/rubies/ruby-2.5.8/bin:/opt/elasticbeanstalk/lib/ruby/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/sbin:/sbin:/bin<br>  +++ /opt/rubies/ruby-2.5.8/bin/ruby -<br>  ++ eval 'export RUBY_ENGINE=ruby;<br>  export RUBY_VERSION=2.5.8;<br>  export GEM_ROOT=&quot;/opt/rubies/ruby-2.5.8/lib/ruby/gems/2.5.0&quot;;'<br>  +++ export RUBY_ENGINE=ruby<br>  +++ RUBY_ENGINE=ruby<br>  +++ export RUBY_VERSION=2.5.8<br>  +++ RUBY_VERSION=2.5.8<br>  +++ export GEM_ROOT=/opt/rubies/ruby-2.5.8/lib/ruby/gems/2.5.0<br>  +++ GEM_ROOT=/opt/rubies/ruby-2.5.8/lib/ruby/gems/2.5.0<br>  ++ ((  0 != 0  ))<br>  + cd /var/app/ondeck<br>  + su -s /bin/bash -c 'bundle exec /opt/elasticbeanstalk/support/scripts/check-for-rake-task.rb db:migrate' webapp<br></code></pre><br><p>I also updated the config.yml with the new environment names:</p><br><p>branch-defaults:<br>master:<br>environment: NewName<br>environment-defaults:<br>NewName:<br>branch: null<br>repository: null<br>RevoltVendor-env:<br>branch: null<br>repository: null<br>global:<br>application_name: App Name<br>default_ec2_keyname: null<br>default_platform: Puma with Ruby 2.5 running on 64bit Amazon Linux<br>default_region: us-east-1<br>include_git_submodules: true<br>instance_profile: null<br>platform_name: null<br>platform_version: null<br>profile: eb-cli<br>sc: git<br>workspace_type: Application</p><br><p>Any help would be appreciated</p><br>
0.0,1.0,0.0,0.0,0.0,0.0,0.0,<h3>How to reduce Connecting time incurred for a request</h3><p>I am trying to optimize speed of website and during this process I have found that. A connecting time of around 117ms is incurred every time I request a page. Please see screenshot below for detailed information<br><a href="https://i.stack.imgur.com/oi60v.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/oi60v.png" alt="enter image description here" /></a><br>This is persistent. Is there any way to reduce this time? What could be the possible reasons:</p><br><p>Is this because of SSL? This website is only accessible.<br>Or is it an issue with DNS? I am using Godaddy DNS as of now. Will switching to AWS Route 53 provide any benefit.<br>I am using apache server.</p><br>
0.0,0.0,1.0,0.6666666666666666,0.0,0.0,0.0,<h3>Will the CreateTable action (with tags supplied) in AWS DynamoDB fail if an SCP is applied that restricts tagging</h3><p>For example; let's say I have account with an SCP applied which denies all tagging and untagging in DynamoDB in that account.</p><br><p>Will all roles in that account be prevented from creating tables in DynamoDB that have tags?</p><br><p>For reference I am talking about the CreateTable action here:</p><br><p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_CreateTable.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_CreateTable.html</a></p><br>
0.0,0.0,1.0,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>How can I sync Terraform state with existing resources?</h3><p>I have a Terraform script that sets up 30 resources in all; including roles; policies and ECS infrastructure.</p><br><p>The state file was destroyed when switching git branches - I should've backed it up; but now I do not have a state file referencing the already existing resources.</p><br><p>I can't use <code>terraform apply</code> because the names already exist; and I can't clear the existing resources with <code>terraform destroy</code> because the state file now thinks the resources don't exist.</p><br><p>Is there a way I could sync with existing resources? I am aware of <code>terraform import</code> but to my knowledge that only works with one resource at a time.</p><br>
0.0,1.0,0.0,0.0,0.6666666666666666,0.0,0.0,<h3>Create my own proxy server by using AWS Linux EC2 instance</h3><p>I have tried to create my own proxy server by using AWS Linux EC2 instance. I followed the steps from this <a href="https://dev.to/viralsangani/be-anonymous-create-your-own-proxy-server-with-aws-ec2-2k63" rel="nofollow noreferrer">article</a> and this <a href="https://medium.com/@ihimanshurawat/how-to-setup-a-proxy-server-on-ec2-655db0c873d8" rel="nofollow noreferrer">article</a>. But after create the instance and configure the Tinyproxy and then configure browser to this proxy; it shows me an error. When I open the browser and try to search it shows me that <code>The connection has timed out. The server at www.reddit.com is taking too long to respond.</code> Here I was tried to open Reddit page. Even I am getting different public IP from curl command and Google. <code>curl ifconfig.co</code> gives me the public IPv4 address of AWS instance. And Google gives me the my devices public IP.</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.3333333333333333,<h3>Putting csvdecode in security group rule terraform</h3><p>I want to call csv function from variable.</p><br><p>Here is my main.tf file of security group</p><br><pre><code>resource &quot;aws_security_group&quot; &quot;names&quot; {<br>  count = length(var.ams_prod_sg_list)<br>  name        = var.ams_prod_sg_list[count.index].sg_name<br>  vpc_id = module.vpc.vpc_id_sg<br>  tags = {<br>    Name = var.ams_prod_sg_list[count.index].sg_tags<br>  }<br>}<br><br>resource &quot;aws_security_group_rule&quot; &quot;sg_rule&quot; {<br>  count             = length(var.ams_prod_sg_list)<br>  security_group_id = &quot;${aws_security_group.this.*.id}&quot;<br>  type              = var.ams_prod_sg_list[count.index].sg_rules.type<br>  protocol          = var.ams_prod_sg_list[count.index].sg_rules.protocol<br>  from_port         = var.ams_prod_sg_list[count.index].sg_rules.from<br>  to_port           = var.ams_prod_sg_list[count.index].sg_rules.to<br>  cidr_blocks       = [var.ams_prod_sg_list[count.index].sg_rules.cidr_blocks]<br>  description       = var.ams_prod_sg_list[count.index].sg_rules.description<br>}<br></code></pre><br><p>Here is the variable.tf file</p><br><pre><code>locals {<br>  test = csvdecode(file(&quot;${path.module}/csv/test.csv&quot;))<br>  test1 = csvdecode(file(&quot;${path.module}/csv/test1.csv&quot;))<br>}<br><br>variable &quot;ams_prod_sg_list&quot; {<br>  description = &quot;sg_name rules&quot;<br>  type        = list(map(string))<br>  default = [<br>    {<br>      sg_name = &quot;test&quot;<br>      sg_rules = local.test<br>      sg_tags = &quot;sg&quot;<br>    };<br>    {<br>      sg_name = &quot;test1&quot;<br>      sg_rules = local.test1<br>      sg_tags = &quot;&quot;<br>    };<br>  ]<br>}<br></code></pre><br><p>When I do terraform apply ; it shows <code>Variables may not be used here which means we cannot use local in variable</code>. And also when I directly put sg_rules = csvdecode(file(&quot;${path.module}/csv/test.csv&quot;)) ; it shows <code>Functions may not be called here</code></p><br><p>Here is the test.csv file</p><br><pre><code>type;protocol;from;to;cidr_blocks;description<br>ingress;-1;0;0;10.100.0.0/16;test<br>ingress;tcp;80;80;10.100.0.0/16;<br></code></pre><br><p>I also tried by putting this in variable and local</p><br><pre><code>variable &quot;ams_prod_sg_list&quot; {<br>  description = &quot;sg_name rules&quot;<br>  type        = list(map(string))<br>  default     = null<br>}<br><br>locals {<br>  default_ams_prod_sg_list = [<br>    {<br>      sg_name = &quot;test&quot;<br>      sg_rule = &quot;${local.test}&quot;<br>      sg_tags = &quot;sg&quot;<br>    };<br>    {<br>      sg_name = &quot;test1&quot;<br>      sg_rule = &quot;${local.test1}&quot;<br>      sg_tags = &quot;&quot;<br>    };<br>  ]<br><br>  ams_prod_sg_list = var.ams_prod_sg_list != null ? var.ams_prod_sg_list : local.default_ams_prod_sg_list<br>}<br></code></pre><br><p>now getting this error</p><br><pre><code>Error: Inconsistent conditional result types<br> <br>   on sg-variable.tf line 46; in locals:<br>   46:   ams_prod_sg_list = var.ams_prod_sg_list != null ? var.ams_prod_sg_list : local.default_ams_prod_sg_list<br>     <br>      local.default_ams_prod_sg_list is tuple with 2 elements<br>      var.ams_prod_sg_list is a list of map of string; known only after apply<br> <br> The true and false result expressions must have consistent types. The given<br> expressions are list of map of string and tuple; respectively.<br></code></pre><br><p>and I also tried putting this</p><br><pre><code>variable &quot;ams_prod_sg_list&quot; {<br>  description = &quot;sg_name rules&quot;<br>  type        = list(map(string))<br>  default     = null<br>}<br><br>locals {<br>  default_ams_prod_sg_list = tolist([<br>    tomap({<br>      sg_name = &quot;test&quot;<br>      sg_rule = &quot;${local.test}&quot;<br>      sg_tags = &quot;sg&quot;<br>    });<br>    tomap({<br>      sg_name = &quot;test1&quot;<br>      sg_rule = &quot;${local.test1}&quot;<br>      sg_tags = &quot;&quot;<br>    });<br>  ])<br><br>  ams_prod_sg_list = var.ams_prod_sg_list != null ? var.ams_prod_sg_list : local.default_ams_prod_sg_list<br>}<br></code></pre><br><p>getting this error</p><br><pre><code>Error: Unsupported attribute<br> <br>   on security-group.tf line 91; in resource &quot;aws_security_group_rule&quot; &quot;sg_rule&quot;:<br>   91:   type              = var.ams_prod_sg_list[count.index].sg_rules.type<br>     <br>      count.index is a number; known only after apply<br>      var.ams_prod_sg_list is a list of map of string; known only after apply<br> <br> This value does not have any attributes.<br><br><br> Error: Unsupported attribute<br> <br>   on security-group.tf line 92; in resource &quot;aws_security_group_rule&quot; &quot;sg_rule&quot;:<br>   92:   protocol          = var.ams_prod_sg_list[count.index].sg_rules.protocol<br>     <br>      count.index is a number; known only after apply<br>      var.ams_prod_sg_list is a list of map of string; known only after apply<br> <br> This value does not have any attributes.<br><br><br> Error: Unsupported attribute<br> <br>   on security-group.tf line 93; in resource &quot;aws_security_group_rule&quot; &quot;sg_rule&quot;:<br>   93:   from_port         = var.ams_prod_sg_list[count.index].sg_rules.from<br>     <br>      count.index is a number; known only after apply<br>      var.ams_prod_sg_list is a list of map of string; known only after apply<br> <br> This value does not have any attributes.<br><br><br> Error: Unsupported attribute<br> <br>   on security-group.tf line 94; in resource &quot;aws_security_group_rule&quot; &quot;sg_rule&quot;:<br>   94:   to_port           = var.ams_prod_sg_list[count.index].sg_rules.to<br>     <br>      count.index is a number; known only after apply<br>      var.ams_prod_sg_list is a list of map of string; known only after apply<br> <br> This value does not have any attributes.<br><br><br> Error: Unsupported attribute<br> <br>   on security-group.tf line 95; in resource &quot;aws_security_group_rule&quot; &quot;sg_rule&quot;:<br>   95:   cidr_blocks       = [var.ams_prod_sg_list[count.index].sg_rules.cidr_blocks]<br>     <br>      count.index is a number; known only after apply<br>      var.ams_prod_sg_list is a list of map of string; known only after apply<br> <br> This value does not have any attributes.<br><br><br> Error: Unsupported attribute<br> <br>   on security-group.tf line 96; in resource &quot;aws_security_group_rule&quot; &quot;sg_rule&quot;:<br>   96:   description       = var.ams_prod_sg_list[count.index].sg_rules.description<br>     <br>      count.index is a number; known only after apply<br>      var.ams_prod_sg_list is a list of map of string; known only after apply<br> <br> This value does not have any attributes.<br><br><br> Error: Invalid function argument<br> <br>   on sg-variable.tf line 34; in locals:<br>   34:     tomap({<br>   35:       sg_name = &quot;test&quot;<br>   36:       sg_rule = &quot;${local.test}&quot;<br>   37:       sg_tags = &quot;sg&quot;<br>   38:     });<br>     <br>      local.test is list of object with 2 elements<br> <br> Invalid value for &quot;v&quot; parameter: cannot convert object to map of any single<br> type.<br><br><br> Error: Invalid function argument<br> <br>   on sg-variable.tf line 39; in locals:<br>   39:     tomap({<br>   40:       sg_name = &quot;test1&quot;<br>   41:       sg_rule = &quot;${local.test1}&quot;<br>   42:       sg_tags = &quot;&quot;<br>   43:     });<br>     <br>      local.test1 is list of object with 1 element<br> <br> Invalid value for &quot;v&quot; parameter: cannot convert object to map of any single<br> type.<br></code></pre><br>
0.0,0.0,0.6666666666666666,1.0,0.0,0.3333333333333333,0.0,<h3>Terraform - using rds snapshot when available</h3><p>I try to find a solution for this problem.</p><br><p>When a rds instance is created with terraform it should use the most recent snapshot available. But when there is no snapshot available the rds instance should created without a snapshot.</p><br><p>I tried to use a data element but the apply always states:</p><br><blockquote><br><p>Your query returned no results. Please change your search criteria and try again.</p><br></blockquote><br><p>How can I achieve this? To give some more context. I would like to create a fresh environment with terraform or ensure that the rds instance is recreated with the latest snapshot with the same code.</p><br>
0.0,0.0,1.0,0.6666666666666666,0.6666666666666666,0.0,0.0,<h3>Why does my GetObjectRequest return a 403 Access Denied error when my Lambda has access to the S3 bucket?</h3><p>I have a Lambda function that makes a <code>GetObject</code> request to an S3 bucket.</p><br><p>However; I'm getting the following error:</p><br><pre><code>AccessDenied: Access Denied<br>    at deserializeAws_restXmlGetObjectCommandError (/node_modules/@aws-sdk/client-s3/dist-cjs/protocols/Aws_restXml.js:6284:41)<br>    at processTicksAndRejections (internal/process/task_queues.js:95:5)<br>    at /node_modules/@aws-sdk/middleware-serde/dist-cjs/deserializerMiddleware.js:6:20<br>    at /node_modules/@aws-sdk/middleware-signing/dist-cjs/middleware.js:11:20<br>    at StandardRetryStrategy.retry (/node_modules/@aws-sdk/middleware-retry/dist-cjs/StandardRetryStrategy.js:51:46)<br>    at /node_modules/@aws-sdk/middleware-logger/dist-cjs/loggerMiddleware.js:6:22<br>    at GetS3Data (/src/input.ts:21:26)<br>    at Main (/src/main.ts:8:34)<br>    at Runtime.run [as handler] (/handler.ts:6:9) {<br>  Code: 'AccessDenied';<br>  RequestId: '3K61PMQGW4825D3W';<br>  HostId: '5PpmWpu2I4WZPx37Y0pRfDAcdCmjX8fchuE+HLpUzy7uqoJirtb9Os0g96kWfluM/ctkn/mEC5o=';<br>  '$fault': 'client';<br>  '$metadata': {<br>    httpStatusCode: 403;<br>    requestId: undefined;<br>    extendedRequestId: '5PpmWpu2I4WZPx37Y0pRfDAcdCmjX8fchuE+HLpUzy7uqoJirtb9Os0g96kWfluM/ctkn/mEC5o=';<br>    cfId: undefined;<br>    attempts: 1;<br>    totalRetryDelay: 0<br>  }<br>}<br></code></pre><br><p>I've given access to the Lambda function to make this request.</p><br><p>What is the issue?</p><br><p><code>serverless.ts</code></p><br><pre><code>import type { AWS } from &quot;@serverless/typescript&quot;;<br><br>const serverlessConfiguration: AWS = {<br>    service: &quot;affiliations&quot;;<br>    frameworkVersion: &quot;2&quot;;<br>    custom: {<br>        esbuild: {<br>            bundle: true;<br>            minify: false;<br>            sourcemap: true;<br>            exclude: [&quot;aws-sdk&quot;];<br>            target: &quot;node14&quot;;<br>            define: { &quot;require.resolve&quot;: undefined };<br>            platform: &quot;node&quot;;<br>        };<br>    };<br>    plugins: [&quot;serverless-esbuild&quot;];<br>    provider: {<br>        name: &quot;aws&quot;;<br>        region: &quot;us-east-2&quot;;<br>        runtime: &quot;nodejs14.x&quot;;<br>        apiGateway: {<br>            minimumCompressionSize: 1024;<br>            shouldStartNameWithService: true;<br>        };<br>        environment: {<br>            AWS_NODEJS_CONNECTION_REUSE_ENABLED: &quot;1&quot;;<br>            NODE_OPTIONS: &quot;--enable-source-maps --stack-trace-limit=1000&quot;;<br>        };<br>        lambdaHashingVersion: &quot;20201221&quot;;<br>        vpc: {<br>            securityGroupIds: [&quot;&lt;redacted&gt;&quot;];<br>            subnetIds: [&quot;redacted&gt;&quot;];<br>        };<br>        iam: {<br>            role: {<br>                statements: [<br>                    {<br>                        Effect: &quot;Allow&quot;;<br>                        Action: [&quot;s3:GetObject&quot;];<br>                        Resource: &quot;&lt;redacted&gt;&quot;;<br>                    };<br>                ];<br>            };<br>        };<br>    };<br>    useDotenv: true;<br>    // import the function via paths<br>    functions: {<br>        run: {<br>            handler: &quot;handler.run&quot;;<br>            timeout: 300;<br>            events: [<br>                {<br>                    sns: {<br>                        arn: &quot;&lt;redacted&gt;&quot;;<br>                    };<br>                };<br>            ];<br>        };<br>    };<br>};<br><br>module.exports = serverlessConfiguration;<br></code></pre><br><p><code>s3.ts</code></p><br><pre><code>export const GetS3Data = async (payload: GetObjectRequest) =&gt; {<br>    try {<br>        const response = await S3Service.getObject(payload);<br><br>        const result = await new Promise((resolve; reject) =&gt; {<br>            const data = [];<br>            response.Body.on(&quot;data&quot;; (chunk) =&gt; data.push(chunk));<br>            response.Body.on(&quot;err&quot;; reject);<br>            response.Body.once(&quot;end&quot;; () =&gt; resolve(data.join(&quot;&quot;)));<br>        });<br><br>        return [result; null];<br>    } catch (err) {<br>        Logger.error({<br>            method: &quot;GetS3Data&quot;;<br>            error: err.stack;<br>        });<br><br>        return [null; err];<br>    }<br>};<br></code></pre><br><p><code>package.json</code></p><br><pre><code>&quot;@aws-sdk/client-s3&quot;: &quot;^3.36.0&quot;;<br></code></pre><br>
0.0,1.0,0.0,0.3333333333333333,1.0,0.0,0.0,<h3>Passing Amazon S3 Gzip HTML file Lambda NodeJS - using Cloudfront</h3><p>I have gzipped html files in an Amazon S3 bucket- they work as expected - I can open and view content in a browser.</p><br><p>I need to deliver these files via Lambda with Cloudfront. I'm finding the contents isn't usable on arrival.</p><br><p>Lambda Function</p><br><pre><code>var AWS = require('aws-sdk');<br>var s3 = new AWS.S3();<br><br>exports.handler = async (event; context; callback) =&gt; {<br>    <br>   <br>}<br></code></pre><br><p>thx</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,1.0,0.0,<h3>Starting Codebuild Job from Step Function Error: .... &quot;not authorized to create managed-rule&quot;</h3><p>I am working on a Step Function where one of the tasks is to start a Codebuild Job:</p><br><pre><code>      &quot;initiate_dbt_run&quot;: {<br>        &quot;Type&quot;: &quot;Task&quot;;<br>        &quot;Next&quot;: &quot;EndState&quot;;<br>        &quot;Resource&quot;: &quot;arn:aws:states:::codebuild:startBuild.sync&quot;;<br>        &quot;Parameters&quot;: {<br>          &quot;ProjectName&quot;: &quot;${Environment}-cbr-delivery-Dbt&quot;;<br>          &quot;EnvironmentVariablesOverride&quot;: [<br>          { <br>              &quot;Name&quot;: &quot;MODEL&quot;;<br>              &quot;Type&quot;: &quot;PLAINTEXT&quot;;<br>              &quot;Value&quot;: &quot;+tag:utc_10&quot;<br>          }<br>         ]<br>        }<br>      };<br></code></pre><br><p>I gave the following permissions to the State Machine ARN:</p><br><pre><code>      - Sid: CodebuildAccess<br>        Effect: Allow<br>        Action:<br>          - codebuild:StartBuild<br>          - codebuild:StopBuild<br>          - codebuild:BatchGetBuilds<br>        Resource:<br>          - !Sub &quot;arn:aws:codebuild:*:${AWS::AccountId}:project/${Environment}-cbr-delivery-Dbt&quot;<br>      - Sid: AllowCloudwatchEvents<br>        Effect: Allow<br>        Action:<br>          - events:PutTargets<br>          - events:PutRule<br>          - events:DescribeRule<br>        Resource:<br>          - !Sub &quot;arn:aws:events:*:${AWS::AccountId}:rule:/StepFunctionsGetEventForCodeBuildStartBuildRule&quot;<br></code></pre><br><p>Also; I'm not sure if this is needed; but through trouble shooting I gave these permissions to the Codebuild ARN</p><br><pre><code>    - PolicyName: statemachines<br>      PolicyDocument:<br>        Version: 2012-10-17<br>        Statement:<br>          - Resource:<br>              - !Sub &quot;arn:aws:states:*:${AWS::AccountId}:stateMachine:FivetranSyncStateMachine-*&quot;<br>            Effect: &quot;Allow&quot;<br>            Action:<br>              - states:StartExecution<br>              - states:StopExecution<br>              - states:DescribeExecution<br></code></pre><br><p>When I deploy the step function I'm met with this error:</p><br><p><code>Resource handler returned message: &quot;'arn:aws:iam::546291546746:role/sam-extract-load-FivetranSyncStateMachineRole-1GNR4UITB9 4K0' is not authorized to create managed-rule. Service:AWSStepFunctions;Status Code: 400;</code></p><br><p>Googling around for granting authorization to created a <code>managed-rule</code> did not yield much except this SO question (<a href="https://stackoverflow.com/questions/60612853/nested-step-function-in-a-step-function-unknown-error-not-authorized-to-cr">Nested Step Function in a Step Function: Unknown Error: &quot;...not authorized to create managed-rule&quot;</a>); which led me to add the Cloudwatch Events perms to the step function arn. Not sure what else to try...</p><br>
0.0,0.0,0.3333333333333333,0.0,1.0,1.0,0.0,<h3>How can I update a server less lambda application?</h3><p>I've cloned a git repository which has nodejs and html files; deployed it and everything worked. Now I'm trying to edit the index.html; and when I deploy the project again; it creates new cloud formation stacks instead of update the current one. How can I save/update changes to the application that is already working through api gateway url that this demo has created?</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>FileAlreadyExistsException: File already exists:s3</h3><p>I have a AWS glue job (PySpark) that needs to load data from a centralized data lake of size 350GB+; prepare it and load into a s3 bucket partitioned by two columns (date and geohash) Mind you that this is PROD data and the environment is PROD. Whenever the job runs; it crashes with the below error.</p><br><p>My glue job has 60 G.1X workers.</p><br><pre><code>++Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: File already exists:s3://&lt;bucket-name&gt;/etl/&lt;directory&gt;/.spark-staging-f163a945-f93c-44b9-bac3-923ec9315275/p_date=2020-11-02/part-00249-f163a945-f93c-44b9-bac3-923ec9315275.c000.snappy.parquet<br>Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 249 in stage 15.0 failed 4 times; most recent failure: Lost task 249.3 in stage 15.0 (TID 16001; 172.36.172.237; executor 21): org.apache.spark.SparkException: Task failed while writing rows.<br>at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)<br>at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)<br>at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)<br>at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)<br>at org.apache.spark.scheduler.Task.run(Task.scala:121)<br>at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)<br>at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)<br>at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)<br>at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>at java.lang.Thread.run(Thread.java:748)<br></code></pre><br><p>I have no idea why this is happening; the spark staging file seems to cause this issue. This wasn't the issue in the DEV environment; I mean the data volume is ceratinly large in PROD but still the code is the same. My SparkConf looks something like this</p><br><pre><code>conf = pyspark.SparkConf().setAll([<br><br>(&quot;spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version&quot;; &quot;2&quot;);<br><br>(&quot;spark.speculation&quot;; &quot;false&quot;);<br><br>(&quot;spark.sql.parquet.enableVectorizedReader&quot;; &quot;false&quot;);<br><br>(&quot;spark.sql.parquet.mergeSchema&quot;; &quot;true&quot;);<br><br>(&quot;spark.sql.crossJoin.enabled&quot;; &quot;true&quot;);<br><br>(&quot;spark.sql.sources.partitionOverwriteMode&quot;;&quot;dynamic&quot;);<br><br>(&quot;spark.hadoop.fs.s3.maxRetries&quot;; &quot;20&quot;);<br><br>(&quot;spark.hadoop.fs.s3a.multiobjectdelete.enable&quot;; &quot;false&quot;)<br><br>])<br><br></code></pre><br><p>Here is my write to S3 code.</p><br><pre><code>finalDF.write.partitionBy('p_date').save(&quot;s3://{bucket}/{basepath}/{table}/&quot;.format(bucket=args['DataBucket']; basepath='etl';<br><br>table='sessions'); format='parquet'; mode=&quot;overwrite&quot;)<br><br></code></pre><br><p>I tried removing the second partition while writing; but still the same issue.</p><br><p>Any help would be appericiated.</p><br>
0.3333333333333333,0.6666666666666666,0.0,0.0,1.0,0.0,0.0,<h3>Not able to access Jupyter notebook via docker running on AWS EC2 instance</h3><p>When running docker on AWS EC2 instance and trying to run &quot;jupyter/pyspark-notebook&quot; image the URL provided to access Jupyter notebook via web is not working.<br><a href="https://i.stack.imgur.com/eRQzU.png" rel="nofollow noreferrer">enter image description here</a></p><br><p>i keep getting localhost refuse to connect error</p><br><p>My security setting on AWS EC2 instances are as follow<br><a href="https://i.stack.imgur.com/rVbAW.png" rel="nofollow noreferrer">enter image description here</a></p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>What options do I have in Amazon RDS for using &#39;fixed_date&#39;?</h3><p>Historically; in Oracle I've used the <code>fixed_date</code> parameter to change the system date to run a series of reports that tie together to verify those links still are correct.</p><br><p>Now that we've moved to Amazon RDS; that capability is not available.</p><br><p>What are my options?</p><br><p>Here's what I've considered</p><br><ul><br><li>changing all calls to 'system_date' to use a custom function that simulates this. (Ugh; this is hundreds of packages; but is possible)</li><br><li>crying :)</li><br></ul><br><p>anyone with an idea?</p><br>
1.0,0.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>Can a Spark job efficiently load partitioned data from AWS S3?</h3><p>I have a pyspark job running on AWS Glue service; which loads partitioned data from S3. The following command loads the data and when files are on a Hadoop file system it allows to filter on the partitions; e.g. load only September-2021 data.</p><br><pre><code>df = spark.read.load(args['INPUT_PATH'])<br></code></pre><br><p>However; my data sits on AWS S3 and I wonder if partitioned data is loaded efficiently. As per my understanding; one of AWS Glue crawlers selling point is; that it allows <a href="https://aws.amazon.com/blogs/big-data/work-with-partitioned-data-in-aws-glue/" rel="nofollow noreferrer">push down</a> loading; therefore I'm questioning if Spark can load data from S3 efficiently?</p><br><p>Thanks.</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>AWS Cognito OIDC Customizations</h3><p><a href="https://consumerdatastandardsaustralia.github.io/standards/#security-profile" rel="nofollow noreferrer">https://consumerdatastandardsaustralia.github.io/standards/#security-profile</a></p><br><p>I am trying to setup AWS Cognito as an OIDC provider. Able to create User pool however there are lots of custom data needed. Such as &quot;.well-known/openid-configuration&quot; of Cognito returns few details but missing introspection_endpoint; revocation_endpoint; claims_supported etc.</p><br><p>Similary; customization of /authorize endpoint with additional claims is needed.</p><br><p>Any help or suggestions would be really helpful.</p><br><p>Regards &amp; Thanks</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Creating an AWS log group?</h3><p>I having issues creating an AWS log group that belongs to a bigger CloudFormation template. So just for testing; I'm creating just the log group with the following template</p><br><pre><code>Parameters:<br>  LogGroupName:<br>    Type: String<br>    Description: 'cloudwatch log group name'<br>    Default: &quot;test-log-group&quot;<br>  LogGroupRetention:<br>    Type: Number<br>    Description: Retention period for log groups in cloudwatch<br>    Default: 30<br>  DelPolicy:<br>    Type: String<br>    Description: 'Deletion policy'<br>    Default: &quot;Retain&quot;<br><br>Resources:<br>  LLGO1WY:<br>    Type: 'AWS::Logs::LogGroup'<br>    Properties:<br>      awslogs-region: !Ref 'AWS::Region'<br>      LogGroupName: !Ref LogGroupName<br>      RetentionInDays: !Ref LogGroupRetention<br>      DeletionPolicy: !Ref DelPolicy<br></code></pre><br><p>When I import the template during the manual stack creation (&quot;Create Stack&quot; button); I get the following when I get to the &quot;Import Overview&quot; page.</p><br><pre><code>There was an error creating this change set<br>The following resources to import [LLGO1WY] must have DeletionPolicy attribute specified in the template.<br></code></pre><br><p>If you look the <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-loggroup.html" rel="nofollow noreferrer">documentation for AWS::Logs::LogGroup</a>; it doesn't even have a <code>DeletionPolicy</code> defined as a property. Note that if I remove that property; I get the same error. Any clues?</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>An error occurred (InternalFailure) when calling the InvokeEndpoint operation: An exception occurred while sending request to model</h3><p>I am trying to host an XGBoost model that I have trained locally on an AWS Sagemaker Endpoint but I am receiving the following error when invoking the endpoint:</p><br><blockquote><br><p>An error occurred (InternalFailure) when calling the InvokeEndpoint operation (reached max retries: 4): An exception occurred while sending request to model. Please contact customer support regarding request.</p><br></blockquote><br><p>The model works as expected locally and I save it using the following before uploading to S3:</p><br><pre><code>model.fit(args)<br>model.save_model(model_save_loc)<br>model_tar_loc = model_save_loc + '.tar.gz'<br>!tar czvf $model_tar_loc $model_save_loc<br></code></pre><br><p>I am hosting the model through the MultiDataModel function;</p><br><pre><code>container = retrieve(&quot;xgboost&quot;; region; &quot;1.3-1&quot;)<br>mme = MultiDataModel(<br>    name=model_name;<br>    role=role;<br>    model_data_prefix=model_data_prefix;<br>    image_uri=container;<br>    sagemaker_session=sagemaker_session;<br>)<br><br>predictor = mme.deploy(<br>    initial_instance_count=1; instance_type=instance_type; endpoint_name=model_name;     <br>)<br></code></pre><br><p>The MultiDataModel deploy works as expected with no errors; and if I do:</p><br><pre><code>list(mme.list_models())<br></code></pre><br><p>It returns the expected list of models:</p><br><pre><code>model_1.tar.gz<br>model_2.tar.gz<br>etc..<br></code></pre><br><p>I invoke the model using the following:</p><br><pre><code>runtime_client = boto3.client(&quot;runtime.sagemaker&quot;)<br><br>response = runtime_client.invoke_endpoint(<br>    EndpointName=&quot;model_name&quot;; ContentType=&quot;text/csv&quot;; Body=payload; TargetModel='model_1.tar.gz'<br>)<br>result = response[&quot;Body&quot;].read().decode(&quot;ascii&quot;)<br></code></pre><br><p>I have experimented with various ways of creating the payload but none change the error message.</p><br><p>The local XGBoost model was trained using XGBoost version 1.3.1 (same as the Docker version).</p><br><p>CloudWatch provides only the following:</p><br><blockquote><br><p>2021-06-26 10:48:36;865 [INFO ] pool-1-thread-1 ACCESS_LOG - /10.32.0.2:37106 &quot;GET /ping HTTP/1.1&quot; 200 0</p><br></blockquote><br><p>There is no way of contacting customer support through the basic plan; as advised by the error.</p><br>
0.0,0.0,0.3333333333333333,0.0,0.6666666666666666,0.3333333333333333,1.0,<h3>Creating budget actions to shutdown Lambda</h3><p>I have registered a free tier AWS Lambda account and created a simple; public service for me and others to play around with. However; since I do not know yet how usage is going to be; I want to be careful for now. Otherwise someone could simply flood my service with one million requests and I get billed for it. I'd rather not have the service available.</p><br><p>Therefore; I want to create a budget action that shuts down all services as soon as $0.01 is exceeded. The way I've done this is that I've granted the Lambda service role (which was auto-created when I setup the lambda service) the budget permission (budgets.amazonaws.com) and then have an IAM action setup that adds the <code>AWSDenyAll</code> managed policy to the role itself once the budget is exceeded.</p><br><p>This does not seem to work. If I manually attach the <code>AWSDenyAll</code> policy; the Lambda service still is available. My understanding of the roles/policies system may be also fundamentally wrong.</p><br><p>How can I achieve a &quot;total shutdown&quot; action that can be triggered from a budget alert?</p><br>
0.0,0.0,0.6666666666666666,1.0,0.0,0.0,0.0,<h3>boto3 giving Access Denied error while uploading file through python</h3><p>When I tried uploading an image to s3 using boto3 in python I am constantly getting errors.<br>The error says:</p><br><pre class="lang-py prettyprint-override"><code>An error occurred (AccessDenied) when calling the PutObject operation: Access Denied<br></code></pre><br><p>My code for uploading the image is</p><br><pre class="lang-py prettyprint-override"><code>def upload_file(file_name; bucket; object_name=None):<br>    &quot;&quot;&quot;Upload a file to an S3 bucket<br><br>    :param file_name: File to upload<br>    :param bucket: Bucket to upload to<br>    :param object_name: S3 object name. If not specified then file_name is used<br>    :return: True if file was uploaded; else False<br>    &quot;&quot;&quot;<br><br>    # If S3 object_name was not specified; use file_name<br>    if object_name is None:<br>        object_name = file_name<br><br>    # Upload the file<br>    s3_client = boto3.client('s3')<br>    try:<br>        response = s3_client.upload_file(file_name; bucket; object_name; ExtraArgs={'ACL':'public-read'})<br>        print(response)<br>    except Exception as e:<br>        print(e)<br>        return False<br>    return True<br><br></code></pre><br>
0.0,0.0,1.0,1.0,0.0,0.0,0.0,<h3>IAM policy to allow S3 access to specific IAM role in the same account</h3><p>I have a AWS account which have some IAM user having access to s3 buckets. I also have a S3 bucket that contains sensitive information. Is there a policy where I can deny the access to all users except the particular role and group ?</p><br>
0.0,0.0,1.0,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>Retrieve Systems Manager Explorer OpsData Using CLI or SDK</h3><p>I'm trying to retrieve below details using <strong>java SDK or CLI</strong>.</p><br><p><a href="https://i.stack.imgur.com/mfovI.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/mfovI.png" alt="OpsData for COMPLIANT items" /></a></p><br><p>I found below CLI commands for that but in <strong>sample responses</strong> I don't see an appropriate CLI command for that.</p><br><ol><br><li><a href="https://docs.aws.amazon.com/cli/latest/reference/ssm/describe-ops-items.html" rel="nofollow noreferrer">describe-ops-items</a></li><br><li><a href="https://docs.aws.amazon.com/cli/latest/reference/ssm/get-ops-item.html" rel="nofollow noreferrer">get-ops-item</a></li><br><li><a href="https://docs.aws.amazon.com/cli/latest/reference/ssm/get-ops-metadata.html" rel="nofollow noreferrer">get-ops-metadata</a></li><br><li><a href="https://docs.aws.amazon.com/cli/latest/reference/ssm/get-ops-summary.html" rel="nofollow noreferrer">get-ops-summary</a></li><br><li><a href="https://docs.aws.amazon.com/cli/latest/reference/ssm/list-ops-item-events.html" rel="nofollow noreferrer">list-ops-item-events</a></li><br><li><a href="https://docs.aws.amazon.com/cli/latest/reference/ssm/list-ops-item-related-items.html" rel="nofollow noreferrer">list-ops-item-related-items</a></li><br><li><a href="https://docs.aws.amazon.com/cli/latest/reference/ssm/list-ops-metadata.html" rel="nofollow noreferrer">list-ops-metadata</a></li><br></ol><br><p>Trying out each CLI is not an option since this is very sensitive data and cannot request permission for all CLI methods..</p><br>
0.6666666666666666,0.0,0.0,0.0,0.0,0.6666666666666666,0.0,<h3>AWS KCL Javascript with Kinesalite and Dynalite Caught exception while sync&#39;ing Kinesis shards and leases</h3><p>I am using the Javascript version of the AWS KCL; and due to the nature of the project I am working on; I am required to create a local instance of our Kinesis consumer application before I migrate to the cloud. When trying to run the basic_sample setup that is given in the <a href="https://github.com/awslabs/amazon-kinesis-client-nodejs/tree/master/samples/basic_sample/consumer" rel="nofollow noreferrer">Javascript KCL Github</a>; I run into the the error below.</p><br><blockquote><br><p>Starting MultiLangDaemon ...<br>2021-02-08 21:24:23;069 [main] INFO  s.a.k.m.MultiLangDaemonConfig [NONE] - Using a cached thread pool.<br>2021-02-08 21:24:23;305 [main] INFO  s.a.k.m.MultiLangDaemonConfig [NONE] - Running kcl to process stream testStream with executable node kcl.js<br>2021-02-08 21:24:23;319 [main] INFO  s.a.k.m.MultiLangDaemonConfig [NONE] - Using workerId: c0c4196e-2d62-4dd9-8c6c-148852dbed37<br>2021-02-08 21:24:23;320 [main] INFO  s.a.k.m.MultiLangDaemonConfig [NONE] - MultiLangDaemon is adding the following fields to the User Agent: amazon-kinesis-client-library-java amazon-kinesis-multi-lang-daemon/1.0.1 nodejs/0.10 node<br>2021-02-08 21:24:24;042 [main] INFO  s.a.k.l.d.DynamoDBLeaseCoordinator [NONE] - With failover time 10000 ms and epsilon 25 ms; LeaseCoordinator will renew leases every 3308 ms; takeleases every 20050 ms; process maximum of 2147483647 leases and steal 1 lease(s) at a time.<br>2021-02-08 21:24:24;056 [multi-lang-daemon-0000] INFO  s.a.kinesis.coordinator.Scheduler [NONE] - Initialization attempt 1<br>2021-02-08 21:24:24;058 [multi-lang-daemon-0000] INFO  s.a.kinesis.coordinator.Scheduler [NONE] - Initializing LeaseCoordinator<br>2021-02-08 21:24:25;053 [multi-lang-daemon-0000] INFO  s.a.kinesis.coordinator.Scheduler [NONE] - Syncing Kinesis shard info<br>2021-02-08 21:24:25;492 [multi-lang-daemon-0000] ERROR s.a.kinesis.leases.ShardSyncTask [NONE] - Caught exception while sync'ing Kinesis shards and leases<br>java.lang.RuntimeException: software.amazon.awssdk.core.exception.SdkClientException<br>at software.amazon.kinesis.retrieval.AWSExceptionManager.apply(AWSExceptionManager.java:65)<br>at software.amazon.kinesis.leases.KinesisShardDetector.listShards(KinesisShardDetector.java:172)<br>at software.amazon.kinesis.leases.KinesisShardDetector.listShards(KinesisShardDetector.java:132)<br>at software.amazon.kinesis.leases.HierarchicalShardSyncer.getShardList(HierarchicalShardSyncer.java:248)<br>at software.amazon.kinesis.leases.HierarchicalShardSyncer.checkAndCreateLeaseForNewShards(HierarchicalShardSyncer.java:81)<br>at software.amazon.kinesis.leases.ShardSyncTask.call(ShardSyncTask.java:67)<br>at software.amazon.kinesis.metrics.MetricsCollectingTaskDecorator.call(MetricsCollectingTaskDecorator.java:53)<br>at software.amazon.kinesis.coordinator.Scheduler.initialize(Scheduler.java:247)<br>at software.amazon.kinesis.coordinator.Scheduler.run(Scheduler.java:213)<br>at software.amazon.kinesis.multilang.MultiLangDaemon$MultiLangRunner.call(MultiLangDaemon.java:95)<br>at software.amazon.kinesis.multilang.MultiLangDaemon$MultiLangRunner.call(MultiLangDaemon.java:86)<br>at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)<br>at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)<br>at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)<br>at java.base/java.lang.Thread.run(Thread.java:834)<br>Caused by: software.amazon.awssdk.core.exception.SdkClientException: null<br>at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:97)<br>at software.amazon.awssdk.core.internal.util.ThrowableUtils.asSdkException(ThrowableUtils.java:98)<br>at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryExecutor.retryIfNeeded(AsyncRetryableStage.java:118)<br>at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage$RetryExecutor.lambda$execute$0(AsyncRetryableStage.java:104)<br>at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)<br>at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)<br>at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)<br>at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)<br>at software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage$ResponseHandler.onError(MakeAsyncHttpRequestStage.java:236)<br>at software.amazon.awssdk.http.nio.netty.internal.NettyRequestExecutor.handleFailure(NettyRequestExecutor.java:228)<br>at software.amazon.awssdk.http.nio.netty.internal.NettyRequestExecutor.makeRequestListener(NettyRequestExecutor.java:126)<br>at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:511)<br>at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:504)<br>at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:483)<br>at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:424)<br>at io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:112)<br>at software.amazon.awssdk.http.nio.netty.internal.http2.HttpOrHttp2ChannelPool.lambda$acquire0$1(HttpOrHttp2ChannelPool.java:84)<br>at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:511)<br>at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:485)<br>at io.netty.util.concurrent.DefaultPromise.access$000(DefaultPromise.java:33)<br>at io.netty.util.concurrent.DefaultPromise$1.run(DefaultPromise.java:435)<br>at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)<br>at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404)<br>at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:474)<br>at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:909)<br>... 1 common frames omitted<br>Caused by: io.netty.handler.codec.http2.Http2Exception: First received frame was not SETTINGS. Hex dump for first 5 bytes: 485454502f<br>at io.netty.handler.codec.http2.Http2Exception.connectionError(Http2Exception.java:85)<br>at io.netty.handler.codec.http2.Http2ConnectionHandler$PrefaceDecoder.verifyFirstFrameIsSettings(Http2ConnectionHandler.java:350)<br>at io.netty.handler.codec.http2.Http2ConnectionHandler$PrefaceDecoder.decode(Http2ConnectionHandler.java:251)<br>at io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:450)<br>at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:502)<br>at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:441)<br>at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278)<br>at io.netty.handler.codec.http2.ForkedHttp2MultiplexCodec.channelRead(ForkedHttp2MultiplexCodec.java:359)<br>at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)<br>at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)<br>at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)<br>at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434)<br>at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)<br>at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)<br>at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965)<br>at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)<br>at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:656)<br>at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:591)<br>at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:508)<br>at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:470)<br>... 2 common frames omitted</p><br></blockquote><br><p>Kinesalite and Dynalite are both running on their respective ports using the code below <em>(Kinesalite code is essentially the same but at port 4567 instead)</em>. I also have configured the .properties file to add the endpoints for the Kinesalite and Dynalite servers. Any help on resolving this error would be much appreciated!</p><br><p><a href="https://i.stack.imgur.com/O8TOc.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/O8TOc.png" alt="Dynalite server" /></a></p><br><p><a href="https://i.stack.imgur.com/S0ti6.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/S0ti6.png" alt="properties file" /></a></p><br>
0.0,0.0,0.0,0.0,0.3333333333333333,1.0,0.3333333333333333,<h3>Ng Build with NGCC Error using ngx-awesome-uploader</h3><p>I'm working on a micro front-end application using single-spa with AngularCDK 9.2.4 and Node v12.16.0 and i'm facing some problems using the ngx-awesome-uploader lib. First; when I tried to run my application locally I got the NGCC's error and after some researches I found the solution that is to add on my package.json; inside scripts tag; &quot;postinstall&quot;: &quot;ngcc&quot; and it worked for me locally.</p><br><p>But since is a work project we use an internal AWS Client to CI/CD and when I get to the Build Stage of my CodePipeline; I get the same error (that i solved locally). I already tried to use the same commands that are used at CodePipeline; since i'm facing this error.</p><br><p>Here's my package.json (couldn't upload since I have restricted upload policy).</p><br><p>&quot;dependencies&quot;: {<br>&quot;@angular/animations&quot;: &quot;~9.1.2&quot;;<br>&quot;@angular/cdk&quot;: &quot;^9.2.4&quot;;<br>&quot;@angular/common&quot;: &quot;~9.1.2&quot;;<br>&quot;@angular/compiler&quot;: &quot;~9.1.2&quot;;<br>&quot;@angular/core&quot;: &quot;~9.1.2&quot;;<br>&quot;@angular/forms&quot;: &quot;~9.1.2&quot;;<br>&quot;@angular/material&quot;: &quot;^9.2.4&quot;;<br>&quot;@angular/platform-browser&quot;: &quot;~9.1.2&quot;;<br>&quot;@angular/platform-browser-dynamic&quot;: &quot;~9.1.2&quot;;<br>&quot;@angular/router&quot;: &quot;~9.1.2&quot;;<br>&quot;@auth0/angular-jwt&quot;: &quot;^4.2.0&quot;;<br>&quot;@ep3/pipepper-tools&quot;: &quot;0.0.5&quot;;<br>&quot;@fortawesome/fontawesome-free&quot;: &quot;^5.15.1&quot;;<br>&quot;@ngrx/store&quot;: &quot;^9.2.0&quot;;<br>&quot;@ngrx/store-devtools&quot;: &quot;^9.2.0&quot;;<br>&quot;bootstrap&quot;: &quot;^4.5.2&quot;;<br>&quot;ngx-awesome-uploader&quot;: &quot;^12.0.2&quot;;<br>&quot;ngx-pagination&quot;: &quot;^5.0.0&quot;;<br>&quot;ngx-toastr&quot;: &quot;^12.1.0&quot;;<br>&quot;rxjs&quot;: &quot;~6.5.4&quot;;<br>&quot;single-spa&quot;: &quot;^5.5.5&quot;;<br>&quot;single-spa-angular&quot;: &quot;4.3.2&quot;;<br>&quot;tslib&quot;: &quot;^1.10.0&quot;;<br>&quot;zone.js&quot;: &quot;~0.10.2&quot;<br>};<br>&quot;devDependencies&quot;: {<br>&quot;@angular-builders/custom-webpack&quot;: &quot;^9&quot;;<br>&quot;@angular-devkit/build-angular&quot;: &quot;~0.901.2&quot;;<br>&quot;@angular/cli&quot;: &quot;~9.1.2&quot;;<br>&quot;@angular/compiler-cli&quot;: &quot;~9.1.2&quot;;<br>&quot;@angular/language-service&quot;: &quot;~9.1.2&quot;;<br>&quot;@types/node&quot;: &quot;^12.11.1&quot;;<br>&quot;@types/jasmine&quot;: &quot;~3.5.0&quot;;<br>&quot;@types/jasminewd2&quot;: &quot;~2.0.3&quot;;<br>&quot;codelyzer&quot;: &quot;^5.1.2&quot;;<br>&quot;jasmine-core&quot;: &quot;~3.5.0&quot;;<br>&quot;jasmine-spec-reporter&quot;: &quot;~4.2.1&quot;;<br>&quot;karma&quot;: &quot;~4.4.1&quot;;<br>&quot;karma-chrome-launcher&quot;: &quot;~3.1.0&quot;;<br>&quot;karma-coverage-istanbul-reporter&quot;: &quot;~2.1.0&quot;;<br>&quot;karma-jasmine&quot;: &quot;~3.0.1&quot;;<br>&quot;karma-jasmine-html-reporter&quot;: &quot;^1.4.2&quot;;<br>&quot;protractor&quot;: &quot;~5.4.3&quot;;<br>&quot;ts-node&quot;: &quot;~8.3.0&quot;;<br>&quot;tslint&quot;: &quot;~6.1.0&quot;;<br>&quot;typescript&quot;: &quot;~3.8.3&quot;<br>}</p><br><p>Here's the error:</p><br><p>Error: Error on worker #7: Error: getInternalNameOfClass() called on a non-ES5 class: expected FilePickerService to have an inner class declaration<br>at Esm5ReflectionHost.getInternalNameOfClass (/codebuild/output/src818/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/host/esm5_host.js:88:23)<br>at DelegatingReflectionHost.getInternalNameOfClass (/codebuild/output/src818/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/host/delegating_host.js:89:34)<br>at extractInjectableMetadata (/codebuild/output/src818/src/s3/00/app/node_modules/@angular/compiler-cli/src/ngtsc/annotations/src/injectable.js:125:69)<br>at InjectableDecoratorHandler.analyze (/codebuild/output/src818/src/s3/00/app/node_modules/@angular/compiler-cli/src/ngtsc/annotations/src/injectable.js:66:24)<br>at NgccTraitCompiler.TraitCompiler.analyzeTrait (/codebuild/output/src818/src/s3/00/app/node_modules/@angular/compiler-cli/src/ngtsc/transform/src/compilation.js:345:40)<br>at analyze (/codebuild/output/src818/src/s3/00/app/node_modules/@angular/compiler-cli/src/ngtsc/transform/src/compilation.js:297:58)<br>at _loop_1 (/codebuild/output/src818/src/s3/00/app/node_modules/@angular/compiler-cli/src/ngtsc/transform/src/compilation.js:319:21)<br>at NgccTraitCompiler.TraitCompiler.analyzeClass (/codebuild/output/src818/src/s3/00/app/node_modules/@angular/compiler-cli/src/ngtsc/transform/src/compilation.js:325:35)<br>at NgccTraitCompiler.analyzeFile (/codebuild/output/src818/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/analysis/ngcc_trait_compiler.js:47:26)<br>at DecorationAnalyzer.analyzeProgram (/codebuild/output/src818/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/analysis/decoration_analyzer.js:134:39)<br>at ClusterMaster.onWorkerMessage (/codebuild/output/src818/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/execution/cluster/master.js:166:27)<br>at /codebuild/output/src818/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/execution/cluster/master.js:50:95<br>at ClusterMaster. (/codebuild/output/src818/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/execution/cluster/master.js:246:57)<br>at step (/codebuild/output/src818/src/s3/00/app/node_modules/tslib/tslib.js:139:27)<br>at Object.next (/codebuild/output/src818/src/s3/00/app/node_modules/tslib/tslib.js:120:57)<br>at /codebuild/output/src818/src/s3/00/app/node_modules/tslib/tslib.js:113:75<br>at new Promise ()<br>at Object.__awaiter (/codebuild/output/src818/src/s3/00/app/node_modules/tslib/tslib.js:109:16)<br>at EventEmitter. (/codebuild/output/src818/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/execution/cluster/master.js:240:32)<br>at EventEmitter.emit (events.js:321:20)<br>Error: NGCC failed.<br>at NgccProcessor.process (/codebuild/output/src818/src/s3/00/app/node_modules/@ngtools/webpack/src/ngcc_processor.js:76:19)<br>at /codebuild/output/src818/src/s3/00/app/node_modules/@ngtools/webpack/src/angular_compiler_plugin.js:579:31<br>at SyncHook.eval (eval at create (/codebuild/output/src818/src/s3/00/app/node_modules/tapable/lib/HookCodeFactory.js:19:10); :7:1)<br>at SyncHook.lazyCompileHook (/codebuild/output/src818/src/s3/00/app/node_modules/tapable/lib/Hook.js:154:20)<br>at webpack (/codebuild/output/src818/src/s3/00/app/node_modules/webpack/lib/webpack.js:55:30)<br>at init (/codebuild/output/src818/src/s3/00/app/node_modules/@angular-devkit/build-angular/src/angular-cli-files/plugins/karma.js:130:20)<br>at Array.invoke (/codebuild/output/src818/src/s3/00/app/node_modules/di/lib/injector.js:75:15)<br>at Injector.get (/codebuild/output/src818/src/s3/00/app/node_modules/di/lib/injector.js:48:43)<br>at /codebuild/output/src818/src/s3/00/app/node_modules/karma/lib/server.js:162:59<br>at Array.map ()<br>at Server._start (/codebuild/output/src818/src/s3/00/app/node_modules/karma/lib/server.js:162:25)<br>at Injector.invoke (/codebuild/output/src818/src/s3/00/app/node_modules/di/lib/injector.js:75:15)<br>at Server.start (/codebuild/output/src818/src/s3/00/app/node_modules/karma/lib/server.js:135:28)<br>29 06 2021 18:55:59.075:ERROR [karma-server]: Server start failed on port 9876: Error: NGCC failed.</p><br><p>Karma's error:</p><br><p>Error: Error on worker #3: Error: getInternalNameOfClass() called on a non-ES5 class: expected FilePickerService to have an inner class declaration<br>at Esm5ReflectionHost.getInternalNameOfClass (/codebuild/output/src196/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/host/esm5_host.js:88:23)<br>at DelegatingReflectionHost.getInternalNameOfClass (/codebuild/output/src196/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/host/delegating_host.js:89:34)<br>at extractInjectableMetadata (/codebuild/output/src196/src/s3/00/app/node_modules/@angular/compiler-cli/src/ngtsc/annotations/src/injectable.js:125:69)<br>at InjectableDecoratorHandler.analyze (/codebuild/output/src196/src/s3/00/app/node_modules/@angular/compiler-cli/src/ngtsc/annotations/src/injectable.js:66:24)<br>at NgccTraitCompiler.TraitCompiler.analyzeTrait (/codebuild/output/src196/src/s3/00/app/node_modules/@angular/compiler-cli/src/ngtsc/transform/src/compilation.js:345:40)<br>at analyze (/codebuild/output/src196/src/s3/00/app/node_modules/@angular/compiler-cli/src/ngtsc/transform/src/compilation.js:297:58)<br>at _loop_1 (/codebuild/output/src196/src/s3/00/app/node_modules/@angular/compiler-cli/src/ngtsc/transform/src/compilation.js:319:21)<br>at NgccTraitCompiler.TraitCompiler.analyzeClass (/codebuild/output/src196/src/s3/00/app/node_modules/@angular/compiler-cli/src/ngtsc/transform/src/compilation.js:325:35)<br>at NgccTraitCompiler.analyzeFile (/codebuild/output/src196/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/analysis/ngcc_trait_compiler.js:47:26)<br>at DecorationAnalyzer.analyzeProgram (/codebuild/output/src196/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/analysis/decoration_analyzer.js:134:39)<br>at ClusterMaster.onWorkerMessage (/codebuild/output/src196/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/execution/cluster/master.js:166:27)<br>at /codebuild/output/src196/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/execution/cluster/master.js:50:95<br>at ClusterMaster. (/codebuild/output/src196/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/execution/cluster/master.js:246:57)<br>at step (/codebuild/output/src196/src/s3/00/app/node_modules/tslib/tslib.js:139:27)<br>at Object.next (/codebuild/output/src196/src/s3/00/app/node_modules/tslib/tslib.js:120:57)<br>at /codebuild/output/src196/src/s3/00/app/node_modules/tslib/tslib.js:113:75<br>at new Promise ()<br>at Object.__awaiter (/codebuild/output/src196/src/s3/00/app/node_modules/tslib/tslib.js:109:16)<br>at EventEmitter. (/codebuild/output/src196/src/s3/00/app/node_modules/@angular/compiler-cli/ngcc/src/execution/cluster/master.js:240:32)<br>at EventEmitter.emit (events.js:321:20)<br>Error: NGCC failed.<br>at NgccProcessor.process (/codebuild/output/src196/src/s3/00/app/node_modules/@ngtools/webpack/src/ngcc_processor.js:76:19)<br>at /codebuild/output/src196/src/s3/00/app/node_modules/@ngtools/webpack/src/angular_compiler_plugin.js:579:31<br>at SyncHook.eval (eval at create (/codebuild/output/src196/src/s3/00/app/node_modules/tapable/lib/HookCodeFactory.js:19:10); :7:1)<br>at SyncHook.lazyCompileHook (/codebuild/output/src196/src/s3/00/app/node_modules/tapable/lib/Hook.js:154:20)<br>at webpack (/codebuild/output/src196/src/s3/00/app/node_modules/webpack/lib/webpack.js:55:30)<br>at init (/codebuild/output/src196/src/s3/00/app/node_modules/@angular-devkit/build-angular/src/angular-cli-files/plugins/karma.js:130:20)<br>at Array.invoke (/codebuild/output/src196/src/s3/00/app/node_modules/di/lib/injector.js:75:15)<br>at Injector.get (/codebuild/output/src196/src/s3/00/app/node_modules/di/lib/injector.js:48:43)<br>at /codebuild/output/src196/src/s3/00/app/node_modules/karma/lib/server.js:162:59<br>at Array.map ()<br>at Server._start (/codebuild/output/src196/src/s3/00/app/node_modules/karma/lib/server.js:162:25)<br>at Injector.invoke (/codebuild/output/src196/src/s3/00/app/node_modules/di/lib/injector.js:75:15)<br>at Server.start (/codebuild/output/src196/src/s3/00/app/node_modules/karma/lib/server.js:135:28)<br>30 06 2021 17:48:07.100:ERROR [karma-server]: Server start failed on port 9876: Error: NGCC failed.</p><br><p>Is it related to any version I'm using?<br>Do I need to upload my Angular's version?<br>Or is it related to AWS's Code Pipeline?</p><br><p>Note: I use Windows but AWS uses Linux-Ubuntu.</p><br><p>Thank you.</p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.3333333333333333,0.3333333333333333,<h3>Heroku Postgres: AWS s3 presigned URL not working</h3><p>I am 6 hours into this error and going mad.</p><br><p>I have created a heroku app using the CLI and pushed the git successfully however I need to initialise a bunch of tables using <code>db.create_all()</code> before the app works; but I have 6gb of data in my localhost postgres that I wish to migrate to heroku for use.</p><br><p>Instantiating the tables in the usual way (create_all) won't work because certain content is required from tables for the index page hence my tables must be pre-populated with said localhost data before it will work.</p><br><p>I have followed the documentation here to the letter and exported my .dump file and uploaded it to my bucket: <a href="https://devcenter.heroku.com/articles/heroku-postgres-import-export#import" rel="nofollow noreferrer">https://devcenter.heroku.com/articles/heroku-postgres-import-export#import</a></p><br><p>Public sharing is enabled; and I have used <code>setx</code> commands to set <code>ACCESS_KEY_ID</code> and <code>SECRET_ACCESS_KEY</code> in the AWS CLI. I have also set <code>heroku config:set AWS_ACCESS_KEY_ID=matching_aws_cli_blah AWS_SECRET_ACCESS_KEY=matching_aws_cli_blah</code>.</p><br><p>Then using the command <code>aws s3 presign s3_url</code> I have copied the output (which I'll add looks different to AWS documentation I used <a href="https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3/presign.html" rel="nofollow noreferrer">https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3/presign.html</a>)</p><br><p>An example of how mine looks (randomly changed some figures)<br><code>https://mybucketname.s3.eu-west-2.amazonaws.com/mydb.dump?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=LLLLL4C7LLLLLLLNLLLQ%2FNNNNNN08%2Feu-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20201208T004613Z&amp;X-Amz-Expires=3600&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=ll9d80nnnnc457ea83298fdnnnn4a25b0c9ll066f54e6ff8acf42dafnnnea8877</code> whereas the documentation provides a nice neat AWSAccessKeyId= variable.</p><br><p>I have only one bucket and the <code>Block public access (bucket settings)</code> is set to OFF and the file itself is share-able publicly. The object URL initialises the download fine; including in incognito.</p><br><p>When using the following command (as per heroku documentation)</p><br><p><code>heroku pg:backups:restore 'https://mybucketname.s3.eu-west-2.amazonaws.com/mydb.dump?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=LLLLL4C7LLLLLLLNLLLQ%2FNNNNNN08%2Feu-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20201208T004613Z&amp;X-Amz-Expires=3600&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=ll9d80nnnnc457ea83298fdnnnn4a25b0c9ll066f54e6ff8acf42dafnnnea8877' DATABASE_URL</code></p><br><p>I get the following output to terminal.</p><br><pre><code>Restoring... !<br> !    An error occurred and the backup did not finish.<br> !<br> !    waiting for restore to complete<br> !    pg_restore finished with errors<br> !    waiting for download to complete<br> !    download finished with errors<br> !    please check the source URL and ensure it is publicly accessible<br> !<br> !    Run heroku pg:backups:info r001 for more details.<br>'X-Amz-Credential' is not recognized as an internal or external command;<br>operable program or batch file.<br>'X-Amz-Date' is not recognized as an internal or external command;<br>operable program or batch file.<br>'X-Amz-Expires' is not recognized as an internal or external command;<br>operable program or batch file.<br>'X-Amz-SignedHeaders' is not recognized as an internal or external command;<br>operable program or batch file.<br>'X-Amz-Signature' is not recognized as an internal or external command;<br>operable program or batch file.<br></code></pre><br><p>which in the log file shows</p><br><pre><code>=== Backup r001<br>Database:         BACKUP<br>Started at:       2020-12-08 00:47:33 +0000<br>Finished at:      2020-12-08 00:47:34 +0000<br>Status:           Failed<br>Type:             Manual<br>Backup Size:      0.00B (0% compression)<br><br>=== Backup Logs<br>2020-12-08 00:47:34 +0000 2020/12/08 00:47:34 aborting: could not write to output stream: Expected HTTP Status 200; received: &quot;400 Bad Request&quot;<br>2020-12-08 00:47:34 +0000 pg_restore: error: could not read from input file: end of file<br>2020-12-08 00:47:34 +0000 waiting for restore to complete<br>2020-12-08 00:47:34 +0000 pg_restore finished with errors<br>2020-12-08 00:47:34 +0000 waiting for download to complete<br>2020-12-08 00:47:34 +0000 download finished with errors<br>2020-12-08 00:47:34 +0000 please check the source URL and ensure it is publicly accessible<br></code></pre><br><p>Please can someone point where I am going wrong?</p><br><p>Thanks!</p><br>
0.0,0.0,0.0,0.0,0.0,0.0,1.0,<h3>Consumer IOT devices with GCP or AWS</h3><p>Were a startup building a consumer IOT device with a connected mobile app. Were building in Flutter and have started with GCP and Firebase for our backend. Firstly; Im new to coding in general; so please forgive my noobery.</p><br><p>We can connect to GCP and have pub/sub and firebase log-in set up. But I'm worried about fleet setup. So far Im finding it difficult with GCP doumentation and just noticed AWS has a ton of <a href="https://docs.aws.amazon.com/iot/latest/developerguide/provision-wo-cert.html#trusted-user" rel="nofollow noreferrer">fleet provisioning documentation</a> for IOT devices. I just want to find an easy way to provision and manage the IOT devices.</p><br><p>How do you set up fleet provisioning in GCP core IoT?</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.3333333333333333,<h3>What is the error for intrinsic function !Ref in this Cloudformation template in AWS?</h3><p>Recently I have started exploring AWS CloudFormation in YAML format. I am getting error message:</p><br><blockquote><br><p>Encountered unsupported property Type</p><br></blockquote><br><p>The description of the YAML code is as below:-</p><br><pre class="lang-yaml prettyprint-override"><code>AWSTemplateFormatVersion: 2010-09-09<br>Resources: <br>  DevEC2Instance:<br>    Type: 'AWS::EC2::Instance'<br>    Properties:<br>      ImageId: ami-04aa88aebb9fefd83<br>      Type: t2.micro<br>      KeyName: Newkey<br>      SecurityGroups:<br>       - default<br>       - !Ref SSHSecurityGroup<br>SSHSecurityGroup:<br>  Type: AWS::EC2::SecurityGroup<br>  Properties:<br>    GroupDescription: Group_For_CloudFormation<br>    SecurityGroupIngress:<br>    - Ipprotocol: tcp<br>      FromPort: '22'<br>      ToPort: '22'<br>      CidrIp: 0.0.0.0/0<br></code></pre><br>
0.0,0.0,1.0,1.0,0.0,0.0,0.0,<h3>What format should the secret-string take to connect to an Aurora Postgres database using AWS data api?</h3><p>First; everything I am doing is from the CLI. I don't have permissions to use the web interface. I am trying to make a call to an existing Aurora Postrgres database using the AWS data api. I am following the directions on this page:</p><br><p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html</a></p><br><p>I am stuck on the section &quot;Storing database credentials in AWS Secrets Manager&quot;.</p><br><p>I know how to create a secret (aws secretsmanager create-secret --name test2 --secret-string &quot;{&quot;Key&quot;:&quot;test&quot;;&quot;Value&quot;:&quot;12345&quot;}&quot;) but I don't know what the --secret-string should be storing the database credentials.</p><br><p>All the documentation says is &quot;Use Secrets Manager to create a secret that contains credentials for the Aurora DB cluster.&quot;; but it doesn't say what format the credentials should take.</p><br><p>When connecting to the database from my IDE I need to include the host; port; user; password; and database name. Do I need to include all of these in the secret-string?</p><br><p>&quot;{&quot;host&quot;:&quot;my host&quot;;&quot;port&quot;:&quot;12345&quot;;&quot;user&quot;:&quot;my user&quot;;&quot;password&quot;:&quot;my password&quot;;&quot;db_name&quot;:&quot;my db name&quot;}&quot;</p><br>
1.0,0.0,0.0,0.3333333333333333,1.0,0.0,0.0,<h3>Deploying Deep Learning model on Amazon ECS</h3><p>I am trying to deploy my Deep Learning <em>Text-to-Speech</em> Model using <strong>Amazon ECS</strong>. I am using a <em>container image</em> of my model and dependencies and uploading it to the <em>ECR Repository</em> and then linking it in the <em>Task definition</em>. The launch type I am using to Run my Task is <strong>Fargate</strong>. However while running the task the <em>logs are not being generated</em>.</p><br><p>My model is stored in <strong>s3 Bucket</strong> and I am getting the model inside my lambda function using <em>GetObject</em> property. Also; For inferencing I do required GPU that's why I switched to this ECS deployment.</p><br><p>I just cant figure out why the logs are not being generated; and whether the model is being loaded from the bucket or not; and what's the proper way to load model from bucket in ECS. I do also need to know that how to I implement the rest API in my code that I provide a <strong>Text</strong>  and it generates an <strong>Audio</strong> of the given text.</p><br>
0.6666666666666666,0.0,0.0,0.6666666666666666,0.0,0.0,0.0,<h3>Improving performance reading from large Redshift table</h3><p>In what ways can I improve reading the entirety of a large table (&gt; 100 mil rows) on Redshift? I have a dotnet program accessing data from a large table and <code>SELECT *</code> is taking around 2 hours to finish reading</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>aws download all custom time cloud watch logs within mentioned time period</h3><p>I am trying to process kinesis messages from aws lambda and pushing to power bi.</p><br><p>In lambda have logged application specific messages.</p><br><p>What is the problem now?</p><br><p>I am able to see my messages in cloud watch logs but not all. When I apply filter I am getting specific period messages which is good but not all messages of that period. Within the mentioned range Every time I scroll my mouse to get more messages and download till that point.</p><br><p>For Ex: In UI for specified time range if I get 100 messages I am able to download till this point. There is something I can see as &quot;load more&quot; When I click on it I get around 150 messages. **This is very tedious ** to scroll each time and get more messages and download .</p><br><p>Is there any automated way where I could download in single shot for mentioned period on all messages ?</p><br><p>Any help will reduce lots of effort</p><br>
0.0,0.0,0.3333333333333333,0.0,0.0,1.0,0.0,<h3>Boto3 SQS send_message seems to ignore most of QueryUrl url parts</h3><p>I'm trying to send a message to a SQS queue from a lambda in a different region. Queue urls do have the region info so I was just trying to do something like:</p><br><pre><code>import boto3<br>sqs = boto3.client('sqs')<br>response = sqs.send_message(<br>        QueueUrl=&quot;https://sqs.us-west-1.amazonaws.com/ACCOUNT_ID/OtherRegionQueue&quot;;<br>        MessageBody=msg_str<br>    )<br></code></pre><br><p>My lambda is in &quot;us-east-1&quot; and I've found out that it's trying to access a SQS in this region instead of &quot;us-west-1&quot; as specified in the QueueUrl param; probably because client() get's the region from config if not specified.</p><br><p>After some small testing looks like send_message() completely ignores QueueUrl parts; except from the protocol and the queue name.</p><br><p>I managed to successfully send_message to another queue in the same region as the lambda with an url just like:</p><br><pre><code>QueueUrl=&quot;https:SameRegionQueue&quot;<br></code></pre><br><p>Is it possible to send messages to queues in other regions without having to create a per-region clients ?</p><br>
0.0,1.0,0.0,0.0,0.3333333333333333,0.0,0.0,<h3>How does amazon ec2 work in regards to routing a domain name to an instance using load balancers?</h3><p>How does amazon ec2 work in regards to routing a domain name to an instance using load balancers?</p><br><p>What is the flow of a request from the domain name to the ec2 instance?</p><br>
0.0,0.3333333333333333,0.3333333333333333,1.0,0.0,0.0,0.0,<h3>Upload a file to Amazon S3 with presigned url + PUT request but get error &quot;SignatureDoesNotMatch&quot; for &quot;GET&quot; in &quot;StringToSign&quot;</h3><p>I am trying to upload files to Amazon S3 with a presigned url with javascript fetch() as following(copy past from the chrome Developer tool)</p><br><pre><code>fetch(&quot;https://.....amazonaws.com/.../copy.mp3?AWSAccessKeyId=...&amp;Signature=...&amp;Expires=1631443785&quot;; {<br>  &quot;headers&quot;: {<br>    &quot;content-type&quot;: &quot;audio/mpeg&quot;;<br>    &quot;sec-ch-ua&quot;: &quot;\&quot;Google Chrome\&quot;;v=\&quot;93\&quot;; \&quot; Not;A Brand\&quot;;v=\&quot;99\&quot;; \&quot;Chromium\&quot;;v=\&quot;93\&quot;&quot;;<br>    &quot;sec-ch-ua-mobile&quot;: &quot;?0&quot;;<br>    &quot;sec-ch-ua-platform&quot;: &quot;\&quot;macOS\&quot;&quot;<br>  };<br>  &quot;referrer&quot;: &quot;http://localhost:3000/&quot;;<br>  &quot;referrerPolicy&quot;: &quot;strict-origin-when-cross-origin&quot;;<br>  &quot;method&quot;: &quot;PUT&quot;;<br>  &quot;mode&quot;: &quot;cors&quot;;<br>  &quot;credentials&quot;: &quot;omit&quot;<br>});<br></code></pre><br><p>and it returns the following error &quot;SignatureDoesNotMatch&quot;:</p><br><pre><code>&lt;Error&gt;<br> &lt;Code&gt;SignatureDoesNotMatch&lt;/Code&gt;<br> &lt;Message&gt;The request signature we calculated does not match the signature you provided. <br> Check your key and signing method.&lt;/Message&gt;<br> &lt;AWSAccessKeyId&gt;...&lt;/AWSAccessKeyId&gt;<br> &lt;StringToSign&gt;GET 1631443785 ..../copy.mp3&lt;/StringToSign&gt;<br>...<br>&lt;/Error&gt;<br></code></pre><br><p><strong>The <code>StringToSign</code> in the error says it's a &quot;GET&quot; request while I think that I am sending a PUT with fetch()</strong>.</p><br><p>Besides; I try to run <code>$curl</code> like:</p><br><pre><code>$curl --request PUT --upload-file copy.mp3  https://....'<br></code></pre><br><p>and it works.</p><br><p>Anything I am doing wrong here?</p><br><p>Thank you in advance.. I have been blocked here for 2 days :(</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,1.0,0.0,<h3>Messages stuck in In-Flight status for SQS FIFO queue generated through Terraform</h3><p>I am new to Terraform.</p><br><p>Actually one weird issue I am facing in terms of SQS FIFO queues generated through terraform.</p><br><p>Initially; I tested my application with SQS queues created manually in the AWS console; which was working fine.</p><br><p>Later; I generated a new SQS FIFO queue through Terraform.</p><br><p>I checked configurations of both the queues in terms of access policy; KMS encryption; etc. They seem to be identical.</p><br><p>The visibility timeout set for both the queues is 30 seconds.</p><br><p>Now while I am testing my application with the terraform generated queue; every message is going to in-flight status and eventually is going to DLQ. None of the messages is getting delivered to the application. I checked the log as well; there seems to be no error as well.</p><br><p>I have configured the maximum receive count for a given message for the normal FIFO queue to be 3; before going to DLQ.</p><br><p>I am not getting any clue regarding why is this happening.</p><br><p>Could anyone please help here?</p><br><p><strong>EDIT</strong></p><br><p>For better clarity; I am adding Terraform code snippet to generate the SQS:</p><br><pre><code>module &quot;queue_1&quot; {<br>  source                      = &quot;./modules/terraform-aws-sqs&quot;<br>  key_alias                   = <br>module.sqs_encryption_queue_dlq.kms_key_alias<br>  service_name                = var.project_name<br>  team_name                   = var.team_name<br>  name                        = &quot;queue_1.fifo&quot;<br>  fifo_queue                  = true<br>  visibility_timeout_seconds  = 30<br>  message_retention_seconds   = 86400<br>  content_based_deduplication = true<br>  redrive_policy = jsonencode({<br>    deadLetterTargetArn = module.dlq_1.sqs_queue_arn<br>    maxReceiveCount     = 3<br>  })<br>  platform_mandatory_tags = var.platform_mandatory_tags<br>}<br><br>module &quot;dlq_1&quot; {<br>  source                      = &quot;./modules/terraform-aws-sqs&quot;<br>  key_alias                   = <br>  module.sqs_encryption_queue_dlq.kms_key_alias<br>  service_name                = var.project_name<br>  team_name                   = var.team_name<br>  name                        = &quot;dlq_1.fifo&quot;<br>  fifo_queue                  = true<br>  content_based_deduplication = true<br>  platform_mandatory_tags     = var.platform_mandatory_tags<br>}<br><br>module &quot;sqs_encryption_queue_dlq&quot; {<br>  source                  = &quot;./modules/terraform-aws-kms2&quot;<br>  key_alias               = &quot;sqs_encryption_queue_dlq_key&quot;<br>  key_description         = &quot;KMS for queue and dlq&quot;<br>  team_name               = var.team_name<br>  project_name            = var.project_name<br>  platform_mandatory_tags = var.platform_mandatory_tags<br>}<br></code></pre><br>
0.0,0.0,1.0,0.6666666666666666,0.0,0.0,0.0,<h3>Is it possible to use Amazon Cognito groups to set permissions on AW resources such as Amazon DynamoDB and Amazon S3?</h3><p>In my application I want to users to be able to create an organization i.e. (OrgA) and then have users sign up under said organization using either an invite code or token. Users in OrgA should have access to an Amazon S3 directory (which stores images and files) and access to a database table that has been created for the said organization.</p><br><p>I could not find a solution on how to implement this online and was wondering if using  Amazon Cognito groups was a good idea to meet requirements.</p><br>
0.0,0.6666666666666666,0.0,0.6666666666666666,0.0,0.3333333333333333,0.0,<h3>Return all items or a single item from DynamoDB from AWS API Gateway endpoint</h3><p>I am using AWS proxy with AWS API Gateway to interact with a DynamoDB table. I have an API resource; under which I have a GET method with the below configuration:</p><br><p><a href="https://i.stack.imgur.com/8x7dO.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/8x7dO.png" alt="API method description" /></a></p><br><p>The API uses the <code>Scan</code> action as seen above to fetch all the items from the DynamoDB table. I also have the following request integration mapping template;</p><br><pre><code>{<br>    &quot;TableName&quot;: tableName<br>}<br></code></pre><br><p>Its really simple. But my problem is that I would like to add another GET method to get each item by their <code>id</code>; which will be supplied in the URL as a <code>param</code>. However; since I have already setup one GET method; I am not able to setup another to fetch only a single item. I am aware I can use mapping templates and <code>Scan</code> as given <a href="https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Scan.html" rel="nofollow noreferrer">in the docs</a> to conditionally fetch items if a param is given; but that would mean scanning the entire table; which is a waste each time I want to fetch a single item.</p><br><p>Is there any other way to do this?</p><br>
0.0,0.0,1.0,0.3333333333333333,0.3333333333333333,0.3333333333333333,0.0,<h3>Multiple template files with autoscaling groups and launch configurations with Terraform</h3><p>I have 3 autoscaling groups all using slightly different template files. The difference in the 3 template files I have is; each template file is attaching a different EBS volume upon startup of an instance in the autoscaling group. I am trying to figure out how I can pass in these different template files to each autoscaling group using count. Currently the Terraform code is setup to where one template file resource is using 1 file; but I need it to determine how to select the 2nd and 3rd template files. I've done some research on this <a href="https://registry.terraform.io/providers/hashicorp/cloudinit/latest/docs/data-sources/cloudinit_config" rel="nofollow noreferrer">Cloudinit Config</a> resource that Terraform has integration with; but am not sure if something like this will help me. Any advice would be appreciated. Below is how my current Terraform code is setup.</p><br><p><strong>Template File</strong></p><br><pre><code>data &quot;template_file&quot; &quot;user_data&quot; {<br>  count    = &quot;${(var.enable ? 1 : 0) * var.number_of_zones}&quot;<br>  template = &quot;${file(&quot;userdata.sh&quot;)}&quot;<br><br>  vars {<br>    node                   = &quot;Node${count.index + 1}&quot;<br>  }<br>}<br></code></pre><br><p><strong>Launch Configuration</strong></p><br><pre><code>resource &quot;aws_launch_configuration&quot; &quot;launch_configuration&quot; {<br>count = &quot;${(var.enable ? 1 : 0) * var.number_of_zones}&quot;<br><br>  name      = &quot;${var.cluster_name}-launch_node_${count.index}&quot;<br>  key_name  = &quot;${var.key_name2}&quot;<br>  image_id  = &quot;${lookup(var.amis; &quot;${var.aws_region}.${var.licensee_key == &quot;&quot; &amp;&amp; var.licensee == &quot;&quot; ? &quot;enterprise&quot; : &quot;byol&quot;}&quot;)}&quot;<br>  user_data = &quot;${element(data.template_file.user_data.*.rendered; count.index)}&quot;<br><br>  security_groups = [<br>    &quot;${aws_security_group.instance_security_group.id}&quot;;<br>  ]<br><br>  instance_type        = &quot;${var.instance_type}&quot;<br>  iam_instance_profile = &quot;${aws_iam_instance_profile.instance_host_profile.name}&quot;<br><br>  ebs_block_device {<br>    device_name = &quot;/dev/sdf&quot;<br>    no_device   = true<br>  }<br><br>    lifecycle {<br>    create_before_destroy = true<br>  }<br>}<br></code></pre><br><p><strong>Userdata script</strong></p><br><pre><code>#!/bin/bash<br><br># Attach the right EBS volume<br>aws ec2 attach-volume --volume-id vol-xxxxxxxxxxxxxxxxx --instance_id `curl http://169.254.169.254/latest/meta-data/instance-id` --device /dev/sdf<br></code></pre><br><p>Each userdata script I have mounts a different volume on startup of an instance. Any advice on how I can pass in a different value without creating multiple template_file resource blocks would be helpful. Version of Terraform being used is 0.11.10.</p><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>Receiving Invalid Grant Type Error Received From AWS Cognito When Supply Auth Code : How do I get Id and access tokens for testing?</h3><p>I am unable to successfully acquire an id token/access token from my AWS cognito user pool when I supply an auth code. I have written a shell script (see below); and receive <code>invalid_grant</code> back from the server.</p><br><p>I have encoded the base64 Authorization Basic header for <code>client_id:client_secret</code> generated with python as:</p><br><pre class="lang-py prettyprint-override"><code>import base64<br><br>encode='my_client_id_string:my_client_secret_string'<br>base64.b64encode(encode)<br></code></pre><br><pre><code>#!/usr/bin/env sh<br><br>curl --location --request POST 'https://&lt;domain&gt;.auth.us-east-2.amazoncognito.com/oauth2/token' \<br>--header 'Content-Type: application/x-www-form-urlencoded' \<br>--header 'Authorization: Basic &lt;base64 encode string client_id:client_secret&gt;' \<br>--data-urlencode 'grant_type=authorization_code' \<br>--data-urlencode 'client_id=&lt;client_id from app settings' \<br>--data-urlencode 'code=&lt;code received from redirect url to my localhost app endpoint&gt;' \<br>--data-urlencode 'redirect_uri=http://localhost:8000/my_redirect'<br></code></pre><br><p>Any ideas?</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>How can I get the url of the folder that is stored in the s3 bucket?</h3><p>So I am able to view the object URL of the files in the S3 bucket; but I'd also like to view the URL of the folder uploaded in that bucket.</p><br><p>Is that even possible?</p><br><p>Let me know; please...</p><br>
0.0,0.0,0.6666666666666666,0.0,0.6666666666666666,0.6666666666666666,0.0,<h3>CodeBuild with custom docker image get error: Failed to get D-Bus connection: Operation not permitted</h3><p>I created a docker image:</p><br><p><strong>Dockerfile</strong>:</p><br><pre><code>FROM amazonlinux:2 AS core<br>WORKDIR /root<br>RUN yum update -y<br>RUN curl -sL https://rpm.nodesource.com/setup_14.x | bash -<br>RUN yum install -y nodejs<br>RUN npm install -g less coffeescript<br>#shodw-utils for use the useradd command.<br>RUN yum install shadow-utils -y<br>RUN useradd ec2-user<br>RUN yum install -y awslogs<br>RUN yum -y install net-tools<br>RUN yum install which -y<br>RUN yum install sudo -y<br>RUN rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm<br>RUN yum install nginx -y<br>RUN yum clean metadata<br>RUN yum install -y python3-devel.aarch64<br>RUN yum install python37<br>RUN /usr/bin/pip3.7 install virtualenv<br>RUN virtualenv -p /usr/bin/python3.7 /var/app/.venv<br>RUN yum install nginx -y<br>RUN yum -y groupinstall Development tools<br>RUN yum -y install zlib-devel<br>RUN yum -y install bzip2-devel openssl-devel ncurses-devel<br>RUN yum -y install sqlite sqlite-devel xz-devel<br>RUN yum -y install readline-devel tk-devel gdbm-devel db4-devel<br>RUN yum -y install libpcap-devel xz-devel<br>RUN yum -y install libjpeg-devel<br>RUN yum -y install wget<br></code></pre><br><p>When i run it with privileged on my local it succeed to run NGINX service with no failure.</p><br><p>CodeBuild (Privileged mode= True) pull this image from ECR but when trying to run NGINX service i got the following error:</p><br><pre><code>Failed to get D-Bus connection: Operation not permitted<br></code></pre><br><p>any ideas?</p><br>
0.0,1.0,0.0,0.0,0.0,0.0,0.0,<h3>How do I reroute to a domain name from an elastic IP</h3><p>I have an Elastic IP in AWS. When I navigate to my elastic IP; I want it to redirect me to a domain name. For example in cmd; I type <code>ping google.com</code> it returns <code>Reply from 172.217.13.238</code>; when i navigate to <code>172.217.13.238</code>; it takes me to <code>google.com</code>. I want to do this same thing but with an elastic IP and a route 53 domain name.<br>Any help would be appreciated!</p><br>
0.0,0.0,1.0,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>AWS Lambda track and limit authenticated calls from user</h3><p>I currently have an S3 bucket running a static website with Cognito user authentication. I call a Lambda function from this bucket which validates the user's JWT token. I am trying to limit the number of calls the user is able to make to the function; such as 1 every hour. How would I go about doing this?</p><br><ul><br><li>My initial plan was to do something on the front-end; but that would easily bypassed.</li><br><li>I looked to see if there was an AWS service for this; but the closest I found was Cloudtrail which doesn't really fit this</li><br><li>My current plan is to create Cognito userAttributes to track how many calls the user has made and update them through lambda; but this also feels like the wrong approach</li><br></ul><br>
0.0,0.6666666666666666,0.0,1.0,0.0,0.0,0.0,<h3>MySQL Workbench won&#39;t connect to AWS RDS DB. ERROR - &quot;Unable to connect to localhost&quot;</h3><p>This image shows the error pop up when I try to test the connection<br><img src="https://i.stack.imgur.com/Nhj13.png" alt="image shows the error pop up when I try to test the connection" /></p><br><p>Current Set Up: 2015 MacBook Pro. OS 10.15.7</p><br><p>Things that I have tried:</p><br><p>System preferences &gt; MySQL &gt; initialize setup.</p><br><p>Checked Username is correct on AWS RDS set up.</p><br><p>Checked the endpoint address.</p><br><p>Tested connection on a 127.0.0.0 database. Works fine.</p><br>
0.0,0.0,0.0,0.0,1.0,0.6666666666666666,0.0,<h3>aws copilot-cli deploy using previously created manifest.yml</h3><p>I initialized aws copilot application; so it created a copilot folder and inside that it created service details with <code>manifest.yml</code> file. And created my AWS infrastructure for me. After that I delete that infra using <code>copilot app delete</code>.</p><br><p>Now I want to deploy this without going to initialization process of copilot. How is that possible?</p><br>
0.0,0.0,0.6666666666666666,0.0,1.0,0.0,0.0,<h3>Kubernetes - force restarting on specific memory usage</h3><p>our server running using Kubernetes for auto-scaling and we use newRelic for observability<br>but we face some issues</p><br><p>1- we need to restart pods when memory usage reaches 1G it automatically restarts when it reaches 1.2G but everything goes slowly.</p><br><p>2- terminate pods when there no requests to the server</p><br><p>my configuration</p><br><pre><code>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: {{ .Release.Name }}<br>  labels:<br>    app: {{ .Release.Name }}<br>spec:<br>  revisionHistoryLimit: 2<br>  replicas: {{ .Values.replicas }}<br>  selector:<br>    matchLabels:<br>      app: {{ .Release.Name }}<br>  template:<br>    metadata:<br>      labels:<br>        app: {{ .Release.Name }}<br>    spec:<br>      containers:<br>        - name: {{ .Release.Name }}<br>          image: &quot;{{ .Values.imageRepository }}:{{ .Values.tag }}&quot;<br>          env:<br>            {{- include &quot;api.env&quot; . | nindent 12 }}<br>          resources:<br>            limits:<br>              memory: {{ .Values.memoryLimit }}<br>              cpu: {{ .Values.cpuLimit }}<br>            requests:<br>              memory: {{ .Values.memoryRequest }}<br>              cpu: {{ .Values.cpuRequest }}<br>      imagePullSecrets:<br>        - name: {{ .Values.imagePullSecret }}      <br>      {{- if .Values.tolerations }}<br>      tolerations:<br>{{ toYaml .Values.tolerations | indent 8 }}<br>      {{- end }}<br>      {{- if .Values.nodeSelector }}<br>      nodeSelector:<br>{{ toYaml .Values.nodeSelector | indent 8 }}<br>      {{- end }}<br></code></pre><br><p>my values file</p><br><pre><code>memoryLimit: &quot;2Gi&quot;<br>cpuLimit: &quot;1.0&quot;<br>memoryRequest: &quot;1.0Gi&quot;<br>cpuRequest: &quot;0.75&quot;<br></code></pre><br><p>thats what I am trying to approach</p><br><p><a href="https://i.stack.imgur.com/kcoV1.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/kcoV1.png" alt="enter image description here" /></a></p><br>
0.0,0.0,1.0,0.0,0.0,1.0,0.0,<h3>AWS-React: The specified key does not exist</h3><p>I made one react app. My app works as expected. This app's target is practice <code>AWS-COGNITO</code>. For Cognito validation I am using <a href="https://www.npmjs.com/package/amazon-cognito-identity-js" rel="nofollow noreferrer">amazon-cognito-identity-js</a> package. I made one helper function where I validate the Congnito. and reuse it in different component. I split my Nav bar into two components. From Congnito current user I made one callback function and use it in useEffect; and dependencies put the callback function; by default <code>getAuthenticatedUser</code> is null. I add condition where it fetch the data; if  <code>getAuthenticatedUser</code> then redirect to <code>signin</code> and <code>signup</code> page. I deployed my app to s3 bucket and this the <a href="http://congnito-auth.s3-website-us-east-1.amazonaws.com" rel="nofollow noreferrer">link</a>. This app runs first time; When I refresh it then got error: <code>404 Not Found</code>. I really don't know what is the issue and somehow the path react path get disappear. I share my code in <a href="https://codesandbox.io/s/admiring-water-qzzbn?file=/src/App.tsx" rel="nofollow noreferrer">code-sandbox</a>.</p><br><p><strong>This is my conditional path</strong></p><br><pre><code>import React from &quot;react&quot;;<br>import SigninLinks from './SigninLinks';<br>import SignoutLinks from './SignoutLinks';<br>import useHandlder from '../configHandler/useHandler';<br><br>const Nav = () =&gt; {<br>  const { getAuthenticatedUser } = useHandlder();<br>  const Links = getAuthenticatedUser() === null ? &lt;SignoutLinks /&gt; : &lt;SigninLinks /&gt;<br>  return (<br>    &lt;nav className=&quot;nav-wrapper grey darken-3&quot;&gt;<br>      &lt;div className=&quot;container&quot;&gt;<br>        &lt;h2 className=&quot;brand-logo&quot;&gt;Logo&lt;/h2&gt;<br>        {<br>          Links<br>        }<br><br>      &lt;/div&gt;<br>    &lt;/nav&gt;<br>  );<br>};<br><br>export default Nav;<br></code></pre><br><p><strong>This is my handler functions</strong></p><br><pre><code>import React; { useCallback; useEffect } from 'react';<br>import { CognitoUserPool } from 'amazon-cognito-identity-js';<br><br>const Pool_Data = {<br>  UserPoolId: &quot;us-east-1_9gLKIVCjP&quot;;<br>  ClientId: &quot;629n5o7ahjrpv6oau9reo669gv&quot;<br>};<br><br>export default function useHandler() {<br><br>  const userPool = new CognitoUserPool(Pool_Data)<br><br>  const getAuthenticatedUser = useCallback(() =&gt; {<br>    return userPool.getCurrentUser();<br>  };<br>    [];<br>  );<br><br>  useEffect(() =&gt; {<br>    getAuthenticatedUser()<br>  }; [getAuthenticatedUser])<br><br>  const signOut = () =&gt; {<br>    return userPool.getCurrentUser()?.signOut()<br>  }<br>  return {<br>    userPool;<br>    getAuthenticatedUser;<br>    signOut<br>  }<br>};<br></code></pre><br>
0.6666666666666666,0.0,0.3333333333333333,0.0,0.0,0.0,0.3333333333333333,<h3>AWS ElasticSearch; Filebeat; Kibana: Indices managed by policy automatically set to NO even after applying policy</h3><p>ES version: 7.9</p><br><p>Hello friends;</p><br><p>I am working on AWS Elasticsearch; we are currently pushing our logs to Kibana via filebeat. To avoid ES space getting filled; we have a lifecyle policy which deletes logs more than 10 days. This has worked for us since 1 month;</p><br><p>but now when I checked the indices; it says the index is not being managed by any policy. No one has changed that setting amongst us. What has caused this change? Any ideas?</p><br><p>My suspect is ilm being set to false via filebeat; but i want to be sure.</p><br><p>We are using following configuration via filebeat:</p><br><pre><code>output.elasticsearch:<br>  hosts: [\&quot;$filebeat_host\&quot;]<br>  protocol: \&quot;https\&quot;<br>output.elasticsearch.index: \&quot;filbeat-${TIER_NAME}\&quot;<br>setup.template.name: \&quot;filebeat-${TIER_NAME}\&quot;<br>setup.template.pattern: \&quot;filebeat-${TIER_NAME}\&quot;<br>setup.ilm.enabled: false<br>setup.pack.security.enabled: false<br>setup.xpack.graph.enabled: false<br>setup.xpack.watcher.enabled: false<br>setup.xpack.monitoring.enabled: false<br>setup.xpack.reporting.enabled: false<br></code></pre><br><p>Any ideas. Thanks a lot. :-)</p><br>
0.0,0.0,0.0,0.0,0.3333333333333333,1.0,0.0,<h3>AWS SQS - setup for horizontal scaling</h3><p>I am using AWS SQS so two of mine applications (lets call them app 1 and app 2) can interact with one another. So far; app 1 was sending a message to queue; while app 2 was listening and processing the message sent from app 1. The flow is that client (reactJS application) sends http request to app 1; app 1 forwards the message to app 2 using AWS SQS; and then app 2 through the websockets broadcast certain message to all other users/clients (the important thing here is that all websocket connections are in app 2)</p><br><p>But; in past period I needed to implement horizontal scaling of app 2 (I am running exactly 2 instances of app 2) and now websocket connections are separated; some websocket connections are on instance 1 of app 2 and some websocket connections are on instance 2 of app 2. So lately I am facing the challenge where app 1 sends the message to queue and then just one of the instances of app 2 reads the message.</p><br><p>So my question would be is there a possibility that both of instances of app 2 reads the SQS message or there is some other way to handle this situation? Thanks in advance!</p><br>
0.0,0.0,0.3333333333333333,0.3333333333333333,0.6666666666666666,0.3333333333333333,0.3333333333333333,<h3>How to deploy a two django based project using apache with postgres database on ubuntu with aws</h3><p>folder structure of my Project</p><br><p><a href="https://i.stack.imgur.com/BqD6f.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/BqD6f.png" alt="folder structure of my Project" /></a></p><br><p>need some help on how to deploy both of these on aws hosting with Apache.</p><br>
0.0,0.0,0.3333333333333333,0.0,0.6666666666666666,1.0,0.3333333333333333,<h3>Rate-limit the email alerts sent by aws lambda function</h3><p>I have used lambda function(python) to receive cloudwatch logs and send alerts(emails) to recipients using SNS service. But; the recipients are getting too many emails within say; a minute. Is there a way to rate-limit the alerts to like; 1 alert per min as long as the alert is the same message?</p><br>
0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.3333333333333333,0.3333333333333333,<h3>Aws ec2 directory does html take priority over php if I upload both?</h3><p>My question is a directory question. If I install a php script in a directory and I install an index.html file. Will the html file take priority when the domain is visited?<br>What I'm trying to do if work on a php script live by typing in <a href="http://www.domainname.com/index.php" rel="nofollow noreferrer">www.domainname.com/index.php</a> but when visitors type in the <a href="http://www.domainname.com" rel="nofollow noreferrer">www.domainname.com</a> I want it to take them to the html index.html. is this possible?</p><br><p>Another question; I installed a domain name and installed a placeholder index.html in the directory; I then created a subdirectory like <a href="http://www.domainname.com/restaurants/" rel="nofollow noreferrer">www.domainname.com/restaurants/</a>. This directory has an index.html file in it but it's not showing up in the browser. I created the directory in file zilla. Could there be a way I can find out why this is?</p><br>
0.0,0.0,0.3333333333333333,0.0,0.3333333333333333,0.0,0.6666666666666666,<h3>Finding credentials of the user from the account linking using lambda python in alexa skill?</h3><p>How to get the user credentials from the account linking used by the user for an Alexa skill in AWS lambda python?</p><br>
0.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.0,<h3>Is there any way to edit big size AWS Lambda function online?</h3><p>Currently I am working with AWS Lambda in my Vue.js project.<br>In the AWS Lambda console; this message is displayed.</p><br><pre><code>&quot;The deployment package of your Lambda function &quot;GraphqlResolver-yukidev&quot; is too large to enable inline code editing. However; you can still invoke your function.&quot;<br></code></pre><br><p>The file which needs change is only one.<br>The size of function is too big so I want to work with only this file.<br>So are there anyway to edit this Lambda function online or alternative ways?</p><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.0,0.0,<h3>Unable to execute PowerShell in Lambda when passing Invoke Command</h3><p>I am looking to execute a PowerShell script through AWS Lambda.</p><br><p>When I execute the script without the invoke-command it executes. When I use the invoke-command the script fails. The lambda is running inside the VPC. I have assigned elastic IP to the lambda interface. Here is my script:</p><br><pre><code>#Requires -Modules @{ModuleName='PSWSMan';ModuleVersion='2.0.0'}<br>  #Requires -Modules @{ModuleName='AWS.Tools.EC2';ModuleVersion='4.1.15.0'}<br>  #Requires -Modules @{ModuleName='AWS.Tools.Common';ModuleVersion='4.1.15.0'}<br><br><br>  $secpasswd = ConvertTo-SecureString &quot;XXXXX&quot; -AsPlainText -Force<br><br>  $myCreds = New-Object -TypeName System.Management.Automation.PSCredential -ArgumentList <br>  &quot;XXX@YYYY.com&quot;; $secpasswd<br><br><br>  Get-EC2NetworkInterface -Filter @(@{name ='description'; values='Created By Amazon <br>  Workspaces for AWS Account ID XXXXXXX'}) | Select-Object -Property PrivateIpAddress | <br>  ForEach-Object -Process {<br>  (Invoke-Command -ComputerName $_.PrivateIpAddress -Credential <br>  $myCreds -ScriptBlock {<br>  ($code = &quot;XXXXXXX&quot;);<br>  ($id = &quot;XXXXXXX&quot;);<br>  ($region = &quot;eu-west-1&quot;); <br>  ($dir = &quot;C:\TEMP\ssm&quot;);<br>  (New-Item -ItemType directory -Path $dir -Force);(cd $dir);(New-Object <br>  System.Net.WebClient).DownloadFile(&quot;https://amazon-ssm-eu-west-1.s3.eu-west- <br>  1.amazonaws.com/latest/windows_amd64/AmazonSSMAgentSetup.exe&quot;; $dir + <br>  &quot;\AmazonSSMAgentSetup.exe&quot;);<br>  (Start-Process .\AmazonSSMAgentSetup.exe -ArgumentList @(&quot;/q&quot;; <br>  &quot;/log&quot;; &quot;install.log&quot;; &quot;CODE=$code&quot;; &quot;ID=$id&quot;; &quot;REGION=$region&quot;) -Wait)})}<br></code></pre><br><p>Here is the error I am getting:</p><br><pre><code>[Error] - Could not find a part of the path '/var/runtime/ref'.<br>  An exception was thrown when the constructor for type 'Demo.Bootstrap' was invoked. Check <br> inner exception for more details.: LambdaException<br><br><br>   at System.RuntimeTypeHandle.CreateInstance(RuntimeType type; Boolean publicOnly; Boolean <br>   wrapExceptions; Boolean&amp; canBeCached; RuntimeMethodHandleInternal&amp; ctor; Boolean&amp; <br>   hasNoDefaultCtor)<br>   at System.RuntimeType.CreateInstanceDefaultCtorSlow(Boolean publicOnly; Boolean <br>   wrapExceptions; Boolean fillCache)<br>   at System.RuntimeType.CreateInstanceDefaultCtor(Boolean publicOnly; Boolean <br>   skipCheckThis; Boolean fillCache; Boolean wrapExceptions)<br>   at System.Activator.CreateInstance(Type type; Boolean nonPublic; Boolean wrapExceptions)<br>   at System.Activator.CreateInstance(Type type)<br>   Could not find a part of the path '/var/runtime/ref'.: DirectoryNotFoundException<br>   at Amazon.Lambda.PowerShellHost.PowerShellFunctionHost..ctor()<br>   at Amazon.Lambda.PowerShellHost.PowerShellFunctionHost..ctor(String <br>   powerShellScriptFileName)<br>   at Demo.Bootstrap..ctor() in **C:\Users\himan\AppData\Local\Temp\Demo\Bootstrap.cs**:line <br></code></pre><br><p>This is a directory on my laptop. Unable to understand why this is showing in the lambda console.</p><br><pre><code>C:\Users\himan\AppData\Local\Temp\Demo\Bootstrap.cs<br></code></pre><br><p>Using this command to publish Lambda:</p><br><pre><code>Publish-AWSPowerShellLambda -ScriptPath .\Demo.ps1 -Name  Demo -Region eu-west-1<br></code></pre><br><p>Thanks<br>Himanshu</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Is it possible to use AWS KMS for key management but keep the keys in memory to encrypt / decrypt locally (without further api calls)?</h3><p>I am expecting very high traffic on one of my services; and I would like to add encryption for a new feature. I know KMS makes an API call each encrypt/decrypt call; but is it possible to use KMS for key management and cache the keys in memory to encrypt/decrypt locally without additional API calls?</p><br>
0.0,0.0,0.6666666666666666,0.0,0.3333333333333333,1.0,0.0,<h3>Create one empty cdk pipeline to deploy any lambda code</h3><p>Can we create a simple cdk pipeline that self update pipeline as shown in (<a href="https://youtu.be/1ps0Wh19MHQ?t=1538" rel="nofollow noreferrer">here</a>)<br>But only one pipeline for many micro services that deploy and update lambda code.<br>Is this possible. So that we don't have to create pipeline for every micro service.</p><br>
0.0,0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,1.0,<h3>Occasional issue with amazon SES</h3><p>I'm experiencing some occasional problems with sending emails from some instances using Amazon EC2 and Amazon SES Swiftmailer v5.4.12.</p><br><p>Most of the emails are sent without problems; while others; without distinction of size; recipient or content; fail to send with the following error:</p><br><pre><code>Code 0<br>Failed to authenticate on SMTP server with username &quot;xxxxxxxx&quot; using 2 possible authenticators.<br></code></pre><br><p>Emails are sent from servers with valid https certificate; using SSL encryption and from verified email addresses. The amazon SES console doesn't report any errors in sending; with a complain rate of 0% and a bounce rate of 0.2%</p><br><p>Has anyone encountered the same problem? Is there a possible solution?</p><br>
0.0,1.0,0.0,0.0,1.0,0.0,0.0,<h3>AWS Load Balancer with ECS. How do I troubleshoot health checks failing?</h3><p>I am deploying a multi container Flask python app (with gunicorn) to ECS with Docker to my ECS cluster that uses a single t2.small EC2 instance. My app runs on port 8000 and runs fine; I can use my app perfectly when using my EC2 DNS: <a href="http://ec2-xx-xxx-xxx-xx.us-east-2.compute.amazonaws.com:8000" rel="nofollow noreferrer">http://ec2-xx-xxx-xxx-xx.us-east-2.compute.amazonaws.com:8000</a></p><br><p>I now want to use my own custom domain instead from GoDaddy. I'm using Route 53 for the nameserver registration; and plan to use an alias that points to my instance via a load balancer (Application Load Balancer).</p><br><p>Before setting up the alias; I want to first check my ALB is is successfully allowing me to access my app on port 8000 via HTTP (port 80) using the target group. My ECS service creates fine and I can see my web app running in the logs; but when I put the ALB DNS into my browser I get: 502 Bad Gateway.</p><br><p>I've checked my Target Group and it seems that the registration of my EC2 instance is failing on port 8000 due to &quot;Health checks fail&quot;. I can't find any further details on the cause of failure; 'Health status details' just says 'Health checks failed' and describe-target-health returns Target.FailedHealthChecks.</p><br><p>I've tried to troubleshoot myself following these steps: <a href="https://www.youtube.com/watch?v=cmRZleI18Yg" rel="nofollow noreferrer">https://www.youtube.com/watch?v=cmRZleI18Yg</a></p><br><p>When I SSH into my EC2 instance on cmd and run telnet  80; I get a 'Connection refused' error rather than HTTP 200 response. When I try the same using my load balancer DNS; It connects successfully on PORT 80. My current thinking is that for some reason; my EC2 instance is not listening on port 80. I have no idea why and have tried the following already:</p><br><p><strong>Ensured correct set-up of security groups and NACL</strong></p><br><p>Yes; EC2 security group set-up to allow all traffic on port 80 and have added rule for my ALB security group on all ports. NACL accepting all inbound and outbound traffic.</p><br><p><a href="https://i.stack.imgur.com/eHuIA.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/eHuIA.png" alt="enter image description here" /></a></p><br><p>**Ensure no firewall on EC2 blocking HTTP **</p><br><p>Have run <code>sudo service iptables status</code> and got the following:</p><br><pre><code>Redirecting to /bin/systemctl status iptables.service<br> iptables.service - IPv4 firewall with iptables<br>   Loaded: loaded (/usr/lib/systemd/system/iptables.service; disabled; vendor preset: disabled)<br>   Active: inactive (dead)<br></code></pre><br><p>which seems to suggest no firewalls in place?</p><br><p><strong>Confirm web server is actually running on EC2 instance</strong></p><br><p>I'm very open to suggestions here; but I assume it's running from a) being able to see logs in ECS and b) I could use the app successfully when using <a href="http://ec2-xx-xxx-xxx-xx.us-east-2.compute.amazonaws.com:8000" rel="nofollow noreferrer">http://ec2-xx-xxx-xxx-xx.us-east-2.compute.amazonaws.com:8000</a>. Is there anything else I can do to validate this?</p><br><p>Logs from ECS:</p><br><p><a href="https://i.stack.imgur.com/INcqp.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/INcqp.png" alt="enter image description here" /></a></p><br><p>Dockerfile</p><br><pre><code>FROM python:3.7.5-slim-buster<br><br>RUN apt-get update &amp;&amp; apt-get install -qq -y \<br>  build-essential libpq-dev --no-install-recommends<br><br>RUN apt-get install libcurl4-openssl-dev libssl-dev -y<br><br>ENV INSTALL_PATH /canopact<br>RUN mkdir -p $INSTALL_PATH<br><br>WORKDIR $INSTALL_PATH<br><br>COPY requirements.txt requirements.txt<br>RUN pip install -r requirements.txt<br><br>COPY . .<br>RUN pip install --editable .<br><br>CMD gunicorn -c &quot;python:config.gunicorn&quot; &quot;canopact.app:create_app()&quot;<br></code></pre><br><p>Gunicorn.py</p><br><pre><code># -*- coding: utf-8 -*-<br><br>bind = '0.0.0.0:8000'<br>keepalive = 120<br>accesslog = '-'<br>access_log_format = '%(h)s %(l)s %(u)s %(t)s &quot;%(r)s&quot; %(s)s %(b)s &quot;%(f)s&quot; &quot;%(a)s&quot; in %(D)ss'<br><br></code></pre><br><p>I'm still using the flask config SERVER_NAME as: &quot;ec2-xx-xxx-xxx-xx.us-east-2.compute.amazonaws.com:8000&quot;; but I have tried changing it to &quot;0.0.0.0:8000&quot; and getting the same result of healthchecks failing</p><br><p><strong>Questions</strong></p><br><p>I'm really really unsure on what else I can do to troubleshoot this. Am I correct to assume that my healthchecks are failing because of the Connection refused when trying to connect to EC2 instance on PORT 80; or should I be investigating something else? Feels like every stackoverflow post suggests trying the above 3 steps to troubleshoot Connection refused; so I'm not sure what else to try.</p><br><p>I've seen some posts suggesting use of NGINX; but I thought ALB could be used independently on NGINX?</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.6666666666666666,<h3>Recording and uploading a wav to Amazon S3</h3><p>I want a user to be able to record and upload a .wav to an S3 bucket. Using <a href="https://www.srijan.net/blog/aws-s3-audio-streaming" rel="nofollow noreferrer">this</a>; I am able to achieve this; working correctly; as a .webm. file. I am now trying to adapt this to use a MediaRecorder bolt-on that allows the support of .wav files. So I am now trying to integrate that code with <a href="https://recordrtc.org/" rel="nofollow noreferrer">RecordRTC</a> which adds .wav support to MediaRecorder.</p><br><p>This functionality essentially works; in that I end up with a .wav file in my Amazon S3 bucket; but the file is corrupted. I think the main place of concern is in the callback function for ondataavailable (a lot of the code afterwards probably can be ignored; but is there just in case). The line console.log(blob); in the following code shows that the blob type is audio/webm.</p><br><p>Any ideas how this can be fixed?</p><br><p><strong>Edit:</strong> The resulting file is actually a .webm file; according to <a href="http://checkfiletype.com/upload-and-check" rel="nofollow noreferrer">link</a>; not a .wav after all! So why not? (However; my computer still shows it as a .wav in the File Inspector)</p><br><pre><code>function isConstructor(obj) {<br>  return !!obj.prototype &amp;&amp; !!obj.prototype.constructor.name;<br>}<br><br>class AudioStream {<br><br>  constructor(region; IdentityPoolId; audioStoreWithBucket) {<br>    this.region = region; //s3 region<br>    this.IdentityPoolId = IdentityPoolId; //identity pool id<br>    this.bucketName = audioStoreWithBucket; //audio file store<br>    this.s3; //variable defination for s3<br>    this.dateinfo = new Date();<br>    this.timestampData = this.dateinfo.getTime(); //timestamp used for file uniqueness<br>    this.etag = []; // etag is used to save the parts of the single upload file<br>    this.recordedChunks = []; //empty Array<br>    this.booleanStop = false; // this is for final multipart complete<br>    this.incr = 0; // multipart requires incremetal so that they can merge all parts by ascending order<br>    this.filename = this.timestampData.toString() + &quot;.wav&quot;; //unique filename<br>    this.uploadId = &quot;&quot;; // upload id is required in multipart<br>    this.recorder; //initializing recorder variable<br><br>    this.audioConstraints = {<br>      audio: true<br>    };<br>  }<br><br>  audioStreamInitialize() {<br><br>      var self = this;<br>    AWS.config.region = self.region;<br>    AWS.config.credentials = new AWS.CognitoIdentityCredentials({<br>      IdentityPoolId: self.IdentityPoolId;<br>    });<br><br><br>      self.s3 = new AWS.S3();<br><br>      navigator.mediaDevices.getUserMedia(self.audioConstraints)<br>    .then(function(stream) {<br><br>                self.recorder = RecordRTC(stream; {<br>                      type: 'audio';<br>                      mimeType: 'audio/wav';<br>                      recorderType: MediaStreamRecorder;<br>                      disableLogs: true;<br>                      // get intervals based blobs<br>                      // value in milliseconds<br>                      timeSlice: 1800000;<br>                      // requires timeSlice above<br>                      // returns blob via callback function<br>                      ondataavailable: function(blob) {<br>                        console.log(&quot;ondata!&quot;)<br><br>         var normalArr = [];<br>        /*<br>          Here we push the stream data to an array for future use.<br>        */<br>        console.log(blob);<br>          self.recordedChunks.push(blob);<br>        normalArr.push(blob);<br><br>        /*<br>          here we create a blob from the stream data that we have received.<br>        */<br>          var bigBlob = new Blob(normalArr; {<br>            type: 'audio/wav'<br>          });<br><br><br>          /*<br>            if the length of recordedChunks is 1 then it means its the 1st part of our data.<br>          So we createMultipartUpload which will return an upload id.<br>          Upload id is used to upload the other parts of the stream<br><br>          else.<br>          It Uploads a part in a multipart upload.<br>          */<br>            if (self.recordedChunks.length == 1) {<br>              self.startMultiUpload(bigBlob; self.filename)<br>            } else {<br>              /*<br>                self.incr is basically a part number.<br>              Part number of part being uploaded. This is a positive integer between 1 and 10;000.<br>              */<br>                self.incr = self.incr + 1<br>              self.continueMultiUpload(bigBlob; self.incr; self.uploadId; self.filename; self.bucketName);<br>            }<br><br>                      } // end ondataavailable<br>                  });<br><br>                /*<br>                    Called to handle the dataavailable event; which is periodically triggered each time timeslice milliseconds of media have been recorded<br>                    (or when the entire media has been recorded; if timeslice wasn't specified).<br>    The event; of type BlobEvent; contains the recorded media in its data property.<br>    You can then collect and act upon that recorded media data using this event handler.<br>    */<br><br>    });<br>  }<br><br>  disableAllButton() {<br>    //$(&quot;#formdata button[type=button]&quot;).attr(&quot;disabled&quot;; &quot;disabled&quot;);<br>  }<br><br>  enableAllButton() {<br>    //$(&quot;#formdata button[type=button]&quot;).removeAttr(&quot;disabled&quot;);<br>  }<br><br>  /*<br>    The MediaRecorder method start(); which is part of the MediaStream Recording API;<br>  begins recording media into one or more Blob objects.<br>  You can record the entire duration of the media into a single Blob (or until you call requestData());<br>  or you can specify the number of milliseconds to record at a time.<br>  Then; each time that amount of media has been recorded; an event will be delivered to let you act upon the recorded media;<br>  while a new Blob is created to record the next slice of the media<br>  */<br>    startRecording(id) {<br>      var self = this;<br>      //self.enableAllButton();<br>      //$(&quot;#record_q1&quot;).attr(&quot;disabled&quot;; &quot;disabled&quot;);<br>      /*<br>        1800000 is the number of milliseconds to record into each Blob.<br>      If this parameter isn't included; the entire media duration is recorded into a single Blob unless the requestData()<br>            method is called to obtain the Blob and trigger the creation of a new Blob into which the media continues to be recorded.<br>        */<br>        /*<br>        PLEASE NOTE YOU CAN CHANGE THIS PARAM OF 1800000 but the size should be greater then or equal to 5MB.<br>        As for multipart upload the minimum breakdown of the file should be 5MB<br>        */<br>        //this.recorder.start(1800000);<br>        this.recorder.startRecording();<br>        Shiny.setInputValue(&quot;timecode&quot;; self.filename);<br><br>    }<br><br><br>    stopRecording(id) {<br>        var self = this;<br>        self.recorder.stopRecording();<br><br>    }<br><br><br>    pauseRecording(id) {<br>        var self = this;<br>        self.recorder.pauseRecording();<br>        //$(&quot;#pause_q1&quot;).addClass(&quot;hide&quot;);<br>        //$(&quot;#resume_q1&quot;).removeClass(&quot;hide&quot;);<br>    }<br><br><br><br>    resumeRecording(id) {<br>        var self = this;<br>        self.recorder.resumeRecording();<br>        //$(&quot;#resume_q1&quot;).addClass(&quot;hide&quot;);<br>        //$(&quot;#pause_q1&quot;).removeClass(&quot;hide&quot;);<br>    }<br><br>    /*<br>        Initiates a multipart upload and returns an upload ID.<br>        Upload id is used to upload the other parts of the stream<br>    */<br>    startMultiUpload(blob; filename) {<br>        var self = this;<br>        var audioBlob = blob;<br>        var params = {<br>            Bucket: self.bucketName;<br>            Key: filename;<br>            ContentType: 'audio/wav';<br>            ACL: 'private';<br>        };<br>        self.s3.createMultipartUpload(params; function(err; data) {<br>            if (err) {<br>                console.log(err; err.stack); // an error occurred<br>            } else {<br>                self.uploadId = data.UploadId<br>                self.incr = 1;<br>                self.continueMultiUpload(audioBlob; self.incr; self.uploadId; self.filename; self.bucketName);<br>            }<br>        });<br>    }<br><br><br>    continueMultiUpload(audioBlob; PartNumber; uploadId; key; bucketName) {<br>        var self = this;<br>        var params = {<br>            Body: audioBlob;<br>            Bucket: bucketName;<br>            Key: key;<br>            PartNumber: PartNumber;<br>            UploadId: uploadId<br>        };<br>        console.log(params);<br>        self.s3.uploadPart(params; function(err; data) {<br>            if (err) {<br>                console.log(err; err.stack)<br>            } // an error occurred<br>            else {<br>                /*<br>                    Once the part of data is uploaded we get an Entity tag for the uploaded object(ETag).<br>                    which is used later when we complete our multipart upload.<br>                */<br>                self.etag.push(data.ETag);<br>                if (self.booleanStop === true) {<br>                    self.completeMultiUpload();<br>                }<br>            }<br>        });<br>    }<br><br><br>    /*<br>        Completes a multipart upload by assembling previously uploaded parts.<br>    */<br>    completeMultiUpload() {<br>        var self = this;<br>        var outputTag = [];<br>        /*<br>            here we are constructing the Etag data in the required format.<br>        */<br>        self.etag.forEach((data; index) =&gt; {<br>            const obj = {<br>                ETag: data;<br>                PartNumber: ++index<br>            };<br>            outputTag.push(obj);<br>        });<br><br>        var params = {<br>            Bucket: self.bucketName; // required<br>            Key: self.filename; // required<br>            UploadId: self.uploadId; // required<br>            MultipartUpload: {<br>                Parts: outputTag<br>            }<br>        };<br><br>        self.s3.completeMultipartUpload(params; function(err; data) {<br>            if (err) {<br>                console.log(err; err.stack);<br>            } // an error occurred<br>            else {<br>                // initialize variable back to normal<br>                self.etag = []; self.recordedChunks = [];<br>                self.uploadId = &quot;&quot;;<br>                self.booleanStop = false;<br>                //self.disableAllButton();<br>                self.removeLoader();<br>                console.log(&quot;sent!&quot;);<br>            }<br>        });<br>    }<br><br><br>    /*<br>        set loader<br>    */<br>    setLoader() {<br>        //$(&quot;#kc-container&quot;).addClass(&quot;overlay&quot;);<br>        //$(&quot;.preloader-wrapper.big.active.loader&quot;).removeClass(&quot;hide&quot;);<br>    }<br><br><br>    /*<br>        remove loader<br>    */<br>    removeLoader() {<br>       // $(&quot;#kc-container&quot;).removeClass(&quot;overlay&quot;);<br>        //$(&quot;.preloader-wrapper.big.active.loader&quot;).addClass(&quot;hide&quot;);<br>    }<br><br>    getFilename() {<br>        return this.filename;<br>    }<br><br>}<br></code></pre><br>
0.0,0.6666666666666666,0.6666666666666666,0.0,0.3333333333333333,0.0,0.3333333333333333,<h3>value of property security group must be of type list of string</h3><pre><code>Parameters:<br>  KeyName:<br>    Description: Name of an existing EC2 KeyPair to enable SSH access to the instance<br>    Type: 'AWS::EC2::KeyPair::KeyName'<br>    ConstraintDescription: must be the name of an existing EC2 KeyPair.<br><br>  MySubnet:<br>    Description: My subnet from my VPC<br>    Type: 'AWS::EC2::Subnet::Id'<br>    Default: subnet-YYYYYYYY<br>  <br>  MySG:<br>    Description: My Security Group from my VPC<br>    Type: 'AWS::EC2::SecurityGroup::GroupName'<br>    Default: SG-YYYYYYYY<br>   <br>Resources:<br><br>  Ec2Instance:<br>    Type: AWS::EC2::Instance<br>    Properties:<br>      InstanceType: t2.micro<br>      ImageId: ami-09e67e426f25ce0d7<br><br>      SecurityGroups: !Ref MySG<br><br>      SubnetId: !Ref MySubnet<br>      <br>      KeyName: !Ref KeyName<br>  <br>      <br></code></pre><br><p>I have this above cloudformation template code which returns &quot;Value of property SecurityGroups must be of type List of String&quot;; my vpc and security groups are simplified in a different cloudformation template; and i want to launch an ec2 in a specific Security group.</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>AWS:SNS Email - Customize SNS Subscribe Confirmation Email for any topic</h3><p>Anyone have an idea; how can I change the SNS subscription confirmation email template?</p><br><p>User Interaction Step By Step to understand the exact question: (All action done using AWS PHP SDK)</p><br><p>1&gt; User A created campaign in AWS: <a href="https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/sns-examples-managing-topics.html#create-a-topic" rel="nofollow noreferrer">https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/sns-examples-managing-topics.html#create-a-topic</a></p><br><p>2&gt; Added N number of subscribers to the campaign: <a href="https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/sns-examples-subscribing-unsubscribing-topics.html#subscribe-an-email-address-to-a-topic" rel="nofollow noreferrer">https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/sns-examples-subscribing-unsubscribing-topics.html#subscribe-an-email-address-to-a-topic</a></p><br><p>3&gt; All of the subscribers will receive an email from AWS SNS saying that; you have opt-in to the ARN:TOPICNAME. Click on the confirmation link to verify subscription</p><br><p><em><strong>The question related to this place: When adding a subscriber to the campaign; SNS send an email with a verification link with its own branding. I want to change the template of the email and want to add our own branding to the email. How can I do that?</strong></em></p><br><p><strong><a href="https://i.stack.imgur.com/iePJq.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/iePJq.png" alt="Image text" /></a></strong></p><br><p>4&gt; Each subscriber will open the email and confirm a subscription.<br>Another one:<br>Is there any way to stop double opt-in as users have prevalidated emails to receive an email from us?</p><br><p>Thank you in advance.</p><br>
0.0,0.0,0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>Which python3.6 patch version is used in AWS serverless function runtime environments?</h3><p>I have a  <code>serverless.yml</code> file that specifies <code>Runtime: python3.6</code> for a serverless function in AWS (Lambda).  When setting up a local virtual Python environment for development; I need to specify a patch number (i.e. 3.6.0 through 3.6.12).  Which patch version would I use in my virtual environment to best replicate the AWS runtime environment?</p><br><p>I think it might be 3.6.12 since Python3 is well beyond minor version 6 now; and patch versions are typically for bug fixes; but I have not been able to find confirmation.  I could also see it maybe depending on what was released when the last build/deployment occurred.</p><br><p>In summary; is there a &quot;correct&quot; patch version I should be using locally <strong>or</strong> a way to determine which patch version is used by my current deployment?</p><br>
1.0,0.0,0.3333333333333333,0.0,0.0,0.3333333333333333,0.0,<h3>AWS MWAA: Glue Crawler issue</h3><p>I have manually provisioned a Glue Crawler and now am attempting to run it via Airflow (in AWS).</p><br><p>Based on the docs from <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_api/airflow/providers/amazon/aws/hooks/glue_crawler/index.html#airflow.providers.amazon.aws.hooks.glue_crawler.AwsGlueCrawlerHook" rel="nofollow noreferrer">here</a>; there seems to be plenty of ways to handle this objective compared to other tasks within the Glue environment. However; I'm having issues handling this seemingly simple scenario.</p><br><p>The following code defines the basic setup for Glue[Crawler]+Airflow. Assume there are some other working tasks that are defined before and after it; which are not included here.</p><br><pre><code>run_crawler = AwsGlueCrawlerHook()<br>run_crawler.start_crawler(crawler_name=&quot;foo-crawler&quot;)<br></code></pre><br><p>Now; here is an example flow:</p><br><pre><code>json2parquet &gt;&gt; run_crawler &gt;&gt; parquet2redshift<br></code></pre><br><p>Given all this; the following error manifests on the Airflow Webserver UI:</p><br><pre><code>Broken DAG: An error occurred (CrawlerRunningException) when calling the StartCrawler operation: Crawler with name housing-raw-crawler-crawler-b3be889 has already started<br></code></pre><br><p>I get it: <em>why</em> don't you use something other than the <code>start_crawler</code> method...? Fair point; but I don't know what else to employ. I just want to start the crawler <em>after</em> some upstream tasks have successfully completed but am unable to.</p><br><p><strong>How should I resolve this problem</strong>?</p><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>AWS Cognito: Getting error in Auth.signIn (Validate that amazon-cognito-identity-js has been linked)</h3><p>I'm new to Amplify integration with Cognito and working on a react-native app using Amplify with Cognito for Authentication. I have configured the user pool and Federated Identity in the AWS console.</p><br><p>I have created my own signup and login interface with the respective screens using the react-navigation 5.x version.<br>Below are the AWS related modules I added in package.json</p><br><pre><code>&quot;@aws-amplify/auth&quot;: &quot;^3.4.24&quot;;<br>&quot;@aws-amplify/core&quot;: &quot;^3.8.16&quot;;<br></code></pre><br><p>Here is the Amplify configuration in the App.js</p><br><pre><code> Amplify.configure({ <br>  Auth: { <br>    identityPoolId: 'eu-west-2:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx';<br>    region: 'eu-west-2';<br>    userPoolId: 'eu-west-2_xxxxxxxx';<br>    userPoolWebClientId: 'xxxxxxxxxxxxxxxxxxxxxx';<br>    authenticationFlowType: 'USER_PASSWORD_AUTH'<br>  }<br>});<br></code></pre><br><p>I'm able to successfully invoke <em><strong>Auth.signUp</strong></em> but getting error when I'm trying to invoke <em><strong>Auth.signIn(username; password)</strong></em></p><br><blockquote><br><p>Validate that amazon-cognito-identity-js has been linked</p><br></blockquote><br><p>How do I able to invoke Auth.signIn successfully; please help in resolving the issue?</p><br>
0.0,0.0,0.0,1.0,0.3333333333333333,0.3333333333333333,0.0,<h3>How to create an Amazon S3 job to move big files</h3><p>I need to copy a file from one folder to another inside an unique Amazon S3 bucket. However; due to files size; I can't simply call copyObject method from AWS SDK S3 class; since it timesout my Lambda function.</p><br><p>That's why I'm trying to create a <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops.html" rel="nofollow noreferrer">S3 Batch Operations</a> job to move this file; but I'm getting an <code>Invalid job operation</code> error when trying to. I'm using AWS SDK <code>S3Control</code> class; trying to invoke method createJob. I'm passing this object as parameter:</p><br><pre><code>{<br>    AccountId: '445084039568';<br>    Manifest: {<br>        Location: {<br>            ETag: 'dbe4a392892992491a7124c10f2fbf03';<br>            ObjectArn: 'arn:aws:s3:::amsp-media-bucket/manifest.csv'<br>        };<br>        Spec: {<br>            Format: 'S3BatchOperations_CSV_20180820';<br>            Fields: ['Bucket'; 'Key']<br>        };<br>        <br>    };<br>    Operation: {<br>        S3PutObjectCopy: {<br>            TargetResource: 'arn:aws:s3:::amsp-media-bucket/bigtest'<br>        }<br>    };<br>    Report: {<br>        Enabled: false<br>    };<br>    Priority: 10; <br>    RoleArn: 'arn:aws:iam::445084039568:role/mehoasumsp-sandbox-asumspS3JobRole-64XWYA3CFZF3'<br>}<br></code></pre><br><p>To be honest; I'm not sure if I'm specifying manifest correctly. This is manifest.csv content:</p><br><pre><code>amsp-media-bucket; temp/37766a92-16ef-4ee2-8e79-3875679dad85.mkv<br></code></pre><br><p>I'm not insecure about the file itself but about the way I define Spec property at param object.</p><br>
0.0,1.0,0.0,0.3333333333333333,0.3333333333333333,0.0,0.0,<h3>Send cookies to EBS backend from CloudFront S3 frontend</h3><p>I am pretty inexperienced with AWS and I have an app that uses a JWT token stored in a cookie to log in users. On page load; a GET request is made to the backend; the backend verifies the token and redirects the user to the dashboard page; which can only be accessed with a valid token. If there's no token; the backend returns a 400 error and the user stays on the home page. This works flawlessly on my local machine but not when I host the project on AWS. I believe there are no problems with how it's hosted because the backend does receive the GET request from the frontend; just without cookies; and I am adding credentials with it. The documentation talks about a <a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesForwardCookies" rel="nofollow noreferrer">Forward Cookies</a> option and so does <a href="https://www.youtube.com/watch?v=V8vR7rA0ubs" rel="nofollow noreferrer">this video</a> by AWS but the console has since changed and this option is no longer available. The second answer in <a href="https://stackoverflow.com/questions/47569633/pass-cookie-to-cloudfront-origin-but-prevent-from-caching">this post</a> suggests that the right way to do it is via custom cache and origin request policies in a distribution behavior but the example given doesn't match my use case and I haven't been able to get it working. I have tried editing the distribution behaviour and both setting &quot;Cookies&quot; to &quot;All&quot; in the legacy cache settings and using custom cache and origin request policies with the same setting but nothing works.</p><br><p><strong>Axios GET request:</strong></p><br><pre><code>axios<br>  .get(`${backendURL}/isUser`; {<br>    withCredentials: true;<br>  })<br>  .then(() =&gt; router.push(&quot;/dashboard&quot;))<br>  .catch((error: AxiosError) =&gt; console.error(error))<br></code></pre><br><p><strong>Development (left) and production (right) requests</strong><br><a href="https://i.stack.imgur.com/XkUdB.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/XkUdB.png" alt="Development (left) and production (right) requests" /></a></p><br><p><strong>Distribution behavior unchanged (just HTTP to HTTPS redirection)</strong><br><a href="https://i.stack.imgur.com/IWG6C.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/IWG6C.png" alt="Distribution behavior unchanged (just HTTP to HTTPS redirection)" /></a></p><br>
0.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.0,<h3>AWS API GW + Lambda Flask</h3><p>I am trying to get an app set up with API Gateway and Lambda with Flask. When I use a simple lambda function to return a string everything works fine.</p><br><pre><code>def lambda_handler(event; context):<br>    return &quot;Hello world&quot; <br></code></pre><br><p>However when I try and run this same thing through flask-lambda I get an error with this code:</p><br><pre><code>from flask_lambda import FlaskLambda<br><br>app = FlaskLambda(__name__)<br><br>@app.route('/'; methods=['GET'; 'POST'])<br>def home():<br>    return &quot;Hello world&quot; <br><br><br>if __name__ == '__main__':<br>    app.run(debug=True)<br></code></pre><br><p>Here is the error I get through the browser:</p><br><pre><code>{&quot;errorMessage&quot;: &quot;'wsgi.url_scheme'&quot;; &quot;errorType&quot;: &quot;KeyError&quot;; &quot;stackTrace&quot;: [&quot;  File \&quot;/var/task/flask_lambda.py\&quot;; line 97; in __call__\n    return super(FlaskLambda; self).__call__(event; context)\n&quot;; &quot;  File \&quot;/var/task/flask/app.py\&quot;; line 2088; in __call__\n    return self.wsgi_app(environ; start_response)\n&quot;; &quot;  File \&quot;/var/task/flask/app.py\&quot;; line 2065; in wsgi_app\n    ctx = self.request_context(environ)\n&quot;; &quot;  File \&quot;/var/task/flask/app.py\&quot;; line 1982; in request_context\n    return RequestContext(self; environ)\n&quot;; &quot;  File \&quot;/var/task/flask/ctx.py\&quot;; line 309; in __init__\n    self.url_adapter = app.create_url_adapter(self.request)\n&quot;; &quot;  File \&quot;/var/task/flask/app.py\&quot;; line 1774; in create_url_adapter\n    return self.url_map.bind_to_environ(\n&quot;; &quot;  File \&quot;/var/task/werkzeug/routing.py\&quot;; line 1651; in bind_to_environ\n    wsgi_server_name = get_host(environ).lower()\n&quot;; &quot;  File \&quot;/var/task/werkzeug/wsgi.py\&quot;; line 113; in get_host\n    environ[\&quot;wsgi.url_scheme\&quot;];\n&quot;]}<br></code></pre><br><p>I don't know if I need to add env variables to the lambda or what. I've added the following env vars to test; just throwing stuff at the wall:</p><br><pre><code>Environment:<br>        Variables: <br>          TABLE_NAME: blah-table<br>          REGION_NAME: !Ref AWS::Region<br>          SERVER_NAME: !Sub 'xxxxxxxxx.execute-api.${AWS::Region}.amazonaws.com'<br>          SERVER_PORT: 443<br>          HTTP_HOST: !Sub 'xxxxxxxx.execute-api.${AWS::Region}.amazonaws.com'<br>          WSGI_SERVER_NAME: !Sub 'xxxxxxxx.execute-api.${AWS::Region}.amazonaws.com'<br></code></pre><br><p>Any nudge in the right direction is much appreciated.</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>I can&#39;t enable &quot;Public access&quot; config in AWS Aurora cluster</h3><p>In the <a href="https://aws.amazon.com/es/premiumsupport/knowledge-center/aurora-private-public-endpoints/" rel="nofollow noreferrer">documentation</a> shows that i should can turn on the &quot;Public Access&quot; in RDS Aurora database but I can't see this setting.</p><br><p><a href="https://i.stack.imgur.com/ZaSJW.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/ZaSJW.png" alt="RDS Autora Connectivity" /></a></p><br><p>This is the screenshot of another RDS Instance (Not Aurora)</p><br><p><a href="https://i.stack.imgur.com/RECrx.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/RECrx.png" alt="enter image description here" /></a></p><br><p>Anyone have any idea to fix that?</p><br><p>Thanks!</p><br>
0.0,0.0,0.3333333333333333,1.0,0.6666666666666666,0.0,0.0,<h3>How do I use lambda function to enable versioning on multiple S3 buckets?</h3><p>I am tasked to write a lambda function that would enable versioning on multiple S3 buckets. How would I do this?</p><br>
0.0,0.0,0.3333333333333333,0.0,0.3333333333333333,0.3333333333333333,0.3333333333333333,<h3>How to auto-update serverless.template file when you update Http-calls in controller?</h3><p>Currently we observed that when every we add any api in controller level we have to manually add it to serverless.template file in .net core Api project.</p><br>
0.0,1.0,0.0,1.0,0.0,0.0,0.0,<h3>Cannot connect to RDS from EC2 instance</h3><p>Tried the various threads still stuck.</p><br><ol><br><li><p>I can successfully connect to RDS from both my local and google data studio; just not from this EC2. (all 3 have inbound rules in my rds security group; see #2)</p><br></li><br><li><p>Added an Inbound Rule to the RDS security group that is the private IP address of my EC2</p><br></li><br><li><p>Added Outbound Rule for port 3306  for my EC2 instance security group to 0.0.0.0/0<br>(although not sure this is needed)</p><br></li><br></ol><br><p>Trying to run the same python script I use in local to connect.<br><code>engine_insert = sqlalchemy.create_engine('mysql+pymysql://root:password@rdsdb.abcdefg.us-east-1.rds.amazonaws.com:3306/database')</code></p><br>
0.0,0.0,1.0,0.0,0.3333333333333333,0.0,0.6666666666666666,<h3>Attach AWS IAM Profile to Azure VM</h3><p>Is there a way where to attach an AWS IAM profile to an Azure VM.</p><br><p>I'm trying to develop a common infrastructure for Azure and AWS and i want to use resources which are in AWS from an Azure VM.</p><br><p>I know this can do this by exporting AWS creds to Azure VM but is there a way where I can attach an already existing AWS IAM profile to the Azure VM (if not directly may be through an interface or a service?) and access the resources (which how is I'm doing from an ec2 instance currently) ?</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.3333333333333333,<h3>Enable the CustomSMSSender for 3rd party provider to send SMS verification code</h3><p>I want to send verification code provided by aws cognito to sms using third party provider. I was going through some documentation.<br><a href="https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-custom-sms-sender.html" rel="nofollow noreferrer">aws documenation</a></p><br><p>But I couldnot find the CustomSMSSender trigger option. Here</p><br><p><a href="https://i.stack.imgur.com/KB5Cs.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/KB5Cs.png" alt="enter image description here" /></a></p><br>
0.0,0.0,0.0,0.3333333333333333,1.0,0.3333333333333333,0.0,<h3>Is there a way to configure AWS Lightsail database public mode by API?</h3><p>I know that I can set the public mode of the database in the AWS Lightsail console.</p><br><p>However; if I want to configure it using the AWS Lightsail API; I can't find a way to do so.</p><br><p>I already looked at Lightsail API reference; but I couldn't discover anything like that.</p><br><p><a href="https://docs.aws.amazon.com/lightsail/2016-11-28/api-reference/Welcome.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/lightsail/2016-11-28/api-reference/Welcome.html</a></p><br><p>My goal is to do the migration with github actions. To do so; I believe I need to put it into public mode once. Then I would like to disable the public mode once the migration is done. I can do it manually but it's more smart using API.</p><br><p>If anyone knows; please tell me how to do.</p><br>
0.0,1.0,1.0,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>How to get ALB name from AWS in Terraform</h3><p>I need to get an ALB name or id to attach WAF rules to it.<br>The ALB is created by Kubernetes and not used anywhere in Terraform.<br>Oficial data resource only supports name and arn with no filtering.</p><br><pre><code>data &quot;aws_lb&quot; &quot;test&quot; {<br>  name = ...<br>  arn = ...<br>}<br></code></pre><br><p>Is there a way to get ALB somehow or attach WAF rules to it?</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>Use a custom classifier in Glue for multi line records</h3><p>I have some files in the following format</p><br><pre><code>AB1|STUFF|1234|<br><br>AB2|SF|STUFF|<br><br>AB1|STUFF|45670|<br><br>AB2|AF|STUFF|<br></code></pre><br><p>Each bit of data is delimited by '|' and a record is made up of the data in lines AB1 and AB2.</p><br><p>I would like to use a custom grok classifier in Glue something like the following:</p><br><pre><code>?&lt;LINE1&gt;(?:AB1)?|%{WORD:ignore1}|%{NUMBER:id}\|\n%{WORD:LINE2}|%{WORD:make}|%{WORD:stuff2}\|<br></code></pre><br><p>That is a multi line grok expression to extract the data from a multi line record as shown above. I am unsure how the classifiers in Glue work any comments or advice would be very helpful.</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Is there an easy way to add GUI search elements to an AWS cloudwatch dashboard?</h3><p>I would like to be able to basically add some GUI drop down and search field elements for a Cloudwatch log-insights search to a dashboard to enable non SQL-savvy admins to change filter and sort options easier based on a few pre-chosen selections. Is this possible without implementing an entirely different tool like Athena?<br>Thanks!</p><br>
0.0,0.3333333333333333,0.0,0.0,1.0,0.0,0.0,<h3>AWS Autoscaling Group EC2 instances go down during cron jobs</h3><p>I tried autoscaling groups and alternatively just a bunch of EC2 instances tied by load balancer. Both configs are working fine at first glance.</p><br><p>But; when the EC2 is a part of autoscaling group it goes down sometimes. Actually it happens very often; almost once a day. And they go down in a &quot;hard reset&quot; way. The ec2 monitoring graphs show that CPU usage goes up to 100%; then the instance become not responsive and then it is terminated by autoscaling group.</p><br><p>And it has nothing to do with my processes on these instances.</p><br><p>When the instance is not a part of Autoscaling groups; it can work without the CPU usage spikes for years.</p><br><p>The &quot;hard reset&quot; on autoscaling group instances are braking my cron jobs. As much as I like the autoscaling groups I cannot use it.</p><br><p>It there a standard way to deal with the &quot;hard resets&quot;?</p><br><p>PS.</p><br><p>The cron jobs are running PHP scripts on Ubuntu in my case. I managed to make only one instance running the job.</p><br>
0.0,0.0,1.0,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>AWS Alarm is always &#39;in alarm&#39; after deleting an EC2 instance</h3><p>I have set up <strong>2 CloudWatch alarms</strong> on a queue metric. Once a batch of files have been uploaded to an S3 bucket; the alarms watch out for a predefined value (in this case it is 10); and launch/terminate an instance accordingly so that my application auto-scales appropriately.</p><br><p>However; since one of the alarms terminates instances once the number of messages on my queue is less than 10; at one point it constantly is in a state of '<strong>ALARM</strong>' as the messages available are less than 10.</p><br><p>Is there any simple way I can fix this?</p><br>
0.0,0.0,1.0,0.6666666666666666,0.0,0.0,0.0,<h3>Importing s3 Bucket in Cloudformation template</h3><p>In a Cloudformation template; I define two S3 Buckets.</p><br><pre><code> Bucket1:<br>  Type: AWS::S3::Bucket<br>  Properties:<br>   ...<br> Bucket2:<br>  Type: AWS::S3::Bucket<br>  Properties:<br>   ...<br><br>Outputs:<br> Bucket1:<br>  Description: S3 Bucket<br>  Value: !Ref Bucket1<br>  Export:<br>   Name: !Sub &quot;${AWS::StackName}:Bucket1&quot;<br> Bucket2:<br>  Description: S3 Bucket<br>  Value: !Ref Bucket2<br>  Export:<br>   Name: !Sub &quot;${AWS::StackName}:Bucket2&quot;<br></code></pre><br><p>I use these exported buckets in two different cloudformation templates.</p><br><p><strong>Template 1</strong></p><br><pre><code>Parameters:<br> LoaderCodeBucket:<br>  Type: String<br><br>Resources:<br> MyLambdaFunction:<br>  Type: AWS::Lambda::Function<br>  Properties:<br>   Code:<br>    S3Bucket:<br>     Fn::ImportValue:<br>      !Sub &quot;${LoaderCodeBucket}:Bucket1&quot;<br></code></pre><br><p><strong>Template 2</strong></p><br><pre><code>Parameters:<br> ProcessorCodeBucket:<br>  Type: String<br><br>Resources:<br> MyOtherLambdaFunction:<br>  Type: AWS::Lambda::Function<br>  Properties:<br>   Code:<br>    S3Bucket:<br>     Fn::ImportValue:<br>      !Sub &quot;${ProcessorCodeBucket}:Bucket2&quot;<br></code></pre><br><p>Template 1 passes <code>aws cloudformation validate-template --template-body ...</code> while Template 2 fails due to</p><br><p><strong>Template error: the attribute in Fn::ImportValue must not depend on any resources; imported values; or Fn::GetAZs</strong>.</p><br><p>The only difference is the lambda function in template 2 is used in an aws analytics application that is also defined in template 2.</p><br><p>I know for sure it's the S3 Bucket causing issues because when I remove that section of code; it passes the validation check.</p><br><p>I have been using this site to try to debug this issue; but none of the questions seem to answer this particular issue.</p><br><p>This is in same region/same account.</p><br><p>My question is:<br>Why is this particular section of code (template 2) throwing a template error when template 1 passes with no error?</p><br>
0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>AWS &amp; PHP: https website redirects to http when there is no slash at the end</h3><p>We have a client that hosts their IIS web server on AWS. When navigating to a particular PHP web application on this server; it works when there is a slash on the end; but not when it is absent.</p><br><p>this works:<br><a href="https://example.com.au/application/" rel="nofollow noreferrer">https://example.com.au/application/</a></p><br><p>However; if one were to enter this into the address bar:<br><a href="https://example.com.au/application" rel="nofollow noreferrer">https://example.com.au/application</a></p><br><p>it redirects to the equivalent http address with a slash on the end:<br><a href="http://example.com.au/application/" rel="nofollow noreferrer">http://example.com.au/application/</a></p><br><p>http is disabled via the firewall; so the result is an error.</p><br><p>Here is the request details in Chrome debugger<br><a href="https://i.stack.imgur.com/gT8sV.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/gT8sV.png" alt="enter image description here" /></a></p><br><p>So my question is; what does my client need to check to ensure this redirect does not occur? or that instead of redirecting to HTTP; it redirects to HTTPS?</p><br><p>Additional info:</p><br><ul><br><li>This same issue does not seem to occur with .NET web applications. Eg 'https://example.com.au/dotnetapp' will not redirect to 'http://example.com.au/dotnetapp/'.</li><br><li>There are no rules configured in &quot;URL rewrite&quot;</li><br><li>IIS logs show requests when the HTTPS url is triggered; but not the HTTP one.<br>Edit: This seems to be due to browser caching. After disabling browser caching; i can see the 301 entry in the log files.</li><br><li>'index.php' is set as a default document</li><br></ul><br>
0.0,0.0,0.6666666666666666,0.0,0.3333333333333333,1.0,0.0,<h3>Auth from lambda to call AppSync with apikey</h3><p>I'm trying to create a timeout function that is calling AppSync</p><br><pre class="lang-js prettyprint-override"><code>const timeout = parseInt(process.env.TIMEOUT);<br>const graphqlEndpoint =<br>  'https://xxxxxxxxxxxxxx.appsync-api.us-east-1.amazonaws.com/graphql';<br>require('isomorphic-fetch'); // Required for 'aws-appsync'<br><br>// Initialize GraphQL client<br>//const AWS = require('aws-sdk/global');<br>const AUTH_TYPE = require('aws-appsync').AUTH_TYPE;<br>const AWSAppSyncClient = require('aws-appsync').default;<br>const gql = require('graphql-tag');<br>const config = {<br>  url: graphqlEndpoint;<br>  region: 'us-east-1';<br>  auth: {<br>    type: AUTH_TYPE.API_KEY;<br>    apiKey: 'xxxxxxxxxx';<br>  };<br>  disableOffline: true;<br>};<br>const gqlClient = new AWSAppSyncClient(config);<br>// The mutation query<br>const mutation = gql`<br>  mutation disconnect($id: ID!) {<br>    disconnect(id: $id)<br>  }<br>`;<br><br>exports.handler = async function () {<br>  const timestamp = Date.now() - timeout; // timeout could be 15 seconds<br>  // Retrieve the list of ids and remove them<br>  try {<br>    const ids = [1; 2; 3]; // TODO: get all ids with timestamp bigger than lastConnectedOn<br>    if (!ids.length) return { expired: 0 };<br>    // Create and send all mutations to AppSync<br>    const promises = ids.map(id =&gt;<br>      gqlClient.mutate({ mutation; variables: { id } })<br>    );<br>    await Promise.all(promises);<br>  } catch (error) {<br>    return error;<br>  }<br>};<br><br></code></pre><br><p>The article is here <a href="https://aws.amazon.com/blogs/gametech/building-a-presence-api-using-aws-appsync-aws-lambda-amazon-elasticache-and-amazon-eventbridge/" rel="nofollow noreferrer">https://aws.amazon.com/blogs/gametech/building-a-presence-api-using-aws-appsync-aws-lambda-amazon-elasticache-and-amazon-eventbridge/</a></p><br><p>I changed the auth part because I want to use apiKey for development purposes.<br>But when I run the lambda I get</p><br><pre><code>{<br>  &quot;graphQLErrors&quot;: [];<br>  &quot;networkError&quot;: {<br>    &quot;name&quot;: &quot;ServerError&quot;;<br>    &quot;response&quot;: {<br>      &quot;size&quot;: 0;<br>      &quot;timeout&quot;: 0<br>    };<br>    &quot;statusCode&quot;: 401;<br>    &quot;result&quot;: {<br>      &quot;errors&quot;: [<br>        {<br>          &quot;errorType&quot;: &quot;UnauthorizedException&quot;;<br>          &quot;message&quot;: &quot;Valid authorization header not provided.&quot;<br>        }<br>      ]<br>    }<br>  };<br>  &quot;message&quot;: &quot;Network error: Response not successful: Received status code 401&quot;<br>}<br></code></pre><br><p>I know AppSync needs x-api-key header; but I'm not sure how to add it.</p><br><p>Or am I just plain wrong for trying to use apiKey from the lambda and have to search for another AUTH_TYPE?</p><br><p>Thanks guys</p><br><p>Edit: In this article <a href="https://docs.aws.amazon.com/appsync/latest/devguide/building-a-client-app-node.html#aws-appsync-building-a-client-app-node" rel="nofollow noreferrer">https://docs.aws.amazon.com/appsync/latest/devguide/building-a-client-app-node.html#aws-appsync-building-a-client-app-node</a> there is the option to create an apiKey; but there is no example in how to use it.</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Where do I find the &quot;endpoint&quot; parameter to integrate AWS Secrets?</h3><p>I am pretty new at the AWS SDK world; and my first project is to collect information from secrets using a Spring Application.</p><br><p>I have been using this document <a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/manage-credentials-using-aws-secrets-manager.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/manage-credentials-using-aws-secrets-manager.html</a> all good with the code but something I cannot wrap my head around is the &quot;endpoint&quot;; where do I find this information inside AWS web console? Is it something that companies can personalize?</p><br><p>This would be the first cooperative project... Thanks in advance for the help.</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>What AWS Policies should I select for an admin?</h3><p>Unde IAM; I am setting up new users. For admin; what policies should I select to start with &amp; what permissions should i set as part of best practices?</p><br><p><a href="https://gyazo.com/2d5570ece42a1bb5922e4e3ae282d2f2" rel="nofollow noreferrer">Click to view image</a></p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.0,0.0,<h3>AWS S3 bucket default storage class</h3><p>I've been trying to change default storage class in one of my buckets to glacier for one user using IAM policy but unfortunately it does not work. Can someone tell me what i am doing wrong? Every uploaded file still use Standard storage class.</p><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Sid&quot;: &quot;statement1&quot;;<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Principal&quot;: {<br>                &quot;AWS&quot;: &quot;arn:aws:iam::$MY_ACCOUNT_ID:user/backup&quot;<br>            };<br>            &quot;Action&quot;: &quot;s3:PutObject&quot;;<br>            &quot;Resource&quot;: &quot;arn:aws:s3:::private-backup/*&quot;;<br>            &quot;Condition&quot;: {<br>                &quot;StringEquals&quot;: {<br>                    &quot;s3:x-amz-storage-class&quot;: &quot;GLACIER&quot;<br>                }<br>            }<br>        }<br>    ]<br>}<br></code></pre><br>
0.0,0.0,0.0,0.6666666666666666,0.6666666666666666,0.0,0.3333333333333333,<h3>AWS Lambda - generating PDF issue</h3><p>I'd like to generate a pdf file using AWS Lambda; put it in the S3 bucket and distribute it later to a specific group of recipients. I have managed to generate and upload the pdf to the S3 bucket; however after I manually download the pdf; I cannot open it as the file seems to be corrupted/protected (the size is greater than 0B though; so I expect the information is saved to the file).</p><br><p><strong>Edit: Added full Lambda function</strong></p><br><p>Below the relevant code fragment :</p><br><pre><code>import json<br>import numpy as np<br>import pandas as pd<br>import boto3<br>from matplotlib.backends.backend_pdf import PdfPages<br><br>def lambda_handler(event; context):<br>    <br>    df = pd.DataFrame(np.random.random((10**2;3)); columns = (&quot;Column1&quot;; &quot;Column2&quot;; &quot;Column3&quot;))<br>    <br>    fig; ax = plt.subplots(figsize=(12;4))<br>    ax.axis('tight')<br>    ax.axis('off')<br>    the_table = ax.table(cellText=df.values;colLabels=df.columns;loc='center')<br><br>    <br>    path = &quot;/tmp/doc.pdf&quot;<br>    pp = PdfPages(path)<br>    pp.savefig(fig; bbox_inches='tight')<br>    <br> <br>    s = boto3.client('s3')<br>    s.upload_file(path; 'bucket_name'; 'doc.pdf')<br>    <br><br>    return {<br>        'statusCode': 200;<br>        'body': json.dumps('Hello from Lambda!')<br>    }<br></code></pre><br>
0.0,1.0,0.0,0.6666666666666666,0.3333333333333333,0.0,0.0,<h3>Separate the domain with landing page and web app in AWS managed service</h3><p>I wanna put my landing page in HTML into S3 and running my react web app in Lambda. The configuration that I wish is like the following.</p><br><ol><br><li><a href="http://www.my-own-bm.com" rel="nofollow noreferrer">www.my-own-bm.com</a> =&gt; S3 landing page</li><br><li><a href="http://www.my-own-bm.com/introduction" rel="nofollow noreferrer">www.my-own-bm.com/introduction</a> =&gt; sub landing page in S3. There are several sub landing page.</li><br><li><a href="http://www.my-own-bm.com/signin" rel="nofollow noreferrer">www.my-own-bm.com/signin</a>; signup... =&gt; react app on lambda</li><br></ol><br><p>I suppose Route53 or API Gateway can resolve such an issue. Please share your knowledge or experience if you have met or resolved.</p><br>
0.0,0.0,1.0,0.0,0.3333333333333333,0.0,0.0,<h3>Is the content on disk in cloud (Azure; AWS) zeroized prior to re-releasing to other users?</h3><p>Wanted to know if cloud based platforms such as Azure and Amazon zeroize the content on the hard disk whenever an 'instance' is 'deleted' and prior to making it available for other users?</p><br><p>I've tried using <a href="https://www.toolbox.com/tech/operating-systems/question/use-dd-to-read-data-from-a-raw-disk-device-101809/" rel="nofollow noreferrer">'dd' command</a> on an Amazon-LightSail instance and it appears that the raw data is indeed zeroized. However was not sure if it was by chance (i just tried few random lengths) or if they actually take care to do that.</p><br><p>The concern is; if I leave passwords in configuration files; then someone who comes along would be able to read them (theoretically). Same goes for data in a database.</p><br>
0.0,1.0,0.0,0.0,1.0,0.0,0.0,<h3>Istio; no listener registered when ports are the same</h3><p>I have an EKS cluster with 2 EC2 nodes. I want to use Istio with ALB not the classic ELB; so I modified the gateway from the Istio helm chart to use NodePort like this:</p><br><pre><code>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: istio-ingressgateway<br>  namespace: istio-system<br>  annotations:<br>  labels:<br>    app: istio-ingressgateway<br>    istio: ingressgateway<br>    release: istio<br>    istio.io/rev: default<br>    install.operator.istio.io/owning-resource: unknown<br>    operator.istio.io/component: &quot;IngressGateways&quot;<br>spec:<br>  type: NodePort<br>  selector:<br>    app: istio-ingressgateway<br>    istio: ingressgateway<br>  ports:<br>    -<br>      name: status-port<br>      port: 15021<br>      protocol: TCP<br>      nodePort: 32767<br>    -<br>      name: http2<br>      port: 80<br>      protocol: TCP<br>      nodePort: 31231<br>    -<br>      name: https<br>      port: 443<br>      protocol: TCP<br>      nodePort: 31312<br></code></pre><br><p>Also; I added the Ingress for the gateway:</p><br><pre><code><br>    ---<br>apiVersion: networking.k8s.io/v1<br>kind: IngressClass<br>metadata:<br>  namespace: istio-system<br>  name: aws-load-balancer<br>spec:<br>  controller: ingress.k8s.aws/alb<br>---<br>apiVersion: networking.k8s.io/v1<br>kind: Ingress<br>metadata:<br>  namespace: istio-system<br>  name: ingress<br>  labels:<br>    app: ingress<br>  annotations:<br>    alb.ingress.kubernetes.io/healthcheck-port: &quot;32767&quot;<br>    alb.ingress.kubernetes.io/healthcheck-path: /healthz/ready<br>    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP<br>    alb.ingress.kubernetes.io/subnets: subnet-foo;subnet-bar<br>spec:<br>  ingressClassName: aws-load-balancer<br>  rules:<br>  - http:<br>      paths:<br>      - path: /<br>        pathType: Prefix<br>        backend:<br>          service:<br>            name: istio-ingressgateway<br>            port:<br>              number: 80<br><br></code></pre><br><p>The ALB and the TargetGroup are created as expected; the nodes are healthy according to the TargetGroup health check.</p><br><p>The sample bookinfo <a href="https://github.com/istio/istio/blob/master/samples/bookinfo/platform/kube/bookinfo.yaml" rel="nofollow noreferrer">stack</a> and <a href="https://github.com/istio/istio/blob/master/samples/bookinfo/networking/bookinfo-gateway.yaml" rel="nofollow noreferrer">gateway</a> are installed to a labeled namesapce</p><br><pre><code>% kubectl get ns bookinfo --show-labels                          <br>NAME       STATUS   AGE   LABELS<br>bookinfo   Active   18h   istio-injection=enabled<br><br></code></pre><br><p>Istioctl shows the proxy status</p><br><pre><code>% istioctl proxy-status<br>NAME                                                   CDS        LDS        EDS        RDS          ISTIOD                      VERSION<br>details-v1-79f774bdb9-2scfv.bookinfo                   SYNCED     SYNCED     SYNCED     SYNCED       istiod-75c795985d-pwx9j     1.10.0<br>istio-ingressgateway-8579cc48f8-2d5sd.istio-system     SYNCED     SYNCED     SYNCED     NOT SENT     istiod-75c795985d-pwx9j     1.10.0<br>productpage-v1-6b746f74dc-l795c.bookinfo               SYNCED     SYNCED     SYNCED     SYNCED       istiod-75c795985d-pwx9j     1.10.0<br>ratings-v1-b6994bb9-l2vcp.bookinfo                     SYNCED     SYNCED     SYNCED     SYNCED       istiod-75c795985d-pwx9j     1.10.0<br>reviews-v1-545db77b95-shzkj.bookinfo                   SYNCED     SYNCED     SYNCED     SYNCED       istiod-75c795985d-pwx9j     1.10.0<br>reviews-v2-7bf8c9648f-6k6mk.bookinfo                   SYNCED     SYNCED     SYNCED     SYNCED       istiod-75c795985d-pwx9j     1.10.0<br>reviews-v3-84779c7bbc-6mw5f.bookinfo                   SYNCED     SYNCED     SYNCED     SYNCED       istiod-75c795985d-pwx9j     1.10.0<br></code></pre><br><p>But when I try to reach it it gives back 502.</p><br><pre><code>% curl http://internal-k8s-istiosys-ingress-foo-bar.eu-west-1.elb.amazonaws.com/productpage<br>&lt;html&gt;<br>&lt;head&gt;&lt;title&gt;502 Bad Gateway&lt;/title&gt;&lt;/head&gt;<br>&lt;body&gt;<br>&lt;center&gt;&lt;h1&gt;502 Bad Gateway&lt;/h1&gt;&lt;/center&gt;<br>&lt;/body&gt;<br>&lt;/html&gt;<br></code></pre><br><p>Istio version: 1.10<br>Kubernetes version: 1.19<br>EKS version: eks.5</p><br><p>Edit:</p><br><p>It turned out there are no listeners attached:</p><br><pre><code>% istioctl proxy-config listeners -n istio-system istio-ingressgateway-8579cc48f8-2d5sd.istio-system<br>ADDRESS PORT  MATCH DESTINATION<br>0.0.0.0 15021 ALL   Inline Route: /healthz/ready*<br>0.0.0.0 15090 ALL   Inline Route: /stats/prometheus*<br></code></pre><br><p>However; if I change a port for the Gateway from 80 to 9000; the listeners created but it is need to match with the ingress-gateway port</p><br><pre><code>% istioctl proxy-config listeners -n istio-system istio-ingressgateway-8579cc48f8-qzn59<br>ADDRESS PORT  MATCH DESTINATION<br>0.0.0.0 9000  ALL   Route: http.9000<br>0.0.0.0 15021 ALL   Inline Route: /healthz/ready*<br>0.0.0.0 15090 ALL   Inline Route: /stats/prometheus*<br></code></pre><br>
0.0,0.0,0.6666666666666666,0.0,0.3333333333333333,0.6666666666666666,0.3333333333333333,<h3>AWS project structure using terraform</h3><p>I am working on a Python and AWS course on coursera. However instead of creating s3 ; api gateway and others using boto3 I am using terraform. Till now everything is going fine however I am facing a issue.Below is my lambda directory structure</p><br><p><a href="https://i.stack.imgur.com/agKCp.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/agKCp.png" alt="enter image description here" /></a></p><br><p><a href="https://i.stack.imgur.com/n7xVi.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/n7xVi.png" alt="enter image description here" /></a></p><br><p>Every lambda has a different directory structure and I have to cd in each directory to apply changes using terraform apply.</p><br><p>Example</p><br><p>Below is my lambda code in terraform for one of the lambda function.&lt;&lt;validate.tf&gt;&gt;</p><br><pre><code>provider &quot;aws&quot; {<br>  region = &quot;us-east-2&quot;<br>}<br><br>terraform {<br>  required_version = &quot;&gt; 0.14&quot;<br>  required_providers {<br>    aws = &quot;~&gt; 3.0&quot;<br>  }<br>  backend &quot;s3&quot; {<br>    bucket = &quot;nyeisterraformstatedata2&quot;<br>    key = &quot;api_gateway/lambda_function/terraform_api_gateway_lambda_validate.tfstate&quot;<br>    region = &quot;us-east-2&quot;<br><br>    dynamodb_table = &quot;terraform-up-and-running-locks-2&quot;<br>    encrypt = true<br>  }<br>}<br><br>data &quot;archive_file&quot; &quot;zip_file&quot; {<br>  type        = &quot;zip&quot;<br>  source_dir  = &quot;${path.module}/lambda_dependency_and_function&quot;<br>  output_path = &quot;${path.module}/lambda_dependency_and_function.zip&quot;<br>}<br><br>resource &quot;aws_lambda_function&quot; &quot;get_average_rating_lambda&quot; {<br>  filename      = &quot;lambda_dependency_and_function.zip&quot;<br>  function_name = &quot;validate&quot;<br>  role          = data.aws_iam_role.lambda_role_name.arn<br>  handler       = &quot;validate.lambda_handler&quot;<br><br>  # The filebase64sha256() function is available in Terraform 0.11.12 and later<br>  # For Terraform 0.11.11 and earlier; use the base64sha256() function and the file() function:<br>  # source_code_hash = &quot;${base64sha256(file(&quot;lambda_function_payload.zip&quot;))}&quot;<br>  source_code_hash = filebase64sha256(data.archive_file.zip_file.output_path)<br><br>  runtime = &quot;python3.8&quot;<br><br>  depends_on = [data.archive_file.zip_file]<br>}<br></code></pre><br><p>&lt;&lt;variable.tf&gt;&gt;</p><br><pre><code>data &quot;aws_iam_role&quot; &quot;lambda_role_name&quot; {<br>  name = &quot;common_lambda_role_s3_api_gateway_2&quot;<br>}<br></code></pre><br><p>Based on the comment below I created a main.tf with following code</p><br><pre><code>provider &quot;aws&quot; {<br>  region = &quot;us-east-2&quot;<br>}<br><br><br>module &quot;test&quot; {<br>  source = &quot;../validate&quot;<br><br>}<br></code></pre><br><p>but I am trying to import using import statement its giving me an error and I am not able to figure out how to solve it</p><br><blockquote><br><blockquote><br><p><code>terraform import module.test.aws_lambda_function.test1 get_average_rating_lambda</code></p><br></blockquote><br></blockquote><br><pre><code>Warning: Backend configuration ignored<br> <br>   on ../validate/validate.tf line 10; in terraform:<br>   10:   backend &quot;s3&quot; {<br> <br> Any selected backend applies to the entire configuration; so Terraform expects provider configurations only in the root module.<br> <br> This is a warning rather than an error because it's sometimes convenient to temporarily call a root module as a child module for testing purposes; but this backend configuration block will have no<br> effect.<br><br><br>Error: resource address &quot;module.test.aws_lambda_function.test1&quot; does not exist in the configuration.<br><br>Before importing this resource; please create its configuration in module.test. For example:<br><br>resource &quot;aws_lambda_function&quot; &quot;test1&quot; {<br>  # (resource arguments)<br>}<br></code></pre><br><p>So my question is there a way for terraform to tell which all files have change and apply them in one go rather than one by one.Since I am new to terraform too so if anyone think that this is the wrong way to structing the project please do let me know.Thank you</p><br>
0.0,1.0,0.0,0.0,1.0,0.0,0.0,<h3>Error occured while trying to proxy to: &lt;ip&gt;:3000/api/v1/metadata; react app host in aws</h3><p>I have created react app with nodejs backend server. In my local machine (mac) this is working fine.</p><br><p>But when I host this app in aws ec2 server (ubuntu) ; I'm getting this error when calling the backend apis.</p><br><p><code>Error occured while trying to proxy to: &lt;ip&gt;:3000/api/v1/metadata</code></p><br><p>I am using <code>http-proxy-middleware</code> to proxy requests for backend nodejs server.</p><br><p><code>src/setupProxy.js</code></p><br><pre><code>const createProxyMiddleware = require('http-proxy-middleware')<br><br>module.exports = function (app) {<br>  app.use(<br>    '/api';<br>    createProxyMiddleware({<br>      target: 'http://127.0.0.1:5000';<br>      changeOrigin: true;<br>    })<br>  )<br>}<br></code></pre><br><p><code>inbound rules</code><br><a href="https://i.stack.imgur.com/jMudy.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/jMudy.png" alt="enter image description here" /></a></p><br>
0.0,0.0,0.0,1.0,0.0,0.3333333333333333,0.0,<h3>Can&#39; read images uploaded to AWS S3</h3><p>I'm uploading images to <code>AWS S3</code> as Buffer (as it's required by <code>aws-sdk</code>); but then this image just can't be read as a normal image - it looks in the browser just like a small white square. What I'm missing here?</p><br><p>On Next.js API</p><br><pre><code>const s3 = new AWS.S3({<br>      accessKeyId: &quot;key&quot;;<br>      secretAccessKey: &quot;key&quot;;<br>    });<br>    const params = {<br>      Bucket: &quot;bucket&quot;;<br>      Key: `${Date.now().toString()}.jpg`;<br>      Body: Buffer.from(req.body);<br>      ContentType: &quot;image/png&quot;;<br>    };<br>    s3.upload(params; (err; data) =&gt; {<br>      if (err) {<br>        throw err;<br>      }<br>      console.log(`File uploaded successfully. ${data}`);<br>    });<br></code></pre><br><p>Client side:</p><br><pre><code>const handleImageUpload = (e: React.ChangeEvent&lt;HTMLInputElement&gt;) =&gt; {<br>    const { files } = e.target;<br>    const formData = new FormData();<br>    if (files &amp;&amp; files[0]) {<br>      formData.append(&quot;image&quot;; files[0]);<br>      fetch(&quot;/api/upload.image&quot;; {<br>        method: &quot;POST&quot;;<br>        body: formData;<br>      });<br>    }<br>  };<br></code></pre><br><p>Here is an example of the answer from server:</p><br><pre><code>&lt;ref *2&gt; ManagedUpload {<br>  _events: {};<br>  body: &lt;Buffer 2d 2d 2d 2d 2d 2d 57 65 62 4b 69 74 46 6f 72 6d 42 6f 75 6e 64 61 72 79 51 41 62 63 59 76 32 45 4e 6c 34 77 77 79 70 57 0d 0a 43 6f 6e 74 65 6e 74 2d ... 93798 more bytes&gt;;<br>  sliceFn: [Function: slice];<br>  callback: [Function (anonymous)];<br>  parts: {};<br>  completeInfo: [];<br>  fillQueue: [Function: fillBuffer];<br>  partSize: 5242880;<br>  service: Service {<br>    config: Config {<br>      credentials: [Credentials];<br>      credentialProvider: [CredentialProviderChain];<br>      region: 'us-east-1';<br>      logger: null;<br>      apiVersions: {};<br>      apiVersion: null;<br>      endpoint: 's3.amazonaws.com';<br>      httpOptions: [Object];<br>      maxRetries: undefined;<br>      maxRedirects: 10;<br>      paramValidation: true;<br>      sslEnabled: true;<br>      s3ForcePathStyle: false;<br>      s3BucketEndpoint: false;<br>      s3DisableBodySigning: true;<br>      s3UsEast1RegionalEndpoint: undefined;<br>      s3UseArnRegion: undefined;<br>      computeChecksums: true;<br>      convertResponseTypes: true;<br>      correctClockSkew: false;<br>      customUserAgent: null;<br>      dynamoDbCrc32: true;<br>      systemClockOffset: 0;<br>      signatureVersion: 'v4';<br>      signatureCache: true;<br>      retryDelayOptions: {};<br>      useAccelerateEndpoint: false;<br>      clientSideMonitoring: false;<br>      endpointDiscoveryEnabled: undefined;<br>      endpointCacheSize: 1000;<br>      hostPrefixEnabled: true;<br>      stsRegionalEndpoints: 'legacy';<br>      accessKeyId: 'xxx';<br>      secretAccessKey: 'xxx';<br>      params: [Object]<br>    };<br>    isGlobalEndpoint: false;<br>    endpoint: Endpoint {<br>      protocol: 'https:';<br>      host: 's3.amazonaws.com';<br>      port: 443;<br>      hostname: 's3.amazonaws.com';<br>      pathname: '/';<br>      path: '/';<br>      href: 'https://s3.amazonaws.com/'<br>    };<br>    _events: { apiCallAttempt: [Array]; apiCall: [Array] };<br>    MONITOR_EVENTS_BUBBLE: [Function: EVENTS_BUBBLE];<br>    CALL_EVENTS_BUBBLE: [Function: CALL_EVENTS_BUBBLE];<br>    _clientId: 2<br>  };<br>  totalBytes: 93848;<br>  failed: false;<br>  partPos: 5242880;<br>  isDoneChunking: true;<br>  numParts: 1;<br>  totalPartNumbers: 1;<br>  singlePart: &lt;ref *1&gt; Request {<br>    domain: null;<br>    service: Service {<br>      config: [Config];<br>      isGlobalEndpoint: false;<br>      endpoint: [Endpoint];<br>      _events: [Object];<br>      MONITOR_EVENTS_BUBBLE: [Function: EVENTS_BUBBLE];<br>      CALL_EVENTS_BUBBLE: [Function: CALL_EVENTS_BUBBLE];<br>      _clientId: 2<br>    };<br>    operation: 'putObject';<br>    params: {<br>      Body: &lt;Buffer 2d 2d 2d 2d 2d 2d 57 65 62 4b 69 74 46 6f 72 6d 42 6f 75 6e 64 61 72 79 51 41 62 63 59 76 32 45 4e 6c 34 77 77 79 70 57 0d 0a 43 6f 6e 74 65 6e 74 2d ... 93798 more bytes&gt;;<br>      Bucket: 'codest-images';<br>      Key: '1613572736403.jpg';<br>      ContentType: 'image/png';<br>      ACL: 'public-read'<br>    };<br>    httpRequest: HttpRequest {<br>      method: 'PUT';<br>      path: '/1613572736403.jpg';<br>      headers: [Object];<br>      body: &lt;Buffer 2d 2d 2d 2d 2d 2d 57 65 62 4b 69 74 46 6f 72 6d 42 6f 75 6e 64 61 72 79 51 41 62 63 59 76 32 45 4e 6c 34 77 77 79 70 57 0d 0a 43 6f 6e 74 65 6e 74 2d ... 93798 more bytes&gt;;<br>      endpoint: [Object];<br>      region: 'us-east-1';<br>      _userAgent: 'aws-sdk-nodejs/2.840.0 darwin/v14.15.4 callback';<br>      virtualHostedBucket: 'codest-images';<br>      stream: [ClientRequest]<br>    };<br>    startTime: 2021-02-17T14:38:56.406Z;<br>    response: Response {<br>      request: [Circular *1];<br>      data: null;<br>      error: null;<br>      retryCount: 0;<br>      redirectCount: 0;<br>      httpResponse: [HttpResponse];<br>      maxRetries: 3;<br>      maxRedirects: 10<br>    };<br>    _asm: AcceptorStateMachine { currentState: 'send'; states: [Object] };<br>    _haltHandlersOnError: false;<br>    _events: {<br>      validate: [Array];<br>      afterBuild: [Array];<br>      restart: [Array];<br>      sign: [Array];<br>      validateResponse: [Array];<br>      send: [Array];<br>      httpHeaders: [Array];<br>      httpData: [Array];<br>      httpDone: [Array];<br>      retry: [Array];<br>      afterRetry: [Array];<br>      build: [Array];<br>      extractData: [Array];<br>      extractError: [Array];<br>      httpError: [Array];<br>      beforePresign: [Array];<br>      success: [Array];<br>      complete: [Array];<br>      httpUploadProgress: [Array]<br>    };<br>    emit: [Function: emit];<br>    API_CALL_ATTEMPT: [Function: API_CALL_ATTEMPT];<br>    API_CALL_ATTEMPT_RETRY: [Function: API_CALL_ATTEMPT_RETRY];<br>    API_CALL: [Function: API_CALL];<br>    _managedUpload: [Circular *2];<br>    signedAt: 2021-02-17T14:38:56.415Z<br>  }<br>}<br></code></pre><br>
0.0,0.0,0.0,1.0,0.6666666666666666,0.0,0.0,<h3>XFS grow not working -- disk with no partition</h3><p>So I have an EC2 ubuntu based linux instance running<br>with an ebs volume mounted on mount point <code>/mnt/xxx</code></p><br><p>I want to modify/increase the size of the volume while I am ssh'ed in<br>that instance. I do not want to modify it via 'console' or 'aws cli'.<br>Is there a way to do this?</p><br><p>I have used the <code>lsblk</code> to figure out whether there is a partition. But there is 0 partition of that block device.<br>So the typical description found on aws (<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html</a>) isn't helpful since it assumes that we are dealing with a volume that has a partition; hence the need to use</p><br><blockquote><br><p>sudo growpart xxx 1</p><br></blockquote><br><p>however if there's no partition; then what am i supposed to do?<br>Am I supposed to use <code>growpart</code> still? and use <code>0</code> instead of <code>1</code>. I doubt it. Or am I supposed to skip this step entirely and go directly to <code>xfs_growfs</code></p><br><blockquote><br><p>sudo xfs_growfs -d /mnt/point</p><br></blockquote><br><p>but this doesn't work; as it doesn't increase the size.<br>I've tried. What am I missing here? Please assist.</p><br><p>(Yes the type of filesystem is xfs).</p><br><p>Note this is not a duplicate <a href="https://stackoverflow.com/questions/25262518/xfs-grow-not-working">XFS grow not working</a> since I am dealing with volume that has no partition.</p><br>
1.0,0.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>PySpark read DynamoDB formatted json</h3><p>I'm not a pro with spark; so ask for help.</p><br><p>I made a migration from DynamoDB table into S3 with built in service. It saves files in *<strong>.json</strong> format. Let's say below we have an example of a row (each rows data is a dict nested under key &quot;Item&quot;).</p><br><pre><code>    {<br>    &quot;Item&quot;: {<br>        &quot;accept_languages&quot;:       {<br>            &quot;M&quot;: {<br>                &quot;en&quot;:    {&quot;N&quot;: &quot;0.9&quot;};<br>                &quot;en-US&quot;: {&quot;N&quot;: &quot;1&quot;}<br>            }<br>        };<br>        &quot;accept_mimetypes&quot;:       {<br>            &quot;M&quot;: {<br>                &quot;*/*&quot;:        {&quot;N&quot;: &quot;0.8&quot;};<br>                &quot;image/*&quot;:    {&quot;N&quot;: &quot;1&quot;};<br>                &quot;image/apng&quot;: {&quot;N&quot;: &quot;1&quot;};<br>                &quot;image/webp&quot;: {&quot;N&quot;: &quot;1&quot;}<br>            }<br>        };<br>        &quot;id&quot;:                     {&quot;S&quot;: &quot;5cddbd53b870c2619f1083ed&quot;};<br>        &quot;ip&quot;:                     {&quot;S&quot;: &quot;11.11.111.11&quot;};<br>        &quot;landing_page__type&quot;:     {&quot;S&quot;: &quot;PageMain&quot;};<br>        &quot;location__city&quot;:         {&quot;S&quot;: &quot;Scituate&quot;};<br>        &quot;location__country&quot;:      {&quot;S&quot;: &quot;United States&quot;};<br>        &quot;location__country_code&quot;: {&quot;S&quot;: &quot;US&quot;};<br>        &quot;location__region&quot;:       {&quot;S&quot;: &quot;MA&quot;};<br>        &quot;location__zip&quot;:          {&quot;S&quot;: &quot;02066&quot;};<br>        &quot;origin_url&quot;:             {&quot;S&quot;: &quot;https://www.bing.com/&quot;};<br>        &quot;session&quot;:                {&quot;S&quot;: &quot;b4d58fd18&quot;};<br>        &quot;source&quot;:                 {&quot;S&quot;: &quot;bing&quot;};<br>        &quot;user_agent__browser&quot;:    {&quot;S&quot;: &quot;Chrome&quot;};<br>        &quot;user_device&quot;:            {&quot;S&quot;: &quot;t&quot;}<br>    }<br>}<br></code></pre><br><p>As we see each rows data is nested.<br>I want to create a *<strong>.csv</strong> file as a result from it.<br>Any recommendations how I can parse it?<br>Currently I have a UDF (custom function) to transform a dict itself from DynamoDB to regular view.<br>How can I extract data from each row and apply that function to it; for example.</p><br><p>Thanks</p><br>
0.0,0.6666666666666666,0.0,0.0,0.6666666666666666,0.0,0.3333333333333333,<h3>Wordpress migration results in infinite redirect loop</h3><p>I am trying to migrate a WordPress website from a hosting service with domain &quot;A&quot; to a new AWS EC2 instance with domain &quot;B&quot;.</p><br><p>So I created a new EC2 instance with domain B directing to it; installed php; apache and mysql.<br>I placed the wordpress files in /var/www/html and imported the database to mysql.</p><br><p>This results in: when I browse to domain B I am redirected to domain A.</p><br><p>I notice the table &quot;B5F_options&quot; has the rows with &quot;option_value&quot; = &quot;home&quot; and &quot;option_value&quot; = &quot;siteurl&quot; have the A domain as &quot;option_value&quot;.</p><br><p>So I updated these two rows with the B domain.</p><br><p>However this results in an infinite redirect loop.</p><br><p>So how can I finish the migration successfully?</p><br>
0.0,0.0,1.0,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>No tags being created</h3><p>As I'm new to terraform I'm creating a very simple piece of code to create an EC2 instance with tags on AWS:</p><br><pre><code>provider &quot;aws&quot; {<br>    region = &quot;eu-west-2&quot;<br>}<br><br>resource &quot;aws_instance&quot; &quot;assignment2&quot; {<br>    ami = &quot;ami-06dc09bb8854cbde3&quot;<br>    instance_type = &quot;t2.micro&quot;<br>    tags = {<br>        Name = &quot;test&quot;<br>    }<br>}<br></code></pre><br><p>Its currently creating the EC2 instance but not adding the tags. Can anyone tell me what I'm missing please. Thank you.</p><br><p>EDIT: The current output (that shows no tags are being added by terraform) is:</p><br><p>Terraform will perform the following actions:</p><br><pre><code>  # aws_instance.example will be created<br>  + resource &quot;aws_instance&quot; &quot;example&quot; {<br>      + ami                                  = &quot;ami-06dc09bb8854cbde3&quot;<br>      + arn                                  = (known after apply)<br>      + associate_public_ip_address          = (known after apply)<br>      + availability_zone                    = (known after apply)<br>      + cpu_core_count                       = (known after apply)<br>      + cpu_threads_per_core                 = (known after apply)<br>      + get_password_data                    = false<br>      + host_id                              = (known after apply)<br>      + id                                   = (known after apply)<br>      + instance_initiated_shutdown_behavior = (known after apply)<br>      + instance_state                       = (known after apply)<br>      + instance_type                        = &quot;t2.micro&quot;<br>      + ipv6_address_count                   = (known after apply)<br>      + ipv6_addresses                       = (known after apply)<br>      + key_name                             = (known after apply)<br>      + outpost_arn                          = (known after apply)<br>      + password_data                        = (known after apply)<br>      + placement_group                      = (known after apply)<br>      + primary_network_interface_id         = (known after apply)<br>      + private_dns                          = (known after apply)<br>      + private_ip                           = (known after apply)<br>      + public_dns                           = (known after apply)<br>      + public_ip                            = (known after apply)<br>      + secondary_private_ips                = (known after apply)<br>      + security_groups                      = (known after apply)<br>      + source_dest_check                    = true<br>      + subnet_id                            = (known after apply)<br>      + tags_all                             = (known after apply)<br>      + tenancy                              = (known after apply)<br>      + vpc_security_group_ids               = (known after apply)<br><br>      + ebs_block_device {<br>          + delete_on_termination = (known after apply)<br>          + device_name           = (known after apply)<br>          + encrypted             = (known after apply)<br>          + iops                  = (known after apply)<br>          + kms_key_id            = (known after apply)<br>          + snapshot_id           = (known after apply)<br>          + tags                  = (known after apply)<br>          + throughput            = (known after apply)<br>          + volume_id             = (known after apply)<br>          + volume_size           = (known after apply)<br>          + volume_type           = (known after apply)<br>        }<br><br>      + enclave_options {<br>          + enabled = (known after apply)<br>        }<br><br>      + ephemeral_block_device {<br>          + device_name  = (known after apply)<br>          + no_device    = (known after apply)<br>          + virtual_name = (known after apply)<br>        }<br><br>      + metadata_options {<br>          + http_endpoint               = (known after apply)<br>          + http_put_response_hop_limit = (known after apply)<br>          + http_tokens                 = (known after apply)<br>        }<br><br>      + network_interface {<br>          + delete_on_termination = (known after apply)<br>          + device_index          = (known after apply)<br>          + network_interface_id  = (known after apply)<br>        }<br><br>      + root_block_device {<br>          + delete_on_termination = (known after apply)<br>          + device_name           = (known after apply)<br>          + encrypted             = (known after apply)<br>          + iops                  = (known after apply)<br>          + kms_key_id            = (known after apply)<br>          + tags                  = (known after apply)<br>          + throughput            = (known after apply)<br>          + volume_id             = (known after apply)<br>          + volume_size           = (known after apply)<br>          + volume_type           = (known after apply)<br>        }<br>    }<br></code></pre><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.0,<h3>AWS Lambda returns Unable to import module &#39;main&#39;: No module named &#39;main&#39; when modules are there</h3><p>So I'm trying to set up a function in AWS Lambda to run some python code I imported from a zip.</p><br><p><a href="https://i.stack.imgur.com/eSx0Z.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/eSx0Z.png" alt="file name" /></a></p><br><p>I've edited the handler to run the file then the function I want to run <a href="https://i.stack.imgur.com/Js1WG.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/Js1WG.png" alt="hander" /></a></p><br><p><a href="https://i.stack.imgur.com/lsU1m.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/lsU1m.png" alt="function I want to run" /></a></p><br><p>I've tried having the file in the directory created when I imported the zip folder; after which I I moved it to the main function directory. Neither worked</p><br><p>Not too sure what is wrong here;<br>the full error returned when I run test is:</p><br><pre><code>Response<br>{<br>  &quot;errorMessage&quot;: &quot;Unable to import module 'main': No module named 'main'&quot;;<br>  &quot;errorType&quot;: &quot;Runtime.ImportModuleError&quot;;<br>  &quot;stackTrace&quot;: []<br>}<br></code></pre><br><p>Edit: really new to Lambda so please excuse any silly mistakes</p><br>
0.0,1.0,0.0,0.0,0.3333333333333333,0.0,0.0,<h3>I created 4 EC2 and created 2 target group each having 2 EC2. i created the load balancer and created the rules as per snippet</h3><p>if I am launching the Load balancer <strong>DNS only</strong> then I am getting the output but as per rule if I am adding the text. getting <strong>invalid URL</strong></p><br><p><img src="https://i.stack.imgur.com/7g9as.png" alt="having the text images as per rule" /></p><br><p><img src="https://i.stack.imgur.com/YtVEu.png" alt="DNS of load Balancer" /></p><br><p><img src="https://i.stack.imgur.com/WR4mp.png" alt="created rules of load balancer" /></p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.0,0.0,<h3>Pre signing AWS S3 files</h3><p>I have a bucket that allows for open files. I have uploaded a test file called <code>test.gsm</code> and have tried to presign the file by doing</p><br><pre><code>root@server2:~# aws s3 presign  s3://dovid-ft/test.gsm  --expires-in 604800<br>https://dovid-ft.s3.amazonaws.com/test.gsm?AWSAccessKeyId=AKIAJSDPJKCCGAZ257VQ&amp;Signature=0zbBU2B%2FKVrqgOXFQNTGh3gme%2Fo%3D&amp;Expires=1625658191<br>root@server2:~# <br></code></pre><br><p>If I then try to grab that file I get a 403.</p><br><pre><code>root@server2:~# wget 'https://dovid-ft.s3.amazonaws.com/test.gsm?AWSAccessKeyId=AKIAJSDPJKCCGAZ257VQ&amp;Signature=0zbBU2B%2FKVrqgOXFQNTGh3gme%2Fo%3D&amp;Expires=1625658191'<br>--2021-06-30 07:49:21--  https://dovid-ft.s3.amazonaws.com/test.gsm?AWSAccessKeyId=AKIAJSDPJKCCGAZ257VQ&amp;Signature=0zbBU2B%2FKVrqgOXFQNTGh3gme%2Fo%3D&amp;Expires=1625658191<br>Resolving dovid-ft.s3.amazonaws.com (dovid-ft.s3.amazonaws.com)... 52.217.88.204<br>Connecting to dovid-ft.s3.amazonaws.com (dovid-ft.s3.amazonaws.com)|52.217.88.204|:443... connected.<br>HTTP request sent; awaiting response... 403 Forbidden<br>2021-06-30 07:49:21 ERROR 403: Forbidden.<br><br>root@server2:~# <br></code></pre><br><p>I also tried to decode the HTML of the key to see if that would help and it did not.</p><br><pre><code>root@server2:~# wget 'https://dovid-ft.s3.amazonaws.com/test.gsm?AWSAccessKeyId=AKIAJSDPJKCCGAZ257VQ&amp;Signature=0zbBU2B/KVrqgOXFQNTGh3gme/o=&amp;Expires=1625658191'<br>--2021-06-30 07:49:37--  https://dovid-ft.s3.amazonaws.com/test.gsm?AWSAccessKeyId=AKIAJSDPJKCCGAZ257VQ&amp;Signature=0zbBU2B/KVrqgOXFQNTGh3gme/o=&amp;Expires=1625658191<br>Resolving dovid-ft.s3.amazonaws.com (dovid-ft.s3.amazonaws.com)... 52.217.32.100<br>Connecting to dovid-ft.s3.amazonaws.com (dovid-ft.s3.amazonaws.com)|52.217.32.100|:443... connected.<br>HTTP request sent; awaiting response... 403 Forbidden<br>2021-06-30 07:49:37 ERROR 403: Forbidden.<br><br>root@server2:~#<br></code></pre><br><p>Is there any way to get logs or see what the issue is and why my request is being rejected? As of now the only way to be able to get the file is to make it publicly available which I don't want to do.</p><br>
0.0,0.0,0.3333333333333333,1.0,0.6666666666666666,0.6666666666666666,0.0,<h3>Lambda Function to Stop RDS-Instances in all Regions Based on a Tag Filter Using Boto3</h3><p>I have put together this code to stop rds-instances in all aws-regions. Currently; this code stops only instances in my current default region. Is there something that I am missing here?<br>Tag:<br>The instances are first filtered by a tag whose Key = ttl and Value = some date<br>Filter:<br>The filter returns only instances whose tag value is smaller than today.</p><br><pre><code>import boto3<br>from datetime import datetime<br> <br><br>current_date = datetime.today().strftime('%Y-%m-%d')<br>available_regions = boto3.Session().get_available_regions('rds')<br><br>def lambda_handler(event; context):<br><br>    for region in available_regions:<br>        rds = boto3.client('rds'; region_name=region)<br>        <br>      # get all instances <br>    instances = rds.describe_db_instances()<br><br>    stopInstances = []   <br>    <br>      # Locate all instances that are tagged for stop based on date.<br>    for instance in instances[&quot;DBInstances&quot;]:<br>        # Example RDS Instance tags:<br>        tags = rds.list_tags_for_resource(ResourceName=instance[&quot;DBInstanceArn&quot;])<br>            <br>        for tag in tags[&quot;TagList&quot;]:<br><br>              if tag['Key'] == 'ttl' or tag['Key'] == '' :<br><br>                  if tag['Value'] &lt; current_date:<br><br>                      stopInstances.append(instance[&quot;DBInstanceIdentifier&quot;])<br>                      rds.stop_db_instance(DBInstanceIdentifier=instance[&quot;DBInstanceIdentifier&quot;])    <br>                    <br>                      pass<br><br>                      pass<br><br><br>      # print  if instances will stop. <br>    if len(stopInstances) &gt; 0:<br>       print (&quot;stopInstances&quot;)<br>    else:<br>       print (&quot;No rds instances to shutdown.&quot;)<br></code></pre><br>
0.0,1.0,0.0,0.0,0.0,0.0,0.0,<h3>AWS ELB - How to add wildcard for path param?</h3><p>I want to add a path-pattern in a listenerRule that looks something like this:</p><br><pre><code>/x/*/y<br></code></pre><br><p>The trouble is that the wildcard also matches forward slash; so it matches much more than I want.<br>I've also tried to use ? as a workaround; but there is an undocumented limit of 5 wildcards; so that won't work either. Are there any other workarounds?</p><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.3333333333333333,<h3>The stream or file &quot;/var/app/current/storage/logs/laravel.log&quot;</h3><p>I have a Laravel project deployed on <code>AWS EB</code> throw <code>Github</code> to <code>AWS Pipeline</code> all things are fine; but when I visit my domain I got this error:</p><br><pre><code>UnexpectedValueException<br>The stream or file &quot;/var/app/current/storage/logs/laravel.log&quot; could not be opened in<br>append mode: failed to open stream: Permission denied<br></code></pre><br><p>After some search I got this one:</p><br><p>In the main project dir; I created <code>.ebextensions</code> folder contains file called <code>set.config</code> that contain:</p><br><pre><code>container_commands:<br> 00_run_bootstrap_command:<br>  command: &quot;chmod -R 775 storage&quot;<br>  cwd: &quot;/var/app/staging&quot;<br><br> 01_run_bootstrap_command:<br>  command: &quot;chmod -R 775 bootstrap/cache&quot;<br>  cwd: &quot;/var/app/staging&quot;<br><br>02_run_config_cache_command:<br>  command: &quot;php artisan config:cache&quot;<br>  cwd: &quot;/var/app/staging&quot;<br><br>03_run_config_clear_command:<br>  command: &quot;php artisan config:clear&quot;<br>  cwd: &quot;/var/app/staging&quot;<br><br>04_run_optimize_command:<br>  command: &quot;php artisan optimize:clear&quot;<br>  cwd: &quot;/var/app/staging&quot;<br></code></pre><br><p>When I re-deploy this file still the same error!</p><br><p>Also; I used <code>chmod -R 777 storage</code> still same error</p><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.0,<h3>Configure elastic beanstalk to install Development Tools</h3><p>I have a nodejs/expressjs application that contains some platform dependent modules like <code>sharp</code> which uses <code>node-gyp</code> to compile for linux platform when i deploy to elastic beanstalk the deployment fails because <code>Development tools (c++;gcc;make)</code> is not installed on <code>ec2</code> instances i have tried various solutions started by<br>adding a <code>.ebextensions/01_install_required_packages.config</code> with the following content</p><br><pre><code>commands:<br>    00_install_packages:<br>        command: &quot;yum groupinstall 'Development Tools'&quot;<br></code></pre><br><p><code>node-gyp</code> command fails and i have read in some resources that <code>Procfile</code> is the way i added <code>packages</code> key as recommended by AWS documentation so my <code>Procfile</code> content is as follows</p><br><pre><code>web: ./node_modules/pm2/bin/pm2-runtime ./bin/server.js -i max<br>packages: yum 'Development Tools'<br></code></pre><br><p>the <code>Procfile</code> solution sometimes work and sometimes give the <code>node-gyp</code> error which fails to compile the module</p><br><h3>what is happening</h3><br><p>i think sometimes when using the <code>Procfile</code> it does actually install the Development tools but when i redeploy sometimes Elastic beanstalk selectes different <code>ec2</code> instance to do the deployment test which does not contain <code>Development tools</code></p><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.6666666666666666,0.0,<h3>Running puppeteer with Firefox on AWS Lambda</h3><p>I'm trying to run puppeteer with Firefox on AWS lambda for printing PDFs.</p><br><p>My AWS Lambda Layer: nodejs-&gt;node_modules</p><br><p>node_modules contains Firefox binaries; as it works in my local dev environment.</p><br><p>Lambda Function:</p><br><pre><code><br>exports.handler = async (event) =&gt; {<br>    const browser = await puppeteer.launch({<br>    product: 'firefox';<br>    headless: true<br>  });<br>  <br>  const page = await browser.newPage();<br>  await page.goto(&quot;http://google.com/&quot;);<br>  console.log(&quot;loaded&quot;);<br>  <br>    const response = {<br>        statusCode: 200;<br>        body: JSON.stringify('Hello from Lambda!');<br>    };<br>    return response;<br>};<br></code></pre><br><p>The error that I get</p><br><pre><code>{<br>  &quot;errorType&quot;: &quot;Error&quot;;<br>  &quot;errorMessage&quot;: &quot;Could not find expected browser (firefox) locally. Run `PUPPETEER_PRODUCT=firefox npm install` to download a supported Firefox browser binary.&quot;;<br>  &quot;trace&quot;: [<br>    &quot;Error: Could not find expected browser (firefox) locally. Run `PUPPETEER_PRODUCT=firefox npm install` to download a supported Firefox browser binary.&quot;;<br>    &quot;    at FirefoxLauncher.launch (/opt/nodejs/node_modules/puppeteer/lib/cjs/puppeteer/node/Launcher.js:194:23)&quot;;<br>    &quot;    at async Runtime.exports.handler (/var/task/index.js:8:21)&quot;<br>  ]<br>}<br></code></pre><br><p>Is there a way to fix it? Firefox is super important for me since it supports OpenType SVG fonts.</p><br>
0.0,0.0,0.0,0.0,0.3333333333333333,0.3333333333333333,1.0,<h3>PushNNotification is not receiving from AWS Pinpoint</h3><pre><code>const AWS = require('aws-sdk');<br></code></pre><br><p>const region = 'us-east-1';</p><br><p>var title = 'Test message sent from Amazon Pinpoint.';</p><br><p>var message = 'This is a sample message sent from Amazon Pinpoint by using the '</p><br><p>var applicationId = 'ce796be37f32f178af652b26eexample';</p><br><p>var recipient = {<br>'token': 'a0b1c2d3e4f5g6h7i8j9k0l1m2n3o4p5q6r7s8t9u0v1w2x3y4z5a6b7c8d8e9f0';<br>'service': 'GCM'<br>};</p><br><p>var action = 'URL';</p><br><p>var priority = 'normal';</p><br><p>var silent = false;</p><br><p>function CreateMessageRequest() {<br>var token = recipient['token'];<br>var service = recipient['service'];<br>if (service == 'GCM') {<br>var messageRequest = {<br>'Addresses': {<br>[token]: {<br>'ChannelType' : 'GCM'<br>}<br>};<br>'MessageConfiguration': {<br>'GCMMessage': {<br>'Action': action;<br>'Body': message;<br>'Priority': priority;<br>'SilentPush': silent;<br>'Title': title;<br>'TimeToLive': ttl;<br>'Url': url<br>}<br>}<br>};<br>} else if (service == 'APNS') {<br>var messageRequest = {<br>'Addresses': {<br>[token]: {<br>'ChannelType' : 'APNS'<br>}<br>};<br>'MessageConfiguration': {<br>'APNSMessage': {<br>'Action': action;<br>'Body': message;<br>'Priority': priority;<br>'SilentPush': silent;<br>'Title': title;<br>'TimeToLive': ttl;<br>'Url': url<br>}<br>}<br>};<br>} else if (service == 'BAIDU') {<br>var messageRequest = {<br>'Addresses': {<br>[token]: {<br>'ChannelType' : 'BAIDU'<br>}<br>};<br>'MessageConfiguration': {<br>'BaiduMessage': {<br>'Action': action;<br>'Body': message;<br>'SilentPush': silent;<br>'Title': title;<br>'TimeToLive': ttl;<br>'Url': url<br>}<br>}<br>};<br>} else if (service == 'ADM') {<br>var messageRequest = {<br>'Addresses': {<br>[token]: {<br>'ChannelType' : 'ADM'<br>}<br>};<br>'MessageConfiguration': {<br>'ADMMessage': {<br>'Action': action;<br>'Body': message;<br>'SilentPush': silent;<br>'Title': title;<br>'Url': url<br>}<br>}<br>};<br>}</p><br><p>return messageRequest<br>}</p><br><p>function ShowOutput(data){<br>if (data[&quot;MessageResponse&quot;][&quot;Result&quot;][recipient[&quot;token&quot;]][&quot;DeliveryStatus&quot;]<br>== &quot;SUCCESSFUL&quot;) {<br>var status = &quot;Message sent! Response information: &quot;;<br>} else {<br>var status = &quot;The message wasn't sent. Response information: &quot;;<br>}<br>console.log(status);<br>console.dir(data; { depth: null });<br>}</p><br><p>function SendMessage() {<br>var token = recipient['token'];<br>var service = recipient['service'];<br>var messageRequest = CreateMessageRequest();</p><br><p>var credentials = new AWS.SharedIniFileCredentials({ profile: 'default' });<br>AWS.config.credentials = credentials;</p><br><p>AWS.config.update({ region: region });</p><br><p>var pinpoint = new AWS.Pinpoint();<br>var params = {<br>&quot;ApplicationId&quot;: applicationId;<br>&quot;MessageRequest&quot;: messageRequest<br>};</p><br><p>pinpoint.sendMessages(params; function(err; data) {<br>if (err) console.log(err);<br>else     ShowOutput(data);<br>});<br>}</p><br><p>SendMessage()</p><br><p>we are trying to send push notification from lambda using pinpoint(FCM)<br>but in the above code pinpoint.sendmessage function is not being invoked.</p><br>
0.0,1.0,0.3333333333333333,0.3333333333333333,0.0,0.0,0.0,<h3>How can I activate a automatic IP change in AWS RDS security classes</h3><p>I am using RDS with postgresql and to access it I need my inbound data rules to be set to my IP. But everyday I have to go back into AWS to change it back manually. Is there a way to do it automatically?</p><br>
0.0,0.0,0.0,0.6666666666666666,0.6666666666666666,0.3333333333333333,0.3333333333333333,<h3>gz to zip conversion with S3 and Python</h3><p>I am having a use case where very large .gz files are generated by a snowflake and stored in S3.</p><br><p>Now I want to convert each file separately into zip files using Python; AWS Lamda; and boto3.</p><br><p>Getting exception with below code. can someone help me with it</p><br><pre><code>    def convert_gz_s3_files(prefix):<br>    s3 = boto3.session.Session().client('s3')<br>    gzFiles = getGZFiles(s3; prefix)<br>    try:<br>        for gzipFile in gzFiles:<br>            obj = s3.get_object(Bucket=bucket; Key=gzipFile[0])<br>            with gzip.GzipFile(fileobj=obj['Body']) as gz:<br>                data = gz.read().decode()<br>                archive = zipfile.ZipFile(data; mode=&quot;r&quot;)<br><br>                # with zipfile.ZipFile(data; mode='w'; allowZip64 = True) as zip:<br>                #     zipData = zip.read().decode()<br>                    # zipObject = s3.Object(bucket; prefix+'file_name.csv.zip')<br>                    # zipRes = zipObject.put(body=zipData)<br>                    # print('zip result =&gt; {}'; zipRes)<br><br>            print(gzipFile)<br>    except Exception as e:<br>        logger.error({'error': str(e); 'message': 'failed gzip decoding'})<br>        raise e<br><br>    print(gzFiles)<br></code></pre><br>
0.0,0.6666666666666666,0.3333333333333333,0.0,0.0,1.0,0.0,<h3>AWS CodeBuild without Internet?</h3><p>We use a hybrid cloud setup where we connect to AWS services via VPC endpoints; but security forbids connections to the internet; which means no internet or NAT gateways. The documentation of AWS codebuild states that if it is bound to a VPC; it needs a NAT gateway for internet access in order to connect to other AWS services.</p><br><p>Does this mean we cannot use AWS CodeBuild? Or is there a way to configure it to connect via VPC endpoints?</p><br>
0.0,0.3333333333333333,0.0,0.0,1.0,0.3333333333333333,0.6666666666666666,<h3>Nestjs on AWS Lambda (Serverless Framework) | How to access the event parameter?</h3><p>I'm hosting a Nestjs application on AWS Lambda (using the Serverless Framework).<br>Please note that the implementation is behind AWS API Gateway.</p><br><p><strong>Question:</strong> How can I access to <code>event</code> parameter in my Nest <code>controller</code>?</p><br><p>This is how I bootstrap the NestJS server:</p><br><pre><code>import { APIGatewayProxyHandler } from 'aws-lambda';<br>import { NestFactory } from '@nestjs/core';<br>import { AppModule } from './app.module';<br>import { Server } from 'http';<br>import { ExpressAdapter } from '@nestjs/platform-express';<br>import * as awsServerlessExpress from 'aws-serverless-express';<br>import * as express from 'express';<br><br>let cachedServer: Server;<br><br>const bootstrapServer = async (): Promise&lt;Server&gt; =&gt; {<br>    const expressApp = express();<br>    const adapter = new ExpressAdapter(expressApp);<br>    const app = await NestFactory.create(AppModule; adapter);<br>    app.enableCors();<br>    await app.init();<br>    return awsServerlessExpress.createServer(expressApp);<br>}<br><br>export const handler: APIGatewayProxyHandler = async (event; context) =&gt; {<br>    if (!cachedServer) {<br>        cachedServer = await bootstrapServer()<br>    }<br>    return awsServerlessExpress.proxy(cachedServer; event; context; 'PROMISE')<br>        .promise;<br>};<br></code></pre><br><p>Here is a function in one controller:</p><br><pre><code>@Get()<br>getUsers(event) { // &lt;-- HOW TO ACCESS event HERE?? This event is undefined.<br>    return {<br>        statusCode: 200;<br>        body: &quot;This function works and returns this JSON as expected.&quot;<br>    }<br></code></pre><br><p>I'm struggling to understand how I can access the <code>event</code> paramenter; which is easily accessible in a &quot;normal&quot; node 12.x Lambda function:</p><br><pre><code>module.exports.hello = async (event) =&gt; {<br>    return {<br>        statusCode: 200;<br>        body: 'In a normal Lambda; the event is easily accessible; but in NestJS its (apparently) not.'<br>    };<br>}; <br></code></pre><br>
0.0,0.0,0.3333333333333333,0.0,0.3333333333333333,1.0,0.0,<h3>AWS CodeBuild Unzipped size must be smaller than 350198 bytes</h3><p>I am trying to deploy and update code in multiple lambdas at the same time; but when making a push to my branch and deploying CodeBuild; I getting the following error:</p><br><blockquote><br><p>An error occurred (InvalidParameterValueException) when calling the<br>UpdateFunctionCode operation: Unzipped size must be smaller than<br>350198 bytes</p><br><p>[Container] 2021/04/24 00:09:31 Command did not exit successfully aws<br>lambda update-function-code --function-name my_lambda_03 --zip-file<br>fileb://my_lambda_03.zip exit status 254 [Container] 2021/04/24<br>00:09:31 Phase complete: POST_BUILD State: FAILED [Container]<br>2021/04/24 00:09:31 Phase context status code: COMMAND_EXECUTION_ERROR<br>Message: Error while executing command: aws lambda<br>update-function-code --function-name my_lambda_03 --zip-file<br>fileb://my_lambda_03.zip. Reason: exit status 254</p><br></blockquote><br><p>This is the <strong>buildspec.yml</strong>:</p><br><pre><code>version: 0.2<br>phases:<br>  install:<br>    runtime-versions:<br>      python: 3.x<br>    commands:<br>      - echo &quot;Installing dependencies...&quot;<br>  build:<br>    commands:<br>      - echo &quot;Zipping all my functions.....&quot;<br>      - cd my_lambda_01/<br>      - zip -r9 ../my_lambda_01.zip .<br>      - cd ..<br>      - cd my_lambda_02/<br>      - zip -r9 ../my_lambda_02.zip .<br>      - cd ..<br>      - cd my_lambda_03/<br>      - zip -r9 ../my_lambda_03.zip .<br>      ...<br>      - cd my_lambda_09/<br>      - zip -r9 ../my_lambda_09.zip .<br>      - cd ..      <br><br>  post_build:<br>    commands:<br>      - echo &quot;Updating all lambda functions&quot;<br>      - aws lambda update-function-code --function-name my_lambda_01 --zip-file fileb://my_lambda_01.zip<br>      - aws lambda update-function-code --function-name my_lambda_02 --zip-file fileb://my_lambda_02.zip<br>      - aws lambda update-function-code --function-name my_lambda_03 --zip-file fileb://my_lambda_03.zip<br>      ...<br>      - aws lambda update-function-code --function-name my_lambda_09 --zip-file fileb://my_lambda_09.zip<br>      - echo &quot;Done&quot;<br></code></pre><br><p>Thanks for any help.</p><br>
0.0,0.0,0.3333333333333333,0.0,0.0,0.3333333333333333,1.0,<h3>How to set AWS SES identity in Laravel 8</h3><p>I am getting the error message <code>User arn:aws:iam:blabla:user/blabla is not authorized to preform ses:SendRawEmail on resource arn:aws:ses:locationLblabla:identity/[domain-of-the-to-address].com</code> my DevOps informs me that <code>[domain-of-the-to-address]</code> should me a specific/our domain. The only way I can find to manipulate that is changing the to address. How do i properly set that? I have tried that following</p><br><pre class="lang-php prettyprint-override"><code>// services.php<br><br>return [<br>  'ses' =&gt; [<br>    'key' =&gt; '...';<br>    'secret' =&gt; '...';<br>    'region' =&gt; '...';<br>    'options' =&gt; [<br>      'SourceARN' =&gt; 'arn:aws:ses:locationLblabla:identity/[correct-domain].com'<br>      'Source' =&gt; 'test@[correct-domain].com'<br>    ];<br>  ]<br>];<br></code></pre><br><p>and</p><br><pre class="lang-php prettyprint-override"><code>// MyMailable.php<br><br>$this-&gt;withSwiftMessage(function (Swift_Message $message) {<br>  $message-&gt;getHeaders()-&gt;addTextHeader('X-SES-SOURCE-ARN'; 'arn:aws:ses:locationLblabla:identity/[correct-domain].com');<br>});<br></code></pre><br><p>Edit: In other words. I DO have the <code>ses:SendRawEmail</code> permission. I'm sending an email like:<br>to: joe-bob@gmail.com<br>from: do-not-reply@my-comp.com</p><br><p>but some how laravel driver or aws sdk is getting it backwards setting my identity as <code>arn:aws:ses:locationLblabla:identity/gmail.com</code> not <code>arn:aws:ses:locationLblabla:identity/my-comp.com</code> and we don't and wouldn't have an identity setup in out aws for ever single email domain with the <code>ses:SendRawEmail</code> so it won't work.</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>Customer Error: imread read blank (None) image for file- Sagemaker AWS</h3><p>I am following this <a href="https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_mscoco_multi_label/Image-classification-multilabel-lst.ipynb" rel="nofollow noreferrer">tutorial</a> with my custom data and my custom S3 buckets where train and validation data are. I am getting the following error:</p><br><pre><code>Customer Error: imread read blank (None) image for file: /opt/ml/input/data/train/s3://image-classification/image_classification_model_data/train/img-001.png<br></code></pre><br><p>I have all my training data are in one folder named '<code>train</code>' I have set up my <code>lst</code> file like this suggested by <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html" rel="nofollow noreferrer">doc</a>;</p><br><pre><code>22  1   s3://image-classification/image_classification_model_data/train/img-001.png<br>86  0   s3://image-classification/image_classification_model_data/train/img-002.png<br>...<br></code></pre><br><p>My other configurations:</p><br><pre><code>s3_bucket = 'image-classification'<br>prefix =  'image_classification_model_data'<br><br><br>s3train = 's3://{}/{}/train/'.format(s3_bucket; prefix)<br>s3validation = 's3://{}/{}/validation/'.format(s3_bucket; prefix)<br><br>s3train_lst = 's3://{}/{}/train_lst/'.format(s3_bucket; prefix)<br>s3validation_lst = 's3://{}/{}/validation_lst/'.format(s3_bucket; prefix)<br><br><br><br>train_data = sagemaker.inputs.TrainingInput(s3train; distribution='FullyReplicated'; <br>                        content_type='application/x-image'; s3_data_type='S3Prefix')<br><br>validation_data = sagemaker.inputs.TrainingInput(s3validation; distribution='FullyReplicated'; <br>                             content_type='application/x-image'; s3_data_type='S3Prefix')<br><br>train_data_lst = sagemaker.inputs.TrainingInput(s3train_lst; distribution='FullyReplicated'; <br>                        content_type='application/x-image'; s3_data_type='S3Prefix')<br><br>validation_data_lst = sagemaker.inputs.TrainingInput(s3validation_lst; distribution='FullyReplicated'; <br>                             content_type='application/x-image'; s3_data_type='S3Prefix')<br><br><br>data_channels = {'train': train_data; 'validation': validation_data; 'train_lst': train_data_lst; <br>                 'validation_lst': validation_data_lst}<br></code></pre><br><p>I checked the images downloaded and checked physically; I see the image. Now sure what this error gets thrown out as <code>blank</code>. Any suggestion would be great.</p><br>
0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,0.0,0.3333333333333333,<h3>Is it acceptable to store profile pictures publicly?</h3><p>Straight to the point; I store user's profile pictures in an AWS S3 bucket; using the following format: <code>&lt;company-uuid&gt;&lt;user-uuid&gt;.jpg.</code> This naming convention results in generated URLs that are almost impossible to guess by outsiders.</p><br><p>Example: <code>https://---.---.com/8794ee24-24ae-49f1-9cff-22d23b0ebef7957a74be-f1ac-493b-b866-b90311bf63a2.png</code></p><br><p>Is it acceptable to store these profile pictures (using the naming convention listed above) publicly; or should some type of authentication middleware be used nonetheless?</p><br>
0.0,0.0,1.0,1.0,0.0,0.0,0.0,<h3>Iam permission vs resource permissions</h3><p>In aws an IAm user can be given say read access to an s3 bucket using permissions.  Similarly a policy (permission) can be attached to an s3 bucket to allow certain user access to that s3 bucket.  My question is why there are two ways to do it.  Should you define both?  What if user 1 is allowed to access an s3 folder but IAM policy at resource level allows user 2 access to it.  Who wins in this situation?  What is the order of evaluation?</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>transformation_ctx value is not stored for the incremental purpose in the glue job temp dir</h3><p>I am trying to load <code>incremental</code> data from <code>Redshift</code> to the <code>s3</code>. I have set up <code>redshift_temp_dir</code> and <code>temp dir</code> for <code>glue</code> jobs(using glue console).</p><br><p>Below is my code:</p><br><pre><code>my_conn_options = {<br>            &quot;url&quot;: &quot;&quot;;<br>            &quot;dbtable&quot;: &quot;&quot;;<br>            &quot;user&quot;: &quot;&quot;;<br>            &quot;password&quot;: &quot;&quot;;<br>            &quot;redshiftTmpDir&quot;: &quot;s3://madl-temp/redshift_temp/&quot;<br>        }<br>        <br>data = glueContext.create_dynamic_frame_from_options(connection_type=&quot;redshift&quot;;<br>                                                         connection_options=my_conn_options;<br>                                                         transformation_ctx=table_name;<br>                                                         additional_options={<br>                                                             &quot;jobBookmarkKeys&quot;: [&quot;timestamp&quot;];<br>                                                             &quot;jobBookmarkKeysSortOrder&quot;: &quot;asc&quot;}).toDF()<br><br>log.info(str(data.count()))<br></code></pre><br><p>Where <code>date_col</code> is a timestamp column in redshift. I have also used <code>job.init(</code>) and <code>job.commit()</code> in my code and after every run I am getting the complete data load count and not the newly added rows in my redshift table.<br>As per the documentation below I can use Redshift as my JDBC source for the incremental purpose too.</p><br><p><a href="https://i.stack.imgur.com/HUqUP.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/HUqUP.png" alt="enter image description here" /></a></p><br><p>I am not able to see any <code>metadata</code> for <code>transformation_ctx</code> in the glue temp directory. So; what could be the reason for my problem? Or am I missing something?</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>how to apply Deny policy on &quot;tag:UntagResources&quot; Action in AWS</h3><p>I have this policy which should prevent users to remove tagging from any recourses in AWS. but tags are still being removed from resources.</p><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Action&quot;: [<br>                &quot;ec2:Delete*&quot;;<br>                &quot;s3:Delete*&quot;;<br>                &quot;s3:ReplicateTags&quot;;<br>                &quot;iam:Untag*&quot;;<br>                &quot;tag:UntagResources&quot;<br>            ];<br>            &quot;Effect&quot;: &quot;Deny&quot;;<br>            &quot;Resource&quot;: &quot;*&quot;<br>        };<br>        {<br>            &quot;Action&quot;: [<br>                &quot;s3:Create*&quot;;<br>                &quot;s3:Describe*&quot;;<br>                &quot;s3:Get*&quot;;<br>                &quot;s3:List*&quot;;<br>                &quot;s3:Put*&quot;;<br>                &quot;s3:Update*&quot;;<br>                &quot;s3:Replicate*&quot;;<br>                &quot;s3:RestoreObject&quot;;<br>                &quot;s3:ObjectOwnerOverrideToBucketOwner&quot;<br>            ];<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Resource&quot;: &quot;*&quot;<br>        };<br>        {<br>            &quot;Action&quot;: [<br>                &quot;ec2:Create*&quot;;<br>                &quot;ec2:Describe*&quot;;<br>                &quot;ec2:Get*&quot;;<br>                &quot;ec2:Modify*&quot;;<br>                &quot;ec2:StartInstances&quot;;<br>                &quot;ec2:StopInstances&quot;<br>            ];<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Resource&quot;: &quot;*&quot;<br>        };<br>        {<br>            &quot;Action&quot;: [<br>                &quot;iam:Tag*&quot;;<br>                &quot;tag:TagResources&quot;;<br>                &quot;tag:GetResources&quot;<br>            ];<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Resource&quot;: &quot;*&quot;<br>        };<br>        {<br>            &quot;Action&quot;: [<br>                &quot;iam:Untag*&quot;;<br>                &quot;tag:UntagResources&quot;<br>            ];<br>            &quot;Effect&quot;: &quot;Deny&quot;;<br>            &quot;Resource&quot;: &quot;*&quot;<br>        }<br>    ]<br>}<br></code></pre><br><p>As I'm new to AWS; I have no Idea what's gone Wrong. other permissions works fine. just un-tagging isn't working. how to Deny for un-tagging recourses? thanks in advance.</p><br><p>How do I make <code>tag:UntagResources</code> work?</p><br>
0.0,1.0,0.6666666666666666,0.0,0.6666666666666666,0.0,0.0,<h3>SSL certificate not working in laravel project aws</h3><p>I am trying to implement SSL certificate on my EC2 instance which is running a laravel project. I have issued the certificate and it is also in use but when I try https://domainName my browser shows</p><br><blockquote><br><p>Unable to connect</p><br></blockquote><br><p>I have used:</p><br><ol><br><li>EC2</li><br><li>Route53</li><br><li>Certificate Manager</li><br><li>Load Balancer</li><br><li>Elastic Beanstalk</li><br></ol><br><p><a href="https://infra.engineer/aws/36-aws-ssl-offloading-with-an-application-load-balancer" rel="nofollow noreferrer">This</a> is exactly how I configured my Load Balancer; Then added my DNS Name to Route53.</p><br><p>I didn't know what details should I provide so please do ask for the information.</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Fetching login times for cognito user</h3><p>I'm fairly new to AWS and was wondering if there is a place in AWS that stores cognito users login times. I want to be able to present a table consisting of the previous succesful login attempts of a specific user. I know I could do that from the client side by storing the time and date in a db everytime they user logs in; but I was just wondering if there was an API I could call that already stores that data for me.</p><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.0,<h3>How to Install CrystalReport for Visual Studo in AWS Elastic BeanStalk?</h3><p>I already deploy my project to AWS Elastic Beanstalk (.Net); and it required CrystalReport to run my web; how to install my CrystalReport.msi to my environment of AWS Elastic Beanstalk?</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>AWS SageMaker Notebook Extension Installation</h3><p>I am Using SageMaker Notebook for some deep learning tasks. However; raw SageMaker does not provide much of nbextension options such as auto complete for Hinter; not even provide a <code>configurator</code> as traditional Notebook. So; I installed <code>jupyter_contrib_nbextensions</code> and try to enable <code>Hinter</code> by <code>jupyter nbextension enable Hinter/main</code>.<br>However; the functionality still does not show up.<br>When I run <code>Jupyter nbextension list</code>; it shows like:<br><a href="https://i.stack.imgur.com/WzwSA.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/WzwSA.png" alt="enter image description here" /></a><br>Can someone tell me how to successfully add and use nbextension on SageMaker Notebook?<br>Thanks in advance</p><br>
0.3333333333333333,0.3333333333333333,0.0,0.6666666666666666,0.6666666666666666,0.3333333333333333,0.0,<h3>How can I use AWS Lambda to read a csv in S3 and output the record count to HTML?</h3><p>I'm very new to AWS and this question could very well be impossible in AWS. I'm looking to create a static webpage with S3 that will show how many records are in a csv file. The csv is some fake data about employee attrition; so every row corresponds to an employee. The csv is already in an S3 bucket; and I have already set up the bucket to show a simple static webpage (see below).</p><br><p>Is it possible to use Lambda or anything else to display this record count on the webpage? Seems simple enough; yet I am struggling.</p><br><p>Below is my current webpage; and ideally I would like the record count to be where the big red &quot;X&quot; is.</p><br><p><a href="https://i.stack.imgur.com/jiAj7.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/jiAj7.png" alt="Ideal Output" /></a></p><br><p>Here is my lambda function so far:</p><br><pre class="lang-py prettyprint-override"><code>import json<br>import os<br>import boto3<br>import csv<br><br>def lambda_handler(event;  context):<br>    for record in event['Records']:<br>        bucket = record['s3']['bucket']['fakeemployee']<br>        file_key = record['s3']['object']['Employee Attrition Data.xlsx']<br>        s3 = boto3.client('s3')<br>        csvfile = s3.get_object(Bucket=bucket; Key=file_key)<br></code></pre><br><p>Here is my test configuration in AWS so far:</p><br><pre class="lang-py prettyprint-override"><code>{<br>  &quot;Records&quot;: [<br>    {<br>      &quot;s3&quot;: {<br>        &quot;bucket&quot;: {<br>          &quot;name&quot;: &quot;fakeemployee&quot;;<br>          &quot;arn&quot;: &quot;arn:aws:s3:::fakeemployee&quot;<br>        };<br>        &quot;object&quot;: {<br>          &quot;key&quot;: &quot;Employee Attrition Data.xlsx&quot;;<br>          &quot;size&quot;: 242.1;<br>          &quot;eTag&quot;: &quot;3df14b4d8bda007b946b9f176b89c9b5&quot;;<br>          &quot;sequencer&quot;: &quot;0A1B2C3D4E5F678901&quot;<br>        }<br>      }<br>    }<br>  ]<br>}<br></code></pre><br><p>Any advice?</p><br><p>EDIT: I have tried to create a API HTTP Gateway but have run into a &quot;message not found&quot; error when invoking. I used <a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-develop.html#apigateway-http-api-create.console" rel="nofollow noreferrer">this</a> tutorial to create the API gateway.</p><br>
0.0,0.0,0.0,0.3333333333333333,0.0,1.0,0.3333333333333333,<h3>Using spyOn with Jest to mock an AWS SNS publish call</h3><p>I'm trying to use <code>spyOn</code> in Jest to mock an AWS SNS <code>publish</code> call. I'm not well versed in AWS's ecosystem; nor am I super experienced with writing Jest tests; so I'm a bit stuck. In the following test code; when <code>spyOn</code> is called for the DynamoDB <code>put</code> method I receive no errors; but when it's called for the SNS <code>publish</code> method; I receive the error that follows. Most of this code was written by someone else; and I'm attempting to follow the same templating for the <code>publish</code> mock.</p><br><pre><code>const lambda = require('../../../src/handlers/sign-up.js');<br><br>const dynamodb = require('aws-sdk/clients/dynamodb'); <br>const AWS = require(&quot;aws-sdk&quot;);<br>AWS.config.update({region: 'us-east-1'})<br>const sns = new AWS.SNS();<br><br>const allowOrigin = process.env.ALLOW_ORIGIN<br><br>describe('Test signUpHandler'; function () {<br>    let putSpy; <br>    let publishSpy;<br><br>    beforeAll(() =&gt; { <br>        putSpy = jest.spyOn(dynamodb.DocumentClient.prototype; 'put'); <br>        publishSpy = jest.spyOn(sns.prototype; 'publish')<br>    }); <br> <br><br>    afterAll(() =&gt; { <br>        publishSpy.mockRestore();<br>        putSpy.mockRestore(); <br>    });<br> <br><br>    it('should add email to the table'; async () =&gt; {<br>        const returnedItem = { email: 'test@example.com' };<br> <br>        putSpy.mockReturnValue({ <br>            promise: () =&gt; Promise.resolve(returnedItem) <br>        }); <br> <br>        const event = { <br>            httpMethod: 'POST'; <br>            headers: {<br>                origin: allowOrigin<br>            };<br>            body: '{&quot;email&quot;: &quot;test@example.com&quot;}'<br>        }; <br>     <br>        const result = await lambda.signUpHandler(event);<br>        const expectedResult = { <br>            statusCode: 200;<br>            headers: {&quot;Access-Control-Allow-Origin&quot;: allowOrigin};<br>            body: JSON.stringify({&quot;success&quot;: true})<br>        }; <br><br>        expect(result).toEqual(expectedResult); <br>    }); <br>}); <br></code></pre><br><p>When I run the test; I get this error:</p><br><pre><code>Cannot spyOn on a primitive value; undefined given<br><br>      20 |         putSpy = jest.spyOn(dynamodb.DocumentClient.prototype; 'put'); <br>    &gt; 21 |         publishSpy = jest.spyOn(sns.prototype; 'publish')<br>         |                           ^<br>      22 |     }); <br></code></pre><br><p>Am I approaching this wrong? Any help would be greatly appreciated.</p><br>
0.0,1.0,0.0,0.0,0.3333333333333333,0.0,0.0,<h3>Internal access for AWS Route 53 public record pointing to ALB</h3><p>For my case; I have a Route 53 public DNS record; let say abc.com. This abc.com has a A record pointing to an ALB which routed to an EC2 (let's name it as EC2 A).</p><br><p>Normally; there are requests from internet calling this abc.com which will then routed to EC2 A. But for some case; EC2 in the same VPC of EC2 A will call abc.com as well. For this case; it will still go through the external path to reach EC2 A. But as both EC2 is in the same VPC; is there any way calling abc.com can be routed internally instead of going through the external path?</p><br>
0.0,0.0,1.0,0.0,0.0,0.6666666666666666,0.0,<h3>Python mock AWS SSM</h3><p>I have written a code that will fetch SSM parameters for me</p><br><pre><code>import boto3<br>    <br>client = boto3.client('ssm')<br>    <br>def lambda_handler(event; context):<br>    return client.get_parameter(Name=event[&quot;param&quot;]; WithDecryption=True)<br><br>if __name__ == '__main__':<br>    print(lambda_handler({&quot;param&quot;: &quot;/mypath/password&quot;}; &quot;&quot;))<br></code></pre><br><p>However; I am not able to write a test case for it I have tried using <code>moto</code> but for some reason; it still gives me the actual value from the SSM store</p><br><pre><code>import os<br><br>import boto3<br>from moto import mock_ssm<br>import pytest<br><br>from handler import lambda_handler<br><br>@pytest.fixture<br>def aws_credentials():<br>    os.environ[&quot;AWS_ACCESS_KEY_ID&quot;] = &quot;testing&quot;<br>    os.environ[&quot;AWS_SECRET_ACCESS_KEY&quot;] = &quot;testing&quot;<br>    os.environ[&quot;AWS_SECURITY_TOKEN&quot;] = &quot;testing&quot;<br>    os.environ[&quot;AWS_SESSION_TOKEN&quot;] = &quot;testing&quot;<br><br>@mock_ssm<br>def test_ssm():<br>    ssm = boto3.client('ssm')<br>    ssm.put_parameter(<br>        Name=&quot;/mypath/password&quot;;<br>        Description=&quot;A test parameter&quot;;<br>        Value=&quot;this is it!&quot;;<br>        Type=&quot;SecureString&quot;<br>    )<br>    resp = lambda_handler({&quot;param&quot;: &quot;/mypath/password&quot;}; &quot;&quot;)<br>    assert resp[&quot;Parameter&quot;][&quot;Value&quot;] == &quot;this is it!&quot;<br></code></pre><br><p>Am I missing something overhear; what should I do to make it work; or is there an alternative way to mock SSM in python.</p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.0,0.0,<h3>aws cli command from bash with jq to get S3 logs from logging bucket failing</h3><p>The below script is intended to get the content of each entry in the S3 logging bucket and save it to a file</p><br><pre><code>#!/bin/bash<br>#<br># Get the content of each entry in the S3 logging bucket and save it to a file <br>#<br><br>LOGGING_BUCKET=dtgd-hd00<br><br>aws s3api list-objects-v2 --bucket &quot;$LOGGING_BUCKET&quot; | jq '.Contents' &gt;&gt; entries.json &amp;&amp;<br>keys=$(jq '.[].Key' entries.json )<br><br>for key in $keys;do<br>  echo $key<br>  aws s3api get-object --bucket &quot;$LOGGING_BUCKET&quot; --key &quot;$key&quot; ouput_file_&quot;$key&quot;<br>done<br></code></pre><br><p>Once executed I got:</p><br><blockquote><br><p>An error occurred (NoSuchKey) when calling the GetObject operation:<br>The specified key does not exist.</p><br><p>&quot;dtgd-hd00/logs2021-08-10-05-43-18-01393D975686FA45&quot;</p><br></blockquote><br><p>However; if I do it from  the CLI:</p><br><pre><code>aws s3api get-object --bucket dtgd-hd00 \<br>    --key &quot;dtgd-hd00/logs2021-08-10-05-43-18-01393D975686FA45&quot; \<br>    output_file_&quot;$key&quot;<br></code></pre><br><p>It works perfectly; getting the content and saving it to an output file as requested.</p><br><p>What could be wrong ??</p><br>
0.0,0.0,0.0,0.6666666666666666,0.0,0.6666666666666666,0.3333333333333333,<h3>Upload to s3; response object</h3><p>In my react app  I upload file to S3</p><br><p>Seems that the upload ok but I don't get any response from the server</p><br><p>this is my code:</p><br><pre><code>if(awsUrl.url){<br>      console.log(awsUrl)<br>        const res = await fetch(awsUrl.url; {<br>        method: 'PUT';<br>        body: formData;<br>        headers: {<br>          &quot;Content-Type&quot;: &quot;multipart/form-data&quot;<br>        }<br>      }).then(response =&gt; response)<br>        if(res.ok) {<br>          alert(&quot;upload success&quot;)<br><br>      }<br></code></pre><br><p>The response is just OK</p><br><p>How Can I get the json response from the s3 request</p><br><p><a href="https://i.stack.imgur.com/dKPHZ.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/dKPHZ.png" alt="enter image description here" /></a></p><br>
0.0,0.0,1.0,0.6666666666666666,0.0,0.0,0.0,<h3>error when using lambda to get s3 object : ClientError: An error occurred (AccessDenied) when calling the GetObject operation: Access Denied</h3><p>I have the line of code below:</p><br><pre><code>resp = s3_client.get_object(Bucket=bucket_name; Key=s3_file_name)<br></code></pre><br><p>Currently; it throws an error:</p><br><pre><code>ClientError: An error occurred (AccessDenied) when calling the GetObject operation: Access Denied<br></code></pre><br><p>However; when I go the specific object that is defined by s3_file_name and change these settings it works fine:</p><br><p><a href="https://i.stack.imgur.com/uqqVy.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/uqqVy.png" alt="Permissions" /></a></p><br><p>Is there anyway I can change a setting in the bucket itself or IAM user to not have to individually change the settings for each object?</p><br>
0.0,0.6666666666666666,0.0,1.0,0.0,0.0,0.0,<h3>Signed Url aws is not downloadable</h3><p>I have a method to create a signed URL</p><br><pre><code>public async getSignedUrl(storageKey: string) {<br><br>    const params = {<br>      Bucket: this.bucket;<br>      Key: storageKey;<br>    };<br>    const url = this.aws.s3.getSignedUrl(&quot;getObject&quot;; params);<br>    return url;<br>  }<br></code></pre><br><p>but when I used to download the file I face some issues</p><br><pre><code>export const handleFileDownload = (storageKey) =&gt; {<br>  getSignedUrl(storageKey).then(async (data) =&gt; {<br>    const fileUri = data.data.getSignedUrl;<br>    const response = await fetch(fileUri; {<br>      headers: {<br>        &quot;Access-Control-Allow-Origin&quot;: &quot;*&quot;;<br>      };<br>    });<br>    const url = window.URL.createObjectURL(new Blob([response.data]));<br>    const link = document.createElement(&quot;a&quot;);<br>    link.setAttribute(&quot;href&quot;; url); --&gt; I tried signed url here as well<br>    link.setAttribute(&quot;download&quot;; name);<br>    document.body.appendChild(link);<br>    link.click();<br>    document.body.removeChild(link);<br>  });<br>};<br></code></pre><br><p>my S3 configuration</p><br><pre><code>[<br>    {<br>        &quot;AllowedHeaders&quot;: [<br>            &quot;Authorization&quot;<br>        ];<br>        &quot;AllowedMethods&quot;: [<br>            &quot;PUT&quot;;<br>            &quot;POST&quot;;<br>            &quot;DELETE&quot;<br>        ];<br>        &quot;AllowedOrigins&quot;: [<br>            &quot;*&quot;<br>        ];<br>        &quot;ExposeHeaders&quot;: [];<br>        &quot;MaxAgeSeconds&quot;: 3000<br>    }<br>]<br></code></pre><br><p>I dont know what I am missing<br>I got that <code>preflightmissingalloworgignheader</code></p><br>
0.0,0.0,0.6666666666666666,0.0,1.0,0.0,0.0,<h3>Cannot use existing log group in ECS scheduled task (aws-cdk)</h3><p>I am trying to add an existing log group to my ECS scheduled task to avoid creation of new log groups with random name. But the aws-cdk (typescript) keeps giving me error that the types are not assignable.</p><br><pre><code>const myLogGroup =  LogGroup.fromLogGroupArn(this; 'MyLogGroup'; 'log-group-arn')<br><br>const logging = ecs.LogDrivers.awsLogs({<br>      streamPrefix: 'mylg';<br>      logGroup: myLogGroup<br>});<br><br></code></pre><br><p><code>logGroup: myLogGroup</code> gives the error:</p><br><pre><code>Type 'import(&quot;/Users/praveen/code/cron-cdk/node_modules/@aws-cdk/aws-logs/lib/log-group&quot;).ILogGroup' is not assignable to type 'import(&quot;/Users/praveen/code/cron-cdk/node_modules/@aws-cdk/aws-ec2/node_modules/@aws-cdk/aws-logs/lib/log-group&quot;).ILogGroup'.<br>  The types of 'addStream(...).stack.tags' are incompatible between these types.<br>    Type 'import(&quot;/Users/praveen/code/cron-cdk/node_modules/@aws-cdk/aws-logs/node_modules/@aws-cdk/core/lib/tag-manager&quot;).TagManager' is not assignable to type 'import(&quot;/Users/praveen/code/cron-cdk/node_modules/@aws-cdk/core/lib/tag-manager&quot;).TagManager'.<br>      Types have separate declarations of a private property 'tags'.ts(2322)<br>aws-log-driver.d.ts(51; 14): The expected type comes from property 'logGroup' which is declared here on type 'AwsLogDriverProps'<br>(property) AwsLogDriverProps.logGroup?: ILogGroup | undefined<br></code></pre><br><p>Any thoughts how can I use my existing log group for this?<br>My scheduled task definition  is this -</p><br><pre><code>new ScheduledFargateTask(this; 'cleanup'; {<br>      cluster;<br>      scheduledFargateTaskImageOptions: {<br>        image: ecs.ContainerImage.fromAsset('scripts/cleanup');<br>        memoryLimitMiB: 512;<br>        logDriver: logging;<br>      };<br>      schedule: events.Schedule.expression('rate(12 hours)');<br>      platformVersion: ecs.FargatePlatformVersion.LATEST;<br>    });<br></code></pre><br>
0.0,0.3333333333333333,0.3333333333333333,0.0,1.0,0.0,0.0,<h3>ECS Fargate task fails: CannotPullContainerError: inspect image has been retried 5 time(s): httpReaderSeeker: failed open: unexpected status code</h3><p>We have other ECS Services running which use images from our private ECR repo. However for our Services in the same cluster which are trying to pull from Docker Hub we are getting the following error:</p><br><blockquote><br><p>CannotPullContainerError: inspect image has been retried 5 time(s): httpReaderSeeker: failed open: unexpected status code https://registry-1.docker.io...: 4...</p><br></blockquote><br><p>(The message itself is truncated at the end: it is literally &quot;4...&quot;).</p><br><p>Judging by the fact that it's getting a status code response; that suggests that it's able to talk to Docker Hub and it's not a network connectivity issue within our AWS configuration. We are trying to use an image in our ECS Task from a public repo; one is a Redis image and another is a Hasura image. I'm not sure how to see the status code itself since it's truncated in the AWS console.</p><br><p>When I hit the <a href="https://registry-1.docker.io/v2/hasura/graphql-engine/manifests/sha256:0b62bc854e92f1859b5c9c19ce40c4bfaa643b65b1807e2cf55a4078db30b8a8" rel="nofollow noreferrer">URL</a> from the error in my browser this is the response:</p><br><pre><code>{&quot;errors&quot;:[{&quot;code&quot;:&quot;UNAUTHORIZED&quot;;&quot;message&quot;:&quot;authentication required&quot;;&quot;detail&quot;:[{&quot;Type&quot;:&quot;repository&quot;;&quot;Class&quot;:&quot;&quot;;&quot;Name&quot;:&quot;hasura/graphql-engine&quot;;&quot;Action&quot;:&quot;pull&quot;}]}]}<br></code></pre><br><p>I get a similar response with the Redis image. I didn't think we needed any authentication to pull public images - we've run ECS Tasks in the past without requiring authentication to Docker Hub?</p><br><p><strong>For completeness; I've included the checks below for troubleshooting this error; however as mentioned; since we're getting a response code from Docker Hub it doesn't look like these checks are relevant.</strong></p><br><p>AWS has this <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_cannot_pull_image.html" rel="nofollow noreferrer">guide to troubleshoot</a> 'CannotPullContainer' errors and for this particular error on Fargate there is <a href="https://aws.amazon.com/premiumsupport/knowledge-center/ecs-fargate-pull-container-error/" rel="nofollow noreferrer">this guide</a>. Here are the things from the guide we have checked:</p><br><blockquote><br><p>Confirm that your VPC networking configuration allows your Amazon ECS infrastructure to reach the image repository</p><br></blockquote><br><p>This ECS Task was in a private subnet; and it's route table had the following routes:</p><br><pre><code>10.0.0.0/16 -&gt; local (active)<br>0.0.0.0/0 -&gt; NAT Gateway (active)<br></code></pre><br><p>The NAT Gateway has status available and an Elastic IP address assigned.</p><br><blockquote><br><p>Check the VPC DHCP Option Set</p><br></blockquote><br><p>Looking at the VPC and going to the DHCP options set we can see Domain name servers is set to: 'AmazonProvidedDNS'</p><br><blockquote><br><p>Check the task execution role permissions<br>More details about configuring this are in <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_execution_IAM_role.html" rel="nofollow noreferrer">this guide</a>.</p><br></blockquote><br><p>The same IAM role is used in the task definition for both the 'task role' and 'task execution role.' This has been with the following default policy as defined in the guide mentioned:</p><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Sid&quot;: &quot;&quot;;<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Action&quot;: [<br>                &quot;logs:PutLogEvents&quot;;<br>                &quot;logs:CreateLogStream&quot;;<br>                &quot;ecr:GetDownloadUrlForLayer&quot;;<br>                &quot;ecr:GetAuthorizationToken&quot;;<br>                &quot;ecr:BatchGetImage&quot;;<br>                &quot;ecr:BatchCheckLayerAvailability&quot;<br>            ];<br>            &quot;Resource&quot;: &quot;*&quot;<br>        }<br>    ]<br>}<br></code></pre><br><blockquote><br><p>Check that the image exists</p><br></blockquote><br><p><a href="https://hub.docker.com/layers/hasura/graphql-engine/v1.3.0/images/sha256-45a57d1e3170c6ca06b0c8f979822daafd77461e888daf3db584032aa1f821ba?context=explore" rel="nofollow noreferrer">This is the  image</a> we're trying to pull from Docker Hub. The image exists and I can pull it from my local machine without having to authenticate.</p><br>
0.0,0.0,1.0,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>How to do a heath check for a Spring SOAP web services Application</h3><p>We have a Spring SOAP web services microservice using the below versions of Spring.</p><br><pre><code>Spring Boot - 2.2.x<br>Spring-ws-core - 3.0.8<br></code></pre><br><p>The microservice is running on AWS EC2 instance in a AutoScaling group and connects to an external backend application. For monitoring we are using Prometheus; which  pulls the data from CloudWatch at regular interval through CloudWatch Exporter.</p><br><p>As this is a SOAP application; is it possible to do a health check on application at regular intervals; using Spring in-built tools; to ensure that application is up and running? We don't need any additional metrics at this point of time. The status of the application should feed into Prometheus.</p><br><p>Thanks in advance.</p><br>
0.0,1.0,0.0,0.0,0.6666666666666666,0.0,0.0,<h3>Can&#39;t seem to access my server either through public IP or ssh into the server</h3><p>I have provisioned an EC2 instance in default VPC which has both private and public IP address. Deployed through terraform and configuration includes a user-data script for  an Apache webserver and security groups are opened ports 22; 80; 443 from anywhere. I can neither seem to connect the webserver through my Public IP nor through ssh into the box. My user-data script is below</p><br><pre><code>#! /bin/bash<br># Instance Identity Metadata Reference - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-identity-documents.html<br>sudo yum update -y<br>sudo yum install -y httpd<br>sudo systemctl enable httpd<br>sudo service httpd start<br>sudo systemctl restart sshd &amp;&amp; systemctl status sshd  <br>sudo echo '&lt;h1&gt;Welcome to StackSimplify - APP-1&lt;/h1&gt;' | sudo tee /var/www/html/index.html<br>sudo mkdir /var/www/html/app1<br>sudo echo '&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;body style=&quot;background-color:rgb(250; 210; 210);&quot;&gt; &lt;h1&gt;Welcome - APP-1&lt;/h1&gt; &lt;p&gt;Terraform Demo&lt;/p&gt; &lt;p&gt;Application Version: V1&lt;/p&gt; &lt;/body&gt;&lt;/html&gt;' | sudo tee /var/www/html/app1/index.html<br>sudo curl http://169.254.169.254/latest/dynamic/instance-identity/document -o /var/www/html/app1/metadata.html<br></code></pre><br><p>Where am I going wrong?</p><br>
0.6666666666666666,1.0,0.0,0.0,0.0,0.0,0.0,<h3>Connection error of AWS Redshift to local computer</h3><p>I tried to connect Amazon Redshift to my local computer using pycopg2. However; I got an error message:</p><br><blockquote><br><p>psycopg2.OperationalError: could not connect to server: Operation timed out. Is the server running on host xxx and accepting TCP/IP connecitons on posrt 5439</p><br></blockquote><br><p>I have done two guides with searching google:</p><br><ol><br><li>Changed the Publicly Accessible setting as enable; and</li><br><li>Add <code>0.0.0.0/0</code> and ::/0 to VPC route as gateway.</li><br></ol><br><p>It still doesn't work. Please let me know if you know what the problem is.</p><br>
0.0,0.0,0.0,1.0,0.3333333333333333,0.6666666666666666,0.0,<h3>How to extract tar files from amazonS3 bucket to another s3 in Java</h3><p>I have tar files in a S3 bucket and I'm trying to untar them in another s3 bucket.<br>So far I got all the files in the destBucket but it seems that the putObject makes the files corrupted or nulls. How to read the whole file and write the whole buffered in the putObject ?</p><br><p>Here the code I am using:</p><br><pre><code>TarArchiveInputStream tarInputStream = new TarArchiveInputStream(new BufferedInputStream(objectData));<br><br>        TarArchiveEntry currentEntry;<br><br>        while ((currentEntry = tarInputStream.getNextTarEntry()) != null) {<br>            if (!currentEntry.isDirectory()) {<br>                byte[] objectBytes = new byte[currentEntry.getSize()];<br>                tarInputStream.read(objectBytes);<br>                def entryName = currentEntry.getName()<br>                def fileN = entryName.substring(entryName.lastIndexOf(&quot;/&quot;) + 1; entryName.length())<br><br>                ObjectMetadata metadata = new ObjectMetadata();<br>                metadata.setContentLength(objectBytes.length);<br>                metadata.setContentType(&quot;application/octet-stream&quot;);<br>                s3Client.putObject(destbucket; packagePath + &quot;untar_frames/&quot; + fileN;<br>                        new ByteArrayInputStream(objectBytes); metadata);<br>            }<br>        }<br></code></pre><br>
0.0,0.0,1.0,0.0,1.0,0.3333333333333333,0.0,<h3>What&#39;s the best practice to run the same github python code on several ec2 instances?</h3><p>I'm using venv for my python repo on github; and wanted to run the same code on 10+ ec2 instances (each instance will have a cronjob that just runs the same code on the same schedule)</p><br><p>Any recommendations on how to best achieve this + continue to make sure all instances get the latest release branches on github? I'd like to try and automate any configuration I need to do; so that I'm not doing this:</p><br><ol><br><li><p>Create one ec2 instance; set up all the configurations I need; like download latest python version; etc. Then git clone; set up all the python packages I need using venv. Verify code works on this instance.</p><br></li><br><li><p>Repeat for remaining 10+ ec2 instances</p><br></li><br><li><p>Whenever someone releases a new master branch; I have to ssh into every ec2 instances; git pull to the correct branch; re-update any new configurations I need; repeat for all remaining 10+ ec2 instances.</p><br></li><br></ol><br><p>Ideally I can just run some script that pushes everything that's needed to make the code work on all ec2 instances. I have little experience with this type of thing; but from reading around this is an approach I'm considering. Am I on the right track?:</p><br><ul><br><li>Create a script I run to ssh into all my ec2 instances and git clone/update to correct branch</li><br><li>Use Docker to make sure all ec2 instances are set up properly so the python code works (Is this the right use-case for Docker?). Above script will run the necessary Docker commands</li><br><li>Similar thing with using venv and reading the requirements.txt file so all ec2 instances has the right python packages and versions</li><br></ul><br>
0.0,1.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>pass query params from cloudfront to api gateway</h3><p>I created a lambda function with a API gateway and Cloudfront distribution in the front</p><br><p>in the cloudfront behaviors I disabled caching</p><br><p><a href="https://i.stack.imgur.com/DWIRv.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/DWIRv.png" alt="enter image description here" /></a></p><br><p>this is the lambda function:</p><br><pre><code>exports.handler = async (event) =&gt; {<br>    const response = {<br>        statusCode: 200;<br>        body: JSON.stringify('rawQueryString is: ' + event.rawQueryString);<br>    };<br>    return response;<br>};<br></code></pre><br><p>calling the api gateway I see the querystring in the lambda response<br><a href="https://xxx.execute-api.us-east-1.amazonaws.com/api?name=john" rel="nofollow noreferrer">https://xxx.execute-api.us-east-1.amazonaws.com/api?name=john</a></p><br><p><code>rawQueryString is: '?name=john'</code></p><br><p>calling the cloudfront distribution i can't see the querystring in the lambda response<br><a href="https://xxx.cloudfront.net/api?name=john" rel="nofollow noreferrer">https://xxx.cloudfront.net/api?name=john</a></p><br><p><code>rawQueryString is: ''</code></p><br><p>I tried with &quot;Origin Request Policy&quot;</p><br><p><a href="https://i.stack.imgur.com/FZcc6.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/FZcc6.png" alt="enter image description here" /></a></p><br><p>but now when i call <a href="https://xxx.cloudfront.net/api?name=john" rel="nofollow noreferrer">https://xxx.cloudfront.net/api?name=john</a></p><br><p>I get</p><br><pre><code>{<br>    &quot;message&quot;: &quot;Forbidden&quot;<br>}<br></code></pre><br>
0.0,0.0,0.3333333333333333,1.0,0.3333333333333333,0.0,0.0,<h3>How to connect to mysql database (RDS instance under AWS) from</h3><p>I created RDS instance for mysql database in AWS console.<br>I managed to connect to in my laravel 8 app with parameters in .env file :</p><br><pre><code>DB_CONNECTION=mysql<br>DB_HOST=myrdsinstance.rds.amazonaws.com<br>DB_PORT=3306<br>DB_DATABASE=tads<br>DB_USERNAME=app_admin_name<br>DB_PASSWORD=app_admin_password<br></code></pre><br><p>I tried to connect to this database from MySQL Workbench.<br>Creating RDS instance I set parameter<br>Public Access set Yes for security - I suppose I can to connect to MySQL Workbench with it.</p><br><p>In MySQL Workbench on tab Under Remote Managent :</p><br><ul><br><li>In hostname field I entered : myrdsinstance.rds.amazonaws.com  In<br>usernmne field I entered : app_admin_name In password field I entered<br>: app_admin_password In SSH Key Path field I selected : path to pem<br>file I use for access my aws instance in the console</li><br></ul><br><p>But I got error:<br>ERROR [Errno 13] Permission denied: '/home/user/.ssh/config'</p><br><p>In the console of my ubuntu I run :</p><br><pre><code>sudo chmod  777  /home/user/.ssh/config<br>sudo systemctl restart ssh.service<br></code></pre><br><p>But I got next error :</p><br><pre><code>SSH Connection Failed.<br>Check you SSH Connection settings and whether the SSH server is up.<br>Error : timed out<br></code></pre><br><p>What is wrong and how can it be fixed?</p><br><p><strong>MODIFIED BLOCK :</strong><br>In MySQL Workbench I disabled Remote Management tab; where I tried to connect with ssh and</p><br><p>and on Connection tab I selected</p><br><pre><code>Connection Method :Standart(TCP/IP) In hostname field I entered :<br>    myrdsinstance.rds.amazonaws.com  In port field I entered : 3306 In<br>    usernmne field I entered : app_admin_name In password field I<br>    entered : app_admin_password<br></code></pre><br><p>and in console Under Security Groups I added new group with Inbound rule : <a href="https://prnt.sc/1hxsfhq" rel="nofollow noreferrer">https://prnt.sc/1hxsfhq</a></p><br><p>But in MySQL Workbench I got error :</p><br><pre><code>Failed to Connect to MySQL at myrdsinstance.rds.amazonaws.com:3306 <br>with user tads_admin<br>Can't connect to MySQL server on 'myrdsinstance.rds.amazonaws.com' (110)<br></code></pre><br><p>Thanks!</p><br>
0.0,0.0,0.0,1.0,0.3333333333333333,0.3333333333333333,0.0,<h3>Save image data from a iterator object to AWS S3 in python</h3><p>I am calling an API that returns a iterator object containing image data. I'd like to iterate over them and upload to s3. I could either open them into <code>.png</code> or <code>.jpeg</code> before or after dumping / uploading them to s3.</p><br><pre><code>import boto3<br><br># Download / open photo<br>img_obj = gmaps.places_photo(ph; max_width = 500; max_height = 400)<br>            <br>print(img_obj)<br><br>&lt;generator object Response.iter_content.&lt;locals&gt;.generate at 0x7ffa2dsa7820&gt;<br>            <br>s3 = boto3.client('s3')<br>with open('output_image_{}.png'.format(c); 'w') as data:<br>    for chunk in img_obj:<br>       s3.upload_fileobj(data;'mybucket';'img/{}'.format(chunk))<br></code></pre><br><p>Error:</p><br><pre><code>Input &lt;_io.BufferedWriter name='output_image_1.png'&gt; of type: &lt;class '_io.BufferedWriter'&gt; is not supported.<br></code></pre><br><p>On local machine; I am able to write the file:</p><br><pre><code>with open(&quot;output_image_{}.png&quot;.format(c); &quot;w&quot;) as fp:<br>    for chunk in img_obj:<br>       fp.write(chunk)<br></code></pre><br><p>I'd like to directly save the <code>img_obj</code> on AWS S3.</p><br>
0.0,0.0,0.0,0.0,0.3333333333333333,1.0,0.0,<h3>Not able to inject dependencies inside the AWS Lambda function through Springs framework</h3><p>I am trying to create a SQS poller inside the AWS Lambda; and I am trying to process the messages fetched from the SQS queue. <br>In this code I am using Spring Framework to inject the dependencies; so for now I am not using Guice or Dagger or creating objects using "new" operator.</p><br><br><p>But when I declare all the beans in the ApplicationBeans.java file inside the spring folder; none of my beans gets initialized.</p><br><br><p>My ApplicationBeans.java looks like this:</p><br><br><pre><code>import org.springframework.context.annotation.Configuration;<br>import org.springframework.context.annotation.Bean;<br><br>@Configuration<br>public class ApplicationBeans<br>{<br><br>@Bean<br>-----------<br>-----------<br>----------<br>}<br></code></pre><br><br><p>And my main class where I need to inject the dependencies looks like these:</p><br><br><pre><code>import lombok.AllArgsConstructor;<br>@AllArgsConstructor<br>public class MainClass implements RequestHandler&lt;SQSEvent; Void&gt;<br>{<br>private Dependency1 dependency1;<br>-----<br>-----<br>-----<br><br>}<br></code></pre><br><br><p>But now when I try to access the dependency1 in the code; it gives me a nullPointer exception.<br>What is going wrong in configuring Springs in AWS Lambda?</p><br>
1.0,0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,0.0,<h3>aws elasticsearch getting signature error on post request</h3><p>Got a 403 signature error ; when using the below fetch function: </p><br><br><pre><code>    function elasticsearchFetch(AWS; elasticsearchDomain; endpointPath; options = {}; region = process.env.AWS_REGION) {<br>  return new Promise((resolve; reject) =&gt; {<br>    const { body; method = 'GET' } = options;<br>    const endpoint = new AWS.Endpoint(elasticsearchDomain);<br>    const request = new AWS.HttpRequest(endpoint; region);<br>    request.method = method;<br>    request.path += endpointPath;<br>    request.headers.host = elasticsearchDomain;<br>    if (body) {<br>      request.body = body;<br>      request.headers['Content-Type'] = 'application/json';<br>      request.headers['Content-Length'] = request.body.length;<br>    }<br>    const credentials = new AWS.EnvironmentCredentials('AWS');<br>    const signer = new AWS.Signers.V4(request; 'es');<br>    signer.addAuthorization(credentials; new Date());<br>    const client = new AWS.HttpClient();<br>    client.handleRequest(request; null; (res) =&gt; {<br>      let chunks = '';<br>      res.on('data'; (chunk) =&gt; {<br>        chunks += chunk;<br>      });<br>      res.on('end'; () =&gt; {<br>        if (res.statusCode !== 201) console.log('Got these options STATUSCODE'; JSON.stringify(options; false; 2));<br>        return resolve({ statusCode: res.statusCode; body: chunks });<br>      });<br>    }; (error) =&gt; {<br>      console.log('Got these options ERROR'; JSON.stringify(options; false; 2));<br>      return reject(error);<br>    });<br>  });<br>}<br></code></pre><br><br><p>This is the options used for the request in above function : </p><br><br><pre><code>{<br>    "method": "POST";<br>    "body": "{\"prefix\":\"image_233/ArtService/articles-0/GB/ART-60297885/\";\"id\":\"ART-60297885\";\"retailUnit\":\"GB\";\"commercial\":{\"name\":{\"en-GB\":\"FRBTTRA\"}};\"schemaType\":\"product\";\"productType\":\"ART\"}"<br>}<br></code></pre><br><br><p>and got this error : </p><br><br><pre><code>{<br>    "statusCode": 403;<br>    "body": "{\"message\":\"The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\"}"<br>}<br></code></pre><br><br><p>This is the endpoint : <code>233/_doc/</code> </p><br>
0.0,0.0,0.0,0.6666666666666666,0.0,0.3333333333333333,0.0,<h3>How to use fetch to fetch data from AWS RDS in a react application?</h3><p>I'm learning Node + React. In my Node + React application; I have the following component <code>FetchData.js</code> to fetch data from <code>localhost</code>:</p><br><pre><code>import React from 'react';<br><br>class FetchData extends React.Component {<br><br>    static async getData() {<br>        let response = await fetch('http://localhost:4000/my-project');<br>        let body = await response.json();<br>        return body<br>    }<br>}<br><br>export default FetchData;<br></code></pre><br><p>I could see the json objects by typing <code>localhost:4000</code> in the browser.</p><br><p>Now I'm hosting the data in AWS RDS. I have an endpoint like this:</p><br><pre><code>my-project.abc12345def.eu-west-2.rds.amazonaws.com:3306<br></code></pre><br><p>How can I fetch data from the endpoint like what I did in <code>FetchData.js</code>? I tried to replace <code>localhost</code> with the endpoint url; no luck; tried to type the endpoint url in a browser; could not see its content. I suppose I need to be able to see its content to be able to fetch data?</p><br>
1.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>botocore.errorfactory.ProvisionedThroughputExceededException from boto3 for DynamoDB while running spark job</h3><p>I am getting data from Kafka Stream and saving them into DynamoDB. While doing so; I am getting the following error:</p><br><br><pre><code>Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):<br>  File "/mnt/yarn/usercache/hadoop/appcache/application_1577444134805_0063/container_1577444134805_0063_01_000004/pyspark.zip/pyspark/worker.py"; line 377; in main<br>    process()<br>  File "/mnt/yarn/usercache/hadoop/appcache/application_1577444134805_0063/container_1577444134805_0063_01_000004/pyspark.zip/pyspark/worker.py"; line 372; in process<br>    serializer.dump_stream(func(split_index; iterator); outfile)<br>  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py"; line 1007; in func_with_open_process_close<br>  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py"; line 1000; in func_with_open_process_close<br>  File "./pyemr.zip/pyemr/dynamowriter.py"; line 117; in process<br>    Item=event<br>  File "/usr/local/lib/python3.6/site-packages/boto3/dynamodb/table.py"; line 101; in put_item<br>    self._add_request_and_process({'PutRequest': {'Item': Item}})<br>  File "/usr/local/lib/python3.6/site-packages/boto3/dynamodb/table.py"; line 110; in _add_request_and_process<br>    self._flush_if_needed()<br>  File "/usr/local/lib/python3.6/site-packages/boto3/dynamodb/table.py"; line 131; in _flush_if_needed<br>    self._flush()<br>  File "/usr/local/lib/python3.6/site-packages/boto3/dynamodb/table.py"; line 137; in _flush<br>    RequestItems={self._table_name: items_to_send})<br>  File "/usr/local/lib/python3.6/site-packages/botocore/client.py"; line 276; in _api_call<br>    return self._make_api_call(operation_name; kwargs)<br>  File "/usr/local/lib/python3.6/site-packages/botocore/client.py"; line 586; in _make_api_call<br>    raise error_class(parsed_response; operation_name)<br>botocore.errorfactory.ProvisionedThroughputExceededException: An error occurred (ProvisionedThroughputExceededException) when calling the BatchWriteItem operation (reached max retries: 9): The level of configured provisioned throughput for the table was exceeded. Consider increasing your provisioning level with the UpdateTable API.<br><br>    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)<br>    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)<br>    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)<br>    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)<br>    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)<br>    at org.apache.spark.sql.execution.python.PythonForeachWriter.close(PythonForeachWriter.scala:66)<br>    at org.apache.spark.sql.execution.streaming.sources.ForeachDataWriter.commit(ForeachWriterProvider.scala:129)<br>    at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:127)<br>    at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)<br>    at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)<br>    at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)<br>    at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)<br>    at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)<br>    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)<br>    at org.apache.spark.scheduler.Task.run(Task.scala:121)<br>    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)<br>    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)<br>    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)<br>    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>    at java.lang.Thread.run(Thread.java:748)<br></code></pre><br><br><p>As I understand from the error; the amount of data I am getting from kafka is too much for DynamoDB to write in table as it's exceeding the capacity of the database. <code>Write capacity units</code> in my DynamoDB table is set to 20 units.</p><br><br><p>The code for <code>writeStream</code> is as follows:</p><br><br><pre><code>def save_source_events_output(self; *; app_name; source_events; sink_type=None; writerClass=None; trigger_freq="2 seconds"; out_put_mode='update'):<br>        output = (<br>            source_events<br>            .writeStream<br>            .outputMode(out_put_mode)<br>            .foreach(writerClass(**self.job_config_data))<br>            .trigger(processingTime=trigger_freq)<br>            .start()<br>        )<br>        output.awaitTermination()<br></code></pre><br><br><p>Can someone please tell me how can I get rid of this error as I need to run the <code>spark-submit</code> job to write in DynamoDB as long as it gets data from kafka?</p><br>
0.0,0.0,0.0,1.0,0.0,1.0,0.0,<h3>How do I retrieve the &#39;id&#39; of an entry/row from an &#39;item&#39; in the entry/row?</h3><p>Building an app using React Native (for iOS) using AWS Amplify</p><br><br><p>I want to do something seemingly so simple; but i am a bit lost as to how to do it: I have a table with user information already in it. Here's the Schema:</p><br><br><pre><code>type Users @model {<br>  id: ID!<br>  userName: String<br>  firstname: String<br>  weblink: String<br>  email: String<br>  mobileNum: String<br> .<br> .<br> .<br>}<br><br><br>//**Here's my current Query.js**<br>export const getUsers = `query GetUsers($id: ID!) {<br>  getUsers(id: $id) {<br>    id<br>    userName<br>    firstname<br>    weblink<br>    email<br>    .<br>    .<br>    .<br>  }<br>}<br>`;<br></code></pre><br><br><p>This table is populated in DynamoDB when i check my AWS console. What i need is to be able to get the id from the table using the userName (not vice versa). The id is generated when i createUser() and it's used throughout my app to get all my user's information. However when a user signs in on a new phone; this id isn't available anymore. So when they sign in via Cognito; i do know the userName and all i need to do is retrieve this id. Because there's only one unique <strong>userName</strong>; it should only return one <strong>id</strong></p><br><br><p>Here's what i'm thinking so far: use a GSI (global secondary index). So change my schema to: </p><br><br><pre><code>type Users @model<br>    @key(<br>       name: "ByUsername"<br>       fields: ["userName"]<br>       queryField: "getIdFromUserName"<br>    )  <br>{<br>  id: ID!<br>  userName: String<br>  firstname: String<br>  weblink: String<br>  email: String<br>  mobileNum: String<br> .<br> .<br> .<br>}<br></code></pre><br><br><p>Then call in my app:</p><br><br><pre><code>const data = await API.graphql(graphqlOperation(getIdFromUserName; { userName }));<br></code></pre><br><br><p>5 questions: </p><br><br><p>1) Is there a simpler way than GSI?</p><br><br><p>2) Is that how you add the GSI? Or is it more robust to do it in the AWS console?</p><br><br><p>3) What should my <strong>Query.js</strong> then look like?</p><br><br><p>4) Do i need to make a custom resolver; or is this sufficient?</p><br><br><p>5) Am i missing anything else; or can i just </p><br><br><pre><code>amplify push  ?<br><br>//11/04/2020<br>//Resolver<br><br>## [Start] Prepare DynamoDB PutItem Request. **<br>$util.qr($context.args.input.put("createdAt"; $util.defaultIfNull($ctx.args.input.createdAt; $util.time.nowISO8601())))<br>$util.qr($context.args.input.put("updatedAt"; $util.defaultIfNull($ctx.args.input.updatedAt; $util.time.nowISO8601())))<br>$util.qr($context.args.input.put("__typename"; "Users"))<br>#set( $condition = {<br>  "expression": "attribute_not_exists(#id)";<br>  "expressionNames": {<br>      "#id": "id"<br>  }<br>} )<br>#if( $context.args.condition )<br>  #set( $condition.expressionValues = {} )<br>  #set( $conditionFilterExpressions = $util.parseJson($util.transform.toDynamoDBConditionExpression($context.args.condition)) )<br>  $util.qr($condition.put("expression"; "($condition.expression) AND $conditionFilterExpressions.expression"))<br>  $util.qr($condition.expressionNames.putAll($conditionFilterExpressions.expressionNames))<br>  $util.qr($condition.expressionValues.putAll($conditionFilterExpressions.expressionValues))<br>#end<br>#if( $condition.expressionValues &amp;&amp; $condition.expressionValues.size() == 0 )<br>  #set( $condition = {<br>  "expression": $condition.expression;<br>  "expressionNames": $condition.expressionNames<br>} )<br>#end<br>{<br>  "version": "2017-02-28";<br>  "operation": "PutItem";<br>  "key": #if( $modelObjectKey ) $util.toJson($modelObjectKey) #else {<br>  "id":   $util.dynamodb.toDynamoDBJson($util.defaultIfNullOrBlank($ctx.args.input.id; $util.autoId()))<br>} #end;<br>  "attributeValues": $util.dynamodb.toMapValuesJson($context.args.input);<br>  "condition": $util.toJson($condition)<br>}<br>## [End] Prepare DynamoDB PutItem Request. **<br></code></pre><br>
0.0,0.0,0.0,1.0,0.3333333333333333,1.0,0.0,<h3>How to reduce runtime for Lambda function that reads JSON and puts data in DynamoDB</h3><p>My Lambda function does a get request to a JSON api and puts the data in DynamoDB. However; it takes way too long run; because the JSON is quite large. How can I adjust my code so that my runtime reduces? Or can I adjust my DynamoDB settings to make the write process work faster?</p><br><br><pre><code>exports.handler = async (event) =&gt; {<br>    try {<br>        const data = await httprequest();<br><br>         for (var i = 0; i &lt; data.d.results.length; i++){<br>         var iden = Date.now();<br>        var identifier = iden.toString();<br><br>            var params = {<br>            Item: {<br>                ID: identifier;           <br>                journal: data.d.results[i].journal<br>            };<br><br>            TableName: ''test"<br>        };<br><br>        await docClient.put(params).promise();<br><br>        }<br><br><br>        console.log('Document inserted.');<br><br>        return JSON.stringify(data);<br>    } catch(err) {<br>        console.log(err);<br><br>        return err;<br>    }<br>};<br></code></pre><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>What permissions besides AssumeRole are needed to be able to switch role to another account in AWS console?</h3><p>I have a:</p><br><br><ul><br><li>a IAM role in AccountB <code>myrole</code> with a trust relationship to AccountA</li><br><li>the role in AccountB has a policy associated to <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-requested-region.html" rel="nofollow noreferrer">only allow <code>eu-north-1</code></a>.</li><br><li>an IAM user in AccountA with a policy to allow <code>AssumeRole</code> to role <code>myrole</code> in Account B</li><br></ul><br><br><p>This <em>works for my test user</em> where only this policy that allows <code>sts:AssumeRole</code> is attached.</p><br><br><p>But I have a user in AccountA with a <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-requested-region.html" rel="nofollow noreferrer"><code>DenyAllOutsideEU</code> policy</a> where the switch role operation in the aws console gives me a "<em>Invalid information in one or more fields. Check you information or contact your administrator</em>". </p><br><br><pre><code>{<br>    "Version": "2012-10-17";<br>    "Statement": [<br>        {<br>            "Sid": "DenyAllOutsideEU";<br>            "Effect": "Deny";<br>            "NotAction": [<br>                "cloudfront:*";<br>                "iam:*";<br>                "route53:*";<br>                "support:*"<br>            ];<br>            "Resource": "*";<br>            "Condition": {<br>                "StringNotEquals": {<br>                    "aws:RequestedRegion": [<br>                        "eu-west-1"<br>                    ]}}}]}<br></code></pre><br><br><p>But when I do <code>aws sts assume-role  --role-arn arn:aws:iam::xxxxxxxxx:role/myrole --role-session-name yyyy</code> for that user in AccountA it successfully assumes the role. <strong>So I know the <code>sts:AssumeRole</code> is not being denied.</strong></p><br><br><p>I know is this <code>DenyAllOutsideEU</code> policy that is causing it because if I add <code>sts:*</code> to the list of <code>NotAction</code> in the <code>DenyAllOutsideEu</code> policy in AccountA it <strong>will start working</strong>. </p><br><br><p>So what exactly happens after <code>sts:AssumeRole</code>; and what other permission is needed that is denied. I know I can just fixit by adding <code>sts:*</code> to the <code>DenyAllOutsideEU</code> but I want to understand exactly what is going on. I was expecting that after the successful <code>sts:AssumeRole</code> the AWS console would not do further request to AccountA so none of the permissions of user in AccountA would matter; but it's obviously not the case. </p><br><br><p><strong>Is AWS Console doing some other STS request user in AccountA towards another region after the <code>sts:AssumeRole</code>?</strong></p><br>
0.3333333333333333,0.0,0.0,0.6666666666666666,0.0,1.0,0.0,<h3>How to put item in dynamodb on the fly</h3><p>Below code will help to put the data in dynamodb</p><br><pre><code>import boto3<br><br>dynamodb = boto3.resource('dynamodb')<br>table = dynamodb.Table('employees')<br>table.put_item(Item={<br>    'emp_id': '2';<br>    'name': 'kammana';<br>    'salary': 20000<br>})<br></code></pre><br><p>I want to put_item below details on press on <code>test</code> button i need to pass</p><br><pre><code>{'emp_id': '2'; 'name': 'kammana'; 'salary': 20000}          <br></code></pre><br>
0.0,0.0,0.0,0.0,0.3333333333333333,1.0,0.0,<h3>Static Images and CSS files are not loading Through IIS</h3><p>I have developed asp.net code in which I am passing some static image and css files.<br>When i am trying my local PC through Visual Studio it's loading as expected but When I am hosting through IIS in AWS EC2; My static images and CSS files is not loading.<br>I think it seems to be Static Content issue in IIS so I enabled IIS feature for static content; still the issue is same.<br>Please Help me in this.</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Why do people use .env file on server?</h3><p>Why do people put a <code>.env</code> file to store all their secrets in a server? If someone hacks it; isn't the <code>.env</code> equally accessible as all the other files? Thanks!</p><br>
0.0,0.0,0.3333333333333333,0.0,0.6666666666666666,0.6666666666666666,0.0,<h3>ec2 instance skip terminated instances</h3><p>The script seen; it works but I noticed another issue which I would appreciate a hint how to fix.   when it comes across an instance that is terminated it barffs; it seems to complete but at the very end it barffs. not sure if that will always be the results. I am wondering if a try/except will work here of I just need to skip. here is my code; but once it hits a terminated instance I this error. I think we have a few instances that rebuild themselves on a constant bases. I assume it is barffing because it can't find the vpc and or subnet. </p><br><br><p>I am thinking I need to pass them or ignore them but im kind of lost here..</p><br><br><pre><code>EC2 State is: terminated<br>Traceback (most recent call last):<br>  File "./list_ec2_instance.py.3"; line 57; in &lt;module&gt;<br>    main()<br>  File "./list_ec2_instance.py.3"; line 53; in main<br>    list_instance()<br>  File "./list_ec2_instance.py.3"; line 37; in list_instance<br>    print("VPC Id is: {}".format(instance_id['VpcId']))<br>KeyError: 'VpcId'<br></code></pre><br><br><p>my code is:</p><br><br><pre><code>for item in response['Reservations']:<br>    #pprint(item['Instances'])<br>        print("AWS Account ID: {}".format(item['OwnerId']))<br>        for instance_id in item['Instances']:<br>            #print(instance_id)<br>            Tags = instance_id['Tags']<br>            tag_name_value = ""<br>            for tag in Tags:<br>                if tag['Key'] == "Name":<br>                    tag_name_value = tag["Value"]<br>                    break<br>            #Tags = instance_id['Tags']['Value'] <br>            State = instance_id['State']['Name']<br>            #print("EC2 Name: {}".format(Tags)) <br>            print("EC2 Name: {}".format(tag_name_value))<br>            print("Instance Id is: {}\nInstance Type is: {}".format(instance_id['InstanceId'];instance_id['InstanceType']))<br>            print("EC2 State is: {}".format(State))<br>            print("VPC Id is: {}".format(instance_id['VpcId']))<br>            print("Subnet Id is: {}".format(instance_id['SubnetId']))<br></code></pre><br>
0.0,0.0,0.0,0.0,0.3333333333333333,1.0,0.0,<h3>Base64 to Pdf export issue AWS Lambda</h3><p>I'm creating pdf document using AWS Lambda Puppeteer Nodejs package.</p><br><br><pre><code>const page = await browser.newPage()<br>page.setContent(html)<br><br>const pdf = await page.pdf({<br>  format: 'A4';<br>  printBackground: true;<br>  margin: { top: '1cm'; right: '1cm'; bottom: '1cm'; left: '1cm' }<br>})<br><br>var pdf_base64 = await pdf.toString('base64');<br><br>const response = {<br> headers: {<br>   'Content-Type': 'application/pdf';<br>   'Content-Disposition': 'attachment;filename=downloaded.pdf'<br>  };<br> statusCode: 200;<br> body: pdf_base64;<br> isBase64Encoded: true<br>}<br></code></pre><br><br><p>Pdf is getting downloaded but seems corrupted or not properly encoded. <br>While opening chrome shows <code>Failed to load PDF document.</code></p><br><br><p>But; if i remove headers and export the base 64 &amp; save it to pdf using any online service proper pdf is generated and working. </p><br><br><p>Even if i open downloaded pdf using text-editor and copy the base64 content and re-generated pdf from base64 using any online tool it works. </p><br><br><p>What can be the issue while returning.</p><br>
1.0,0.0,0.0,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>How to delete index which is older than 15 days in ElasticSearch using lambda function which is in Python</h3><p>As I am new to lambda;So Please help me with code which is in python.<br>I want to delete ElasticSearch index <code>staging-logs-yy-mm--dd</code> which is older than 15 days.</p><br>
0.0,1.0,0.0,0.0,0.3333333333333333,0.0,0.0,<h3>Failing to delete Network Interface AWS</h3><p>I am trying to delete my vpc and I first need to delete my network interface. However; those cannot be deleted because they are still in use. I terminated all my ec2 instances and still; the interfaces are in use and cannot be detached even cannot be forced detached. They are of the type efs mount target.<a href="https://i.stack.imgur.com/vO67S.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/vO67S.png" alt="enter image description here"></a><br><a href="https://i.stack.imgur.com/qXdTS.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/qXdTS.png" alt="Here is a view of the network interface"></a></p><br>
0.0,0.3333333333333333,1.0,0.0,0.6666666666666666,0.0,0.0,<h3>Attaching &quot;Application Load Balancer&quot; to &quot;Auto Scaling Group&quot; in Terraform gives error</h3><p>When one creates an ASG (Auto Scaling Group) in AWS Console there is option which can be checked &quot;receive traffic from one or more load balancers&quot;?</p><br><p>I was trying to do same using the &quot;aws_autoscaling_attachment&quot; resource; however I'm getting error below. I can see that the &quot;MyALBWP&quot; is present in the console.</p><br><blockquote><br><p>ERROR: Failure attaching AutoScaling Group MyWPReaderNodesASGroup with Elastic Load Balancer: arn:aws:elasticloadbalancing:eu-west-2:262702952852:loadbalancer/app/MyALBWP/ef1dd71d87b8742b:<br>ValidationError: Provided Load Balancers may not be valid. Please ensure they exist and try again.</p><br></blockquote><br><pre><code>resource &quot;aws_launch_configuration&quot; &quot;MyWPLC&quot; {<br>  name          = &quot;MyWPLCReaderNodes&quot;<br>  #count                = 2     Was giving error as min; max size is mentioned in ASG<br>  #name_prefix          = &quot;LC-&quot;  Error: &quot;name_prefix&quot;: conflicts with name<br>  image_id      =  aws_ami_from_instance.MyWPReaderNodes.id<br>  instance_type = &quot;t2.micro&quot;<br>  iam_instance_profile = aws_iam_instance_profile.MyWebInstanceProfile2.name # Attach S3 role to EC2 Instance<br>  security_groups    = [aws_security_group.WebDMZ.id]  # Attach WebDMZ SG<br>  user_data          = file(&quot;./AutoScaleLaunch.sh&quot;)<br>  lifecycle {<br>    #prevent_destroy       = &quot;${var.prevent_destroy}&quot;<br>    create_before_destroy = true<br>  }<br>  #   tags = {     NOT VALID GIVES ERROR<br>  #   Name = &quot;MyWPLC&quot;<br>  # }<br><br>}<br><br># # Create AutoScaling Group for Reader Nodes<br># Name: MyWPReaderNodesASGroup<br># Launch Configuration : MyWPLC<br># Group Size : 2<br># Network : Select your VPC<br># Subnets : Select your public Subnets<br># Receive traffic from Load Balancer   &lt;&lt;&lt; Tried in &quot;aws_autoscaling_attachment&quot; gives <br># Target Group : MyWPInstances<br># Health Check : ELB or EC2; Select ELB<br># Health check grace period : 60 seconds<br># tags name MyWPReaderNodesGroup<br><br>resource &quot;aws_autoscaling_group&quot; &quot;MyWPReaderNodesASGroup&quot; {<br>  name                      = &quot;MyWPReaderNodesASGroup&quot;<br>  # We want this to explicitly depend on the launch config above<br>  depends_on = [aws_launch_configuration.MyWPLC]<br>  max_size                  = 2<br>  min_size                  = 2<br>  health_check_grace_period = 60<br>  health_check_type         = &quot;ELB&quot;<br>  desired_capacity          = 2<br>  force_delete              = true<br>  launch_configuration      = aws_launch_configuration.MyWPLC.id<br>  vpc_zone_identifier       = [aws_subnet.PublicSubNet1.id; aws_subnet.PublicSubNet2.id]<br>  target_group_arns = [aws_lb_target_group.MyWPInstancesTG.arn] #  A list of aws_alb_target_group ARNs; for use with Application or Network Load Balancing.<br>  #target_group_arns = [aws_lb.MyALBWP.id] #  A list of aws_alb_target_group ARNs; for use with Application or Network Load Balancing.<br>  #error: ValidationError: Provided Target Groups may not be valid. Please ensure they exist and try again.<br>  # tags = {        NOT REQUIRED GIVES ERROR  : Error : Inappropriate value for attribute &quot;tags&quot;: set of map of string required.<br>  #   Name = &quot;MyWPReaderNodesGroup&quot;<br>  # }<br>}<br><br># Create a new load balancer attachment<br># ERROR: Failure attaching AutoScaling Group MyWPReaderNodesASGroup with Elastic Load Balancer: arn:aws:elasticloadbalancing:eu-west-2:262702952852:loadbalancer/app/MyALBWP/ef1dd71d87b8742b: <br># ValidationError: Provided Load Balancers may not be valid. Please ensure they exist and try again.<br><br>resource &quot;aws_autoscaling_attachment&quot; &quot;asg_attachment_elb&quot; {<br>  autoscaling_group_name = aws_autoscaling_group.MyWPReaderNodesASGroup.id<br>  elb                    = aws_lb.MyALBWP.id<br>}<br></code></pre><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>How to remotely launch a Juypter Notebook within AWS Sagemaker using AWS Lambda</h3><p>I have a Juypter Notebook set up within AWS Sagemaker. I wanted to find a way to launch this notebook on an autonomous trigger when a new file is uploaded to a certain folder (hence AWS Lambda). I was looking for if there was a streamlined way to trigger a Juypter Notebook with an AWS Lambda trigger.  </p><br><br><p>I have looked into using API and turning Sagemaker into and endpoint; but it didnt work.</p><br><br><p>*edit Sorry if the question was a little vague. I have allot of code written in this notebook on in Juypter. What i was ideally looking for was; when a file is uploaded to "RandomFile" then the code within the notebook will run. I was looking to do this with AWS Lambda by setting up a S3 based trigger. </p><br>
0.0,0.0,0.0,0.0,0.0,0.3333333333333333,0.6666666666666666,<h3>How can I subscribe to a MQTT topic in a Amazon AWS Lambda function?</h3><p>I am using python; I am able to publish a message to AWS IoT using the code below:</p><br><br><pre><code>import boto3<br>import json<br><br>client = boto3.client('iot-data'; region_name='us-east-1')<br><br># Change topic; qos and payload<br>response = client.publish(<br>        topic='$aws/things/pi/shadow/update';<br>        qos=1;<br>        payload=json.dumps({"foo":"bar"})<br>    )<br></code></pre><br><br><p>but there is no method like </p><br><br><blockquote><br>  <p>client.subscribe();</p><br></blockquote><br>
0.0,1.0,0.3333333333333333,0.3333333333333333,0.0,0.6666666666666666,0.0,<h3>403 OPTIONS Cors error in AWS; preflight requests</h3><p>I have deployed React JS + Spring boot app to AWS Cloud. My React JS project is stored in S3 bucket which reference is used in CloudFront. Spring boot app is deployed to Elastic Beanstalk Service.</p><br><br><p>When I try make any request to backend from CloudFront distribution I get 403 error OPTIONS Cors which is following:</p><br><br><pre><code>Access to fetch at 'https://api.divelog.eu/getuserdata/eyJhbGciOiJIUzUxMiJ9.eyJsb2dnZWRB' from origin 'https://divelog.eu' has been blocked by CORS policy: Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs; set the request's mode to 'no-cors' to fetch the resource with CORS disabled.<br></code></pre><br><br><p>I have enabled CORS at the server side in Spring Boot:</p><br><br><pre><code>@Component<br>public class CustomCorsFilter extends OncePerRequestFilter {<br><br>@Override<br>protected void doFilterInternal(HttpServletRequest request; HttpServletResponse response; FilterChain filterChain) throws ServletException; IOException {<br>    response.setHeader("Access-Control-Allow-Origin"; "*");<br>    response.setHeader("Access-Control-Allow-Methods"; "GET; POST; PATCH; PUT; DELETE; OPTIONS");<br>    response.setHeader("Access-Control-Max-Age"; "3600");<br>    response.setHeader("Access-Control-Allow-Headers"; "Origin; Content-Type; Allow; authorization; content-type; xsrf-token");<br>    response.addHeader("Access-Control-Expose-Headers"; "xsrf-token");<br>    if ("OPTIONS".equals(request.getMethod())) {<br>        response.setStatus(HttpServletResponse.SC_OK);<br>    } else {<br>        filterChain.doFilter(request; response);<br>    }<br>}<br>}<br><br>also <br><br>@Configuration<br>@EnableWebSecurity<br>public class SocialConfig extends WebSecurityConfigurerAdapter {<br><br>@Override<br>protected void configure(HttpSecurity http) throws Exception {<br>    http.cors().and()<br>            .csrf().disable()<br>            .antMatcher("/**")<br>            .authorizeRequests()<br>            .antMatchers("/"; "/login**"; "/webjars/**"; "/error**";<br>                    "/signin"; "/getuserdata/**"; "/add/marker/**"; "/get/markers/**"; "/delete/marker/**/**";<br>                    "/logout/**"; "/add/logbook/**/**"; "/get/logbook/**"; "/logbook/**/**"; "/**/**"; "/edit/logbook/**/**";<br>                    "/pdf/logbook/**/**"; "/add/topic"; "/get/topic/posts/**"; "/add/post"; "/delete/post/**/**"; "/post/**/**";<br>                    "/delete/post/file/**/**"; "/get/topic/number/comments/**/**"; "/update/topic/number/displays/**";<br>                    "/topic/likes/vote/**/**"; "/update/topic/**"; "/callback"; "/signin"; "/oauth/request_token")<br>            .permitAll()<br>            .anyRequest()<br>            .authenticated()<br><br>            .and()<br>            .addFilterBefore(new CorsFilter(); ChannelProcessingFilter.class);<br>}<br>}<br></code></pre><br><br><p>In React JS project I pass to fetch and axios those headers: Accept; Content-Type.</p><br><br><p>My CORS in AWS S3 buckets for Spring boot and React JS are set to:</p><br><br><pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;<br>&lt;CORSConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;<br>&lt;CORSRule&gt;<br>&lt;AllowedOrigin&gt;*&lt;/AllowedOrigin&gt;<br>&lt;AllowedMethod&gt;GET&lt;/AllowedMethod&gt;<br>&lt;AllowedMethod&gt;PUT&lt;/AllowedMethod&gt;<br>&lt;AllowedMethod&gt;POST&lt;/AllowedMethod&gt;<br>&lt;AllowedMethod&gt;DELETE&lt;/AllowedMethod&gt;<br>&lt;AllowedHeader&gt;*&lt;/AllowedHeader&gt;<br>&lt;/CORSRule&gt;<br>&lt;/CORSConfiguration&gt;<br></code></pre><br><br><p>Spring boot app hich is deployed to Elastic Beanstalk works fine when I try get access to endpoints via Postman; only if I try make request from S3 bucket instance to backend API I get cors error.</p><br><br><p>I have added listeners for 443 and 80 HTTP HTTPS port in load balancer of elastic beanstalk instance.</p><br><br><p>Have you got any idea why I still get this error ? </p><br><br><p><a href="https://i.stack.imgur.com/XzdrJ.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/XzdrJ.png" alt="I upload picture to request info"></a></p><br><br><ol start="2"><br><li>CloudFront Behaviors</li><br></ol><br><br><p><a href="https://i.stack.imgur.com/7mlrU.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/7mlrU.png" alt="enter image description here"></a></p><br>
0.6666666666666666,0.0,0.6666666666666666,0.0,0.0,0.6666666666666666,0.0,<h3>CI/CD pipeline for AWS Glue</h3><p>I am fairly a new user of AWS Glue; which is one of new AWS managed services to orchestrate batch job workflows with ease.</p><br><br><ul><br><li>I have 3 different AWS IAM account (Dev; Test; Prod). Separate<br>login for each account. </li><br><li>Glue; Scala Jobs are version controlled in my<br>Github repository. </li><br><li>I want to build a CI/CD pipeline to automate the<br>Test; Build and deployment of my Glue job as I commit and push in my Github repository.</li><br></ul><br><br><p>I went through number of blogs and article describing CI/CD best practices. I found a good article (specifically on CI/CD in Data Pipeline workflows) on AWS Blog <a href="https://aws.amazon.com/blogs/big-data/implement-continuous-integration-and-delivery-of-serverless-aws-glue-etl-applications-using-aws-developer-tools/" rel="noreferrer">here</a>. It is quite simple and has details of building CodePipeline using CodeCommit &amp; AWS Cloudformation. But all the phases of CI/CD initiate and ends in the same AWS IAM account. </p><br><br><p>I have two questions:</p><br><br><ol><br><li>Is separating Dev; Test and Prod account (different IAM) for building and managing<br>Data Pipelines/ Data warehouse; a good practice? </li><br><li>If yes; how would I design CI/CD pipeline using 3 different IAM account?</li><br></ol><br><br><p>Any suggestion on best practices to design CI/CD pipeline for AWS-Glue ?</p><br>
1.0,0.0,0.0,0.0,0.6666666666666666,0.0,0.0,<h3>Deploying Tensroflow Library as AWS Lambda Layer</h3><p>I am trying to deploy <strong>Tensorflow</strong> (specifically: <code>libtensorflow.so.1; libtensorflow.so.1.14.0; libtensorflow_framework.so.1; libtensorflow_framework.so.1.14.0</code>) as <strong>AWS Lambda Layer</strong>. I am getting following error:</p><br><br><pre><code>An error occurred: TensorflowLambdaLayer - Unzipped size must be smaller than 262144000 bytes (Service: AWSLambdaInternal; Status Code: 400; Error Code: InvalidParameterValueException; Request ID: d8668bfa-a61c-473c-aeb0-62bff3aa852f).<br></code></pre><br><br><p>How can I change these .so files by deleting some of their content that is irrelevant for my Lambda function which utilizes Tensorflow Lambda Layer?</p><br>
0.0,0.0,0.0,1.0,0.0,1.0,0.0,<h3>Sequelize with asynchronous configuration in nodejs</h3><p>I have been bashing my head for days as I cannot find a valid example of async configuration in Sequelize </p><br><br><p>So as you may know; you can simply config a Sequelize instance like that </p><br><br><pre><code>const sequelize = new Sequelize('postgres://user:pass@example.com:5432/dbname')<br></code></pre><br><br><p>and then declare your Model</p><br><br><pre><code>const User = sequelize.define('User'; {<br>  // Model attributes are defined here<br>  firstName: {<br>    type: DataTypes.STRING;<br>    allowNull: false<br>  };<br>  lastName: {<br>    type: DataTypes.STRING<br>    // allowNull defaults to true<br>  }<br>}; {<br>  // Other model options go here<br>});<br></code></pre><br><br><p>However what happens when the db credentials comes from an external service? </p><br><br><pre class="lang-js prettyprint-override"><code>const credentials = await getDbCredentials();<br>const sequelize = new Sequelize({credentials})<br><br><br></code></pre><br><br><p>since sequelize models creation are coupled with the instance creation (unlike many others ORMs) this becomes a big problem.</p><br><br><p>My current solution is the following:</p><br><br><pre class="lang-js prettyprint-override"><code><br>const Sequelize = require("sequelize");<br><br>// Models<br>const { User } = require("./User");<br><br>const env = process.env.NODE_ENV || "development";<br>const db = {};<br><br>let sequelize = null;<br><br>const initSequelize = async () =&gt; {<br>  if (!sequelize) {<br>      let configWithCredentials = {};<br><br>      if (env === "production") {<br>        const credentials = await getDbCredentials();<br>        const { password; username; dbname; engine; host; port } = credentials;<br>        configWithCredentials = {<br>          username;<br>          password;<br>          database: dbname;<br>          host;<br>          port;<br>          dialect: engine;<br>          operatorsAliases: 0<br>        };<br>      }<br><br>      const config = {<br>        development: {<br>          // Dev config <br>        };<br>        production: configWithCredentials;<br>      };<br><br>      sequelize = new Sequelize(config[env]);<br><br>      sequelize.authenticate().then(() =&gt; {<br>         console.log("db authenticated")<br>        });<br>      });<br>  }<br><br>  db.User = User;<br><br>  db.sequelize = sequelize;<br>  db.Sequelize = Sequelize;<br>};<br><br>initSequelize().then(() =&gt; {<br>  console.log("done");<br>});<br><br>module.exports = db;<br><br></code></pre><br><br><p>However I feel that this is not a good approach because of the asynchronous nature of the initialization and sometimes the <code>db</code> is undefined.<br>Is there a better way to approach this thing?<br>Thanks </p><br>
0.0,0.0,1.0,0.0,0.0,1.0,0.0,<h3>is it wrong/dangerous to include aws-exports.js file in source control?</h3><p>amplify auto-ignores <code>aws-exports.js</code> in <code>.gitignore</code> possibly simply because it may change frequently and is fully generated - however maybe there are also security concerns?</p><br><p>For this project my github project is private so that is not a concern; but I am wondering for future projects that could be public.</p><br><p>The reason I ask is because if I want to run my app setup/build/test through github workflows then I need this file for the build to complete properly on github machines?</p><br><p>Also I appear to need it for my amplify CI hosting to work on amplify console (I have connected my amplify console build-&gt;deploy to my github master branch and it all works perfectly but only when aws-exports.js is in source control).</p><br><p>Here is my <code>amplify.yml</code>; I am using reason-react with nextjs; and my amplify console is telling me I have connected to the correct backend:</p><br><pre><code>version: 1<br>frontend:<br>  phases:<br>    preBuild:<br>      commands:<br>        - yarn install<br>    build:<br>      commands:<br>        - yarn run build<br>  artifacts:<br>    baseDirectory: out<br>    files:<br>      - '**/*'<br>  cache:<br>    paths:<br>      - node_modules/**/*<br></code></pre><br>
0.6666666666666666,0.3333333333333333,0.0,1.0,0.0,0.0,0.0,<h3>Enabled S3Guard in CDH 5.16.2 cluster but still seeing S3 eventual consistency issues</h3><p>We were facing issues when we're writing data to S3 due to eventual consistency problem. Later found <code>S3Guard</code> helps to fix this issue.</p><br><p>We've enabled S3Guard in our cluster; but still we're seeing job failures.</p><br><p>I have verified data in DynamoDB; and it definitely writes the data; and also I see that we also have reads for DynamoDB.</p><br><p>May I know is there a way in CDH cluster to enable logs to test if every request is going through DynamoDB and log the status of DynamoDB write/read? If there is a way to enable logging may I know how to enable it and check the logs?</p><br><p>Please note that we've added S3Guard into our cluster using IAM role based authentication; with all IAM roles that required access; also added S3connectivity only for our master roles.</p><br><p><a href="https://i.stack.imgur.com/irkCx.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/irkCx.png" alt="enter image description here" /></a></p><br>
0.0,1.0,0.0,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>cdk api gateway route53 lambda custom domain name not working</h3><p>Similar questions has been made but none of them were able to help me fix the issue that I'm facing.<br>What I'm trying to do is to connect my api-gateway/lamnda function with a custom domain name and for some reason when calling the api/domain is not returning what I expected.</p><br><p><code>cdk version: 1.53.0</code></p><br><pre><code>    const lambdaFunction = new lambda.Function(this; 'LambdaApi'; {<br>      functionName: 'lambda-api';<br>      handler: 'lambda.handler';<br>      runtime: lambda.Runtime.NODEJS_12_X;<br>      code: new lambda.AssetCode(join(process.cwd(); '../api/dist'));<br>      memorySize: 128;<br>      timeout: cdk.Duration.seconds(5);<br>    })<br><br>    const zone = route53.HostedZone.fromLookup(scope; 'Zone'; {<br>     'example.com';<br>     privateZone: false;<br>    })<br><br>    const certificate = certificatemanager.Certificate.fromCertificateArn(<br>     this;<br>     'Certificate';<br>     CERT_ARN;<br>    )<br><br>    const api = new apigateway.LambdaRestApi(this; 'LambdaApiGateway'; {<br>      handler: lambdaFunction;<br>      proxy: true;<br>      endpointTypes: [apigateway.EndpointType.EDGE];<br>      defaultCorsPreflightOptions: {<br>        allowOrigins: apigateway.Cors.ALL_ORIGINS;<br>      };<br>      options: {<br>        restApiName: 'gateway-api';<br>        domainName: {<br>          domainName: 'api.example.com';<br>          certificate;<br>        };<br>        deployOptions: {<br>          stageName: 'prod';<br>          metricsEnabled: true;<br>          loggingLevel: apigateway.MethodLoggingLevel.INFO;<br>          dataTraceEnabled: true;<br>        };<br>      };<br>    })<br><br>    new route53.ARecord(this; 'CustomDomainAliasRecord'; {<br>      zone;<br>      recordName: 'api';<br>      target: route53.RecordTarget.fromAlias(new targets.ApiGateway(api));<br>    })<br></code></pre><br><p>The deployment process works fine; a ARecord is created on route53 that is pointing to the api-gateway domain name; the api mappings is created as well pointing to <code>prod</code> as specified on <code>stageName</code> but when calling the domain name it doesnt work but when calling the api-gateway endpoint it does.</p><br><p><code>api.example.com/ping</code> returns <code>healthy</code></p><br><p><code>{id}.execute-api.us-east-1.amazonaws.com/prod/ping</code> returns the current date</p><br><p>Been researching but I'm not able to find out why the <code>api.example.com/ping</code> is not working</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Add new Output and Resource to existing AWS CloudFormation Stack</h3><p>We have a previous stack with some resources and outputs; and we want to add new resources; but when we add a new environment variable that reference a new stack output and we try to deploy with serverless we found that:</p><br><br><p>serverless deploy --stage Zzzz</p><br><br><p><code>Trying to request a non exported variable from CloudFormation. Stack name: "Xxxx-Cognito-Zzzz" Requested  variable: "MyNewVariable".</code></p><br><br><p>I have the following .yml:</p><br><br><pre class="lang-yaml prettyprint-override"><code>environment:<br>    MY_NEW_ENVIRONMENT_VARIABLE: ${cf:${self:custom.serviceName}-Cognito-${self:custom.stage}.MyNewVariable}<br></code></pre><br>
0.0,0.0,0.6666666666666666,0.6666666666666666,0.3333333333333333,0.0,0.0,<h3>How add extra blockdevices to machineset in OKD4.4</h3><p>I have created a machineset YAML file with 2 EBS disks. Here is a snip of the YAML file:</p><br><br><pre><code>apiVersion: awsproviderconfig.openshift.io/v1beta1<br>      blockDevices:<br>        - ebs:<br>            iops: 0<br>            volumeSize: 120<br>            volumeType: gp2<br>        - ebs: <br>            iops: 0<br>            volumeSize: 2048<br>            volumeType: gp2<br></code></pre><br><br><p>When I run <code>oc create -f machineset.yaml</code> the instance is created in AWS but without an extra disk. So my question is: how can I add an extra disk to my machineset YAML file. </p><br>
0.0,1.0,0.0,0.0,0.0,0.0,0.0,<h3>Public/Private Subnet Architecture on AWS</h3><p>I am having trouble understanding <a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html" rel="nofollow noreferrer">this</a> image:</p><br><p><a href="https://i.stack.imgur.com/qTmrX.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/qTmrX.png" alt="enter image description here" /></a></p><br><p>Specifically what I want to have is the Global Accelerator attached to every region; with each region having a VPC and Load Balancer. Then in each AZ in a region; have a public and private subnet. The private subnet or database servers will do all the computation and rendering. The public subnet is the Load Balancer (or does the public subnet need to be a set of instances?).</p><br><p>The public subnet also has an Internet Gateway and NAT Gateway attached. The docs say a private subnet can access the internet using a NAT Gateway in the public subnet; but I don't understand why it doesn't just use the Internet Gateway.</p><br><p>I have a route table in the public subnet; and the private subnet. In the public subnet; the route table goes to <code>0.0.0.0/0</code> pointing to the internet gateway. In the private subnet; it goes to <code>0.0.0.0/0</code> pointing to the NAT gateway.</p><br><p>There are also an Elastic IP associated with the NAT Gateway; presumably so that's what the internet sees when I make a request from a private server.</p><br><p>My question is; what is connected to what? The docs aren't clear enough on a practical example. I would like to have a public subnet; which I think doesn't require having any instances (am I wrong?); only the load balancer. Then the private subnet is the computation/database subnet which <em>does</em> consist of instances which can only connect to the internet through the load balancer connected to the NAT gateway.</p><br><p>I am confused. I have read the docs over and over again but still don't see how this is supposed to be wired up. Any help explaining what is connected to what in this diagram (extending it to include the Global Accelerator) would be greatly appreciated.</p><br>
0.0,0.0,1.0,0.6666666666666666,0.0,0.0,0.0,<h3>Bucket policy to prevent bucket delete</h3><p>I am looking for a bucket policy which allows only the root account user and the bucket creator to delete the bucket. something like below. Please suggest. How to restrict to only bucket creator and root?</p><br><br><pre><code>{<br>"Version": "2012-10-17";<br>"Id": "PutObjBucketPolicy";<br>"Statement": [<br>       {<br>        "Sid": "Prevent bucket delete";<br>        "Effect": "Allow";<br>        "Principal": {<br>            "AWS": "arn:aws:iam::xxxxxxx:root"<br>        };<br>        "Action": "s3:DeleteBucket";<br>        "Resource": "arn:aws:s3:::test-bucket-s3"<br>    };<br>     {<br>        "Sid": "Prevent bucket delete";<br>        "Effect": "Deny";<br>        "Principal": *;<br>        "Action": "s3:DeleteBucket";<br>        "Resource": "arn:aws:s3:::test-bucket-s3"<br>    }<br>]<br>}<br></code></pre><br>
0.0,1.0,0.0,0.0,0.6666666666666666,0.0,0.0,<h3>Mixing EC2 and Fargate Tasks behind an Elastic Load Balancer</h3><p>I'm attempting load balance a single application that's deployed with both fargate and ec2 ecs services.   The reason being that I want to scale up with fargate briefly if necessary to cope with unexpected high loads.  I can't seem to figure out how to configure the target groups/elastic load balancer.</p><br><br><p>For each service; I've created a corresponding target group with target_type set to "ip" and "instance" respectively for the fargate and ec2 service.</p><br><br><p>I'm not sure how to have the load balancer forward traffic to both of these target groups equally.</p><br>
0.0,0.0,0.0,1.0,1.0,0.0,0.0,<h3>AWS EC2 Lost Data after attaching volume</h3><p>I'm sort of new to AWS EC2 instances; but also comfortable.</p><br><br><p><strong>16.04 Ubuntu Linux ec2 instance</strong></p><br><br><p>I attempted to attach an extra harddrive volume to an existing ec2 instance. The initial default volume of 8 gigs was not enough to satisfy my needs. </p><br><br><p>I created the new 500gig volume and attached it to my instance; however it became the boot/root volume; so I detached and modified volume to a different device name <code>/dev/sdf</code> and reattached it keeping the instance's <code>/dev/sda1</code> drive I <em>had</em> data on.</p><br><br><p>Upon restarting; the data in my initial <code>/dev/sda1</code> was reverted back to it's initial snapshot data when the volume was created ( AMI image ); instead of still holding the data that was on it - which is what I was expecting it to do. </p><br><br><p>Is there anything I can do get that data back? Where is it? No popup or warning said that any data was going to be erased to the volume's initial snapshot and my backup couldn't catch it before hand. The <code>/dev/sda1</code> was never detached or touched in this process; but the data is gone.</p><br><br><p>What happened? Where is my data?</p><br>
0.0,0.0,1.0,0.6666666666666666,0.0,0.3333333333333333,0.0,<h3>upload file to s3 from local using AWS CLI without hard-coded credentials(access id and secret access key)</h3><p>My requirement to upload file from local to s3 using aws cli  but don't want to use access ID and secret access key while running in command line.</p><br><br><p>Any suggestions!</p><br>
0.0,0.3333333333333333,1.0,0.0,0.0,0.0,0.0,<h3>Is it possible to Store AWS API gateway API keys in secret manager and rotate them</h3><p>I want to create multiple API keys and distribute among the customers; Once I create an API key and attach it to usage plan I want to save it to secrets manager; Can secret manager apply the rotation policy to the API keys in API gateway or we need to develop our own mechanism for it?<br>I see it can apply rotation policy to AWS database credentials but didn't see the same for AWS API keys.</p><br>
0.0,0.3333333333333333,0.3333333333333333,0.0,0.0,0.6666666666666666,0.3333333333333333,<h3>Setting &quot;SameSite=Lax&quot; value with my cookie</h3><p>I just got a warning in Chrome that the way I've been setting a cookie needs to be updated with the "SameSite" Attribute. I found this php page here: <a href="https://wiki.php.net/rfc/same-site-cookie" rel="nofollow noreferrer">https://wiki.php.net/rfc/same-site-cookie</a></p><br><br><p>Based on this; I have updated my setCookie to:</p><br><br><pre><code>setcookie("foo"; $res["ID"] . "|" . $_POST["bar"]; time() + (86400 * 30); "/"; $_SERVER['HTTP_HOST']; false; false; "Lax"); // 86400 = 1 day<br></code></pre><br><br><p>from</p><br><br><pre><code>setcookie("foo"; $res["ID"] . "|" . $_POST["bar"]; time() + (86400 * 30); "/"); // 86400 = 1 day<br></code></pre><br><br><p>My lamp project is hosted on AWS; but I also test on my local box. </p><br><br><p>My question are:</p><br><br><p>(a) have I done this correctly? I would normally test this before coming here; but in this case testing means deploying to AWS and debugging from there; which is tedious and makes my site potentially unstable.</p><br><br><p>(b) In PHP; is there any way to just set SameSite="Lax" while keeping the defaults for domain; secure; and httponly parameters?</p><br>
0.0,0.0,0.0,0.0,1.0,0.6666666666666666,0.0,<h3>Implement lambda layer in lambda function using .net</h3><p>how can I call lambda layers within lambda function  in .net Run time?</p><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.0,<h3>AWS ECS Auto Scaling for Windows Containers</h3><p>We have couple of (&gt; 5) windows framework 4.8 .Net MVC Web application workloads hosted in an Multi-AZ ECS cluster (EC2 type :<strong>Windows</strong>) exposed outside by an ALB. All of those applications are  working fine for quite a period. Now its required to introduce auto scaling for these applications selectively (Out of 5; 3 needs scaling out/in) .<br>We are thinking of leveraging below two features <strong>together</strong> to achieve this .</p><br><ol><br><li><p><strong>ECS Service auto scaling</strong> to scale up each <strong>container instances</strong>(task level).</p><br></li><br><li><p><strong>ECS cluster auto scaling using ECS capacity providers</strong> in an <strong>EC2 Instance</strong> level. Which provides space for containers spun up by task in Step 1.</p><br></li><br></ol><br><p><strong>My question is</strong> ;is this achievable?; or is this the right approach for <strong>Windows Containers</strong>? . Why I am stressing Windows container is because AWS ECS lack many feature compared to Linux containers ; for example We can't set Container memory Soft limit(memory reservation) but should mention a hard limit(memory) while configuring the task itself; which I think is a major limitation  .</p><br><p>if this is not achievable what are the options ?  We are not in a position to move to EKS now and obviously there is no Windows support for Fargate.</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>How to monitor whether a remote process has crashed?</h3><p>I have a large number of instances across multiple cloud providers. Each of them is running a single Java program. I want to check that all of these Java programs are running and haven't crashed; and if/when one of them crashes; I want to be alerted about it.</p><br><br><p>At the moment I have a hacked-together solution that I run from my local computer; which will loop through an array of all the IP addresses; and send a command through SSH to each of them to check <code>ps -ef</code> and count the number of Java processes are running. If that number is zero then I will popup something on my screen to alert me.</p><br><br><p>Is there a better solution? Ideally I could use a Zabbix-style tool to handle it for me but I don't know if anything exists that serves this need.</p><br>
0.0,0.6666666666666666,0.0,0.0,1.0,0.0,0.0,<h3>Will an Amazon EC2 instance automatically refresh its own IP at some time interval?</h3><p>A running EC2 instance will automatically change its old public ip to the new public ip at some point of interval.  Is that true??</p><br><br><p>If so how many days it will take to reflect the new ip in our instance. Can some one pls explain.</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>How create a file in S3 without copy</h3><p>I want create a file in AWS S3; I dont want do a copy; I want create a file in S3; this is possible? I search on internet and not found the answer. I want do this use Docker Criu</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>React.js Read User Info from AWS and set variable state</h3><p>I would like to read user information and set Admin variable to yes if it's custom attributes is equal to 1.<br>The problem is that when I want to read user attributes the program crashes with TypeError: Cannot read property attributes of undefined"</p><br><br><p>This is my code:</p><br><br><pre><code>import React; { useState; useEffect } from "react";<br>import './App.css';<br>import { Link; withRouter } from "react-router-dom";<br>import { Nav; Navbar; NavItem; Image } from "react-bootstrap";<br>import { LinkContainer } from "react-router-bootstrap";<br>import Routes from "./Routes";<br>import { Auth } from "aws-amplify";<br>import logo from './img/download.svg';<br><br><br>function App(props) {<br>    const [isAuthenticating; setIsAuthenticating] = useState(true);<br>    const [isAuthenticated; userHasAuthenticated] = useState(false);<br>    const [userData; setUserData] = useState([]);<br>    const [isAdmin; setIsAdmin] = useState(true);<br><br>    useEffect(() =&gt; {<br>      onLoad();<br>    }; [];);<br><br>    async function handleLogout() {<br>      await Auth.signOut();<br>      userHasAuthenticated(false);<br>      setIsAdmin(false);<br>      props.history.push("/login");<br>    }<br><br>    async function onLoad() {<br>      try {<br>        await Auth.currentSession();<br>        userHasAuthenticated(true);<br>        await Auth.currentUserInfo().then(user =&gt; setUserData(user));<br>        if (userData.attributes['custom:isAdmin'] === "1" ) {<br>          setIsAdmin(true)<br>        }<br>      }<br>      catch(e) {<br>        if (e !== 'No current user') {<br>        alert(e);<br>      }<br>    }<br><br>    setIsAuthenticating(false);<br>  }<br><br>return (...)<br></code></pre><br>
0.0,0.0,0.0,0.3333333333333333,0.6666666666666666,1.0,0.0,<h3>S3Client and Quarkus Native App Issueu with Runn</h3><p>I am trying to create a lambda S3 listener leveraging Lambda as a native image. The point is to get the S3 event and then do some work by pulling the file; etc. To get the file I am using het AWS 2.x S3 client as below</p><br><br><pre><code>S3Client.builder().httpClient().build();<br></code></pre><br><br><p>This code results in </p><br><br><pre><code>2020-03-12 19:45:06;205 ERROR [io.qua.ama.lam.run.AmazonLambdaRecorder] (Lambda Thread) Failed to run lambda: software.amazon.awssdk.core.exception.SdkClientException: Unable to load an HTTP implementation from any provider in the chain. You must declare a dependency on an appropriate HTTP implementation or pass in an SdkHttpClient explicitly to the client builder.<br></code></pre><br><br><p>To resolve this I added the aws apache client and updated the code to do the following:</p><br><br><ol><br><li><code>SdkHttpClient httpClient = ApacheHttpClient.builder().<br>            maxConnections(50).<br>            build()</code></li><br><li><code>S3Client.builder().httpClient(httpClient).build();</code></li><br><li>I also had to add:<br><code>[<br>["org.apache.http.conn.HttpClientConnectionManager"; <br>"org.apache.http.pool.ConnPoolControl";"software.amazon.awssdk.http.apache.internal.conn.Wrapped"]<br>]</code></li><br></ol><br><br><p>After this I am now getting the following stack trace:</p><br><br><pre><code>Caused by: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty<br>at java.security.cert.PKIXParameters.setTrustAnchors(PKIXParameters.java:200)<br>at java.security.cert.PKIXParameters.&lt;init&gt;(PKIXParameters.java:120)<br>at java.security.cert.PKIXBuilderParameters.&lt;init&gt;(PKIXBuilderParameters.java:104)<br>at sun.security.validator.PKIXValidator.&lt;init&gt;(PKIXValidator.java:86)<br>... 76 more<br></code></pre><br><br><p>I am running version 1.2.0 of qurkaus on 19.3.1 of graal. I am building this via Maven and the the provided docker container for Quarkus. I thought the trust store was added by default (in the build command it looks to be accurate) but am I missing something? Is there another way to get this to run without the setting of the HttpService on the S3 client?</p><br>
0.0,0.0,0.0,0.0,0.0,0.6666666666666666,0.3333333333333333,<h3>Select2 is not working in Amazon Web Services (AWS)</h3><p>I have used select2 in my mvc web application. But problem is; select2 is working fine on localhost but when uploaded on AWS; it does not work. I tried to give references of online cdn too but it also never worked for me. When I open inspect; I see select2 js has worked fine but select2 css is not working to load select2 design. </p><br>
0.0,1.0,0.0,0.0,0.3333333333333333,0.0,0.0,<h3>AWS ALB seems to be slower than CLB</h3><p>We are trying to attain performance improvement in our current API.</p><br><br><p>Which is in a EC2 container with Flask Restful API on NGINX reverse proxy with Gunicorn.</p><br><br><p>As tested with Chrome; It seems that TTFB is fluctuating from 200+ms to 500+ms.</p><br><br><p>We don't noticed this in CLB by the way.</p><br><br><p>Now anyone has made some configuration updates on this? would love to hear others thoughts on this</p><br>
0.0,0.0,0.0,1.0,0.0,0.6666666666666666,0.0,<h3>How to create a java OutputStream for an S3 object and write value to it?</h3><p>Existing ways of adding content to an S3 file using methods in AmazonS3 class are</p><br><br><ul><br><li>by putObject with an InputStream</li><br><li>Creating a local file with content and uploading it to S3.</li><br></ul><br><br><p>Is there a way an OutputStream can be created for an existing S3 object to which values from a list can be written into? I see there are no APIs for doing so.</p><br>
0.0,0.0,0.0,0.0,0.0,0.0,1.0,<h3>Use For and save and saa in diferent files</h3><p>I'm trying to take 2 files from one command; in one file I only put 1 entries and the other complete a list; this is the example:</p><br><br><p>I tried various commands</p><br><br><pre><code>#!/bin/bash<br>    for i in range 4<br>     do<br>      echo "test" &gt;one  &gt;&gt;list<br>    done <br></code></pre><br><br><p>I need what in the "one" save the last one loop and in the "list" everyone.</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Cloudformation: How to pass a string through cloudformation script into UserData?</h3><p>Cloudformation command:</p><br><p><code>- aws cloudformation create-stack --stack-name stackName --template-body file://cloudformation.yaml --parameters ParameterKey=keyName;ParameterValue=myKeys</code></p><br><p>I need to paste env variable into UserData cloudformation sctipt. So at the end it would have             docker pull quay.io/apple/myRepo:myVarName</p><br><pre><code>UserData:<br>      Fn::Base64: !Sub |<br>        docker pull quay.io/apple/myRepo:master<br></code></pre><br>
1.0,0.0,0.3333333333333333,0.0,0.0,0.0,0.0,<h3>Deleting all Transcribe Jobs in one CLI Command for AWS</h3><p>I am trying to delete all of my AWS Transcribe jobs at the same time. I know I can go through and delete them one by one through the console; and I can also delete them all through the CLI through the following command:</p><br><br><pre><code>$ aws transcribe delete-transcription-job --transcription-job-name YOUR_JOB_NAME<br></code></pre><br><br><p>The issue with this is that I have to do this for each individual job! I am dealing with them on a mass scale (about 1000 jobs). I have tried the following code; however this does not work:</p><br><br><pre><code>for jobName in ${aws transcribe list-transcription-jobs --query '[TranscriptionJobSummaries[*].TranscriptionJobName]' --output text}; do aws delete-transcription-job --transcription-job-name $jobName<br></code></pre><br><br><p>When I run this code; it does nothing. Any ideas how to fix this?</p><br>
0.0,0.6666666666666666,0.3333333333333333,0.0,0.0,1.0,0.0,<h3>Working example of AWS API Gateway integration with SNS; including MessageAttributes</h3><p>I have a working API Gateway integration with SNS; including CloudWatch logging for the API Gateway (which took some figuring out). The last piece I need now is to get the SNS messages to include <code>MessageAttributes</code>. Hunting for a complete example in AWS documentation seems difficult/ if not impossible... Could someone please share a working example please?</p><br><p>Here is what I have so far:</p><br><pre><code>  SfdcPlatformEventsApi:<br>    Type: AWS::Serverless::Api<br>    DependsOn:<br>      - SfdcPlatformEventsApiLogGroup<br>    Properties:<br>      Name: !Sub '${EnvironmentName}-${ApplicationName}-api'<br>      StageName: !Ref EnvironmentName<br>      AccessLogSetting:<br>        DestinationArn: !GetAtt SfdcPlatformEventsApiLogGroup.Arn<br>        # 'requestId' is useless... but it is required for AccessLogSetting<br>        Format: '{<br>                     &quot;request_id&quot;: &quot;$context.requestId&quot;;<br>                     &quot;api_id&quot;: &quot;$context.apiId&quot;;<br>                     &quot;domain&quot;: &quot;$context.domainPrefix&quot;<br>                     &quot;path&quot;: &quot;$context.path&quot;;<br>                     &quot;param&quot;: &quot;$integration.request&quot;;<br>                     &quot;http_method&quot;: &quot;$context.httpMethod&quot;;<br>                     &quot;source_ip&quot;: &quot;$context.identity.sourceIp&quot;;<br>                     &quot;user-agent&quot;: &quot;$context.identity.userAgent&quot;;<br>                     &quot;api_key&quot;: &quot;$context.identity.apiKey&quot;;<br>                     &quot;status&quot;: &quot;$context.status&quot;;<br>                     &quot;responseLatency&quot;: &quot;$context.responseLatency&quot;;<br>                     &quot;requestTime&quot;: &quot;$context.requestTime&quot;;<br>                     &quot;wafResponseCode&quot;: &quot;$context.wafResponseCode&quot;;<br>                     &quot;xrayTraceId&quot;: &quot;$context.xrayTraceId&quot;;<br>                     &quot;error_message&quot;: &quot;$context.error.message&quot;;<br>                     &quot;error.validationErrorString&quot;: &quot;$context.error.validationErrorString&quot;<br>                 }'<br>      MethodSettings:<br>        - DataTraceEnabled: true<br>          HttpMethod: '*'<br>          LoggingLevel: INFO<br>          ResourcePath: '/*'<br>          MetricsEnabled: true<br>      # enable xRay tracing<br>      TracingEnabled: true<br>      DefinitionBody:<br>        swagger: 2.0<br>        info:<br>          title: !Sub '${EnvironmentName}-${ApplicationName}-api'<br>        paths:<br>          /{proxy+}:<br>            post:<br>              responses:<br>                '202':<br>                  description: 'Published'<br>              x-amazon-apigateway-integration:<br>                type: aws<br>                httpMethod: POST<br>                uri: !Sub 'arn:aws:apigateway:${AWS::Region}:sns:action/Publish'<br>                credentials: !GetAtt SfdcPlatformEventsApiRole.Arn<br>                parameters:<br>                  name: proxy<br>                  in: path<br>                  required: true<br>                  type: string<br>                requestParameters:<br>                  method.request.path.proxy: true<br>                  integration.request.querystring.Type: 'method.request.body.attributes.type'<br>                  integration.request.querystring.MessageAttributes: 'method.request.body.attributes'<br>                  integration.request.querystring.Message: 'method.request.body'<br>                  integration.request.querystring.TopicArn: !Sub &quot;'${SfdcPlatformEventsSnsQueue}'&quot; #It looks funky here with the double and single quotes!! But it needs it!<br>                responses:<br>                  default:<br>                    statusCode: 202<br>              security:<br>                - API_KEY: []<br>              x-amazon-apigateway-api-key-source: HEADER<br>              x-amazon-apigateway-gateway-responses:<br>                ACCESS_DENIED:<br>                  statusCode: 403<br>                  responseTemplates:<br>                    application/json: !Ref AccessDeniedMsg<br>        securityDefinitions:<br>          API_KEY:<br>            type: 'apiKey'<br>            name: 'x-api-key'<br>            in: 'header'<br></code></pre><br>
0.0,0.0,1.0,0.3333333333333333,0.3333333333333333,1.0,0.0,<h3>Issue reading CloudWatch Logs in .gz format after create_export_task in S3</h3><p>Created a Lambda to export the logs from CloudWatch on a schedule CloudWatch Event; that&#39;s all fine.</p><br><br><p>So the CloudWatch logs are now sat my S3 bucket in .gz format (that&#39;s what CloudWatch saves them as in S3 from create_export_task)</p><br><br><p>I now want to itirate that bucket and read the .gz files and convert the logs from JSON to CSV (don't ask) but the problem i have that any which way I try to read the files I keep getting the below result.</p><br><br><pre><code>{<br>  "errorMessage": "Not a gzipped file ('Pe')";<br>  "errorType": "BadGzipFile";<br>  "stackTrace": [<br>    "  File \"/var/task/lambda_function.py\"; line 28; in lambda_handler\n    content = gzipfile.read()\n";<br>    "  File \"/var/lang/lib/python3.8/gzip.py\"; line 292; in read\n    return self._buffer.read(size)\n";<br>    "  File \"/var/lang/lib/python3.8/gzip.py\"; line 479; in read\n    if not self._read_gzip_header():\n";<br>    "  File \"/var/lang/lib/python3.8/gzip.py\"; line 427; in _read_gzip_header\n    raise BadGzipFile(&amp;amp;#39;Not a gzipped file (%r)&amp;amp;#39; % magic)\n"<br>  ]<br>}<br></code></pre><br><br><p>Below is what i have been trying Opt 1; 2 &amp; 3 all return the same result above.</p><br><br><pre><code>from io import BytesIO<br>import json<br>import gzip<br>import csv<br>import datetime<br>import base64<br>import boto3<br><br><br>def lambda_handler(event; context):<br><br>    logBucketName = event['s3BucketName']<br>    logFolderName = event['logFolderName'] + '/'<br><br>    s3 = boto3.resource('s3')<br>    my_bucket = s3.Bucket(logBucketName)<br>    client = boto3.client('s3')<br><br>    for object_summary in my_bucket.objects.filter(Prefix= logFolderName; MaxKeys=1):<br><br>        obj = s3.Object(object_summary.bucket_name; object_summary.key)<br><br>        # opt 1<br>        content = gzip.GzipFile(fileobj=obj.get()['Body']).read()<br>        print(content)<br><br>        # opt 2<br>        with gzip.GzipFile(fileobj=obj.get()["Body"]) as gzipfile:<br>            content = gzipfile.read()<br>        print(content)<br><br>        # opt3<br>        bytes = io.BytesIO(obj.get()['Body'].read())<br>        gzipfile = gzip.GzipFile(fileobj=bytes)<br>        content = gzipfile.read()        <br>        print(content)<br></code></pre><br><br><p>In the AWS document on Subscription Filters it states that </p><br><br><blockquote><br>  <p>The <strong>Data</strong> attribute in the Lambda record is Base64 encoded and<br>  compressed with the gzip format. The actual payload that Lambda<br>  receives is in the following format { "awslogs": {"data":<br>  "BASE64ENCODED_GZIP_COMPRESSED_DATA"} }</p><br></blockquote><br><br><p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#LambdaFunctionExample" rel="nofollow noreferrer">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#LambdaFunctionExample</a></p><br><br><p>However I don't beleve this to be the case in CloudWatch logs that have been exported to S3 using the create_export_task as in S3 there are lots of .gz files and if you download and decompress the log file is readable without and Base64 decoding so that appears to be just for streamed logs.</p><br><br><p>Has anyone already solved this problem and share their advise?</p><br>
0.0,0.3333333333333333,0.3333333333333333,0.0,1.0,0.0,0.0,<h3>Connection failure between services on kubernetes worker nodes</h3><p>I have Nodejs services which run on AWS EKS cluster. The cluster has two worker nodes. service connects with another service. </p><br><br><p>Problem is; sometimes when I re-create the deployment; service fails to connect with another service; where it runs on another worker node. </p><br><br><p>EX: service 1 runs on worker node 1; service 2 runs on worker node 2. if service 1 runs as service1:3001(internal DNS) and when service 2 tries to connect that service; it fails</p><br><br><p>What I did to solve this time to time;</p><br><br><p><strong>Solution 1:</strong> Update AWS Control-Plane</p><br><br><pre><code>eksctl utils update-kube-proxy --name acceptance --approve<br>eksctl utils update-aws-node --name acceptance --approve<br>eksctl utils update-coredns --name acceptance --approve<br>eksctl update cluster --name acceptance --approve<br></code></pre><br><br><p><strong>Note:</strong> <code>I did this once; when solution 2 and 3 did not work</code></p><br><br><p><strong>Solution 2:</strong> Delete coredns pods and let start themself.</p><br><br><pre><code>kubectl delete po coredns-workernode-1 coredns-workernode-2; <br></code></pre><br><br><p>** Note:** <code>I only do this if solution 3 does not work</code></p><br><br><p><strong>Solution 3:</strong> <br>Restarted the service again after I re-create the deployment.</p><br><br><p>These are solutions that I did to solve this connection failure between services.</p><br><br><p><strong>Note:</strong> This does not happen when services run on the same worker node.</p><br><br><p><strong>Moreinfo</strong>:</p><br><br><blockquote><br>  <p>worker node AMI: ami-0b7127e7a2a38802a<br><br>  EC2 Type: t2.medium<br><br>  Kubernetes verson : 1.13.10</p><br></blockquote><br><br><p>Seems those are temporary solutions and seems CoreDNS doesn't work properly. No more troubleshooting ideas to solve this permanently.   </p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>HIVE_BAD_DATA: Wrong type using parquets on AWS Athena</h3><p>I've created a Glue Crawler to read files from S3 and create a table for each S3 path. The table health_users were created using a wrong type for a specific column: the column two_factor_auth_enabled were created as int instead of string.</p><br><br><p>Manually; I went to Glue Catalog and updated the schema of table health_users.</p><br><br><p>After that; I tried to run the query again on Athena and it still throwing the same error:</p><br><br><blockquote><br>  <p>Your query has the following error(s):</p><br>  <br>  <p>HIVE_BAD_DATA: Field two_factor_auth_enabled's type BOOLEAN in parquet is incompatible with type int defined in table schema<br>  This query ran against the "test_parquets" database; unless qualified by the query. Please post the error message on our forum or contact customer support with Query Id: c3a86b98-70a2-4c70-97d8-8bc377c455b8. </p><br></blockquote><br><br><p>I've checked the table structure on Athena and the column two_factor_auth_enabled is a string (the file attached shows table definition):</p><br><br><p><a href="https://i.stack.imgur.com/6wIl5.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/6wIl5.png" alt="enter image description here"></a></p><br><br><p>What's wrong with my solution? How can I fix this error? </p><br>
0.3333333333333333,0.0,0.0,0.0,0.0,1.0,0.0,<h3>Need to remove array in JSON object</h3><p>My json object is: <code>{"values": {"empid": 20000; "empName": "Sourav"; "empSal": 8200}}</code><br>But I want to remove <code>"Values: "</code>. How can I do this? I have written a code in Python.<br>In the background It is taking the streaming data from MySQL and sending to Kinesis.</p><br><br><pre><code>def main():<br>  connection = {<br>    "host": "127.0.0.1";<br>    "port": int(sys.argv[1]);<br>    "user": str(sys.argv[2]);<br>    "passwd": str(sys.argv[3])}<br>  kinesis = boto3.client("kinesis";region_name='ap-south-1')<br>  stream = BinLogStreamReader(<br>            connection_settings=connection;<br>            only_events=[DeleteRowsEvent; WriteRowsEvent; UpdateRowsEvent];<br>            server_id=100;<br>            blocking=True;<br>            log_file='mysql-bin.000003';<br>            resume_stream=True;<br>        )<br>  for binlogevent in stream:<br>    for row in binlogevent.rows:<br>      print (json.dumps(row;cls=DateTimeEncoder)) <br>      kinesis.put_record(StreamName=str(sys.argv[4]); Data=json.dumps(row;cls=DateTimeEncoder); <br>                         PartitionKey="default";)<br></code></pre><br>
0.0,0.3333333333333333,1.0,0.3333333333333333,0.0,0.0,0.0,<h3>How should I set up my security groups so that hackers will not infiltrate so easily</h3><p>So I have an EC2 instance on AWS and it runs Mongodb. I have been having issues with hackers for a few months now and I can't seem to figure out how to keep them out. Luckily; I don't have anything important on there. </p><br><br><p>I did notice that my security group on AWS is basically open to all. For example; my inbound rules:</p><br><br><p>Port 80; tcp; 0.0.0.0/0<br>Port 8080; tcp; 0.0.0.0/0<br>Port all; tcp; 0.0.0.0/0<br>Port 22; tcp; 0.0.0.0/0<br>Port 27017; tcp; 0.0.0.0/0<br>Port 443; tcp; 0.0.0.0/0</p><br><br><p>If i change the source is there a convention I should follow? How should I set the source? I am new to this as I did not set up my security groups. Just trying to figure out how I can keep out the hackers. They have been going at it for a while now.</p><br>
0.0,1.0,0.3333333333333333,0.0,0.3333333333333333,0.0,0.0,<h3>Request Timeout ( HTTP 408) : loadbalancer backed by ecs-fargate with nginx image</h3><p>I am trying to create following infrastructure using terraform.<br><em><strong>LoadBalancer -&gt; ECS-Service -&gt; Fargate (nginx images; count=2)</strong></em></p><br><p>After applying terraform plan; I can see that a target groups shows two healthy targets. But when i try to access loadbalancer dns name from browser; I am getting request-timeout. Ping is also not working for lb dns name.</p><br><p>Loadbalancer is an non-internal application loadbalancer with security-group allowing all traffic on 80 port to all IPv4.</p><br><p>Need help.</p><br>
0.0,0.0,0.6666666666666666,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>How to update PHP versions with Yum on AWS Linux / CentOS</h3><p>I have a web server with PHP 7.3 installed (And Apache + Wordpress); plus a number of PHP related modules. This server is on AWS Linux. </p><br><br><p>When the AWS repo makes PHP 7.4 available; what's the best way to upgrade the PHP 7.3 packages to 7.4 and preserve my PHP configs? I'm assuming that <code>sudo yum upgrade</code> isn't going to cut it. Will I need to uninstall php73-* and then install php74-*?</p><br><br><p>By way of showing what I have:</p><br><br><pre><code>yum list installed *php*<br>Loaded plugins: priorities; update-motd; upgrade-helper<br>Installed Packages<br>php73.x86_64                       7.3.17-1.25.amzn1               @amzn-updates<br>php73-cli.x86_64                   7.3.17-1.25.amzn1               @amzn-updates<br>php73-common.x86_64                7.3.17-1.25.amzn1               @amzn-updates<br>php73-gd.x86_64                    7.3.17-1.25.amzn1               @amzn-updates<br>php73-json.x86_64                  7.3.17-1.25.amzn1               @amzn-updates<br>php73-mbstring.x86_64              7.3.17-1.25.amzn1               @amzn-updates<br>php73-mysqlnd.x86_64               7.3.17-1.25.amzn1               @amzn-updates<br>php73-pdo.x86_64                   7.3.17-1.25.amzn1               @amzn-updates<br>php73-process.x86_64               7.3.17-1.25.amzn1               @amzn-updates<br>php73-xml.x86_64                   7.3.17-1.25.amzn1               @amzn-updates<br></code></pre><br>
0.0,0.0,0.0,0.6666666666666666,0.0,1.0,0.0,<h3>Errors while putting object on S3 bucket</h3><p>I am trying to get temperature value from DynamoDB table and calculating the average of that values and sending average value to S3 bucket. I calculated the Average value but when putting that value to s3 bucket I got the error:</p><br><br><p>Response:</p><br><br><pre><code>{<br>  "errorMessage": "Syntax error in module 'lambda_function': positional argument follows keyword argument (lambda_function.py; line 45)";<br>  "errorType": "Runtime.UserCodeSyntaxError";<br>  "stackTrace": [<br>    "  File \"/var/task/lambda_function.py\" Line 45\n        s3.put_object(Bucket='mys3pooja'; Key='time'; Body=val)\n"<br>  ]<br>}<br></code></pre><br>
0.0,0.0,0.6666666666666666,0.6666666666666666,0.0,0.0,0.0,<h3>Access Denied while trying to create AWS S3 Bucket</h3><p>I'm just starting to learn AWS am trying to create an S3 bucket through the 'Get Started' button on the S3 console. After naming my bucket; selecting the region; and altering nothing on the 'configure options' and 'set permissions' pages; when I go to create the bucket I get an error saying 'Access Denied.' </p><br><br><p>How can I go about fixing this issue?</p><br>
0.0,1.0,0.0,0.0,0.6666666666666666,0.0,0.0,<h3>Handle high load of websocket traffic with AWS ALB</h3><p>I have a cluster of socket servers on AWS; with an ALB in front of them. We are using socket.io in the code.</p><br><p>Recently; the load of the system got very high; causing servers to crash (AWS auto scale group was replacing them). I've learned that due to the nature of websocket connections; the ALB cannot distribute the traffic evenly like with HTTP.</p><br><p>I am looking for ideas to improve this situation - perhaps there are ways to even the load; or maybe use a logic that rejects new connections above some threshold; signaling the ALB to send the connection to another server.</p><br><p>Would appreciate any advise</p><br>
0.0,0.6666666666666666,0.6666666666666666,0.0,0.3333333333333333,0.6666666666666666,0.0,<h3>AWS API Gateway and static HTML: &quot;Execution failed due to configuration error: statusCode should be an integer which defined in request template&quot;</h3><p>I am trying to serve a static content using AWS API Gateway.<br>When I attempt to invoke the URL; both from the test page and from <code>curl</code>; I get the error:  </p><br><br><blockquote><br>  <p>"Execution failed due to configuration error: statusCode should be an integer which defined in request template".</p><br></blockquote><br><br><p>This is my configuration on Terraform:</p><br><br><pre><code>resource "aws_api_gateway_rest_api" "raspberry_api" {<br>  name        = "raspberry_api"<br>}<br><br>resource "aws_acm_certificate" "raspberry_alexa_mirko_io" {<br>  domain_name       = "raspberry.alexa.mirko.io"<br>  validation_method = "DNS"<br><br>  lifecycle {<br>    create_before_destroy = true<br>  }<br>}<br><br>resource "aws_route53_record" "raspberry_alexa_mirko_io_cert_validation" {<br>  name    = aws_acm_certificate.raspberry_alexa_mirko_io.domain_validation_options.0.resource_record_name<br>  type    = aws_acm_certificate.raspberry_alexa_mirko_io.domain_validation_options.0.resource_record_type<br>  zone_id = var.route53_zone_id<br>  records = [aws_acm_certificate.raspberry_alexa_mirko_io.domain_validation_options.0.resource_record_value]<br>  ttl     = 60<br>}<br><br>resource "aws_route53_record" "raspberry_alexa_mirko_io" {<br>  zone_id = var.route53_zone_id<br>  name = aws_acm_certificate.raspberry_alexa_mirko_io.domain_name<br>  type = "A"<br>  alias {<br>    name = aws_api_gateway_domain_name.raspberry_alexa_mirko_io.cloudfront_domain_name<br>    zone_id = aws_api_gateway_domain_name.raspberry_alexa_mirko_io.cloudfront_zone_id<br>    evaluate_target_health = true<br>  }<br>}<br><br>resource "aws_acm_certificate_validation" "raspberry_alexa_mirko_io" {<br>  certificate_arn         = aws_acm_certificate.raspberry_alexa_mirko_io.arn<br>  validation_record_fqdns = [aws_route53_record.raspberry_alexa_mirko_io_cert_validation.fqdn]<br>  provider = aws.useast1<br>}<br><br>resource "aws_api_gateway_domain_name" "raspberry_alexa_mirko_io" {<br>  certificate_arn = aws_acm_certificate_validation.raspberry_alexa_mirko_io.certificate_arn<br>  domain_name     = aws_acm_certificate.raspberry_alexa_mirko_io.domain_name<br>}<br><br>resource "aws_api_gateway_base_path_mapping" "raspberry_alexa_mirko_io_base_path_mapping" {<br>  api_id      = aws_api_gateway_rest_api.raspberry_api.id<br>  domain_name = aws_api_gateway_domain_name.raspberry_alexa_mirko_io.domain_name<br>}<br><br>resource "aws_api_gateway_resource" "home" {<br>  rest_api_id = aws_api_gateway_rest_api.raspberry_api.id<br>  parent_id   = aws_api_gateway_rest_api.raspberry_api.root_resource_id<br>  path_part   = "login"<br>}<br><br>resource "aws_api_gateway_method" "login" {<br>  rest_api_id   = aws_api_gateway_rest_api.raspberry_api.id<br>  resource_id   = aws_api_gateway_resource.home.id<br>  http_method   = "GET"<br>  authorization = "NONE"<br>}<br><br>resource "aws_api_gateway_integration" "integration" {<br>  rest_api_id             = aws_api_gateway_rest_api.raspberry_api.id<br>  resource_id             = aws_api_gateway_resource.subscribe_raspberry.id<br>  http_method             = aws_api_gateway_method.subscribe.http_method<br>  integration_http_method = "POST"<br>  type                    = "AWS_PROXY"<br>  uri                     = aws_lambda_function.raspberry_lambda.invoke_arn<br>  # This was just a failed attempt. It did not fix anything<br>  request_templates = {<br>    "text/html" = "{\"statusCode\": 200}"<br>  }<br>}<br><br>resource "aws_api_gateway_integration" "login_page" {<br>  rest_api_id          = aws_api_gateway_rest_api.raspberry_api.id<br>  resource_id          = aws_api_gateway_resource.home.id<br>  http_method          = aws_api_gateway_method.login.http_method<br>  type                 = "MOCK"<br>  timeout_milliseconds = 29000<br>}<br><br>resource "aws_api_gateway_method_response" "response_200" {<br>  rest_api_id = aws_api_gateway_rest_api.raspberry_api.id<br>  resource_id = aws_api_gateway_resource.home.id<br>  http_method = aws_api_gateway_method.login.http_method<br>  status_code = "200"<br>}<br><br>resource "aws_api_gateway_integration_response" "login_page" {<br>  rest_api_id = aws_api_gateway_rest_api.raspberry_api.id<br>  resource_id = aws_api_gateway_resource.home.id<br>  http_method = aws_api_gateway_method.login.http_method<br>  status_code = aws_api_gateway_method_response.response_200.status_code<br>  response_templates = {<br>    "text/html" = data.template_file.login_page.rendered<br>  }<br>}<br><br>resource "aws_api_gateway_deployment" "example" {<br>  depends_on = [<br>    aws_api_gateway_integration.login_page<br>  ]<br>  rest_api_id = aws_api_gateway_rest_api.raspberry_api.id<br>  stage_name  = "production"<br>}<br><br></code></pre><br><br><p>I have followed the instructions as in <a href="https://blog.it-playground.eu/display-html-page-using-only-api-gateway/" rel="nofollow noreferrer">this blog</a>; with no success.</p><br>
0.0,0.6666666666666666,0.6666666666666666,0.6666666666666666,0.0,0.0,0.0,<h3>Direct access a database in a private subnet without SSH tunnel</h3><p>I have a database set up (use RDS) in a private subnet; and a bastion is set up in front of it in a public subnet. The traditional way to access this database from local laptops is to set up an ssh tunnel on that bastion/jumpbox and map the database port to local. But this is not convenient to development because we need to set up that tunnel everytime before we want to connect. I am looking for a way to access this database without setting up an ssh tunnel first. I have seen a case where the local laptop directly uses that bastion's ip and its 3306 port to connect to the database behind. I have no idea how it is done.</p><br><br><p>BTW; in that case I saw; they don't use port forwarding because I didn't find any special rules in the bastion's iptable.</p><br>
0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.6666666666666666,0.0,<h3>how to configure two flask applications for domain and sub domain name on same aws ec2 instance?</h3><p>I have one flask web application running on AWS EC2 instance and I have connected a domain name to it. Now I have created a subdomain name pointed to the same EC2 instance where the main domain name is running and I want to run another Python flask web app for the subdomain name on the same instance. I don't know what kind of configuration I need to do? If there is any documentation or any procedure to follow please let me know.</p><br>
0.0,0.0,0.6666666666666666,1.0,0.0,0.0,0.3333333333333333,<h3>How to restrict users from download files uploaded to aws s3</h3><p>I am developing a LMS in Laravel and uploading all the video files to aws s3 bucket and can play them using video js player. But problem is; users can download the video files; which I want to stop. Can anybody suggest me is it possible or not? If possible; can anyone tell me how can I do that?</p><br>
0.0,0.0,0.0,0.6666666666666666,0.3333333333333333,1.0,0.0,<h3>DynamoDB getItem call not giving a response</h3><p>I'm trying to read a basic DynamoDB table in AWS Lambda; following the AWS tutorials.  I've got some basic code; that seems to be running OK (I'm not seeing any errors logged); but I can't get any output:</p><br><br><pre><code>const AWS = require('aws-sdk');<br>AWS.config.update({region: 'eu-west-1'});<br>const ddb = new AWS.DynamoDB({apiVersion: '2012-08-10'});<br><br>function readData(){<br><br>console.log("In the readData() function");<br><br>var params = {<br>    TableName: "desks";<br>    Key: {<br>        "desk_id": {N:'1'}<br>    }<br>};<br>console.log("Set params");<br>// Call DynamoDB to read the item from the table<br>ddb.getItem(params; function(err; data) {<br>    console.log("In getItem callback function");<br>    if (err) {<br>        console.log("Error"; err);<br>    }<br>    else {<br>        console.log("Success"; data.Item);<br>    }<br>});<br>console.log("Completed call");<br>}<br></code></pre><br><br><p>When my function above is called; the logs show the output "Set params" and "Completed call"; but it's like the callback function doesn't get executed.  Am I missing something around the execution flow?</p><br><br><p>Edit: I'm using Node.js 8.10 and I believe I've set up the appropriate role permissions (full access on the database).</p><br>
0.6666666666666666,0.6666666666666666,0.0,0.0,0.0,0.0,0.0,<h3>Spark/Yarn UI views all use dns name not IP:port name; any way to change?</h3><p>So; through the spark resource manager; all of the links to different applications; logs; etc all use DNS name</p><br><br><p><code>ip-xx-xx-xx-xx.company.com:20888/proxy/application_11111</code> for example</p><br><br><p>I must manually copy this and cange it to <code>xx.xx.xx.xx:20888/proxy/application_11111</code> to view this webpage.  Is it possible to get the original link working or change all of the links to <code>xx.xx.xx.xx:20888...</code> format?</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>AWS Console switch role transitively twice in a row</h3><p>I use the AWS Console; to switch my role from A to B on the same account (account1). Then I would like to switch from B to role C which is on another account (account2). The switch from A to B is working; but then from B to C is not.</p><br><br><ul><br><li>B trusts in <code>arn:aws:iam::account1.id:role/A</code> </li><br><li>C trusts in <code>arn:aws:iam::account2.id:role/B</code></li><br></ul><br><br><p>Is this a limitation of AWS; or should it be possible to switch roles transitively?</p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.0,0.0,<h3>Files are disappearing from s3</h3><p>I am having files just disappear from s3 without a trace.<br>The following is turned on with the bucket concerned:</p><br><br><ul><br><li>Versoning</li><br><li>Server Access Logging</li><br><li>Object-Level Logging</li><br></ul><br><br><p>There is absolutely no logs about deletes either from within AWS or our own application (We log all activity there as well).</p><br><br><p>Permissions are locked down and files only accessable via Cloudfront. </p><br><br><p>Also; there is no versioning of the missing files so I can't even restore them. This issue has happened twice now in the last month and I am at a total loss as to what is happening.</p><br><br><p>As there are no deleteObject logs there must be something else that is happening here and I dont know what.</p><br><br><p>Any help or insight would be of great help?</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>How can i get the aws region for my profile in typescript</h3><p>I have my aws region configured in the .aws/config for the specific aws profiles.<br><br>Is it possible to get the value of the set region (e.g. us-west-1) in my node.js service?<br><br>Is there maybe an environment variable that i can read from?</p><br>
1.0,0.0,0.0,0.0,0.0,0.6666666666666666,0.0,<h3>JavaScript returns undefined - AWS Comprehend</h3><p>I'm trying to set up a function in which detects the language of the text sample (through AWS API) but I get in nodeJs I get undefined.</p><br><pre class="lang-js prettyprint-override"><code>const fs = require('fs');<br>var AWS = require('aws-sdk');<br>AWS.config.update({ region: 'eu-west-1' });<br><br><br>function languageDetection(test) {<br><br>    const params = { TextList: [test] };<br>    var comprehend = new AWS.Comprehend();<br>    comprehend.batchDetectDominantLanguage(params; function (err; data) {<br>        if (err) console.log(err; err.stack);<br>        else {<br>            const { ResultList } = data;<br>            const languages = ResultList[0][&quot;Languages&quot;];<br>            const detectedLanguage = languages[0]<br>            const detectedLanguageText = detectedLanguage[&quot;LanguageCode&quot;];<br>            return detectedLanguageText<br>        }<br>    });<br>}<br><br>let aTextExample = 'This is a sample text'<br>const language = languageDetection(aTextExample)<br>console.log(language)<br></code></pre><br>
0.0,0.3333333333333333,0.3333333333333333,0.0,1.0,0.6666666666666666,0.0,<h3>Passing Custom Context to Lambda Authoriser</h3><p>I'm trying to create an AWS API Gateway with a Lambda function as an authoriser.</p><br><br><p>The Lambda function will check various things like wether the user is enabled and if they have a certain level of access.</p><br><br><p>When defining a route and a method I'd like to be able to pass in custom fields into the context for the Lambda function so I can define the things mentioned above on a per request / method basis.</p><br><br><p>Is there anyway of doing something like this or am I going about this the wrong way?</p><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.3333333333333333,0.0,<h3>AWS CLI EC2: option modify-default-credit-specification not working</h3><p>It says in the EC2 userguide that you can change the t2/t3/t3a default-credit-specification for your AWS account <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances-how-to.html#burstable-performance-instance-set-default-credit-specification-for-account" rel="nofollow noreferrer">Link Here</a>.<br>When I run the command <code>aws ec2 modify-default-credit-specification --region us-east-1 --instance-family t2 -cpu-credits unlimited</code>; it says <code>Invalid choice: 'modify-default-credit-specification'</code>. How do I change the default specification of t2/t3/t3a instances</p><br>
1.0,0.0,0.0,0.5,0.5,0.0,0.0,<h3>Read Large CSV from S3 using Lambda</h3><p>I have multiple compressed (.gzip) csv file in S3 which I wish to parse using preferably Lambda. The largest compressed file seen so far is 80MB. On decompressing; the file size becomes 1.6GB. It is approximately that a single uncompressed file can be approximately 2GB (the file be stored in compressed in S3).</p><br><p>After parsing; I am interested in selected rows from the csv file. I do not expect the memory used by filtered rows to be more than 200MB.</p><br><p>However; given Lambda's limit on time(15 min) &amp; memory (3GB); is using Lambda for such use case a feasible option in longer run? Any alternatives to consider?</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.6666666666666666,0.0,<h3>Is there a way to write git check status back to git when there is a failure in awscode pipeline</h3><p>I am setting up aws code pipeline based on the git developer branch.<br>when the developer commits his code; the pipeline will be trigger based on the webhook. Now the idea is when there is a failure in the pipeline; and when the developer triggers a pull-request; the reviewer should know that this is bad branch. He should be able to see the git status of the branch showing there is a failure.</p><br><br><p>Earlier I have used a build tool called Codeship which has a git-hub app to do this stuff. Now I have gone through git-hub API<br><a href="https://developer.github.com/v3/checks/runs/#create-a-check-run" rel="nofollow noreferrer">https://developer.github.com/v3/checks/runs/#create-a-check-run</a><br>But not sure where to start.</p><br>
1.0,0.0,0.0,0.0,0.6666666666666666,0.0,0.0,<h3>How kinesis keep the offset and push the record again when an event fails in lambda</h3><p>I am new to AWS lambda and Kinesis. Please help with the following question</p><br><br><p>I have a kinesis stream as a source to lambda and the target is again kinesis. I have following queries. <br>The system doesnt want to lose a record.</p><br><br><p>if any of the records fails the processing in lambda; How it again pull into the lambda? How it keep the unprocessed records ? How kinesis track the offset to process the next record?</p><br><br><p>Please update.</p><br>
0.0,0.0,0.0,1.0,0.0,0.3333333333333333,0.3333333333333333,<h3>dynamodb low with net core 3.1</h3><p>Some dynamodb readings are very slow; I even have readings that take 27min; but in my load tests I do not have those high peaks in my tests; the maximum time is 500ms; these high times occur in the production environment.</p><br><p>I do not understand what is the reason for this delay<br>Will it be netcore3.1?</p><br><p>i am using asynchronous too</p><br><p>I am using NET CORE 3.1</p><br><p>i am use on-demand</p><br><p>I have 13;848 records in my dynamo table</p><br><p>In the cloudwatch metrics I don't see any spike that represents 27 minutes; I am very concerned about this</p><br>
0.0,1.0,0.0,0.0,0.0,0.0,0.0,<h3>AWS ELB redirect behind SSH tunnel doesn&#39;t work</h3><p>We have an HTTP server listening on port X; and an AWS Classic HTTP Load Balancer; listening on the same port X and forwarding to the host port X.</p><br><br><p>The ELB is not routable to the internet; so to connect to it; we create an SSH tunnel from a local port Y to the ELB port X over a bastion host.</p><br><br><p>Simple HTTP requests work ok; however when the service return an HTTP redirect response (303); the ELB translates it to port Y.</p><br><br><p>So it looks like:<br><a href="http://localhost:Y" rel="nofollow noreferrer">http://localhost:Y</a> --> (303 redirected) <a href="http://localhost:X/login" rel="nofollow noreferrer">http://localhost:X/login</a></p><br><br><p>When skipping the ELB and tunnelling directly to the service host; redirects work fine.</p><br><br><p>Any way to configure the ELB to translate the redirections to the originating port; as specified in the HTTP Host header?</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>How to configure useful feedback when newly set password does not comply with set policy?</h3><p>I've successfully applied a custom password policy for my IAM users in AWS:</p><br><br><p><code>aws iam update-account-password-policy --minimum-password-length 64 --allow-users-to-change-password --password-reuse-prevention 24</code></p><br><br><p>Next I force a password reset - this also works.<br>But the when I deliberately try to set the new password to a non-compliant one; I get this rather opaque error at the top of the page (<a href="https://us-east-1.signin.aws.amazon.com/changepassword" rel="nofollow noreferrer">https://us-east-1.signin.aws.amazon.com/changepassword</a>):</p><br><br><blockquote><br>  <p>Either user is not authorized to perform iam:ChangePassword or entered password does not comply with account password policy set by administrator</p><br></blockquote><br><br><p>Is there a way to configure useful feedback? Such as: <code>Password should be at least 64 characters</code>?</p><br>
0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,0.6666666666666666,0.0,<h3>NodeJS API deployement on AWS with Elastic Beanstalk</h3><p>I'm looking for deploying my NodeJS API with AWS.</p><br><br><p>I tried to use Elastic Beanstalk but I always get this issue: </p><br><br><p><a href="https://i.stack.imgur.com/F9Rd2.png" rel="nofollow noreferrer">Image of the error I get</a></p><br><br><p>There is my package.json</p><br><br><pre><code>  {<br>    "name": "";<br>    "scripts": {<br>      "start": "node server.js"<br>    };<br>    "version": "0.0.0";<br>    "private": true;<br>    "dependencies": {<br>      "bcryptjs": "^2.4.3";<br>      "body-parser": "^1.18.3";<br>      "express": "^4.16.4";<br>      "hammerjs": "^2.0.8";<br>      "jsonwebtoken": "^8.5.1";<br>      "mongoose": "^5.4.20";<br>      "mongoose-unique-validator": "^2.0.2";<br>      "multer": "^1.4.1";<br>      "tslib": "^1.9.0"<br>    }<br>  }<br></code></pre><br><br><p>And I also have a nodecommand.config into a folder named ".ebextensions"</p><br><br><pre><code>    option_settings:<br>      aws:elasticbeanstalk:container:nodejs:<br>        NodeCommand: "npm start"<br></code></pre><br><br><p>Also; is Elastic Beanstalk the best way to deploy my API ?</p><br><br><p>Thx for the answers !</p><br>
0.0,0.0,0.0,1.0,0.0,0.6666666666666666,0.0,<h3>aws DynamoDB issue while getItem</h3><p>I am trying to get single Item through dynamoDB using Javascript here my code</p><br><br><pre><code>var params = {<br>    TableName: 'sharedata';<br>    Key: {<br>        id: _id<br>    };<br>    ProjectionExpression: 'ATTRIBUTE_NAME'<br>    };<br><br>ddb.getItem(params; function(err; data) {<br>    if (err) {<br>        console.log("Error"; err);<br>    } else {<br>        console.log("Success"; data.Item);<br>    }<br>    });<br></code></pre><br><br><p>and here my table in dynamoDB.</p><br><br><p><a href="https://i.stack.imgur.com/ez2rz.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/ez2rz.png" alt="enter image description here"></a></p><br><br><p>I am facing error: Expected params.Key['id'] to be a structure. What I am missing I am trying same as per docs  <a href="https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/dynamodb-example-table-read-write.html" rel="nofollow noreferrer">reading writing a single Item in dynamoDB</a></p><br>
0.0,0.6666666666666666,0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>AWS - Lambda unable to invoke another lambda through Python SDK</h3><p>I am trying to invoke a lambda function from another lambda using python SDK. Both the lambda functions belong to the same VPC. The trigger lambda only contains a python script that invokes the second lambda (loader_development). The loader_development lambda's APIGateway is private and it has a resource policy configured with it which denies access to all IP addresses which don't belong to that particular VPC. </p><br><br><p>My Python script in the trigger lambda is: </p><br><br><pre><code>from __future__ import print_function<br><br>import json<br>import logging<br>import os<br>from urllib2 import urlopen;Request;HTTPError<br>import boto3<br><br>logger = logging.getLogger()<br>logger.setLevel(logging.INFO)<br>region = os.environ['AWS_REGION']<br><br>def lambda_handler(event; context):<br>    invokeLambda = boto3.client('lambda'; region_name = 'us-east-1')  <br>    request = {'resource':'/bucketstatus/latest';'path':'/bucketstatus/latest';'httpMethod':'GET'}<br>    invoke_response = invokeLambda.invoke(FunctionName='loader_development';<br>                                           InvocationType='RequestResponse';<br>                                           Payload=json.dumps(request))<br>    print(invoke_response['Payload'].read())<br><br>logger.info('Process Complete')<br></code></pre><br><br><p>So <code>/bucketstatus/latest</code> is a <code>GET</code> request and this endpoint resides in the loader_development lambda's APIGateway (which is private). The loader_development lambda is a spring boot application whereas the trigger lambda is a standalone lambda that only has a python script to invoke an endpoint of the loader_development lambda to get the response.</p><br><br><p>While testing this script; it gives a status of 500 and an Internal Server Error. <br>The error:</p><br><br><pre><code>2019-10-09 10:09:09.279 ERROR 1 --- [ main] c.a.s.proxy.AwsProxyExceptionHandler : Called exception handler for:<br>com.amazonaws.serverless.exceptions.InvalidRequestEventException: The incoming event is not a valid request from Amazon API Gateway or an Application Load Balancer<br>10:09:09 at com.amazonaws.serverless.proxy.internal.servlet.AwsProxyHttpServletRequestReader.readRequest(AwsProxyHttpServletRequestReader.java:41)<br>10:09:09 at com.amazonaws.serverless.proxy.internal.servlet.AwsProxyHttpServletRequestReader.readRequest(AwsProxyHttpServletRequestReader.java:28)<br>10:09:09 at com.amazonaws.serverless.proxy.internal.LambdaContainerHandler.proxy(LambdaContainerHandler.java:174)<br>10:09:09 at com.amazonaws.serverless.proxy.internal.LambdaContainerHandler.proxyStream(LambdaContainerHandler.java:209)<br>10:09:09 at com.trimble.roadrunner.StreamLambdaHandler.handleRequest(StreamLambdaHandler.java:49)<br>10:09:09 at lambdainternal.EventHandlerLoader$2.call(EventHandlerLoader.java:888)<br>10:09:09 at lambdainternal.AWSLambda.startRuntime(AWSLambda.java:293)<br>10:09:09 at lambdainternal.AWSLambda.&lt;clinit&gt;(AWSLambda.java:64)<br>10:09:09 at java.lang.Class.forName0(Native Method)<br>10:09:09 at java.lang.Class.forName(Class.java:348)<br>10:09:09 at lambdainternal.LambdaRTEntry.main(LambdaRTEntry.java:114)<br></code></pre><br><br><p>The weird part is that when I try to invoke some other lambda (microservice); the request gets processed and I get a status of 200. The example lambda is also inside a VPC and has a private APIGateway.</p><br><br><p>I am really not sure what I am missing. Any help would be greatly appreciated!</p><br>
0.0,1.0,0.6666666666666666,0.0,0.0,0.0,0.0,<h3>AWS: How to Block IP making more than 1 request per second?</h3><p>I am hosting a monolith web app in AWS. In my web app; there are 2 heavy pages which load complicated dataset by running 3 heavy MySQL queries. From time to time; some authenticated user of my system ends up making more than 1 request/second to those pages using a browser plugin. Thus this hampers server performance and slows down other users of the system. I know a bit about <code>AWS WAF</code>. Is there any way to detect such behavior and block such an IP address automatically and not slow down other users? Any suggestions would be really appreciated. :)</p><br>
0.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.0,<h3>How do I get my EC2 Instance to connect to ECS Cluster?</h3><p>I have an ECS cluster defined in AWS and an Auto Scaling Group that I use to add/remove instance to handle tasks as necessary. I have the ASG setup so that it is creating the EC2 instance at the appropriate time; but it won't connect to the ECS Cluster unless I manually go in and disable/enable the ECS service.</p><br><p>I am using the Amazon Linux 2 ami on the EC2 machines and everything is in the same region/account etc.<br>I have included my user data below.</p><br><pre><code>#!/bin/bash <br>yum update -y <br>amazon-linux-extras disable docker <br>amazon-linux-extras install -y ecs <br>echo &quot;ECS_CLUSTER={CLUSTERNAME}&quot; &gt;&gt; /etc/ecs/ecs.config<br>systemctl enable --now ecs<br></code></pre><br><p>As mentioned this installs the ECS service and sets the config file properly but the enable doesn't actually connect the machine; but running the same disable/enable commands on the machine once running connects without problem. What am I missing?</p><br>
0.0,0.6666666666666666,0.0,0.0,0.0,0.3333333333333333,0.3333333333333333,<h3>API is working but aws api gateway menu is empty</h3><p>i'm working with aws api for my flask server.<br>the api endpoint is working; <strong>but the api menu in aws is empty.</strong> </p><br><br><p>Is there any reason that I can access the api but it's menues and configurations are totally empty in aws.com?</p><br>
0.0,0.0,1.0,0.0,0.0,0.6666666666666666,0.3333333333333333,<h3>Aws Lambda errors to CloudWatch; SNS then Slack</h3><p>Trying to push lambda errors to CloudWatch logs and SNS to Slack. Errors are not making it to Slack. The current code is shown below where i have three files main.tf; lambda.tf variables.tf and index.js. I'm not recieving errors anywhere from the looks of things.</p><br><p>#main.tf</p><br><pre><code>provider &quot;aws&quot; {<br>  region = &quot;eu-west-1&quot;<br>}<br><br>resource &quot;aws_cloudwatch_metric_alarm&quot; &quot;calculator-time&quot; {<br>  alarm_name          = &quot;kfk-test-calculator-execution-time&quot;<br>  comparison_operator = &quot;GreaterThanOrEqualToThreshold&quot;<br>  evaluation_periods  = &quot;1&quot;<br>  metric_name         = &quot;Duration&quot;<br>  namespace           = &quot;AWS/Lambda&quot;<br>  period              = &quot;60&quot;<br>  statistic           = &quot;Maximum&quot;<br>  threshold           = aws_lambda_function.test_lambda.timeout * 1000 * 0.75<br>  alarm_description   = &quot;Calculator Execution Time&quot;<br>  treat_missing_data  = &quot;ignore&quot;<br><br>  insufficient_data_actions = [<br>    &quot;${aws_sns_topic.test_slack_channel.arn}&quot;;<br>  ]<br><br>  alarm_actions = [<br>    &quot;${aws_sns_topic.test_slack_channel.arn}&quot;;<br>  ]<br><br>  ok_actions = [<br>    &quot;${aws_sns_topic.test_slack_channel.arn}&quot;;<br>  ]<br><br>  dimensions = {<br>    FunctionName = &quot;${aws_lambda_function.test_lambda.function_name}&quot;<br>    Resource     = &quot;${aws_lambda_function.test_lambda.function_name}&quot;<br>  }<br>}<br><br><br>module &quot;notify_slack&quot; {<br>  source  = &quot;terraform-aws-modules/notify-slack/aws&quot;<br>  version = &quot;~&gt; 3.0&quot;<br><br>  sns_topic_name = aws_sns_topic.test_slack_channel.name<br><br>  slack_webhook_url = var.slack_web_hook_url<br>  slack_channel     = &quot;aws-notification&quot;<br>  slack_username    = &quot;my username&quot;<br>}<br><br><br>resource &quot;aws_sns_topic&quot; &quot;test_slack_channel&quot; {<br>  name = &quot;test-slack-notifications&quot;<br>  application_success_feedback_role_arn    = aws_iam_role.example.arn<br>  application_success_feedback_sample_rate = 100<br>  application_failure_feedback_role_arn    = aws_iam_role.example.arn<br>  lambda_success_feedback_role_arn         = aws_iam_role.example.arn<br>  lambda_success_feedback_sample_rate      = 90<br>  lambda_failure_feedback_role_arn         = aws_iam_role.example.arn<br>  http_success_feedback_role_arn           = aws_iam_role.example.arn<br>  http_success_feedback_sample_rate        = 80<br>  http_failure_feedback_role_arn           = aws_iam_role.example.arn<br>}<br><br>resource &quot;aws_iam_role&quot; &quot;example&quot; {<br>  name = &quot;kfk-sns-delivery-status-role&quot;<br>  path = &quot;/&quot;<br>  assume_role_policy = &lt;&lt;EOF<br>{<br>  &quot;Version&quot;: &quot;2012-10-17&quot;;<br>  &quot;Statement&quot;: [<br>    {<br>      &quot;Effect&quot;: &quot;Allow&quot;;<br>      &quot;Principal&quot;: {<br>        &quot;Service&quot;: &quot;sns.amazonaws.com&quot;<br>      };<br>      &quot;Action&quot;: &quot;sts:AssumeRole&quot;<br>    }<br>  ]<br>}<br>EOF<br>}<br>resource &quot;aws_iam_role_policy&quot; &quot;example&quot; {<br>  name = &quot;kfk-sns-delivery-status-role-policy&quot;<br>  role = aws_iam_role.example.id<br>  policy = &lt;&lt;EOF<br>{<br>  &quot;Version&quot;: &quot;2012-10-17&quot;;<br>  &quot;Statement&quot;: [<br>    {<br>      &quot;Effect&quot;: &quot;Allow&quot;;<br>      &quot;Action&quot;: [<br>        &quot;logs:CreateLogGroup&quot;;<br>        &quot;logs:CreateLogStream&quot;;<br>        &quot;logs:PutLogEvents&quot;;<br>        &quot;logs:PutMetricFilter&quot;;<br>        &quot;logs:PutRetentionPolicy&quot;<br>      ];<br>      &quot;Resource&quot;: [<br>        &quot;*&quot;<br>      ]<br>    }<br>  ]<br>}<br>EOF<br>}<br><br>resource &quot;aws_sns_topic_subscription&quot; &quot;sns-topic&quot; {<br>  topic_arn = aws_sns_topic.sns-topic.arn<br>  protocol  = &quot;sqs&quot;<br>  endpoint  = aws_sqs_queue.sqs-queue.arn<br>}<br></code></pre><br><p>#lambda.tf</p><br><pre><code># Simple AWS Lambda Terraform Example<br># requires 'index.js' in the same directory<br># to test: run `terraform plan`<br># to deploy: run `terraform apply`<br><br>data &quot;archive_file&quot; &quot;lambda_zip&quot; {<br>    type          = &quot;zip&quot;<br>    source_file   = &quot;index.js&quot;<br>    output_path   = &quot;lambda_function.zip&quot;<br>}<br><br>resource &quot;aws_lambda_function&quot; &quot;test_lambda&quot; {<br>  filename         = &quot;lambda_function.zip&quot;<br>  function_name    = &quot;test_lambda&quot;<br>  role             = aws_iam_role.iam_for_lambda_tf.arn<br>  handler          = &quot;index.handler&quot;<br>  source_code_hash = data.archive_file.lambda_zip.output_base64sha256<br>  runtime          = &quot;python3.7&quot;<br>}<br><br>resource &quot;aws_iam_role&quot; &quot;iam_for_lambda_tf&quot; {<br>  name = &quot;iam_for_lambda_tf&quot;<br><br>  assume_role_policy = &lt;&lt;EOF<br>{<br>  &quot;Version&quot;: &quot;2012-10-17&quot;;<br>  &quot;Statement&quot;: [<br>    {<br>      &quot;Action&quot;: &quot;sts:AssumeRole&quot;;<br>      &quot;Principal&quot;: {<br>        &quot;Service&quot;: &quot;lambda.amazonaws.com&quot;<br>      };<br>      &quot;Effect&quot;: &quot;Allow&quot;;<br>      &quot;Sid&quot;: &quot;&quot;<br>    }<br>  ]<br>}<br>EOF<br>}<br><br>resource &quot;aws_iam_role_policy_attachment&quot; &quot;basic&quot; {<br>  policy_arn = &quot;arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole&quot;<br>  role       = aws_iam_role.iam_for_lambda_tf.name<br>}<br></code></pre><br><p>variables.tf</p><br><pre><code>variable &quot;lambda_function_name&quot; {<br>    default = &quot;kfk-test-lambda-function&quot;<br>}<br><br>variable &quot;slack_web_hook_url&quot; {<br>    default = &quot;https://hooks.slack.com/services/identifier1/identifier2/identifier3&quot;<br>}<br></code></pre><br><p>#index.js</p><br><pre><code>// 'Hello World' nodejs6.10 runtime AWS Lambda function<br>exports.handler = (event; context; callback) =&gt; {<br>    console.log('Hello; logs!');<br>    callback(null; 'great success');<br>}<br></code></pre><br>
0.0,0.6666666666666666,0.3333333333333333,0.0,0.3333333333333333,0.6666666666666666,0.0,<h3>The provided route key is not formatted properly for HTTP protocol</h3><blockquote><br><p>An error occurred: <strong>HttpApiRoutePostv1Banks</strong> - The provided route key is<br>not formatted properly for HTTP protocol. Format should be &quot;[HTTP<br>METHOD] /[RESOURCE PATH]&quot; or &quot;$default&quot; (Service: AmazonApiGatewayV2;<br>Status Code: 400; Error Code: BadRequestException; Request ID:<br>38370b30-9c11-4a66-9f2d-710fd2c25329; Proxy: null).</p><br></blockquote><br><pre><code>provider:<br>  ...<br>  httpApi:<br>    payload: '2.0' # Define Http format needed for API GW<br>  ...<br>functions:<br>  banksCreate:<br>    handler: src/banks.create<br>    events:<br>      - httpApi: 'POST v1/banks'<br></code></pre><br><p>I had set up serverless.yml file according to the <a href="https://www.serverless.com/framework/docs/providers/aws/events/http-api/" rel="nofollow noreferrer">official documentation</a></p><br><p>when I run serverless offline everything working fine; but when I try to deploy error above occurs.</p><br><p>I am not sure where I make a mistake?</p><br><p>note that deployment was ok with <strong>restApi</strong> event</p><br>
0.0,0.0,1.0,0.0,1.0,0.0,0.0,<h3>Terraform cycle with AWS and Kubernetes provider</h3><p>My Terraform code describes some AWS infrastructure to build a Kubernetes cluster including some deployments into the cluster. When I try to destroy the infrastructure using <code>terraform plan -destroy</code> I get a cycle:</p><br><br><pre><code>module.eks_control_plane.aws_eks_cluster.this[0] (destroy)<br>module.eks_control_plane.output.cluster<br>provider.kubernetes<br>module.aws_auth.kubernetes_config_map.this[0] (destroy)<br>data.aws_eks_cluster_auth.this[0] (destroy)<br></code></pre><br><br><p>Destroying the infrastructure works by hand using just <code>terraform destroy</code> works fine. Unfortunately; Terraform Cloud uses <code>terraform plan -destroy</code> to plan the destructuion first; which causes this to fail. Here is the relevant code:</p><br><br><p>excerpt from eks_control_plane module:</p><br><br><pre><code>resource "aws_eks_cluster" "this" {<br>  count = var.enabled ? 1 : 0<br><br>  name     = var.cluster_name<br>  role_arn = aws_iam_role.control_plane[0].arn<br>  version  = var.k8s_version<br><br>  # https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html<br>  enabled_cluster_log_types = var.control_plane_log_enabled ? var.control_plane_log_types : []<br><br>  vpc_config {<br>    security_group_ids = [aws_security_group.control_plane[0].id]<br>    subnet_ids         = [for subnet in var.control_plane_subnets : subnet.id]<br>  }<br><br>  tags = merge(var.tags;<br>    {<br>    }<br>  )<br><br>  depends_on = [<br>    var.dependencies;<br>    aws_security_group.node;<br>    aws_iam_role_policy_attachment.control_plane_cluster_policy;<br>    aws_iam_role_policy_attachment.control_plane_service_policy;<br>    aws_iam_role_policy.eks_cluster_ingress_loadbalancer_creation;<br>  ]<br>}<br><br>output "cluster" {<br>  value = length(aws_eks_cluster.this) &gt; 0 ? aws_eks_cluster.this[0] : null<br>}<br></code></pre><br><br><p>aws-auth Kubernetes config map from aws_auth module:</p><br><br><pre><code>resource "kubernetes_config_map" "this" {<br>  count = var.enabled ? 1 : 0<br><br>  metadata {<br>    name      = "aws-auth"<br>    namespace = "kube-system"<br>  }<br>  data = {<br>    mapRoles = jsonencode(<br>      concat(<br>        [<br>          {<br>            rolearn  = var.node_iam_role.arn<br>            username = "system:node:{{EC2PrivateDNSName}}"<br>            groups = [<br>              "system:bootstrappers";<br>              "system:nodes";<br>            ]<br>          }<br>        ];<br>        var.map_roles<br>      )<br>    )<br>  }<br><br>  depends_on = [<br>    var.dependencies;<br>  ]<br>}<br><br></code></pre><br><br><p>Kubernetes provider from root module:</p><br><br><pre><code>data "aws_eks_cluster_auth" "this" {<br>  count = module.eks_control_plane.cluster != null ? 1 : 0<br>  name  = module.eks_control_plane.cluster.name<br>}<br><br>provider "kubernetes" {<br>  version = "~&gt; 1.10"<br><br>  load_config_file       = false<br>  host                   = module.eks_control_plane.cluster != null ? module.eks_control_plane.cluster.endpoint : null<br>  cluster_ca_certificate = module.eks_control_plane.cluster != null ? base64decode(module.eks_control_plane.cluster.certificate_authority[0].data) : null<br>  token                  = length(data.aws_eks_cluster_auth.this) &gt; 0 ? data.aws_eks_cluster_auth.this[0].token : null<br>}<br></code></pre><br><br><p>And this is how the modules are called:</p><br><br><pre><code>module "eks_control_plane" {<br>  source  = "app.terraform.io/SDA-SE/eks-control-plane/aws"<br>  version = "0.0.1"<br>  enabled = local.k8s_enabled<br><br>  cluster_name          = var.name<br>  control_plane_subnets = module.vpc.private_subnets<br>  k8s_version           = var.k8s_version<br>  node_subnets          = module.vpc.private_subnets<br>  tags                  = var.tags<br>  vpc                   = module.vpc.vpc<br><br>  dependencies = concat(var.dependencies; [<br>    # Ensure that VPC including all security group rules; network ACL rules;<br>    # routing table entries; etc. is fully created<br>    module.vpc;<br>  ])<br>}<br><br><br># aws-auth config map module. Creating this config map will allow nodes and<br># Other users to join the cluster.<br># CNI and CSI plugins must be set up before creating this config map.<br># Enable or disable this via `aws_auth_enabled` variable.<br># TODO: Add Developer and other roles.<br>module "aws_auth" {<br>  source  = "app.terraform.io/SDA-SE/aws-auth/kubernetes"<br>  version = "0.0.0"<br>  enabled = local.aws_auth_enabled<br><br>  node_iam_role = module.eks_control_plane.node_iam_role<br>  map_roles = [<br>    {<br>      rolearn  = "arn:aws:iam::${var.aws_account_id}:role/Administrator"<br>      username = "admin"<br>      groups = [<br>        "system:masters";<br>      ]<br>    };<br>    {<br>      rolearn  = "arn:aws:iam::${var.aws_account_id}:role/Terraform"<br>      username = "terraform"<br>      groups = [<br>        "system:masters";<br>      ]<br>    }<br>  ]<br>}<br></code></pre><br><br><p>Removing the aws_auth config map; which means not using the Kubernetes provider at all; breaks the cycle. The problem is obviously that Terraform tries to destroys the Kubernetes cluster; which is required for the Kubernetes provider. Manually removing the resources step by step using multiple <code>terraform apply</code> steps works fine; too.</p><br><br><p>Is there a way that I can tell Terraform first to destroy all Kubernetes resources so that the Provider is not required anymore; then destroy the EKS cluster?</p><br>
0.0,0.6666666666666666,0.0,0.6666666666666666,1.0,0.0,0.0,<h3>AWS Elastic Beanstalk selected Security Group update</h3><p>It seems something has changed with the Elastic Beanstalk Security Group handling; tonight at UTC 00:00 the webserver lost the connection with the RDS. <br>I logged in to the EC2 console; everything looks good; the security group that belongs to the RDS instance has an Inbound rule that accepts MySql port to the security group that's been selected to the Elastic Beanstalk config.  </p><br><br><p>When I set the RDS Security Group Inbound Rule to Anywhere the webserver and the RDS server can connect. When I add the rule back for the EB's security group the webserver can't connect to the RDS anymore.</p><br><br><p>This has been worked for years; nothing has changed; it just doesn't work anymore and I can't find the solution. </p><br><br><p>Any suggestions? </p><br>
0.0,0.0,1.0,0.0,0.6666666666666666,0.0,0.0,<h3>Is there a way to have cloudformation not replace an existing resource on update?</h3><p>This resource creates a beanstalk application version: <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-beanstalk-version.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-beanstalk-version.html</a></p><br><p>Having that in a template is convenient and useful because the beanstalk environment and the application version can be deployed at the same time</p><br><p>Because its defined as a resource in the template though when the version is updated to a new one its completely replaced</p><br><p>This is a problem because the beanstalk UI makes it easy to deploy previous versions- you just select one and deploy it to the environment. But because cloudformation is destroying/creating the version each time there is only one. Its useful to have the UI fallback for tracking/redeploying previous version- even if its mostly going to be done through cloudformation</p><br><p>I want cloudformation to create a new version (when it changes) and leave the old one untouched- instead of destroying it.</p><br><p>I only want this behaviour for this one resource in the template- other resources can use the default behaviour</p><br>
0.0,0.0,0.0,1.0,0.0,0.3333333333333333,0.0,<h3>Error in .local(drv; ...) : Failed to connect to database: Error: Unknown database &#39;database1&#39;</h3><p>I am trying to connect to MySQL hosted on AWS free tier. </p><br><br><p>For some reason I am getting an error stating my database is not found. I was wondering what I am doing wrong.</p><br><br><p>Below is my code along with a picture stating what my database name is on AWS.</p><br><br><pre><code>library(dplyr)<br>library(dbplyr)<br>library(pool)<br><br>host = "database1.creyniq1gyij.us-east-2.rds.amazonaws.com"<br>dbname = "database1"<br>user = "jordan1"<br>pass = "mysecurepass"<br><br><br>con &lt;-  dbPool(RMySQL::MySQL(); <br>                  username=user;<br>                  password=pass;<br>                  host=host;<br>                  port=3306;<br>                  dbname="database1"<br>)<br><br><br>Error in .local(drv; ...) : <br>  Failed to connect to database: Error: Unknown database 'database1'<br></code></pre><br><br><p>Here is my amazon screen shot of my database<br><a href="https://i.stack.imgur.com/4xznQ.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/4xznQ.png" alt="enter image description here"></a></p><br><br><p>From @makeshift-programmer answer; I removed dbname in the call and I was able to connect. However; I am not to sure how to create a dbname/schema; so I can start uploading data into the sever. </p><br>
0.3333333333333333,0.0,0.0,0.0,0.3333333333333333,0.0,0.6666666666666666,<h3>Facing an error while building a custom skil for amazon alexa</h3><p>I am trying to a build a basic custom alexa skill. I have created an intent schema and am using AWS lambda function as the endpoint. ]</p><br><br><p>My Intent schema:</p><br><br><pre><code>{<br>    "interactionModel": {<br>        "languageModel": {<br>            "invocationName": "toit brewpub";<br>            "modelConfiguration": {<br>                "fallbackIntentSensitivity": {<br>                    "level": "LOW"<br>                }<br>            };<br>            "intents": [<br>                {<br>                    "name": "AMAZON.FallbackIntent";<br>                    "samples": []<br>                };<br>                {<br>                    "name": "AMAZON.CancelIntent";<br>                    "samples": []<br>                };<br>                {<br>                    "name": "AMAZON.HelpIntent";<br>                    "samples": []<br>                };<br>                {<br>                    "name": "AMAZON.StopIntent";<br>                    "samples": []<br>                };<br>                {<br>                    "name": "AMAZON.NavigateHomeIntent";<br>                    "samples": []<br>                };<br>                {<br>                    "name": "GetClosingTime";<br>                    "slots": [];<br>                    "samples": [<br>                        "what time do you close";<br>                        "when is the last order";<br>                        "till what time are you open";<br>                        "What time does the pub close"<br>                    ]<br>                };<br>                {<br>                    "name": "GetPriceOfBeer";<br>                    "slots": [<br>                        {<br>                            "name": "beer";<br>                            "type": "BEERS"<br>                        }<br>                    ];<br>                    "samples": [<br>                        "how much is {beer}";<br>                        "what is the price of {beer}"<br>                    ]<br>                }<br>            ];<br>            "types": [<br>                {<br>                    "name": "BEERS";<br>                    "values": [<br>                        {<br>                            "name": {<br>                                "value": "Toit Red"<br>                            }<br>                        };<br>                        {<br>                            "name": {<br>                                "value": "Tiot Weiss"<br>                            }<br>                        };<br>                        {<br>                            "name": {<br>                                "value": "Basmati Blonde"<br>                            }<br>                        };<br>                        {<br>                            "name": {<br>                                "value": "Tintin Toit"<br>                            }<br>                        };<br>                        {<br>                            "name": {<br>                                "value": "IPA"<br>                            }<br>                        };<br>                        {<br>                            "name": {<br>                                "value": "Dark Knight"<br>                            }<br>                        }<br>                    ]<br>                }<br>            ]<br>        }<br>    }<br>}<br></code></pre><br><br><p>I am using <strong>Node.js</strong> <code>v 10.x</code> for my lamda function which has been built using <strong>Alexa-Skills-NodeJS-Fact-kit</strong>; The region for my aws lambda is <strong>US_EAST- N.VIRGINIA</strong>.</p><br><br><p>Below is the request I receive when I talk to my Test Simulator:</p><br><br><pre><code>{<br>    "version": "1.0";<br>    "session": {<br>        "new": false;<br>        "sessionId": "amzn1.echo-api.session.fd1c5315-ecf8-413f-ba25-e54bd6ae316a";<br>        "application": {<br>            "applicationId": "amzn1.ask.skill.72615503-5f38-4baf-b0dd-cd6edd3b6dfd"<br>        };<br>        "user": {<br>            "userId": ""<br>        }<br>    };<br>    "context": {<br>        "System": {<br>            "application": {<br>                "applicationId": "amzn1.ask.skill.72615503-5f38-4baf-b0dd-cd6edd3b6dfd"<br>            };<br>            "user": {<br>                "userId": ""<br>            };<br>            "device": {<br>                "deviceId": "";<br>                "supportedInterfaces": {}<br>            };<br>            "apiEndpoint": "https://api.eu.amazonalexa.com";<br>            "apiAccessToken": ""<br>        };<br>        "Viewport": {<br>            "experiences": [<br>                {<br>                    "arcMinuteWidth": 246;<br>                    "arcMinuteHeight": 144;<br>                    "canRotate": false;<br>                    "canResize": false<br>                }<br>            ];<br>            "shape": "RECTANGLE";<br>            "pixelWidth": 1024;<br>            "pixelHeight": 600;<br>            "dpi": 160;<br>            "currentPixelWidth": 1024;<br>            "currentPixelHeight": 600;<br>            "touch": [<br>                "SINGLE"<br>            ];<br>            "video": {<br>                "codecs": [<br>                    "H_264_42";<br>                    "H_264_41"<br>                ]<br>            }<br>        };<br>        "Viewports": [<br>            {<br>                "type": "APL";<br>                "id": "main";<br>                "shape": "RECTANGLE";<br>                "dpi": 160;<br>                "presentationType": "STANDARD";<br>                "canRotate": false;<br>                "configuration": {<br>                    "current": {<br>                        "video": {<br>                            "codecs": [<br>                                "H_264_42";<br>                                "H_264_41"<br>                            ]<br>                        };<br>                        "size": {<br>                            "type": "DISCRETE";<br>                            "pixelWidth": 1024;<br>                            "pixelHeight": 600<br>                        }<br>                    }<br>                }<br>            }<br>        ]<br>    };<br>    "request": {<br>        "type": "SessionEndedRequest";<br>        "requestId": "amzn1.echo-api.request.24b64895-3f90-4a5b-9805-9d3b038cd323";<br>        "timestamp": "2020-03-29T08:59:54Z";<br>        "locale": "en-US";<br>        "reason": "ERROR";<br>        "error": {<br>            "type": "INVALID_RESPONSE";<br>            "message": "An exception occurred while dispatching the request to the skill."<br>        }<br>    }<br>}<br></code></pre><br><br><p>I have removed the user Id; device ID and access token while asking the question for security reasons.</p><br><br><p>My Lambda node js function looks like this which i have generated using the code generator :</p><br><br><p><a href="https://github.com/shreyneil/Episodes/blob/master/amazon-echo/lambda-function.js" rel="nofollow noreferrer">https://github.com/shreyneil/Episodes/blob/master/amazon-echo/lambda-function.js</a></p><br><br><p>Url for code-generator: <a href="http://alexa.codegenerator.s3-website-us-east-1.amazonaws.com/" rel="nofollow noreferrer">http://alexa.codegenerator.s3-website-us-east-1.amazonaws.com/</a></p><br><br><p>Url for tutorial that i was using to implement it: <a href="https://www.youtube.com/watch?v=BB3wwxgqPOU" rel="nofollow noreferrer">https://www.youtube.com/watch?v=BB3wwxgqPOU</a></p><br><br><p>Whenever i try to launch the event using ; <code>open toit brewpub</code>  in my test simulator it thorws an error stating :</p><br><br><blockquote><br>  <p>There was a problem with the requested skill's response</p><br></blockquote><br><br><p>Any idea on how to make this work?</p><br><br><p>Any leads would appreciated; Thank you!</p><br>
0.0,0.0,0.3333333333333333,0.0,0.6666666666666666,1.0,0.0,<h3>Best practice for using github and AWS lambda?</h3><p>I'm writing AWS lambda's in the browser and want to improve my version control process from the present setup; which is to copy paste the lambda code into a text file and manually commit to a repository. </p><br><br><p>Is there a better (and preferably straight-forward) way of using AWS lambda with version control (in my case git/github)?</p><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>How to set policy for IAM user group?</h3><p>I'm new to AWS. I have an AWS CodeArtifact repository and I wanted to create a Repository Policy using an IAM user group for the repository.</p><br><p>My Repository Policy using IAM user looks like this.</p><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Principal&quot;: {<br>                &quot;AWS&quot;: &quot;arn:aws:iam::****:user/*********&quot;<br>            };<br>            &quot;Action&quot;: [<br>                &quot;codeartifact:DescribePackageVersion&quot;;<br>                &quot;codeartifact:DescribeRepository&quot;;<br>                &quot;codeartifact:GetPackageVersionReadme&quot;;<br>                &quot;codeartifact:GetRepositoryEndpoint&quot;;<br>                &quot;codeartifact:ListPackages&quot;;<br>                &quot;codeartifact:ListPackageVersions&quot;;<br>                &quot;codeartifact:ListPackageVersionAssets&quot;;<br>                &quot;codeartifact:ListPackageVersionDependencies&quot;;<br>                &quot;codeartifact:ReadFromRepository&quot;<br>            ];<br>            &quot;Resource&quot;: &quot;*&quot;<br>        }<br>    ]<br>}<br></code></pre><br><p>How do I do it for a user group ?</p><br><p>I tried to do like what I had done with IAM user for group but there seems to be some error.<br>It looked like this</p><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Principal&quot;: {<br>                &quot;AWS&quot;: &quot;arn:aws:iam::*****:group/*********&quot;<br>            };<br>            &quot;Action&quot;: [<br>                &quot;codeartifact:DescribePackageVersion&quot;;<br>                &quot;codeartifact:DescribeRepository&quot;;<br>                &quot;codeartifact:GetPackageVersionReadme&quot;;<br>                &quot;codeartifact:GetRepositoryEndpoint&quot;;<br>                &quot;codeartifact:ListPackages&quot;;<br>                &quot;codeartifact:ListPackageVersions&quot;;<br>                &quot;codeartifact:ListPackageVersionAssets&quot;;<br>                &quot;codeartifact:ListPackageVersionDependencies&quot;;<br>                &quot;codeartifact:ReadFromRepository&quot;<br>            ];<br>            &quot;Resource&quot;: &quot;*&quot;<br>        }<br>    ]<br>}<br></code></pre><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.6666666666666666,<h3>How Can I Generate An OpenApiDocument from an assembly?</h3><p>AWS Schema Registry (AWS EventBridge) requires OpenApi3 schema documents.</p><br><br><p>I have a dotnet assembly with types that are used for Detail of an AWS EventBridge message in an AWS Lambda Function assembly.</p><br><br><p>I would like to generate the OpenApi3 document for these types.</p><br><br><p>I have completed a spike where these types are return types of ASP.NET application and checked the swagger output.  These desired types are present in the swagger output.  But; these types do not live in the ASP.NET application; nor are they used in the ASP.NET core application.   So; I know they can be generated into the schema.</p><br><br><p>I want to leave them in the AWS Lambda Function assembly but generate the schema into an OpenApiDocument.  I have tried using nswag studio to examine the assembly; but since they are neither inputs or outputs of controller method (there are no controllers - it is an AWS Lambda assembly); they are not generating.</p><br><br><p>Any ideas on how I can generate the OpenApi3 schema document from types located in a library assembly like this?</p><br>
0.0,0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,0.0,<h3>How can I use AWS-RunPowerShellScript on Windows to run a command that contains a space?</h3><p>I'm trying to run a program on Windows using AWS Systems Manager (SSM) from some Java code. Here is the code:</p><br><pre><code>String commands = &quot;C:\\Program Files\\MyProgram.exe --key1=value1&quot;;<br>SendCommandRequest commandRequest = new SendCommandRequest()<br>        .withDocumentName( &quot;AWS-RunPowerShellScript&quot; )<br>        .withInstanceIds( instanceId )<br>        .addParametersEntry( &quot;commands&quot;; Collections.singletonList( commands ) );<br></code></pre><br><p>The error I get is:</p><br><pre><code>C:\Program : The term 'C:\Program' is not recognized as the name of a cmdlet; <br>function; script file; or operable program. Check the spelling of the name; or <br>if a path was included; verify that the path is correct and try again.<br></code></pre><br><p>I've tried putting single quotes around the command name; like this:</p><br><pre><code>String commands = &quot;'C:\\Program Files\\MyProgram.exe' --key1=value1&quot;;<br>SendCommandRequest commandRequest = new SendCommandRequest()<br>        .withDocumentName( &quot;AWS-RunPowerShellScript&quot; )<br>        .withInstanceIds( instanceId )<br>        .addParametersEntry( &quot;commands&quot;; Collections.singletonList( commands ) );<br></code></pre><br><p>But then I get this error:</p><br><pre><code>+ 'C:\Program Files\MyProgram.exe' --key1=value1<br>+                                    ~~~~~~~~~~~<br>Unexpected token 'key1=value1' in expression or statement.<br>At C:\ProgramData\Amazon\SSM\InstanceData\i-0ad13762af97f97f1\document\orchestr<br>ation\784e0a71-dc7d-4a9e-944c-8820a7db6530\awsrunPowerShellScript\0.awsrunPower<br>ShellScript\_script.ps1:1 char:1<br>+ 'C:\Program Files\MyProgram.exe' --key1=value1<br>+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br>The '--' operator works only on variables or on properties.<br>    + CategoryInfo          : ParserError: (:) []; ParentContainsErrorRecordEx <br>   ception<br>    + FullyQualifiedErrorId : UnexpectedToken<br></code></pre><br><p>How can I run a command that contains spaces; and combine it with command-line arguments?</p><br>
0.3333333333333333,0.0,0.0,1.0,0.0,0.0,0.0,<h3>How to compare two Amazon RDS snapshots?</h3><p>I have Amazon RDS snapshot that is taken once every day.</p><br><br><p>I want to find only the changes that were made (needing only the delta). Is there any way to find the difference that was made to the snapshot?</p><br>
0.0,0.0,0.0,1.0,0.0,0.6666666666666666,0.0,<h3>Chrome Extension Upload Stream to S3</h3><p>I'm trying to use S3  multipart upload to upload data to S3 using a stream on the client-side.</p><br><p>I'm using <a href="http://browserify.org/" rel="nofollow noreferrer">Browserify</a> to convert the Nodejs code into a single file that can be loaded by the Chrome Extension.</p><br><p>Here is my code:</p><br><pre><code>    const Stream = require('stream');<br>    var inputBytesReadable = new Stream.Readable();<br>    // add data to the Stream<br>    var s3 = new AWS.S3({<br>                    params: {Bucket: bucketName}<br>                });<br>    var params = {<br>      Bucket: bucketName;<br>      Key: fileName;<br>      PartNumber: partNumber; <br>      UploadId: uploadId; <br>      Body: inputBytesReadable<br>    };<br>    s3.uploadPart(params; function(err; data) {<br>      if (err)<br>      {<br>          appendMessage(&quot;Error in uploading &quot;+fileName+&quot;part &quot;+partNumber);<br>          console.log(err; err.stack); // an error occurred<br>      }<br>    });<br><br></code></pre><br><p>However; this results in the error: <strong>InvalidParameterType: Expected params.Body to be a string; Buffer; Stream; Blob; or typed array object</strong></p><br><p>What am I doing incorrectly? Is there any way that I can pass a Stream to S3 in client-side JavaScript?</p><br>
0.0,1.0,0.0,0.0,0.0,0.0,0.0,<h3>AWS loadbalancer stopped dispatching traffic correctly</h3><p>I have a single LB with 3 listeners and 2 target groups: <strong>TargetX</strong> and <strong>targetY</strong>.</p><br><pre><code>TargetX contains two nodes: Node1 in region A and Node2 in region B.<br>TargetY contains one node:  Node3 in region A.<br><br>Listener1 forwards to targetX<br>Listener2 forwards to targetX<br>Listener3 forwards to targetY<br></code></pre><br><p>A few days ago; over a sudden without any modification in configurations etc;<br>LB stopped sending the requests to <strong>TargetX:Node2</strong> and therefore all the requests were forwarded to<br><strong>TargetX:Node1</strong>.</p><br><p><strong>All nodes and LB and targets have been in absolute healthy state all the time and connections to<br>Nodes were all up.</strong></p><br><p>In order to fix the problem I was told by AWS support to define a second <strong>Node4</strong> in <strong>TargetY</strong> to make it<br>look like <strong>TargetX</strong> with two instances. It did not help however; and I ended up enabling the Cross Zone Loadbalancing feature to fix this problem.</p><br><p>I can't understand why I've been suggested to define a second Node in <strong>TargetY</strong> to make it having two<br>Nodes!</p><br><p>To me; targets are independent entities with distinct listeners and a group of nodes to dispatch the<br>traffic and one target's configuration shouldn't impact another target's function.</p><br><p>Am I missing something?</p><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.6666666666666666,0.0,<h3>Boto3: Get autoscaling group name based on multiple tags</h3><p>I have a requirement to get the name of the autoscaling group based on its tags. </p><br><br><p>I have tried following code:</p><br><br><pre><code>kwargsAsgTags = {<br>    'Filters': [<br>        {<br>            'Name': 'key';<br>            'Values': ['ApplicationName']<br>        };<br>        {<br>            'Name': 'value';<br>            'Values': ['my-app-name']<br>        }<br>    ]<br>}<br></code></pre><br><br><p>by using above filter I can get the autoscaling group name but since I have same 'ApplicationName' tag used in multiple environments like dev/qa/uat; the output prints all autoscaling groups belong to all environments. How do I filter the EnvironmentName as well?</p><br><br><p>For that I've tried following but this time it prints all auto-scaling groups belonging to 'dev' environment as well.</p><br><br><pre><code>kwargsAsgTags = {<br>    'Filters': [<br>        {<br>            'Name': 'key';<br>            'Values': ['ApplicationName'; 'EnvName']<br>        };<br>        {<br>            'Name': 'value';<br>            'Values': ['my-app-name'; 'dev']<br>        }<br>    ]<br>}<br></code></pre><br>
0.0,1.0,0.0,0.6666666666666666,1.0,0.3333333333333333,0.0,<h3>Connecting to Aurora Serverless from Lambda Django?</h3><p>I want to connect to my Aurora Serverless mysql database inside of my django Lambda function. Currently; I have:</p><br><br><ul><br><li>a Lambda function inside of the default VPC<br><br><ul><br><li>Uses the default security group</li><br><li>Uses two public subnets I created</li><br><li>Allows inbound requests from TCP ports 1024 - 65535</li><br><li>Allows outbound requests to Aurora/Mysql on Aurora security group</li><br></ul></li><br><li>an Aurora cluster inside of the default VPC<br><br><ul><br><li>Uses the same (default) VPC as the Lambda</li><br><li>Uses two private subnets I created</li><br><li>Allows inbound requests on port 3306 from Lambda security group</li><br></ul></li><br><li>an internet gateway for the default VPC</li><br><li>a NAT gateway which pipes communications to the internet gateway</li><br><li>a public routing table with the target ID of the internet gateway</li><br><li>a private routing table with the target ID of the NAT gateway</li><br></ul><br><br><p>When I try to deploy my Lambda function to API gateway; the request times out:</p><br><br><pre><code>START RequestId: [request id] Version: $LATEST<br>Instancing..<br>END RequestId: [request id]<br>REPORT RequestId: [request id]  Duration: 30030.15 ms   Billed Duration: 30000 ms   Memory Size: 512 MB Max Memory Used: 49 MB  <br>[time] [request id] Task timed out after 30.03 seconds<br><br></code></pre><br><br><p>When I remove the Lambda function from the VPC (setting the VPC to none in the Lambda function's settings); it deploys to API gateway without any problems. This has led me to believe that my problem is with the VPC as opposed to with my database.</p><br><br><p>I used <a href="https://stackoverflow.com/questions/39144688/aws-lambda-invoke-not-calling-another-lambda-function-node-js">this</a> question to try and rearrange the VPC to get it working but it has not worked.</p><br><br><p>Any help with getting the API to run and connect to the Aurora serverless cluster or alternative ways to use a database with django and Lambda functions would be great. Thanks!</p><br>
0.0,0.3333333333333333,0.0,1.0,0.0,0.0,0.0,<h3>Where to find the externally accessible IP address when accessing an RDS instance in a VPC?</h3><p>I have made an RDS instance inside a VPC publicly accessible:</p><br><p><a href="https://i.stack.imgur.com/DZNA0.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/DZNA0.png" alt="enter image description here" /></a></p><br><p>Will there be a publicly accessible IP address now?  Where would I find it in the <code>aws console</code> ?</p><br>
0.0,0.6666666666666666,0.3333333333333333,0.0,0.3333333333333333,0.6666666666666666,0.0,<h3>add AWS API gateway mapping template from sam template or swagger 2.0</h3><p>I am creating a aws based app with sam template So; I have my custom lambda authorizer so I need to add mapping template to the methods and i cant find a template that can do that</p><br>
0.0,0.3333333333333333,0.0,0.0,1.0,0.0,0.3333333333333333,<h3>Migrate Windows Server c3.large (old generation) to t3.large</h3><p>I&#39;m trying to migrate a Windows Server c3.large to t3.large. The first problem I faced is that t3 instances need to have ENA enabled where c3 doesn't.</p><br><br><p>So I get through this documentation following all the steps to update all drivers:<br><a href="https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/migrating-latest-types.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/migrating-latest-types.html</a></p><br><br><p>So the problem is when I've done steps 1; 2 and 3 I stop the instance and I go to the AWS dashboard and change type c3.large to t3.large. At this point the instance can not be started because ENA support is disabled; so I enabled it as this doc explains:<br><a href="https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/enhanced-networking-ena.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/enhanced-networking-ena.html</a></p><br><br><p>But once is enabled I can start the instance but I lose the connectivity with it; I can't connect; I can't even make a screenshot / nor access to the instance log from the aws dashboard.</p><br><br><p>I've read that it may be caused by a problem with the ENA kernel; have any of you faced this issue? Is something I'm missing?</p><br><br><p>Sincerely;</p><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>AWS Cognito Facebook and Phone number login</h3><p>We are building an app that requires social logins (Apple; Facebook; Instagram; Google; etc) and also phone number login (where the user receives a code via SMS to login)</p><br><br><p>This is a similar sign-in model to Tinder/Bumble dating apps</p><br><br><hr><br><br><p>We are using AWS for our backend and want to use AWS Cognito for our Authorization/Authentication <br>I am having difficulty finding out a way to get AWS Cognito working with our setup:</p><br><br><p>Currently; AWS Cognito provides 2 ways of authorization.</p><br><br><ol><br><li>Cognito User Pools</li><br><li>Cognito Identity Pool</li><br></ol><br><br><p>We have developed the Phone number login method with User Pool and the social login methods i.e. Google and Facebook are Federated via. Identity Pool.</p><br><br><p>The issue with the Federated Signing-in method is that the JWT tokens are not returned by Cognito; instead; temporary AWS credentials are returned (which for now works efficiently with the system).</p><br><br><p>But since AWS does not allow us to use two authorizers at a time; we then needed to use the temporary credentials instead of JWT tokens provided by the user pool. The issue here is that the temporary credentials get expired in a maximum time of 12 hours from the time they have been issued and in case of phone number login; the user needs to then login back to generate the credentials again which degrades the User Experience.</p><br><br><p>Though Google and Facebook log in can also be integrated via User Pool and retrieve JWT Token in return; the issue is that a UI hosted by Cognito appears which again prompts the user with several login methods used in the app; which again is a bad UX as user needs to tap on Sign-in in with Facebook twice to sign-in.</p><br><br><p><strong>Is there a way to have Phone number login/social login without:</strong></p><br><br><ol><br><li>Using Hosted UI (My understanding is that the Hosted UI cannot be changed to appear how we want; ie: like our original sign-in page designs)</li><br><li>Forcing the user to log in again ever 12 hours</li><br><li>Forcing the user to log in twice (once in our app and once in the Cognito provided UI)</li><br></ol><br><br><hr><br><br><p>Thank you for your time</p><br>
0.0,0.0,0.0,0.0,1.0,1.0,0.0,<h3>AWS lambda missing few SQS event miss leading to message in flight</h3><p>My Lambda configuration is as below<a href="https://i.stack.imgur.com/jriYU.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/jriYU.png" alt="Lambda cofiguration" /></a></p><br><p>Lambda Concurrency is set to 50<br>And SQS trigger batch size is set to 1</p><br><p>Issue:<br>When my queue is flooded with 200+ messages; some of the sqs triggers are missed and the message from the queue goes to inflight state without even triggering the lambda. This is adding a latency in processing by the timeout value set for lambda as I need to wait for the message to come out of flight for it to be reprocessed.</p><br><p>Any inputs will be highly appreciated.</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>AWS Cognito Password Regex - Specific to AWS Cognito</h3><p>Can someone give me the regex to match a valid AWS Cognito password - with numbers; special characters (their list); lower and upper case letters</p><br><p>The AWS Cognito default length limit is 6 characters and has it's own list of special characters</p><br><p>Note that the AWS Congito password regex is specific to AWS Congnito - not just a general password regex.</p><br>
0.0,0.0,0.0,1.0,0.0,0.6666666666666666,0.0,<h3>The specified method is not allowed against this resource in Amazon aws</h3><p>Guys I may or may not be wrong; But seriously; I am struggling with file uploading problem in Amazon S3 bucket. When I am trying to hit on the request then I am getting the following error. </p><br><br><blockquote><br>  <p><strong>MethodNotAllowed</strong> and <strong>The specified method is not allowed against this resource</strong></p><br></blockquote><br><br><p>The above message is the sort of the below response. </p><br><br><pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;Error&gt;&lt;Code&gt;MethodNotAllowed&lt;/Code<br>&lt;Message&gt;Thespecified method is not allowed against this resource.&lt;/Message&gt;<br>&lt;Method&gt;POST&lt;/Method&gt;&lt;ResourceType&gt;OBJECT&lt;/ResourceType&gt;<br>&lt;RequestId&gt;xxx&lt;/RequestId&gt;&lt;HostId&gt;xxxx&lt;/HostId&gt;&lt;/Error&gt;<br></code></pre><br><br><p>The above message is the complete message and below is the code whatever I have written for uploading files to amazon s3 server. </p><br><br><pre><code>public synchronized boolean uploadFile(String url; String name; File file) {<br>    HttpEntity entity = MultipartEntityBuilder.create()<br>            .addPart("file"; new FileBody(file)).build();<br>    HttpPost request = new HttpPost(url);<br>    request.setEntity(entity);<br>    HttpClient client = HttpClientBuilder.create().build();<br>    try {<br>        HttpResponse response = client.execute(request);<br>        entity = response.getEntity();<br>        if (entity != null) {<br>            try (InputStream in_stream = entity.getContent()) {<br>                BufferedReader in = new BufferedReader(new InputStreamReader(in_stream));<br>                String inputLine;<br>                StringBuilder responseBuffer = new StringBuilder();<br>                while ((inputLine = in.readLine()) != null) {<br>                    responseBuffer.append(inputLine);<br>                }<br>                in.close();<br>                String a = responseBuffer.toString();<br>                Utils.print("\n\n" + a + "\n\n");<br>            }<br>        }<br>        return true;<br>    } catch (Exception e) {<br>        Utils.print(e);<br>    }<br>    return false;<br>}<br></code></pre><br><br><p>Please suggest me what to do for this? I will be very thankful for your expected answer. </p><br>
0.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.0,<h3>How to get memory/cpu values for instance types programatically?</h3><p>I want to find out memory/cpu info from AWS instance type pro-grammatically. Let consider I know my instance type is t2.medium. From this instance type information how can I get memory and cpu. This particular instance type has 4 GB RAM and 2 vCPU.</p><br>
0.6666666666666666,1.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>Redshift ODBC Driver test fails: Is the server running on host and accepting TCP/IP connections on port 5439?</h3><p>I have installed the 64x Amazon Redshift ODBC driver on my Windows 10 and it shows up under <em>System DSN</em> tab of <em>ODBC Data Source Administrator (64-bit)</em>. I click on <strong>Configure</strong> and as I enter the connection information from my Amazon Redshift cluster properties; I face the following error:</p><br><p><a href="https://i.stack.imgur.com/hapVp.jpg" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/hapVp.jpg" alt="enter image description here" /></a></p><br><p>Here are the current Network Security settings of the cluster. The only thing I have changed is that I have made it <em>Publicly accessible</em> (the value changed from No to Yes). But I haven't changed anything in VPC and VPC security group. As far as I see; all Inbound and Outbound traffic is allowed.</p><br><p><a href="https://i.stack.imgur.com/G2ueH.jpg" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/G2ueH.jpg" alt="enter image description here" /></a></p><br><p>Anything I might have missed?<br>Should I add a firewall rule to my local machine for this port as well?</p><br><p><strong>UPDATE:</strong></p><br><p>Inbound rules for the VPC Group<br><a href="https://i.stack.imgur.com/3sOl4.jpg" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/3sOl4.jpg" alt="enter image description here" /></a></p><br><p>Outbound rules for the VPC Group<br><a href="https://i.stack.imgur.com/rIVb3.jpg" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/rIVb3.jpg" alt="enter image description here" /></a></p><br>
0.3333333333333333,0.0,0.0,0.0,0.0,1.0,0.3333333333333333,<h3>AWS qna-bot returning messages with new line</h3><p>I am working with the Cloudformation script from <a href="https://github.com/aws-samples/aws-ai-qna-bot" rel="nofollow noreferrer">https://github.com/aws-samples/aws-ai-qna-bot</a> to build a chatbot. A lambda function is used to return a list of open jobs and the newline characters for python and html both are absorbed from the text but the new line does not appear. I've found that to fix this we must modify the message.vue file but I have been unable to find any fixes.<br>Here is what it currently looks like: <a href="https://i.stack.imgur.com/qrLJN.png" rel="nofollow noreferrer">Chatbot Jobs</a></p><br>
0.0,1.0,0.3333333333333333,0.0,0.3333333333333333,0.3333333333333333,0.0,<h3>No response or output running aws command</h3><p>There's a task I need to complete:</p><br><ol><br><li>list out all the count of AWS Network Interface - which I ran this</li><br></ol><br><pre><code>aws ec2 describe-network-interfaces --filters Name=attachment.status;Values=attached --query 'length(NetworkInterfaces)' <br></code></pre><br><p>But it returns blank; nothing shows<br>[enter image description here][1]</p><br><p><a href="https://i.stack.imgur.com/VRIb5.png" rel="nofollow noreferrer">https://i.stack.imgur.com/VRIb5.png</a></p><br><p>Would anyone know any troubleshooting steps I can take to figure out why its showing blank?</p><br><p>Ideally; I need to get a count and list of all network interfaces that are not in used; so I can remove them.</p><br>
0.0,1.0,1.0,0.0,0.0,0.0,0.0,<h3>creating an iam role for a specific domain</h3><p>How can I create a role where an EC2 instance has full access to route53 except a specific domain? I have been looking at the condition and can't figure out how to give full access except a specific domain</p><br>
0.0,1.0,0.6666666666666666,0.0,0.0,0.0,0.0,<h3>associate custom Elastic IP to NAT Gateway with AWS CDK</h3><p>After struggling with that for several hours; here is my question. I am using CDK to create a VPC in the most simple form currently:</p><br><pre><code>let vpc = new Vpc(this; &quot;myVpc&quot;; {maxAzs: 1});<br></code></pre><br><p>This gets me a public Subnet and a private one with the all the Gateways (internet and NAT). My NAT Gateway got a public EIP from the AWS pool. Of course when i destroy the stack and re-create it; i will get a new EIP from AWS; but <strong>THIS</strong> i dont want.</p><br><p><strong>What i want is:</strong><br>Creating an Elastic IP outside of my CDK project (manually via CLI or AWS Console) and attach it to my NAT GW; so that even after destroying the stack; i can re-attach my (external) EIP to the &quot;new&quot; NAT GW.</p><br><p>So there must be a way to <em>not</em> have the AWS::EC2::NatGateway created automatically by the VPC but manually with the proper EIP association and then attach it to the VPC / Public Subnet. Pretty much the same way i can explicitly define Subnets and associate them with the VPC instead of CDK construct magic.</p><br>
0.0,0.0,0.0,1.0,0.0,0.3333333333333333,0.0,<h3>How to delete nested JSON attribute in dynamodb / python</h3><p>I have a simple dynamodb database that uses &quot;League&quot; as a partition key and &quot;Team&quot; as a sort key to store all roster data under a &quot;Players&quot; attribute field that uses a JSON format. I would like to access and delete a specific player (Larry Bird or Jayson Tatum in this case); however; I am having trouble accessing the schema correctly to delete the specific key and values; especially given that Players.Jayson Tatum wont work because it is two separate words. Here is the basic skeleton code for the function so far:</p><br><pre><code>def lambda_handler(event; context):<br><br>    newLeague = None<br>    newTeam = None<br>    newPlayer = None<br>    statusCode = 200<br><br>    if checkKey(event; 'League'):<br>        newLeague = event['League']<br>    <br>    if checkKey(event; 'Team'):<br>        newTeam = event['Team']<br>    <br>    if checkKey(event; 'Player'):<br>        newPlayer = event['Player']<br><br>    if newLeague != None and newTeam != None and newPlayer != None:<br>        retrievedData = table.delete_item(<br>           Key = {<br>               'League': newLeague;<br>               'Team': newTeam;<br>           }<br>        )<br></code></pre><br><p>Database layout in Dynamodb for reference</p><br><p><a href="https://i.stack.imgur.com/AWC4L.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/AWC4L.png" alt="Database layout in Dynamodb for reference" /></a></p><br>
0.0,0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,0.3333333333333333,<h3>Terragrunt path resolution - locally referenced modules - splitting into multiple environments</h3><p>For business-related reasons; I am not able to split my infrastructure using versioned modules. As it is still beneficial to split the environments; and I would like to avoid the copy/paste root modules; which are basically just instantiation of child modules; over multiple directories; each representing its own environment; I would like to specify the source of root module in <em>terragrunt.hcl</em></p><br><p>To make life easier; I have built a small part of the current infrastructure; just with a single module to speed up the development.</p><br><p>The current structure of the project looks like this:</p><br><pre><code> config<br>    common.tfvars<br>    terragrunt.hcl<br> environments<br>    dev<br> infrastructure-modules<br>    ecs<br> terraform-modules<br>    terraform-aws-ecr<br></code></pre><br><p>All my infrastructure is described in <code>infrastructure-modules/ecs/*.tf</code> files; which are basically just instantiating child modules declared in <code>terraform-modules/terraform-aws-*/</code>.</p><br><p>With that; I can simply execute the terragrunt (terraform commands) from the <code>infrastructure-modules/ecs</code> directory.</p><br><p>To have a possibility to create the same environment in another account; I have introduced a new directory <code>environments/dev/eu-central-1/ecs</code> as shown on tree output from the root directory.</p><br><p>The <code>environments/dev/eu-central-1/ecs</code>; consists just of two files; <em>terragrunt.hcl</em> and <em>common.tfvars</em>.</p><br><p>I guess; that the usage of the <em>common.tfvars</em> is quite self-explanatory; where my <em>terragrunt.hcl</em> consists of <code>remote_state {}</code> and <code>terraform {}</code> blocks.</p><br><p>The important part of the terragrunt configuration file:</p><br><pre><code>remote_state {}<br><br>terraform {<br>  source = &quot;../../../../infrastructure-modules/ecs&quot;<br><br>  {...}<br>}<br></code></pre><br><p>Above I am basically referencing my root modules; declared in <code>infrastructure-modules/ecs/*.tf</code>. Where my root modules are instantiating child-modules declared in <code>terraform-modules/terraform-aws-*/</code>.</p><br><p>Child modules from <code>infrastructure-modules/ecs/*.tf</code> are instantianed like this:</p><br><pre><code>module my_module {<br>  source = &quot;../../terraform-modules/terraform-aws-*&quot;<br><br>  {...}<br>}<br></code></pre><br><p>In an ideal world; I would be able to execute terragrunt (terraform) commands from <code>environments/dev/eu-central-1/ecs</code> directory; but as I am using local (relative) paths; this is failing during the <em>initialization</em> of the modules; as the root module <strong>my_module</strong> loads the child module with following relative path:</p><br><pre><code>module my_module {<br>  source = &quot;../../terraform-modules/terraform-aws-*&quot;<br><br>  {...}<br>}<br></code></pre><br><p>This is causing a module instantiation in <code>environments/dev/eu-central-1/ecs</code> to fail as the relative path is different; based on parent module instantiation.</p><br><pre><code>Initializing modules...<br>- my_module in <br><br>Error: Unreadable module directory<br><br>Unable to evaluate directory symlink: lstat ../../terraform-modules: no such<br>file or directory<br></code></pre><br><p>So far; according to the documentation; <code>path_relative_*</code>; should be able to return the relative path between the path specified in its include block and the current terragrunt.hcl; but the problem here is that I am not having any <code>include {}</code> block(s) within my <code>terragrunt.hcl</code> files and thus this approach doesn't works. Symlinks are the last option.</p><br><p><strong>EDIT</strong></p><br><p>If I inspect the <code>.terragrunt-cache/*</code> on path <code>environments/dev/eu-central-1/ecs</code> I can confirm that all the &quot;root&quot; modules have been downloaded(copied over) into cache directory.</p><br><p>However; the module is being instantiated like this; and it tries to fetch the actual modules (Terraform modules) from directory two levels above.</p><br><pre><code>module my_modules {<br>  source = &quot;../..//terraform-modules/terraform-aws-ecr&quot;<br></code></pre><br><p>So basically; I need to tell Terragrunt to download/fetch the modules from other path.</p><br><p><strong>EDIT 2:</strong></p><br><p>Inspecting .terragrunt-cache in the directory where I am running <code>init</code> shows; the the <code>terraform-modules</code> are never downloaded in the<br><code>terraform-infrastructure/environments/dev/eu-central-1/ecs/.terragrunt-cache/</code>.</p><br><p>If I change my <code>terraform-infrastructure/infrastructure-modules/ecs/ecr-repos.tf</code>; from</p><br><pre><code>module ecr_lz_ingestion {<br>  source = &quot;../../terraform-modules//terraform-aws-ecr&quot;<br><br>{&lt;MODULE_ARGUMENTS&gt;}<br>  }<br>}<br></code></pre><br><p>to:</p><br><pre><code>module ecr_lz_ingestion {<br>  source = &quot;../../../../../../../../terraform-modules//terraform-aws-ecr&quot;<br><br>{&lt;MODULE_ARGUMENTS&gt;}<br>  }<br>}<br></code></pre><br><p>Terraform is able to initialize the child-modules as I have given a relative path to <code>terraform-modules/</code> in the directory root; which is obviously a workaround.</p><br><p>Somehow I am expecting the Terragrunt to download both directories; <code>terraform-modules</code> and <code>infrastructure-modules</code> for the relative paths in module instantiation to work.</p><br>
0.3333333333333333,0.0,0.0,0.3333333333333333,0.0,0.0,1.0,<h3>In AWS IoT analytics; for the Service-managed store; which AWS database is used in the background and which AWS service is used to query the dataset?</h3><p>In AWS IoT analytics; for the Service-managed store; which AWS database is used in the background to hold the data?</p><br><p>And when this data store is queried via the IoT analytics dataset; does it use the AWS Athena service in the background?</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>AWS SageMaker - How to load trained sklearn model to serve for inference?</h3><p>I am trying to deploy a model trained with sklearn to an endpoint and serve it as an API for predictions. All I want to use sagemaker for; is to deploy and server model I had serialised using <code>joblib</code>; nothing more. every blog I have read and sagemaker python documentation showed that sklearn model had to be trained on sagemaker in order to be deployed in sagemaker.</p><br><p>When I was going through the SageMaker documentation I learned that sagemaker does let users <a href="https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#load-a-model" rel="nofollow noreferrer">load a serialised model</a> stored in S3 as shown below:</p><br><pre><code>def model_fn(model_dir):<br>    clf = joblib.load(os.path.join(model_dir; &quot;model.joblib&quot;))<br>    return clf<br></code></pre><br><p>And this is what documentation says about the argument <code>model_dir</code>:</p><br><blockquote><br><p>SageMaker will inject the directory where your model files and<br>sub-directories; saved by save; have been mounted. Your model function<br>should return a model object that can be used for model serving.</p><br></blockquote><br><p>This again means that training has to be done on sagemaker.</p><br><p>So; is there a way I can just specify the S3 location of my serialised model and have sagemaker de-serialise(or load) the model from S3 and use it for inference?</p><br><h2>EDIT 1:</h2><br><p>I used code in the answer to my application and I got below error when trying to deploy from notebook of SageMaker studio. I believe SageMaker is screaming that training wasn't done on SageMaker.</p><br><pre><code>---------------------------------------------------------------------------<br>ValueError                                Traceback (most recent call last)<br>&lt;ipython-input-4-6662bbae6010&gt; in &lt;module&gt;<br>      1 predictor = model.deploy(<br>      2     initial_instance_count=1;<br>----&gt; 3     instance_type='ml.m4.xlarge'<br>      4 )<br><br>/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py in deploy(self; initial_instance_count; instance_type; serializer; deserializer; accelerator_type; endpoint_name; use_compiled_model; wait; model_name; kms_key; data_capture_config; tags; **kwargs)<br>    770         &quot;&quot;&quot;<br>    771         removed_kwargs(&quot;update_endpoint&quot;; kwargs)<br>--&gt; 772         self._ensure_latest_training_job()<br>    773         self._ensure_base_job_name()<br>    774         default_name = name_from_base(self.base_job_name)<br><br>/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py in _ensure_latest_training_job(self; error_message)<br>   1128         &quot;&quot;&quot;<br>   1129         if self.latest_training_job is None:<br>-&gt; 1130             raise ValueError(error_message)<br>   1131 <br>   1132     delete_endpoint = removed_function(&quot;delete_endpoint&quot;)<br><br>ValueError: Estimator is not associated with a training job<br></code></pre><br><p>My code:</p><br><pre><code>import sagemaker<br>from sagemaker import get_execution_role<br># from sagemaker.pytorch import PyTorchModel<br>from sagemaker.sklearn import SKLearn<br>from sagemaker.predictor import RealTimePredictor; json_serializer; json_deserializer<br><br>sm_role = sagemaker.get_execution_role()  # IAM role to run SageMaker; access S3 and ECR<br><br>model_file = &quot;s3://sagemaker-manual-bucket/sm_model_artifacts/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix<br><br>class AnalysisClass(RealTimePredictor):<br>    def __init__(self; endpoint_name; sagemaker_session):<br>        super().__init__(<br>            endpoint_name;<br>            sagemaker_session=sagemaker_session;<br>            serializer=json_serializer;<br>            deserializer=json_deserializer;   # To be able to use JSON serialization<br>            content_type='application/json'   # To be able to send JSON as HTTP body<br>        )<br><br>model = SKLearn(model_data=model_file;<br>                entry_point='inference.py';<br>                name='rf_try_1';<br>                role=sm_role;<br>                source_dir='code';<br>                framework_version='0.20.0';<br>                instance_count=1;<br>                instance_type='ml.m4.xlarge';<br>                predictor_cls=AnalysisClass)<br>predictor = model.deploy(initial_instance_count=1;<br>                         instance_type='ml.m4.xlarge')<br></code></pre><br>
1.0,0.0,0.3333333333333333,0.3333333333333333,0.0,0.3333333333333333,0.0,<h3>Trigger AWS Rekognition Custom Labels via Lambda</h3><p>My s3 bucket and AWS Rekognition model are both in us-east-1. My lambda which is also in us-east-1 is triggered by an upload into my s3 bucket. I pasted the auto generated code from the model (for python) and used it in my lambda function.  I have even tried giving full access to my s3 bucket (allowing public access with full permission) but when I trigger the lambda I get this exception</p><br><pre><code>[ERROR] InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectCustomLabels operation: Unable to get object metadata from S3. Check object key; region and/or access permissions.<br>Traceback (most recent call last):<br>  File &quot;/var/task/lambda_function.py&quot;; line 80; in lambda_handler<br>    label_count=show_custom_labels(model;bucket;photo; min_confidence)<br>  File &quot;/var/task/lambda_function.py&quot;; line 59; in show_custom_labels<br>    response = client.detect_custom_labels(Image={'S3Object': {'Bucket': bucket; 'Name': photo}};<br>  File &quot;/var/runtime/botocore/client.py&quot;; line 357; in _api_call<br>    return self._make_api_call(operation_name; kwargs)<br>  File &quot;/var/runtime/botocore/client.py&quot;; line 676; in _make_api_call<br>    raise error_class(parsed_response; operation_name)<br></code></pre><br><p>I am printing out my bucket name and key in the logs and they are fine. My key has a folder path included (folder1/folder2/image.jpg).</p><br><p>How can I get over this error?</p><br>
0.0,0.0,1.0,0.0,0.0,1.0,0.0,<h3>AWS Cognito Identity Service Provider appears to store access token in local storage. Is this safe?</h3><p>We are developing an application that uses a React front end website hosted on AWS using Amplify. This communicates with a .NET Core 3.1 Web API running on EC2 / Elastic Beanstalk. Cognito is used for user authentication with the Web API configured to use JWT tokens.</p><br><p>It works OK; but we have noticed that the Cognito provider stores the JWT access token in the browser local storage. This is what we see using F12 in Chrome and inspecting local storage.</p><br><p><a href="https://i.stack.imgur.com/JX1cw.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/JX1cw.png" alt="enter image description here" /></a></p><br><p>From what we have read; storing access tokens in local storage is not advised as it makes the application susceptible to XSS attacks. Strange then; that the Cognito identity provider chooses to store sensitive information here.</p><br><p>If this approach is not considered safe; can the provider be configured to store this information elsewhere; such as cookies?</p><br><p>Alternatively; as we control both front and back ends; is there an alternative method that can be used to secure the API that does not involve tokens? Obviously the API needs to know which user is logged on to the web application in order to perform authorization checks. [Note authorization in the application is record level and defined in database tables; so it goes beyond simple user profile attributes.]</p><br><p>Many thanks in advance for your advice.</p><br><p>Doug</p><br>
0.3333333333333333,0.0,0.0,1.0,0.0,0.0,0.0,<h3>Amazon RDS or S3 Bucket to store game scores?</h3><p>I am building a game using Unity and I want to track how well an individual does over a period of time. So I want to save their time; level; high score; etc. I am writing the scores to either a .txt file or .json file at the end of the game. The game will be deployed to Android OS (maybe IOs). I want the file to be sent off before the game returns to the home menu.</p><br><p>I wanted to know what is the better option for collecting the game data. Amazon RDS or S3 Bucket?</p><br>
0.0,0.0,0.0,1.0,0.3333333333333333,1.0,0.0,<h3>Post request with strange behavior</h3><p>I have a node / express server set up; which is communicating with a mysql database (both of which are hosted on AWS; in elastic beanstalk environment and RDS instance respectively). I have created a simple create account authentication project in react; which I am currently running locally. I communicate to the server with a post request.</p><br><pre><code>createAccount = (event) =&gt; {<br>  console.log(&quot;hello from createAccount&quot;);<br>  const isValid = this.validate();<br>  if (!isValid) {<br>    console.log(&quot;not valid&quot;);<br>    return false;<br>  } else {<br>    const formData = new FormData();<br>    formData.append(&quot;firstName&quot;; this.state.firstName);<br>    formData.append(&quot;lastName&quot;; this.state.lastName);<br>    formData.append(&quot;username&quot;; this.state.username);<br>    formData.append(&quot;passcode&quot;; this.state.passcode);<br>    console.log(formData);<br><br>    fetch(&quot;https://www.myurl.com/createUser&quot;; {<br>      method: &quot;POST&quot;;<br>      headers: new Headers({<br>        &quot;Access-Control-Allow-Origin&quot;: &quot;*&quot;;<br>      });<br>      body: formData;<br>    });<br>  }<br>};<br></code></pre><br><p>And here is the server side code:</p><br><pre><code>var mysqlConnection = mysql.createConnection({<br>  host: &quot;my-endpoint&quot;;<br>  user: &quot;****&quot;;<br>  password: &quot;****&quot;;<br>  port: &quot;****&quot;;<br>  database: &quot;****&quot;;<br>});<br><br>app.post(&quot;/createUser&quot;; function (request; response) {<br>  console.log(&quot;/createUser&quot;);<br>  console.log(request.body);<br>  const createUser = `CALL CreateUser('${request.body.firstName}'; '${request.body.lastName}'; '${request.body.username}'; '${request.body.passcode}')`;<br>  mysqlConnection.query(createUser; (error; result) =&gt; {<br>    if (error) {<br>      console.log(error);<br>    } else {<br>      console.log(&quot;SUCCESSFULLY CREATED A USER ACCOUNT&quot;);<br>      response.set(&quot;Access-Control-Allow-Origin&quot;; &quot;*&quot;);<br>      mysqlConnection.end();<br>    }<br>  });<br>});<br></code></pre><br><p>However; my post request is doing something strange...about 2/3 of the time; all goes as expected. The user creates an account and the information is successfully added to my database. However; the rest of the time everything seems to run fine (through examination of my console.log check points); but the information is not added to my database...the user does not successfully create an account. When this happens; I open up the logs from AWS to examine what the server is doing; and nothing is printed that I would expect. My new account information is not printed (while is usually is); and there is not an error printed either. This is a head scratcher. Is there some reason my post requests would not reach my server; BUT only part of the time? I am at a loss...</p><br><p>I ran the application totally in my local environment (changed my request to my local server as opposed to the one hosted on elastic beanstalk). This worked 100% of the time (please note that this is the same exact server code). I am perplexed as to this behavior; which is apparently something to do with the hosted version on elastic beanstalk in AWS. What is causing this behavior; and how can I fix it?</p><br><p>I switched back to the hosted server for further testing. I was just spam creating accounts to see if I could notice any patterns. The application creates a successful account about 1 in every 5 times. The other four times it will not save any info to the database. It is not a problem with my field data (I have implemented a little error checking); because I just spam the same info over and over until it finally creates. It eventually does; but only after about 4 tries.</p><br><p>Below is my post request with the added connection.end(); as well as where I initially connection to my database as requested in the comments.</p><br><pre><code>var mysqlConnection = mysql.createConnection({<br>  host: &quot;my-endpoint&quot;;<br>  user: &quot;****&quot;;<br>  password: &quot;****&quot;;<br>  port: &quot;****&quot;;<br>  database: &quot;****&quot;;<br>});<br><br>app.post(&quot;/createUser&quot;; function (request; response) {<br>  console.log(&quot;/createUser&quot;);<br>  console.log(request.body);<br>  const createUser = `CALL CreateUser('${request.body.firstName}'; '${request.body.lastName}'; '${request.body.username}'; '${request.body.passcode}')`;<br>  mysqlConnection.query(createUser; (error; result) =&gt; {<br>    if (error) {<br>      console.log(error);<br>    } else {<br>      console.log(&quot;SUCCESSFULLY CREATED A USER ACCOUNT&quot;);<br>      response.set(&quot;Access-Control-Allow-Origin&quot;; &quot;*&quot;);<br>      mysqlConnection.end();<br>    }<br>  });<br>});<br></code></pre><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>aws cli does not ask for MFA code on the test user</h3><p>It was recent past that I started working on AWS IAM.</p><br><p>My task is to ensure for a particular user; MFA code needs to be asked for all the commands when triggered from AWS CLI using temporary access credentials.</p><br><p>Here is what I did;</p><br><p>Using <code>get-session-token</code> I created the temporary credentials and set them in a profile.</p><br><p>when i execute <code>aws s3 ls --profile &lt;profile_name&gt;</code>; the cli does not ask for MFA code.</p><br><p>Unfortunately; nothing helped me out even though I referred many articles and responses on stackoverflow.</p><br><p>Please find the policy and the profile configuration that were set and used.</p><br><pre><code>{<br>&quot;Version&quot;: &quot;2012-10-17&quot;;<br>&quot;Statement&quot;: [<br>    {<br>        &quot;Sid&quot;: &quot;VisualEditor0&quot;;<br>        &quot;Effect&quot;: &quot;Allow&quot;;<br>        &quot;Action&quot;: &quot;*&quot;;<br>        &quot;Resource&quot;: &quot;*&quot;;<br>        &quot;Condition&quot;: {<br>            &quot;BoolIfExists&quot;: {<br>                &quot;aws:MultiFactorAuthPresent&quot;: &quot;true&quot;<br>            }<br>        }<br>    }<br>  ]<br>}<br></code></pre><br><p>./aws/credentials file</p><br><pre><code>[mfa_user]<br>aws_access_key_id = &lt;AccessKeyId&gt;<br>aws_secret_access_key = &lt;SecretAccessKey&gt;<br>aws_session_token = IQoJb3JpZ2luX2VjEKn//////////<br>mfa_serial = arn:aws:iam::9xxxxxxxxxxxx:mfa/some-user<br></code></pre><br><p>Is there something that I am missing?</p><br><p>I followed the various online articles and nothing helped me out.</p><br><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/authenticate-mfa-cli/" rel="nofollow noreferrer">https://aws.amazon.com/premiumsupport/knowledge-center/authenticate-mfa-cli/</a></p><br><p><a href="https://stackoverflow.com/questions/28177505/enforce-mfa-for-aws-console-login-but-not-for-api-calls">enforce MFA for AWS console login; but not for API calls</a></p><br>
0.0,0.6666666666666666,0.3333333333333333,0.0,0.3333333333333333,0.0,0.0,<h3>AWS already made EC2 instance and forgot to create port 5000</h3><p>I need a port for my ec2 instance; and I only have ports 80 443 22 and 8080 in the security group. Is there any port one can use that doesn't require to be put in the security group?<br>Thanks!</p><br>
1.0,0.0,0.3333333333333333,0.0,0.0,0.0,0.0,<h3>Send log data to kinesis stream in another account</h3><p>I have a log group in Account A that I want to send to a kinesis stream in another account B.<br>I tried the steps mentioned in <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CrossAccountSubscriptions.html</a> but it fails.</p><br><p>I am new to AWS and I am unsure if I implemented it correctly. Please help me figure this thing out.</p><br><p>I did the following :</p><br><ol><br><li>Created a kinesis stream in Account B.</li><br></ol><br><pre><code>        const kinesisStream = new Stream(this; 'KinesisStream'; {<br>              streamName: &quot;KinesisStream&quot;;<br>              shardCount: 2<br>        });<br></code></pre><br><ol start="2"><br><li>Created an IAM role in Account B.</li><br></ol><br><pre><code>        const crossAccountRole = new iam.Role(this; 'crossAccountRole'; {<br>              assumedBy: new iam.ServicePrincipal('logs.amazonaws.com');<br>        });<br></code></pre><br><ol start="3"><br><li>Attached a trust policy and permission policy to the above role in Account B.</li><br></ol><br><pre><code>         crossAccountRole.assumeRolePolicy?.addStatements(new iam.PolicyStatement({<br>              effect: iam.Effect.ALLOW;<br>              principals: [new iam.ServicePrincipal('logs.amazonaws.com')];<br>              actions: ['sts:AssumeRole']<br>        }));<br><br>        crossAccountRole.addToPolicy(new iam.PolicyStatement({<br>              actions: ['kinesis:PutRecord'];<br>              effect: iam.Effect.ALLOW;<br>              resources: [kinesisStream.streamArn]<br>        }));<br></code></pre><br><ol start="4"><br><li>Created a destination in Account B</li><br></ol><br><pre><code>        const crossAccountDestination = new CrossAccountDestination(this; 'LogDestination'; {<br>              targetArn: &quot;kinesisStream.streamArn&quot;;<br>              role: crossAccountRole;<br>              destinationName: &quot;LogDestination&quot;<br>        });<br><br> const cfnCrossAccountDestination = crossAccountDestination.node.defaultChild as CfnDestination;<br><br>        cfnCrossAccountDestination.destinationPolicy = JSON.stringify({<br>            Version : '2012-10-17';<br>            Statement : [<br>                {<br>                    Sid : '';<br>                    Effect : 'Allow';<br>                    Principal : {<br>                        &quot;AWS&quot; : '111111111111' // Account A<br>                    };<br>                    Action : 'logs:PutSubscriptionFilter';<br>                    Resource : 'arn:aws:logs:${props.region}:AccountB:destination:LogDestination'<br>                }<br>            ]<br>        });<br></code></pre><br><ol start="5"><br><li>Created a subscription filter for the logs in Account A.</li><br></ol><br>
0.0,1.0,0.3333333333333333,0.0,0.0,0.0,0.0,<h3>How to measure the time taken by ALB/NLB to route a request to a target group?</h3><p>It is my understanding that both ALB and NLB take finite time to route a request to any target group. NLB is stated to be much faster than ALB at this. I am trying to understand how much time each of those take in order to make a decision about which to pick (for an application for which such tiny distinction matters).</p><br><p>I am trying to understand what would be the time difference between these two hits:</p><br><ul><br><li>Make a request ALB/NLB and measure response time</li><br><li>Make a request to an instance directly and measure response time</li><br></ul><br><p>Essentially; how much time would an ALB/NLB add to a request processing time.</p><br><p>I found the following cloudwatch metric; but it doesn't seem to answer the question:</p><br><p>AWS/ApplicationELB/TargetResponseTime:</p><br><blockquote><br><p>The time elapsed; in seconds; after the request leaves the load balancer until a response from the target is received. This is equivalent to the target_processing_time field in the access logs.</p><br><p>Ref: <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-cloudwatch-metrics.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-cloudwatch-metrics.html</a></p><br></blockquote><br><p>Is such a metric available through cloudwatch? If not; are there any direct ways to measure it (without having to devise ways to do that from client-side)?</p><br>
0.0,0.6666666666666666,0.3333333333333333,0.0,0.0,1.0,0.0,<h3>Failing to call GraphQL from Lambda using Amazon Amplify</h3><p>I'm using Amazon Amplify to create a function that accesses the contentment of an AppSync GraphQL endpoint.<br>Using the command line I create the new function and API and I can call it correctly from my front end application; however; I keep receiving this error:</p><br><pre><code>2020-12-16T01:39:40.524Z    979abb8d-4d64-4929-937a-03b0cb495174    INFO    error posting to appsync:  Error: Request failed with status code 401<br>    at createError (/var/task/node_modules/axios/lib/core/createError.js:16:15)<br>    at settle (/var/task/node_modules/axios/lib/core/settle.js:17:12)<br>    at IncomingMessage.handleStreamEnd (/var/task/node_modules/axios/lib/adapters/http.js:244:11)<br>    at IncomingMessage.emit (events.js:326:22)<br>    at endReadableNT (_stream_readable.js:1223:12)<br>    at processTicksAndRejections (internal/process/task_queues.js:84:21) {<br>  config: {<br>    url: 'https://y63v24inqneirizeuiig7x5g54.appsync-api.us-east-1.amazonaws.com/graphql';<br>    method: 'post';<br>    data: '{&quot;query&quot;:&quot;query listCompetitions {\\n  listCompetitions {\\n    items {\\n      createdAt\\n      id\\n      likes\\n      updatedAt\\n      votes\\n      websiteImageKey\\n      websiteUrl\\n      user {\\n        username\\n        imageKey\\n      }\\n      websiteDescription\\n    }\\n  }\\n}\\n&quot;}';<br>    headers: {<br>      Accept: 'application/json; text/plain; */*';<br>      'Content-Type': 'application/json;charset=utf-8';<br>      'x-api-key': '7aeiasrvb5bczhmlt2nnbpfuyi';<br>      'User-Agent': 'axios/0.21.0';<br>      'Content-Length': 284<br>    };<br></code></pre><br><p>The code I'm using to call GraphQL from my function is this from this <a href="https://docs.amplify.aws/guides/functions/graphql-from-lambda/q/platform/js#query" rel="nofollow noreferrer">link</a>.</p><br><pre><code>const graphqlData = await axios({<br>      url: process.env.API_MAINDATA_GRAPHQLAPIENDPOINTOUTPUT;<br>      method: &quot;post&quot;;<br>      headers: {<br>        &quot;x-api-key&quot;: process.env.API_MAINDATA_GRAPHQLAPIIDOUTPUT;<br>      };<br>      data: {<br>        query: print(listCompetition);<br>      };<br>    });<br></code></pre><br><p>The URL and KEY is added to the env by Amplify; I already added API_KEY auth for the API in question; so I can't understand why I still get <code>401 error</code>.</p><br><p>One thing I notice is that the document says to use:</p><br><p><code>'x-api-key': process.env.API_&lt;YOUR_API_NAME&gt;_GRAPHQLAPIKEYOUTPUT</code></p><br><p>However <code>_GRAPHQLAPIKEYOUTPUT</code> is not been generated so I use <code>_GRAPHQLAPIIDOUTPUT</code> instead; not sure if this is something that changed or if I'm missing an env constant.</p><br>
0.0,0.3333333333333333,0.0,1.0,1.0,1.0,0.3333333333333333,<h3>Express serving static files vs AWS S3</h3><p>are there any benefits to running a static website via S3/CloudFront vs running one via Express (via Fargate) sitting behind API Gateway? In terms of performance or any other considerations? The latter approach allowing api endpoints to also be hosted via Express/API Gateway on the same domain.</p><br>
1.0,0.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>AWS Glue job to unzip a file from S3 and write it back to S3</h3><p>I'm very new to AWS Glue; and I want to use AWS Glue to unzip a huge file present in a S3 bucket; and write the contents back to S3.</p><br><p>I couldn't find anything while trying to google this requirement.</p><br><p>My questions are:</p><br><ol><br><li>How to add a zip file as data source to AWS Glue?</li><br><li>How to write it back to same S3 location?</li><br></ol><br><p>I am using AWS Glue Studio. Any help will be highly appreciated.</p><br>
0.0,0.0,1.0,1.0,0.0,0.0,0.0,<h3>IAM Policy to limit access to S3 Buckets and Object not working</h3><p>I am trying to write a simple IAM policy which</p><br><ol><br><li>Gives access to list all the Buckets from the Console (ListAllMyBuckets and GetBucketLocation)</li><br><li>Allows to writes files to only one of the s3 buckets.</li><br></ol><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Action&quot;: [<br>                &quot;s3:GetObject&quot;;<br>                &quot;s3:PutObject&quot;;<br>                &quot;s3:DeleteObject&quot;<br>            ];<br>            &quot;Resource&quot;: &quot;arn:aws:s3:::test/*&quot;;<br>            &quot;Effect&quot;: &quot;Allow&quot;<br>        };<br>        {<br>            &quot;Action&quot;: [<br>                &quot;s3:ListBucket&quot;<br>            ];<br>            &quot;Resource&quot;: &quot;arn:aws:s3:::test&quot;;<br>            &quot;Effect&quot;: &quot;Allow&quot;<br>        };<br>        {<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Action&quot;: [<br>                &quot;s3:GetBucketLocation&quot;;<br>                &quot;s3:ListAllMyBuckets&quot;<br>            ];<br>            &quot;Resource&quot;: &quot;*&quot;<br>        }<br>    ]<br>}<br></code></pre><br><p>The above policy is still allowing the user to list files from each and every bucket ? I want the user to view the buckets in the console and not the files/folders in the buckets except for one bucket - test.<br>What is wrong with my policy ?</p><br><p>Thanks</p><br>
0.0,0.0,0.0,1.0,0.0,0.3333333333333333,0.0,<h3>Download s3 file only if upload completed</h3><p>At the moment I get a list of files from s3 by a prefix and download them.<br>I want to make sure the file uploaded completely before I start the download.</p><br><p>How is that possible?</p><br><pre><code>bucket='bucket-name'<br>prefix='files_prefix')<br><br>s3_client = boto3.client('s3')<br>objs = s3_client.list_objects_v2(Bucket=bucket; Prefix=prefix)['Contents']<br><br>for key in objs:<br>    # need to add an if here to check if file upload completed<br>    s3_client.download_file(bucket; key['Key'];'{}/{}'.format('./data/';key['Key']))<br></code></pre><br>
0.0,0.0,0.3333333333333333,0.0,0.6666666666666666,0.0,0.0,<h3>AWS [EC2] - I can&#39;t get my Windows password</h3><p>I lost my .PEM key. I created an image of my instance and created a new key; executed the new instance; however; when trying to recover my password it shows the message &quot;Please wait at least 4 minutes after launching an instance before trying to retrieve the auto-generated password. &quot;. But it's been 24 hours and nothing.</p><br><p>Has anyone had a similar problem? What can I do?</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>MEaning of this CLoudWatch log below. (New to AWS)</h3><p>[ERROR] ClientError: An error occurred (AccessDeniedException) when calling the CreateImportJob operation: User: arn:aws:sts::570646387953:assumed-role/ajklsf/poop is not authorized to perform: iam:PassRole on resource: arn:aws:iam::570646387953:role/service-role/ajklsf</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>AWS Amplify - Changing Between Stages</h3><p>I have an Amplify project  and an API Gateway; I would like to be able to specify to use my &quot;dev&quot; stage of the api from my &quot;dev&quot; git branch or in my &quot;dev&quot; amplify env but from what I've read I can only change what stage is used by changing the aws-export.js file which explicitly says</p><br><p><code>&quot;// WARNING: DO NOT EDIT. This file is automatically generated by AWS Amplify. It will be overwritten.&quot;</code></p><br><p>I have seen in the docs (<a href="https://docs.aws.amazon.com/amplify/latest/userguide/build-settings.html#branch-specific-build-settings" rel="nofollow noreferrer">https://docs.aws.amazon.com/amplify/latest/userguide/build-settings.html#branch-specific-build-settings</a> ) that I can add branch specific build settings; would it be possible to change my api endpoint or stage with these? Or is there another way to do this?</p><br><p>Thanks!</p><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.0,<h3>docker context create ecs myecs - requires exactly one argument</h3><p>I'm trying to create a Docker context that will automatically integrate with AWS's ECS.<br><a href="https://aws.amazon.com/blogs/containers/deploy-applications-on-amazon-ecs-using-docker-compose/" rel="nofollow noreferrer">I'm following this tutorial</a></p><br><p>The author just does:<br><code>docker context create ecs myecs</code> and gets a &quot;pick an integration&quot; prompt; whereas I get an error saying it needs exactly 1 argument.</p><br><pre><code>docker context create&quot; requires exactly 1 argument.<br>See 'docker context create --help'.<br><br>Usage:  docker context create [OPTIONS] CONTEXT<br><br>Create a context<br></code></pre><br>
0.0,0.0,1.0,0.0,0.0,1.0,0.0,<h3>Cannot access Identity Pool with token return from User pool in Nodejs</h3><p>I'm trying to access Identity pool with token return from User pool after user login successful; but now I get: <code>These credentials are not valid for accessing this resource</code>. I already set UserpoolId and ClientId in Authenticate provider of Identity pool.<br>Here is my code</p><br><pre><code>    AWS.config.region = 'us-xxx-xx';<br>    AWS.config.credentials = new AWS.CognitoIdentityCredentials({<br>        IdentityPoolId: 'us-xxxxxx';<br>        Logins: {<br>            'cognito-idp.us-xxx-xx.amazonaws.com/us-xxx-xxx': 'token-return-from-user-pool'<br>        };<br>    });<br>    const cognito = new CognitoIdentity;<br>    const openIdToken = await cognito.getOpenIdTokenForDeveloperIdentity({<br>        IdentityPoolId: 'us-xxxxxx';<br>        Logins: {<br>            'cognito-idp.us-xxx-xx.amazonaws.com/us-xxx-xxx': 'token-return-from-user-pool'<br>        };<br>        TokenDuration: 3600;<br>    }).promise();<br></code></pre><br><p>I get this error: <code>These credentials are not valid for accessing this resource</code></p><br><p>Where is my wrong? Please help me</p><br>
1.0,0.0,0.0,0.0,0.0,0.3333333333333333,0.0,<h3>AWS Glue mapping function with multiple parameters</h3><p>I need to apply a mapping function to all the dynamic records inside a dynamic frame within a Glue Job. For that purpose; I am intending to use the apply method from Glue's Map class; as you can see in this example:</p><br><p><a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-transforms-map.html#aws-glue-api-crawler-pyspark-transforms-map-example" rel="nofollow noreferrer">https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-transforms-map.html#aws-glue-api-crawler-pyspark-transforms-map-example</a></p><br><p>Here is a short description of the method.</p><br><pre class="lang-py prettyprint-override"><code>from awsglue import transforms as gtf<br><br><br>class LoadJsonData(object):<br><br>    def __init__(self; config; dynamic_frame):<br>        self.config = config<br>        self.dynamic_frame = dynamic_frame<br><br>    <br>    def load_jsons_to_dataframe(self):<br>        # Apply Map function to all records inside dynamic frame.<br>        mapped_dyf =  gtf.Map.apply(frame = self.dynamic_frame; f = self.transform_function)<br>        <br>        return mapped_dyf<br><br><br>    def transform_function(self; rec):<br>        # upper case record<br>        is_upper = self.config.is_upper()<br>    <br>        if is_upper == &quot;true&quot;:<br>            rec.upper()<br>    <br>        return rec<br>        <br></code></pre><br><p>As far as I am concerned; the mapping function that I will be implementing is ready to accept a record as a parameter like this:</p><br><pre><code>def transform_function(rec):<br>    # upper case record<br>    rec.upper()<br>    <br>    return rec<br></code></pre><br><p>But what I need in fact is to access my loaded configurations inside transform_function like this:</p><br><pre class="lang-py prettyprint-override"><code>def transform_function(self; rec):<br>    # upper case record<br>    is_upper = self.config.is_upper()<br>    <br>    if is_upper == &quot;true&quot;:<br>        rec.upper()<br>    <br>    return rec<br></code></pre><br><p>When I run this; I get the following error:</p><br><pre><code>PicklingError: Could not serialize object: Py4JError: An error occurred while calling o192.__getstate__. Trace:<br>py4j.Py4JException: Method __getstate__([]) does not exist<br>    at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)<br>    at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)<br>    at py4j.Gateway.invoke(Gateway.java:274)<br>    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)<br>    at py4j.commands.CallCommand.execute(CallCommand.java:79)<br>    at py4j.GatewayConnection.run(GatewayConnection.java:238)<br>    at java.lang.Thread.run(Thread.java:748)<br></code></pre><br><p>Considering the glue lib is not ready to handle more parameters than the dynamic record; what do you recommend to do here?</p><br><p>Thanks.</p><br>
0.3333333333333333,0.3333333333333333,0.0,0.0,1.0,0.0,0.0,<h3>Distribute an incoming data stream into separate containers within the same network (Twitter API &amp; AWS ECS)</h3><p>I am building a data pipeline on AWS which streams the data from a Twitter's v1.1 <a href="https://developer.twitter.com/en/docs/twitter-api/v1/tweets/filter-realtime/api-reference/post-statuses-filter" rel="nofollow noreferrer">POST statuses/filter</a>. A streamer app sits within an ECS (i.e. docker) container from which it sends the initial POST request. The app then sends the tweets to an AWS Kinesis Firehose stream (so it's possible to send the data to the same KF stream from different places/agents).</p><br><p>I am using a VPC; hence it's possible to run multiple containers within the same network.</p><br><p>The question is: <strong>Is it possible to distribute an incoming (Twitter) data stream into multiple containers within the same network (VPC)? And if yes; any hints how to do that?</strong></p><br><p><strong>UPD.</strong> My pipeline is <code>Twitter API -&gt; [ECS container] streamer app -&gt; S3 -&gt; Lambda (predictions) -&gt; Elasticsearch</code>; and I'm talking about the streamer app part.</p><br><p>The ultimate goal here is to be able to scale depending on the intensity of the stream. E.g.; have one small (memory; CPU) container when the traffic from Twitter is low and spin up more of them when the stream is more intense.</p><br>
0.0,0.0,0.0,0.3333333333333333,0.6666666666666666,0.0,0.6666666666666666,<h3>AWS Free Tier usage limit exhausted when used Gradle-Beanstalk plugin for spring boot app deployment</h3><p>I am using AWS free tier; specifically AWS Elastic Beanstalk for my Java application deployment using gradle-beanstalk plugin. Internally it uses S3 for storing archives. I have deployed hardly 100 versions for my application; still the monthly limit for <strong>2;000 Put; Copy; Post or List Requests of Amazon S3</strong> is exhausted within nearly 10 days.</p><br><p>snippet from my build.gradle -</p><br><pre><code>plugins {<br>        id &quot;fi.evident.beanstalk&quot; version &quot;0.2.3&quot;<br>}<br>...<br>beanstalk{<br>    profile = 'my-profile'<br>    s3Endpoint = &quot;s3-us-west-1.amazonaws.com&quot;<br>    beanstalkEndpoint = &quot;elasticbeanstalk.us-west-1.amazonaws.com&quot;<br>    deployments{<br>        staging{<br><br>            file = tasks.jar<br>            application = 'test-app'<br>            environment = 'test-app-staging'<br>        }<br>    }<br>}<br></code></pre><br><p>Am I missing something here? Please guide me on how to optimize this.</p><br><p>Thanks!</p><br>
0.0,1.0,0.0,0.0,0.0,0.0,0.3333333333333333,<h3>Heroku - AWS Cloudfront</h3><p>We have got an application deployed on Heroku and we use AWS Cloudfront to deliver the content.</p><br><p>It was all working fine until last week but now the application itself is working fine on Heroku or from wherever it is accessed but when we try to access the custom URL used with the Cloudfront; it is giving the login page but users are not able to login.</p><br><p>On the dev console; we get the below error:</p><br><p><em><strong>Failed to load resource: the server responded with a status of 502 (Bad Gateway)</strong></em></p><br><p>All the certificates are up to date on both the custom URL on AWS and also on the domain name used with Heroku.</p><br><p>Any help will be appreciated.</p><br>
0.0,1.0,0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>url not working without https/http in aws elasticbeanstalk</h3><p>I have hosted a website on AWS using elastic beanstalk.<br>I have registered a domain from aws route 53(abfrlawards2020.com) and ssl certificate from aws as well<br>I am using nodejs as backend.<br /><br>It works properly if I write <a href="http://example.com" rel="nofollow noreferrer">http://example.com</a> or <a href="https://example.com" rel="nofollow noreferrer">https://example.com</a> but when I just write abfrlawards2020.com on address bar it doesnot work it just keeps loading</p><br>
0.0,0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,0.0,<h3>Reproducible go binaries on aws codebuild</h3><p>I use codebuild to create a go binary using go 1.14.7 on linux:</p><br><p><code>GOOS=linux GOARCH=amd64 go build -trimpath -ldflags=-buildid=&quot;&quot; -o myBin main.go</code></p><br><p>I check the hash of the binary each run (<code>shasum256 myBin</code>) and it's always different.  I see codebuild sets a unique GOPATH for each run.  I'm using mod but in case it could affect the binary I set it to <code>/go</code> (seems impossible to unset).  Locally I have downloaded the different source zips used by codebuild; the build path is different each time but locally; in each of the downloaded sources I'm able to build the binary and the hashes match.</p><br><p>On codebuild though the hash is different on every run.  What might be causing the binary to always have a unique hash?</p><br><p>Thanks;</p><br>
0.0,0.0,0.0,1.0,0.0,0.6666666666666666,0.0,<h3>dynamodb python; new items ID should increment by 1 and therefor be unique</h3><p>Creating CRUD operations on dynamo; I want item_id to be an integer and I want it to increment by 1 with every new entry to the DB; alternatively I can generate a unique integer but it has to be an integer and it has to be unique as I want it to be like the primary key; I know we don't call it that with noSQL I think it's called the partition key.</p><br><p>Using boto3 client will probably insert like this but where should I create the item_id; keep in mind I am using an aws lambda so would not like to rely on keeping a value in memory or anything like that.</p><br><pre><code>client.put_item(<br>        TableName=ITEMS_TABLE;<br>        Item={<br>            'item_id': {'N': item_id};<br>            'file_name': {'S': file_name};<br>            'media_type': {'S': media_type};<br>            'created_at': {'N': created_at};<br>            'updated_at': {'N': updated_at};<br>        }<br></code></pre><br><p>Thanks</p><br>
0.6666666666666666,0.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>Simple query running forever in Redshift</h3><p>I am running a simple query in redshift where the source table has 26 million records and the other table has 120k records. One issue is I don't have a direct relation between the two tables so I am using another foreign key which is common in both the table to make the join. When I gave a limit the query is completing within seconds; however when running without limit it is running forever</p><br><pre><code> `select a.person_name; b.city from persons a  left outer join  address b on a.zip_code= b.zip_code`<br><br>`QUERY PLAN<br>XN Hash Left Join DS_BCAST_INNER  (cost=1559.11..39246081357.77 rows=224197624453 width=15)<br>           Hash Cond: (&quot;outer&quot;.zip_code = &quot;inner&quot;.zip_code)<br>  -&gt;  XN Seq Scan on persons a  (cost=0.00..269134.10 rows=26913410 width=16)<br>  -&gt;  XN Hash  (cost=1247.29..1247.29 rows=124729 width=15)<br>        -&gt;  XN Seq Scan on address b  (cost=0.00..1247.29 rows=124729 width=15)`<br><br><br>Any help would be appreciated.<br></code></pre><br>
0.0,0.0,0.3333333333333333,0.0,0.0,1.0,0.0,<h3>In AWS CodePipeline do we have an option to provide a parameter at run time</h3><p>I have an angular project and i wanted to have a single pipeline to build for uat; develop and production. I know in the codebuild we can provide an environment variable but if this is hardcoded each time i need to edit the codebuild.<br><code>Like jenkins is there any option which ask for parameter which needs to inject to codebuild ? </code></p><br>
0.3333333333333333,0.0,0.0,0.0,0.0,0.6666666666666666,0.0,<h3>AWS Airflow - &quot;variables set KEY VALUE &quot; command in API / CLI returns 400 &quot;Airflow command parsing error&quot;</h3><p>I'm calling Airflow API to set an airflow variables but it returns me a HTTP 400 Bad Request and error message <code>Airflow command parsing error</code></p><br><p>The full call is taken from the documentation:</p><br><p><a href="https://docs.aws.amazon.com/mwaa/latest/userguide/access-airflow-ui.html#CreateCliToken" rel="nofollow noreferrer">https://docs.aws.amazon.com/mwaa/latest/userguide/access-airflow-ui.html#CreateCliToken</a></p><br><pre><code>aws mwaa create-cli-token --name MWAAEnvironmentName | export CLI_TOKEN=$(jq -r .CliToken) &amp;&amp; curl --request POST &quot;https://$WEB_SERVER_HOSTNAME/aws_mwaa/cli&quot; \<br>    --header &quot;Authorization: Bearer $CLI_TOKEN&quot; \<br>    --header &quot;Content-Type: text/plain&quot; \<br>    --data-raw &quot;variables set MY_KEY MY_VALUE&quot;<br></code></pre><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>how to allow users to change email without sending verification code in aws cognito?</h3><p>I am also facing the same issue. while updating the email address. Cognito is sending the code to that new email address. I followed the above solution but still; the verification code is sent.</p><br><p>my lambda function code is:</p><br><pre><code>if (event.triggerSource === 'CustomMessage_UpdateUserAttribute')<br>{<br>    const params = {<br>        UserAttributes: [<br>          {<br>              Name: 'email_verified';<br>              Value: 'true';<br>          };<br>        ];<br>        UserPoolId: event.userPoolId;<br>        Username: event.userName;<br>    };<br>    var cognitoIdServiceProvider = new AWS.CognitoIdentityServiceProvider();<br>    cognitoIdServiceProvider.adminUpdateUserAttributes(params; function(err; data) {<br>        if (err) context.done(err; event); // an error occurred<br>        else context.done(null; event); // successful response<br>    });<br>}<br>else<br>{<br>    context.done(null; event);<br> }<br></code></pre><br><p>};</p><br><p>Anyone can help me with this?? Thanks in advance</p><br>
0.0,0.0,0.0,0.0,0.0,0.0,1.0,<h3>Google Apps Script: Amazon SP-API Add parameters to the request</h3><p>I'm trying to add additional parameters to my call to Amazon Selling Partner API.</p><br><p>The calls I'm doing at the moment is point at <a href="https://github.com/amzn/selling-partner-api-docs/blob/main/references/orders-api/ordersV0.md" rel="nofollow noreferrer">Amazon Get Orders</a></p><br><p>I'd like to add two parameters: <code>MarketplaceIds</code>(required; array) and <code>OrderStatuses</code>(optional; string; e.g.: &quot;Shipped&quot;).</p><br><p>My question is:</p><br><ul><br><li>shall this go only as part of the request or it also impacts on the canonical request?</li><br><li>and how to implement it?</li><br></ul><br><p>This is my code so far:</p><br><pre><code>function GetOrders(){<br>  var access_token = AccessToken();<br>  //Time variables<br>  var currentDate = new Date();<br>  var isoDate = currentDate.toISOString();<br>  var isoString = isoDate.replace(/-/g; &quot;&quot;).replace(/:/g; &quot;&quot;).replace(/(\.\d{3})/; &quot;&quot;);<br>  var yearMonthDay= Utilities.formatDate(currentDate; 'GTM-5'; 'yyyyMMdd');<br><br>  //API variables<br>  var end_point = 'https://sellingpartnerapi-eu.amazon.com';<br><br>  //Credential variables<br>  var aws_region = &quot;eu-west-1&quot;;<br>  var service = &quot;execute-api&quot;;<br>  var termination_string = &quot;aws4_request&quot;;<br><br>  //CanonicalRequest = httpRequestMethod + '\n' + CanonicalURI + '\n' + CanonicalQueryString + '\n' + CanonicalHeaders + '\n' + SignedHeaders + '\n' + HexEncode(Hash(RequestPayload));<br>  //CanonicalRequest components:<br>  var httpRequestMethod = 'GET';<br>  var canonicalURI = '/orders/v0/orders';<br>  var canonicalQueryString = '?marketplaceId=A1PA6795UKMFR9';<br>  var canonicalheaders = 'host:' + &quot;sellingpartnerapi-eu.amazon.com&quot; + '\n' + 'user-agent:' + 'Mozilla/5.0 (compatible; Google-Apps-Script; beanserver; +https://script.google.com; id: UAEmdDd-KyWEWcR137UzUzWb1fu3rUgNviHA)' + '\n' + 'x-amz-access-token:' + access_token + '\n' + 'x-amz-date:' + isoDate;<br>  var signedheaders = 'host;user-agent;x-amz-access-token;x-amz-date';//;user-agent &amp; host; al comienzo<br>  var requestPayloadHashed = Utilities.computeDigest(Utilities.DigestAlgorithm.SHA_256; &quot;&quot;);<br>  requestPayloadHashed = requestPayloadHashed.map(function(e) {return (&quot;0&quot; + (e &lt; 0 ? e + 256 : e).toString(16)).slice(-2)}).join(&quot;&quot;);<br><br>  //Building the canonical request<br>  var canonical_string = httpRequestMethod + '\n' + canonicalURI + '\n' + &quot;marketplaceId=A1PA6795UKMFR9&quot; + '\n' + canonicalheaders + '\n\n' + signedheaders + '\n' + requestPayloadHashed;//UPDATED<br><br>  var canonical_signature = Utilities.computeDigest(Utilities.DigestAlgorithm.SHA_256; canonical_string);<br>  canonical_request = canonical_signature.map(function(e) {return (&quot;0&quot; + (e &lt; 0 ? e + 256 : e).toString(16)).slice(-2)}).join(&quot;&quot;);<br><br>  //CredentialScope = Date + AWS region + Service + Termination string;<br>  //StringToSign = Algorithm + \n + RequestDateTime + \n + CredentialScope + \n + HashedCanonicalRequest;<br>  var credential_scope = yearMonthDay + '/' + aws_region + '/' + service + '/' + termination_string;<br>  var string_to_sign = &quot;AWS4-HMAC-SHA256&quot; + '\n' + isoString + '\n' + credential_scope + '\n' + canonical_request;<br>  var kSecret = ACCESS_KEY;<br>  var kDate = Utilities.computeHmacSha256Signature(yearMonthDay; &quot;AWS4&quot; + kSecret);<br>  var kRegion = Utilities.computeHmacSha256Signature(toBytes(aws_region); kDate);<br>  var kService = Utilities.computeHmacSha256Signature(toBytes(service); kRegion);<br>  var kSigning = Utilities.computeHmacSha256Signature(toBytes(termination_string); kService);<br><br>  var signature = hex(Utilities.computeHmacSha256Signature(toBytes(string_to_sign); kSigning));<br><br>  var options = {<br>    'method': 'GET';<br>    'headers': {<br>      //'host': end_point;<br>      'user-agent': 'Mozilla/5.0 (compatible; Google-Apps-Script; beanserver; +https://script.google.com; id: UAEmdDd-KyWEWcR137UzUzWb1fu3rUgNviHA)';<br>      'x-amz-access-token': access_token;<br>      'x-amz-date': isoDate;<br>      'Authorization': 'AWS4-HMAC-SHA256 Credential=' + ACCESS_ID + '/' + credential_scope + '; SignedHeaders=' + signedheaders + '; Signature=' + signature;<br>    };<br>    'muteHttpExceptions': true<br>  }<br>  var getOrders = UrlFetchApp.fetch(end_point + canonicalURI + canonicalQueryString; options);<br>  Logger.log(getOrders);<br>}<br></code></pre><br><p>I already have a variable <code>marketplaceId</code> on my canonicalQueryString parameter but using it as it is I'm getting the following response:</p><br><pre><code>{<br>  &quot;errors&quot;: [<br>    {<br>     &quot;message&quot;: &quot;Missing or invalid request parameters: [MarketplaceIds]&quot;;<br>     &quot;code&quot;: &quot;InvalidInput&quot;<br>    }<br>  ]<br>}<br></code></pre><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.3333333333333333,<h3>AWS Access Prod Error Even with Access Enabled</h3><p>I have requested and received AWS-SES prod access in OHIO region ( us-east-2 ); but even with the status changed to ENABLED on my account:</p><br><p>I still getting the error <strong>Production access not granted. Please request production access prior to sending custom verification emails</strong></p><br><p><strong>I have generated a new SMTP keys settings.<br>I have forced the region using Regions.US_EAST_2.</strong></p><br><p>Is there anything more that I should to do?</p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.3333333333333333,0.0,<h3>Getting 403 (Forbidden) when trying to delete image from s3</h3><p>I am using this library <a href="https://www.npmjs.com/package/react-s3" rel="nofollow noreferrer">react-s3</a> to upload images on S3 and delete from S3. Upload image is working fine but when I am trying to delete the file from s3 but whenever I try to delete I am getting 403.<br>I am passing the following URL in delete function:</p><br><pre><code>https://abc-profile-images.s3.amazonaws.com/1601273978.331.jpeg<br></code></pre><br><p>But In catch block I received the following URL (why is it including url with and without region at the same time)</p><br><pre><code> https://abc-profile-images.s3-ap-southeast-1.amazonaws.com/https://abc-profile-images.s3.amazonaws.com/1601273978.331.jpeg<br></code></pre><br><p>Following is the object received in catch:</p><br><pre><code>Response {type: &quot;cors&quot;; url: &quot;https://abc-profile-images.s3-ap-southeast-1.amages.s3.amazonaws.com/1601273978.331.jpeg&quot;; redirected: false; status: 403; ok: false; }<br>body: (...)<br>bodyUsed: false<br>headers: Headers {}<br>ok: false<br>redirected: false<br>status: 403<br>statusText: &quot;Forbidden&quot;<br>type: &quot;cors&quot;<br>url: &quot;https://abc-profile-images.s3-ap-southeast-1.amazonaws.com/https://abc-profile-images.s3.amazonaws.com/1601273978.331.jpeg&quot;<br>__proto__: Response<br></code></pre><br><p>Following is my code:</p><br><pre><code>import { deleteFile } from &quot;react-s3&quot;;<br>import config from './s3config';<br><br>const options = {<br><br>    accessKeyId: config.ID;<br>    secretAccessKey: config.key;<br>    region: config.region;<br>    bucketName: config.name<br>}<br><br>const removeImage = (uri) =&gt; {<br>    console.log('file : '; uri)<br>    return new Promise((resolve; reject) =&gt; {<br>        deleteFile(uri; options)<br>            .then(result =&gt;  resolve(result))<br>            .catch(err =&gt; reject(err))<br>    })<br>}<br>export {<br>    removeImage<br>};<br></code></pre><br><pre><code>import { removeImage } from '../../common/imageDelete'<br>function MethodList(props) {<br><br>  const removeMethod = (e; page) =&gt; {<br>        let image = e.target.dataset.image;<br>      <br>         removeImage(image)<br>             .then(res =&gt; console.log('s3 delete result : '; res))<br>             .catch(res =&gt; console.log('Image catch'; res))<br>         }<br>    }<br>export default MethodList<br></code></pre><br>
0.0,0.0,0.0,0.0,0.0,0.6666666666666666,0.6666666666666666,<h3>Alexa with AWS: AskSdk.S3PersistenceAdapter Error Could not read item</h3><p>Getting this error from the an Alexa Skill that accesses info using the S3 Persistence Adapter. The skill is deployed to AWS (not with cloudformation) and we double checked the bucket exists in S3; that the lambda function has the 'Bucket' key in its environment variables; and that the lambda function has Administrator permissions.</p><br><blockquote><br><p>2021-03-09T14:54:58.693Z  6f3fbbd3-5cbb-4c39-8577-971461a16a45    INFO    {<br>AskSdk.S3PersistenceAdapter Error: Could not read item<br>(amzn1.ask.account.XXX) from bucket (undefined): Missing required key<br>'Bucket' in params<br>at Object.createAskSdkError (/var/task/node_modules/ask-sdk-runtime/dist/util/AskSdkUtils.js:23:19)<br>at S3PersistenceAdapter.getAttributes (/var/task/node_modules/ask-sdk-s3-persistence-adapter/dist/attributes/persistence/S3PersistenceAdapter.js:49:34)<br>name: 'AskSdk.S3PersistenceAdapter Error' }</p><br></blockquote><br>
0.0,0.0,0.6666666666666666,0.6666666666666666,0.0,0.6666666666666666,0.3333333333333333,<h3>Generic way to retrieve tags from AWS events</h3><p>I have an autotagging engine based on GorillaStack's open source <a href="https://github.com/GorillaStack/auto-tag" rel="nofollow noreferrer">https://github.com/GorillaStack/auto-tag</a>. I added code to use a DynamoDB table that contains additional tags to apply based off of the user-specified tag &quot;BusinessUse&quot;. So if a user specifies the tag BusinessUse to be &quot;Use_Case_XXX&quot; the lambda function should look at the dynamodb table and apply the corresponding tags CostCenter; Department; etc. Everything works fine if the specific field within responseElements that contains the tags matches the resource type.</p><br><p>The problem is that the structure of the responseElements and requestParameters for each this.event vary greatly depending on the resource type. For example; IAM users have the tags inside this simple struct responseElements.user.tags[]:</p><br><pre><code>responseElements:<br>   { user:<br>      { path: '/';<br>        userName: 'test1';<br>        userId: 'AIDAWV5EEBYBXINBXD2JL';<br>        arn: 'arn:aws:iam::123456789012:user/test1';<br>        createDate: 'Sep 25; 2020 4:09:56 PM';<br>        tags: [Array] } }<br></code></pre><br><p>Whereas EC2 instances have their tags inside a much more complex structure; responseElements.instancesSet.tagSet.resourceType[instance].tags[] (UGH!):</p><br><pre><code>responseElements:<br>   { requestId: '73bc65c3-b1d8-4954-aa41-d2f63ca2b5a9';<br>     reservationId: 'r-01a9a50db0c26034f';<br>     ownerId: '123456789012';<br>     groupSet: {};<br>     instancesSet: { items: [ { instanceId: 'i-08687c568bfbe3671';<br>       imageId: 'ami-0947d2ba12ee1ff75';<br>       instanceState: [Object];<br>       privateDnsName: 'ip-10-0-1-14.ec2.internal';<br>       keyName: 'foo';<br>       amiLaunchIndex: 0;<br>       productCodes: {};<br>       instanceType: 't2.micro';<br>       launchTime: 1601050311000;<br>       placement: [Object];<br>       monitoring: [Object];<br>       subnetId: 'subnet-01f8d0d4cabe5a3a7';<br>       vpcId: 'vpc-0568962eafdd8d11b';<br>       privateIpAddress: '10.0.1.14';<br>       stateReason: [Object];<br>       architecture: 'x86_64';<br>       rootDeviceType: 'ebs';<br>       rootDeviceName: '/dev/xvda';<br>       blockDeviceMapping: {};<br>       virtualizationType: 'hvm';<br>       hypervisor: 'xen';<br>       tagSet: [ { resourceType: 'instance'; tags: [Array] };{ resourceType: 'volume'; tags: [Array] } ];<br>       groupSet: [Object];<br>       sourceDestCheck: true;<br>       networkInterfaceSet: [Object];<br>       ebsOptimized: false;<br>       cpuOptions: [Object];<br>       capacityReservationSpecification: [Object];<br>       hibernationOptions: [Object];<br>       enclaveOptions: [Object];<br>       metadataOptions: [Object] } ] }<br> } }<br></code></pre><br><p>So my question is how to extract the tag BusinessUse from this.event in a generic way. There are thirty-three resource types that this stack supports and I would really rather not have to handle each one individually. I'm open to using other API's as well like the ResourceGroupTagging API but from what I can see this returns an ARN and each resource type for this.event also does not have a uniform ARN field (ARG!). I suppose a more general question is how to deal with non-uniform responseElements and requestParameters and why does AWS make it so hard to parse these structures instead of providing a uniform template.</p><br><p>FWIW; here's the code to pull values from the dynamoDB table and return a tag value; the case below works for iam users because it explicitly calls responseElements.user.tags[]. I need to make it generic so that it works for this.event of any resource type:</p><br><pre><code>getTagValueFromTable(projectionExpression) {<br>    console.log(this.event);<br>    let _data;<br>    let done = false;<br>    try {<br>      // Create the DynamoDB service object<br>      var ddb = new AWS.DynamoDB({apiVersion: '2012-08-10'});<br>      var params = {<br>        TableName: 'csv-table';<br>        Key: {<br>          'BusinessUse': {S: this.event.responseElements.user.tags[0].value}<br>        };<br>        ProjectionExpression: projectionExpression<br>      };<br>      // Retrieve the item<br>      ddb.getItem(params).promise().then((data) =&gt; {<br>        _data = data.Item[projectionExpression].S;<br>        done = true;<br>        console.log(projectionExpression + &quot;:&quot; + _data);<br>      });<br>      // Wait for asynchronous call to complete!<br>      require('deasync').loopWhile(function(){return !done;});<br>      return _data;<br>    }catch (error) {<br>       console.error(error);<br>       return &quot;ERROR_ENTRY_NOT_FOUND&quot;;<br>    }<br>  }<br></code></pre><br><p>The original lambda code is at <a href="https://github.com/GorillaStack/auto-tag/releases/download/0.5.3/autotag-0.5.3.zip" rel="nofollow noreferrer">https://github.com/GorillaStack/auto-tag/releases/download/0.5.3/autotag-0.5.3.zip</a></p><br>
0.0,1.0,0.3333333333333333,0.0,0.0,0.0,0.0,<h3>Calling private API Gateway from a public API Gateway</h3><p>I'm trying to call a private API Gateway (B) endpoint from a public one (A); but I'm getting timeout errors when performing the call.</p><br><p>I've read <a href="https://aws.amazon.com/es/premiumsupport/knowledge-center/api-gateway-private-cross-account-vpce/" rel="nofollow noreferrer">https://aws.amazon.com/es/premiumsupport/knowledge-center/api-gateway-private-cross-account-vpce/</a> and <a href="https://stackoverflow.com/questions/61133268/timeout-calling-private-api-gateway-from-another-aws-account">Timeout calling PRIVATE API Gateway from another AWS account</a>; which look similar to my goal; but I'm probably missing something.</p><br><p>The APIs A and B are in different accounts and belong to different VPCs. The VPCs have private DNS option enabled; and the same security group which allows inbound traffic for all protocols and all port ranges.</p><br><p><a href="https://i.stack.imgur.com/y2GkF.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/y2GkF.png" alt="enter image description here" /></a></p><br><p>The API B defines a policy to allow traffic from A's VPCE; as follow:</p><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Effect&quot;: &quot;Deny&quot;;<br>            &quot;Principal&quot;: &quot;*&quot;;<br>            &quot;Action&quot;: &quot;execute-api:Invoke&quot;;<br>            &quot;Resource&quot;: &quot;execute-api:/*/*/*&quot;;<br>            &quot;Condition&quot;: {<br>                &quot;StringNotEquals&quot;: {<br>                    &quot;aws:sourceVpce&quot;: &quot;vpce-A&quot;<br>                }<br>            }<br>        };<br>        {<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Principal&quot;: &quot;*&quot;;<br>            &quot;Action&quot;: &quot;execute-api:Invoke&quot;;<br>            &quot;Resource&quot;: &quot;execute-api:/*/*/*&quot;<br>        }<br>    ]<br>}<br></code></pre><br><p>The following calls from A give a timeout:</p><br><ul><br><li>curl -i https://public-DNS-VPCE-A/stage-name-B/endpoint -H &quot;Host: https://B-Invoke-Url&quot;</li><br><li>curl -i https://public-DNS-VPCE-A/stage-name-B/endpoint -H &quot;x-apigw-api-id: B-id&quot;</li><br><li>curl -i https://public-DNS-VPCE-B/stage-name-B/endpoint -H &quot;Host: https://B-Invoke-Url&quot;</li><br><li>curl -i https://public-DNS-VPCE-B/stage-name-B/endpoint -H &quot;x-apigw-api-id: B-id&quot;</li><br><li>curl -i https://B-Invoke-Url/endpoint</li><br></ul><br><p>Any suggestions for resolving this issue?</p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.6666666666666666,0.0,<h3>AWS S3 Generate a presigned url with a md5 check</h3><p>Im looking to generate a pre signed url with aws s3.<br>It works fine with some condition (mime type for example) but im unable to use 'Content-MD5'.</p><br><p>I use the node js sdk and put the md5 in fields object.</p><br><pre class="lang-js prettyprint-override"><code>const options = {<br>      Bucket: bucket;<br>      Expires: expires;<br>      ContentType: 'multipart/form-data';<br>      Conditions: [{ key }];<br>      Fields: {<br>        'Content-MD5': params.md5;<br>      };<br>} as PresignedPost.Params;<br><br>if (acl) {<br>      options.Conditions.push({ acl });<br>}<br><br>if (params.mimeType) {<br>      options.Conditions.push({ contentType: params.mimeType });<br>}<br></code></pre><br><p>But after when I upload the file; I would like AWS to check by itself the uploaded file with the MD5 given in the presigned request but I always have that error :</p><br><pre class="lang-xml prettyprint-override"><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;<br>&lt;Error&gt;<br>    &lt;Code&gt;AccessDenied&lt;/Code&gt;<br>    &lt;Message&gt;Invalid according to Policy: Policy Condition failed: [&quot;eq&quot;; &quot;$Content-MD5&quot;; &quot;&lt;md5&gt;&quot;]&lt;/Message&gt;<br>    &lt;RequestId&gt;497312AFEEF83235&lt;/RequestId&gt;<br>    &lt;HostId&gt;KY9RxpGZzRog7hjlDk3whjAbItG/mwhpItYDL7rUNNH4BCXMfmLZsbZIPKivmSZZ3VkWxlgstOk=&lt;/HostId&gt;<br>&lt;/Error&gt;<br></code></pre><br><p>My MD5 is generated like that in the browser ( just after recording a video ):</p><br><pre class="lang-js prettyprint-override"><code>const reader = new FileReader();<br>reader.readAsBinaryString(blob);<br>reader.onloadend = () =&gt; {<br>  const mdsum = CryptoJS.MD5(reader.result.toString());<br>  resolve(CryptoJS.enc.Base64.stringify(mdsum));<br>};<br></code></pre><br><p>Maybe it's not the way it works ?</p><br><p><strong>edit :</strong></p><br><p>If I add to the upload form data (md5 hash is the same as set in the presigned)</p><br><pre class="lang-js prettyprint-override"><code>formData.append('Content-MD5'; encodeURI(fields['Content-MD5']));<br></code></pre><br><p>the error becomes</p><br><pre class="lang-xml prettyprint-override"><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;<br>&lt;Error&gt;<br>&lt;Code&gt;BadDigest&lt;/Code&gt;<br>&lt;Message&gt;The Content-MD5 you specified did not match what we received.&lt;/Message&gt;<br>&lt;ExpectedDigest&gt;2b36c76525c8d3a6dada59a6ad2867a7&lt;/ExpectedDigest&gt;&lt;CalculatedDigest&gt;+RifURVLd61O6QCT+SzhBg==&lt;/CalculatedDigest&gt;&lt;RequestId&gt;B4FF38D0FCC2E8F2&lt;/RequestId&gt;&lt;HostId&gt;yS7q200rJpBu48RNcGzsb1oGbDUrN8UK9+gkg6jGMl+EJSGeyQaSCfwfcMRUeNlJYapfmF304Oc=&lt;/HostId&gt;&lt;/Error&gt;<br></code></pre><br><p>answer:</p><br><pre class="lang-js prettyprint-override"><code>const reader = new FileReader();<br>reader.readAsBinaryString(blob);<br>reader.onloadend = () =&gt; {<br>          resolve(CryptoJS.enc.Base64.stringify(CryptoJS.MD5(CryptoJS.enc.Latin1.parse(reader.result.toString()))));<br>        };<br></code></pre><br>
0.0,0.0,1.0,1.0,0.0,0.0,0.0,<h3>AWS S3 sync command gets different results on the same machine</h3><p>I am trying to sync data from S3 to local file.</p><br><p>I create 2 connection sessions to the target host. However; using the same sync command; one session sync data <code>NORMALLY</code> while the other one; I get</p><br><pre><code>fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied<br></code></pre><br><p>Why could this happen and how to avoid this issue?</p><br><p>BTW; they were not run at the same time.</p><br><p>The command is:</p><br><pre><code>/usr/local/bin/aws s3 sync s3://${my_s3_path}/ ${my_local_path} --exclude '*xxx' --page-size 400<br></code></pre><br>
0.0,0.0,1.0,0.0,0.6666666666666666,0.0,0.0,<h3>AWS CloudFormation CodeBuild: Can i use ECR image as Environment image</h3><p>I have created the following Script where i am trying to use a custom image from ECR:</p><br><pre><code>  Adminrole:<br>    Type: AWS::IAM::Role<br>    Properties:<br>      RoleName: !Join<br>                  - &quot;.&quot;<br>                  - - !Ref &quot;AWS::StackName&quot;<br>                    - !Ref &quot;AWS::Region&quot;<br>                    - &quot;codebuild&quot;<br>      Path: &quot;/&quot;<br>      AssumeRolePolicyDocument:<br>        Statement:<br>        - Action: ['sts:AssumeRole']<br>          Effect: Allow<br>          Principal:<br>            Service: [codebuild.amazonaws.com]<br>        Version: '2012-10-17'<br>      Policies:<br>        - PolicyName: &quot;root&quot;<br>          PolicyDocument:<br>            Version: &quot;2012-10-17&quot;<br>            Statement:<br>               - Effect: &quot;Allow&quot;<br>                 Action: &quot;*&quot;<br>                 Resource: &quot;*&quot;<br><br>  ProjectTerrafom:<br>    Type: AWS::CodeBuild::Project<br>    Properties:<br>      Name: !Join<br>              - &quot;_&quot;<br>              - - !Ref &quot;AWS::StackName&quot;<br>                - !Ref &quot;AWS::Region&quot;<br>                - &quot;ProjectTerrafom&quot;<br>      Description: Terraform deployment<br>      ServiceRole: !Ref Adminrole<br>      Artifacts:<br>        Type: no_artifacts<br>      Environment:<br>        Type: LINUX_CONTAINER<br>        ComputeType: BUILD_GENERAL1_SMALL<br>        Image: 111111.cer.ecr.eu-center-1.amazonaws.com/my_terraform<br>      Source:<br>        Location: !Ref &quot;FullPathRepoNameTerraform&quot;<br>        Type: GITHUB_ENTERPRISE<br>      TimeoutInMinutes: 10<br>      Tags:<br>        - Key: Project<br>          Value: &quot;Run Terraform From CodeBuild&quot;<br></code></pre><br><p>When i run the CodeBuild i get the following error:</p><br><pre><code>BUILD_CONTAINER_UNABLE_TO_PULL_IMAGE: Unable to pull customer's container image.<br>CannotPullContainerError: Error response from daemon: pull access denied for<br>111111.cer.ecr.eu-center-1.amazonaws.com/my_terraform; repository does<br> not exist or may `enter code here`require 'docker login': denied: User: CodeBuild<br></code></pre><br><p>Is this a permission issue or are we not allowed to use the ECR images for CodeBuild?</p><br>
0.0,0.0,0.0,0.0,0.6666666666666666,0.6666666666666666,0.0,<h3>How to Return a Caught Error in my Response? (AWS Lambda Node.js 14)</h3><p>I have an AWS Lambda function connected to API Gateway. My function looks basically like this:</p><br><pre><code> exports.handler = async(event)=&gt;{<br>    try {<br>        //to do stuff <br>    } catch (e) {<br>        const response = {<br>            statusCode: 200;<br>            headers: headers;<br>            body: JSON.stringify({<br>                CaughtException: e;<br>                req: event<br>            });<br>        }<br>        return response;<br>    }<br> }<br></code></pre><br><p>When I use the Lambda test Event; I can see the error in the function logs section:<br /><br><a href="https://i.stack.imgur.com/Fl7Oc.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/Fl7Oc.png" alt="enter image description here" /></a></p><br><p>As you can see; I am trying to send this error as part of my response; but it's just an empty object :(</p><br><p><a href="https://i.stack.imgur.com/Zphdd.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/Zphdd.png" alt="enter image description here" /></a></p><br><p>How can I access this error?</p><br>
0.0,0.0,0.3333333333333333,0.0,0.3333333333333333,1.0,0.0,<h3>How do i integrate a Python Lambda Function into the Pipeline of AWS Amplify</h3><p>So i'm trying to build an Ampliy application with Javascript and a Python Lambda function. Everything works just fine. I have setup my CodeCommit Branch for hosting with continous deployment. I add a API with a Lambda function in Python. With amplify push; amplify successfully deploys the corresponding API Gateway and Lambda and i can successfully interact with my lambda function. So; as soon as i  push my commits into my repository; the pipeline gets trigger and crashes during the build phase:</p><br><pre><code>                                 # Starting phase: build<br>                                 # Executing command: amplifyPush --simple<br>2021-02-17T14:01:23.680Z [INFO]: [0mAmplify AppID found: d2l0j3vtlykp8l. Amplify App name is: documentdownload[0m<br>2021-02-17T14:01:23.783Z [INFO]: [0mBackend environment dev found in Amplify Console app: documentdownload[0m<br>2021-02-17T14:01:24.440Z [WARNING]: - Fetching updates to backend environment: dev from the cloud.<br>2021-02-17T14:01:24.725Z [WARNING]:  Successfully pulled backend environment dev from the cloud.<br>2021-02-17T14:01:24.758Z [INFO]: <br>2021-02-17T14:01:26.925Z [INFO]: [33mNote: It is recommended to run this command from the root of your app directory[39m<br>2021-02-17T14:01:31.904Z [WARNING]: - Initializing your environment: dev<br>2021-02-17T14:01:32.216Z [WARNING]:  Initialized provider successfully.<br>2021-02-17T14:01:32.829Z [INFO]: [31mpython3 found but version Python 3.7.9 is less than the minimum required version.[39m<br>                                 [31mYou must have python &gt;= 3.8 installed and available on your PATH as &quot;python3&quot; or &quot;python&quot;. It can be installed from https://www.python.org/downloads[39m<br>                                 [31mYou must have pipenv installed and available on your PATH as &quot;pipenv&quot;. It can be installed by running &quot;pip3 install --user pipenv&quot;.[39m<br>2021-02-17T14:01:32.830Z [WARNING]:  An error occurred when pushing the resources to the cloud<br>2021-02-17T14:01:32.830Z [WARNING]:  There was an error initializing your environment.<br>2021-02-17T14:01:32.832Z [INFO]: [31minit failed[39m<br>2021-02-17T14:01:32.834Z [INFO]: [0mError: Missing required dependencies to package documentdownload[0m<br>                                 [0m    at Object.buildFunction (/root/.nvm/versions/node/v12.19.0/lib/node_modules/@aws-amplify/cli/node_modules/amplify-category-function/src/provider-utils/awscloudformation/utils/buildFunction.ts:21:11)[0m<br>                                 [0m    at processTicksAndRejections (internal/process/task_queues.js:97:5)[0m<br>                                 [0m    at prepareResource (/root/.nvm/versions/node/v12.19.0/lib/node_modules/@aws-amplify/cli/node_modules/amplify-provider-awscloudformation/src/push-resources.ts:474:33)[0m<br>                                 [0m    at async Promise.all (index 0)[0m<br>                                 [0m    at Object.run (/root/.nvm/versions/node/v12.19.0/lib/node_modules/@aws-amplify/cli/node_modules/amplify-provider-awscloudformation/src/push-resources.ts:106:5)[0m<br>2021-02-17T14:01:32.856Z [ERROR]: !!! Build failed<br>2021-02-17T14:01:32.856Z [ERROR]: !!! Non-Zero Exit Code detected<br>2021-02-17T14:01:32.856Z [INFO]: # Starting environment caching...<br>2021-02-17T14:01:32.857Z [INFO]: # Environment caching completed<br></code></pre><br><p>In the previous step PROVISION step is Python 3.8 though..</p><br><pre><code>## Install python3.8<br>RUN wget https://www.python.org/ftp/python/3.8.0/Python-3.8.0.tgz<br>RUN tar xvf Python-3.8.0.tgz<br>WORKDIR Python-3.8.0<br>RUN ./configure --enable-optimizations --prefix=/usr/local<br>RUN make altinstall<br></code></pre><br><p>For now i have no idea why it behaves like this. Pushing the changes locally it works. Can anybody help?</p><br>
0.0,0.0,1.0,0.0,0.0,0.6666666666666666,0.0,<h3>How do I list all users in my Cognito User Pool in my React application?</h3><p>I have a CMS application which site editors can log into. I want to create a dashboard for the editors to be able to create and delete site users. Site user accounts will be maintained through a AWS Cognito User Pool; while site editors log in through their account with the CMS.</p><br><p>To begin with; I want to be able to list all users in a given user pool for the editors to view; so I'm following the SDK docs <a href="https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/clients/client-cognito-identity-provider/index.html" rel="nofollow noreferrer">here</a>; but it doesn't explain how to pass any credentials to the method. This is with v3 of the SDK; and any examples I can find that does something similar uses v2.</p><br><p>I get this far:</p><br><pre class="lang-js prettyprint-override"><code>import { CognitoIdentityProviderClient; ListUsersCommand } from &quot;@aws-sdk/client-cognito-identity-provider&quot;;<br><br>const client = new CognitoIdentityProviderClient({ region: &quot;eu-west-2&quot; });<br>const params = {};<br>const command = new ListUsersCommand({ UserPoolId: &quot;My_User_Pool_Id&quot; });<br><br>try {<br>    const data = await client.send(command);<br>    return this.setState({ users: data });<br>} catch (error) {<br>    console.error(error);<br>    return this.setState({ error: error });<br>} finally {<br>    this.setState({ loading: false });<br>}<br></code></pre><br><p>But this gives me an error when I run the above code and render the component:</p><br><pre><code>Error: Credential is missing<br>    at SignatureV4.credentialProvider (runtimeConfig.browser.js:15)<br>    at SignatureV4.&lt;anonymous&gt; (SignatureV4.js:169)<br>    at step (tslib.es6.js:100)<br>    at Object.next (tslib.es6.js:81)<br>    at tslib.es6.js:74<br>    at new Promise (&lt;anonymous&gt;)<br>    at __awaiter (tslib.es6.js:70)<br>    at SignatureV4.signRequest (SignatureV4.js:165)<br>    at SignatureV4.&lt;anonymous&gt; (SignatureV4.js:85)<br>    at step (tslib.es6.js:100)<br></code></pre><br><p>I assume I need to create some credentials (through IAM?) for my dashboard to access the user pools; but I don't know where to create these or how to pass them to the SDK. Googling around this subject seemed to suggest I should make an unauthenticated <em>Identity</em> Pool and attached IAM permissions to that; then pass that pool's credentials to the SDK; but that gave me permissions errors too:</p><br><pre><code>AccessDeniedException: User: [DashboardAdmin IdentityPool ARN] is not authorized to perform: cognito-idp:ListUsers on resource: [UserPool for Site Users ARN]<br></code></pre><br>
0.0,1.0,0.0,0.0,0.0,0.3333333333333333,0.0,<h3>How can I pass through request body when using API gateway?</h3><p>I deployed a REST api gateway on AWS and enabled &quot;Use HTTP Proxy integration&quot; to pass all requests to a http endpoint. It works fine for <code>GET</code> with path parameters. But it doesn't pass through all BODY in POST request. How can I make it to pass everything to downstream http endpoint?</p><br><p><a href="https://i.stack.imgur.com/fKzCC.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/fKzCC.png" alt="enter image description here" /></a></p><br>
0.0,0.0,1.0,0.0,0.0,0.6666666666666666,0.0,<h3>AWS Codepipeline with bitbucket and how to pass branch name to appspec.yaml</h3><p>I've created a code pipeline for the PHP laravel base project with bitbucket. Passing parameter using AWS SSM to the appspec.yml All are working fine with the development branch. I need to update the parameters from the AWS SSM based on the branch name on appspec.yml file.</p><br><p>FOR DEV</p><br><pre><code>Branch name: develop<br>parameter value: BRANCH_NAME_VALUE (develop_value)<br></code></pre><br><p>FOR QA</p><br><pre><code>Branch name: qa<br>parameter value: BRANCH_NAME_VALUE(qa_value)<br></code></pre><br><p>appspec.yaml file</p><br><pre><code>version: 0.0<br>os: linux<br>files:<br>  - source: /<br>    destination: /var/www/html/<br>    overwrite: true<br>hooks:<br>  BeforeInstall:<br>    - location: scripts/before_install.sh<br>      timeout: 300<br>      runas: root<br>  AfterInstall:<br>    - location: scripts/after_install.sh<br>      timeout: 300<br>      runas: root<br></code></pre><br><p>How I can get the BRANCH_NAME for update the after_install.sh</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>Is it possible to selectively read from AWS SQS?</h3><p>I have a use-case. I want to read from SQS always; except when another event happens.</p><br><p>For instance; I have football news into SQS as messages. I want to retrieve them always; except for times when live matches are happening.</p><br><p>Is there any possibility to read unless there is another event does the job?</p><br><p>I scrolled the docs and Stack Overflow; but I don't see a solution.</p><br><p><strong>COMMENT:</strong> I have a small and week service; and I cannot because of technical limitations increase it (memory/CPU; etc.); but I still want 2 &quot;conflicting&quot; flows to be in the service. They are both supposed to communicate to the same API; and I don't want them to send conflicting requests.</p><br><p>Is there a way to do it; or will I have to write a custom communicator with SQS?</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,1.0,0.0,<h3>AWS SAM CLI local invoke account id not set when --profile is passed as parameter</h3><p>I'm trying to execute aws sam local invoke command in order to test my function locally. The problem occurs when I run this command passing <strong>profile</strong> parameter. The execution container sets account id as a dummy account like <strong>123456789012</strong></p><br><p>Does anybody knows If there is any thing cached here? When I deploy my project using <code>sam deploy --guided</code> everything runs perfect and code is uploaded to AWS</p><br><p>my <strong>~/.aws/config</strong> and <strong>~/.aws/credentials</strong> files are filled</p><br><p><strong>credentials</strong></p><br><pre class="lang-sh prettyprint-override"><code>[adm]<br>aws_access_key_id=XXXXXXXXXXXXXx<br>aws_secret_access_key=XXXXXXXXXXXXXXXXXX<br></code></pre><br><p><strong>config</strong></p><br><pre class="lang-sh prettyprint-override"><code>[adm]<br>region = eu-west-1<br>output = json<br></code></pre><br><pre class="lang-sh prettyprint-override"><code>  POC sam local invoke &quot;ScheduleBusinessRuleFunction&quot;  --profile adm<br>Invoking scheduleBusinessRule (go1.x)<br>Skip pulling image and use local one: amazon/aws-sam-cli-emulation-image-go1.x:rapid-1.18.1.<br><br>Mounting /run/media/urkob/projects/Projects/POC/.aws-sam/build/ScheduleBusinessRuleFunction as /var/task:ro;delegated inside runtime container<br>START RequestId: bc99de19-290b-46fa-908a-744d54547cbe Version: $LATEST<br>operation error CloudWatch Events: PutTargets; https response error StatusCode: 400; RequestID: 8e335486-de38-49bb-a06e-8090e26e9628; api error AccessDeniedException: Access to the resource arn:aws:lambda:us-east-1:123456789012:function:POC-executeBusinessRule is denied. Reason: Adding cross-account target is not permitted.: OperationError<br>null<br>END RequestId: bc99de19-290b-46fa-908a-744d54547cbe<br>REPORT RequestId: bc99de19-290b-46fa-908a-744d54547cbe  Init Duration: 0.14 ms  Duration: 1415.60 ms    Billed Duration: 1500 ms        Memory Size: 128 MB    Max Memory Used: 128 MB<br>{&quot;errorMessage&quot;:&quot;operation error CloudWatch Events: PutTargets; https response error StatusCode: 400; RequestID: 8e335486-de38-49bb-a06e-8090e26e9628; api error AccessDeniedException: Access to the resource arn:aws:lambda:us-east-1:123456789012:function:POC-executeBusinessRule is denied. Reason: Adding cross-account target is not permitted.&quot;;&quot;errorType&quot;:&quot;OperationError&quot;}%                                                                                                          <br>  POC <br></code></pre><br><p><strong>UPDATED: 21/02/16</strong><br>I've been searching what could be the problem. I'm using aws-sdk-go-v2 the example they exposed in ther official github documentation to get an instance of configuration:</p><br><p>I tryied two ways of getting config instance as you can see in this piece of code; the line commented and the line that comes after that is not commented.</p><br><pre class="lang-golang prettyprint-override"><code>import (<br>    &quot;context&quot;<br><br>    &quot;github.com/aws/aws-sdk-go-v2/config&quot;<br>    &quot;github.com/aws/aws-sdk-go-v2/service/cloudwatchevents&quot;<br>)<br><br>//NewCloudWatchService scheduler service<br>func NewCloudWatchService() CWEPutEventsAPI {<br>    cfg; err := config.LoadDefaultConfig(context.TODO())<br>    //Happens the same in local with this line<br>    // cfg; err := config.LoadDefaultConfig(context.TODO(); config.WithSharedConfigFiles(config.DefaultSharedCredentialsFiles))<br>    if err != nil {<br>        panic(&quot;configuration error; &quot; + err.Error())<br>    }<br><br>    return cloudwatchevents.NewFromConfig(cfg)<br>}<br><br></code></pre><br><p>This is my <strong>template.yaml</strong> file. I'm developing usin AWS SAM yaml templates. As you can see I reference <strong>ExecuteLambda</strong> in <strong>ScheduleLambda</strong> using <code>GettAtt</code> method but when the code runs in local the region and accountId are always the same: default values.</p><br><pre class="lang-yaml prettyprint-override"><code>AWSTemplateFormatVersion: &quot;2010-09-09&quot;<br>Transform: AWS::Serverless-2016-10-31<br>Globals:<br>  Function:<br>    Timeout: 5<br><br><br>Resources:<br>  ScheduleBusinessRuleFunction:<br>    Type: AWS::Serverless::Function<br>    Properties:<br>      FunctionName: POC-scheduleBusinessRule<br>      CodeUri: functions/scheduleBusinessRule/<br>      Handler: scheduleBusinessRule<br>      Runtime: go1.x<br>      MemorySize: 128<br>      Timeout: 10<br>      Tracing: Active<br>      Role: !GetAtt SchedulerRole.Arn<br>      Environment:<br>        Variables:<br>          LAMBDA_ARN: !GetAtt ExecuteBusinessRuleFunction.Arn<br>          EVENT_BUS: &quot;default&quot;<br><br>  ExecuteBusinessRuleFunction:<br>    Type: AWS::Serverless::Function<br>    Properties:<br>      FunctionName: POC-executeBusinessRule<br>      CodeUri: functions/executeBusinessRule/<br>      Handler: executeBusinessRule<br>      Runtime: go1.x<br>      MemorySize: 128<br>      Tracing: Active<br></code></pre><br>
0.0,0.0,0.0,1.0,0.0,0.3333333333333333,0.0,<h3>Which AWS S3 API action will trigger which event type?</h3><p>There are several event types for S3 buckets which can trigger a lambda (or something else) which are listed <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html#supported-notification-event-types" rel="nofollow noreferrer">here</a>.</p><br><p>An event type correspond to a given action on a bucket. For instance the event type <code>s3:ObjectCreated:*</code> correspond to new object created event.</p><br><p>I would like to know which <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_Operations.html" rel="nofollow noreferrer">S3 API Action</a> will trigger which event type.</p><br><p>For instance the <code>PutObject</code> Action will obviously trigger an <code>s3:ObjectCreated:*</code> event.<br>But for some other action; that's not obvious.</p><br><p>Is this well documented somewhere ? How can I find this information (without launching tests)?</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>How to add terraform aws_cognito_user_pool_client access token and id token validity?</h3><p>In AWS Cognito Console; App Clients under &quot;General Settings&quot;; there are 3 types of token expirations that can be customized:</p><br><ul><br><li>Refresh token expiration</li><br><li>Access token expiration</li><br><li>ID Token expiration</li><br></ul><br><p>Based on <a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/cognito_user_pool_client" rel="nofollow noreferrer">terraform documentation</a>; the aws_cognito_user_pool_client resource has a &quot;refresh_token_validity&quot; attribute that I could use to specify the expiration time for refresh tokens. However; there's none for access token or ID token validity.</p><br><p>How can I specify those?</p><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.0,0.3333333333333333,<h3>Save Cost by stopping the EC2 instance behind Elastic BeanStalk in AWS?</h3><p>I spawned an Elastic BeanStalk service and stopped the EC2 instance it was using. Within a span of 10 seconds ; a new instance was spawned again.</p><br><p>I was thinking of the option to save money during non-business hours ; by shutting the EC2 instance used by the ElasticBean Stalk ; but a new instance seems to come back up again.</p><br><p>Is there any option that i can use to stop the instance to save money during non-working hours and start it back up; when needed.</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,1.0,0.0,<h3>Cannot access AWS SQS</h3><p>Java code is trying to put message in AWS SQS.But we are getting error</p><br><p><code>AWS Error Code: AccessDenied; Access to the resource https://sqs.eu-west-1.amzon.com/...... is denied Status Code: 403; AWS Request ID: 2fe34c11-7af8-5445-a768-070159a0953e</code></p><br><p>There is no issue with the policy same policy works in test region but we are getting this error in dev.</p><br><p>I also tried with policy &quot;sqs:*&quot;;but this is also not working.</p><br><p>Java application is running in EC2 and we have given this role to EC2.</p><br><p>Also when i tried sending message through AWSCLi from the same EC2 it works.</p><br><p>I am not sure what i am missing here.Please suggest.</p><br><p>We are using proxy to connect to SQS url here.Incase if proxy is not working how it will try to resolve URL using SQS Endpoint?</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>Create SSRS Data Driven Subscriptions in Amazon Redshift</h3><p>I have few SSRS reports which were initially built using the data/stored procedures from SSMS.<br>We have now migrated over to Amazon Redshift and created new views in redshift to support the reports and they are working fine. The last part is creating the data driven/file share subscriptions. These subscriptions are created using the stored procedures in SSMS. Is there a way to create a stored procedure in Amazon Redshift for these data driven subscriptions?</p><br><p>Regards;<br>S</p><br>
0.3333333333333333,0.0,0.0,0.0,1.0,0.0,0.6666666666666666,<h3>Setting up Redash Instance in private subnet. EC2 status check failed</h3><p><strong>Issue Summary</strong></p><br><p>I would like to set up Redash Instance in private subnet; but it didnt work well. The instance status check is 1/2 failed.<br>The question is whether there is some necessary setting in addition to the setting introduced in the website(<a href="https://redash.io/help/open-source/setup" rel="nofollow noreferrer">https://redash.io/help/open-source/setup</a>).</p><br><p>For your information; if I place the redash instance on the public subnet; it works well.</p><br><p><strong>Technical details:</strong></p><br><p>AMI: ami-060741a96307668be</p><br><p>EC2 size: t2.small</p><br><p>the private subnet has NAT Gateway</p><br><p>CloudFormation template is below.I removed parameters because those were kind of secret information. The parameters are correct because I checked those parameters with public subnet. So please check the other part; Thank you.</p><br><pre><code>AWSTemplateFormatVersion: '2010-09-09'<br>Description: This template is used for creating redash analysis foundation<br>Resources:<br>  ####################################################################################################<br>  #### NetWork Setting<br>  ####################################################################################################<br>  RedashInstancePrivateSubnetA:<br>    Type: AWS::EC2::Subnet<br>    Properties:<br>      AvailabilityZone: ap-northeast-1a<br>      CidrBlock: !Ref PrivateSubnetACidrBlock<br>      VpcId: !Ref VpcId<br> PrivateSubnetARoute:<br>   Type: AWS::EC2::SubnetRouteTableAssociation<br>   Properties:<br>     RouteTableId: !Ref PrivateSubnetRouteTable<br>     SubnetId: !Ref RedashInstancePrivateSubnetA<br>PrivateSubnetRouteTable:<br>Type: AWS::EC2::RouteTable<br>Properties:<br>    VpcId: !Ref VpcId<br>  NATGatewayForPrivateSubnetA:<br>    Type: AWS::EC2::NatGateway<br>    Properties:<br>      AllocationId: !GetAtt NATGatewayAEIP.AllocationId<br>      SubnetId: !Ref RedashALBPublicSubnetA<br>  NATGatewayAEIP:<br>    Type: AWS::EC2::EIP<br>    Properties:<br>      Domain: vpc<br>  PrivateARoute:<br>    Type: AWS::EC2::Route<br>    Properties:<br>      RouteTableId: !Ref PrivateSubnetRouteTable<br>      DestinationCidrBlock: 0.0.0.0/0<br>      NatGatewayId: !Ref NATGatewayForPrivateSubnetA<br>  RedashALBPublicSubnetA:<br>    Type: AWS::EC2::Subnet<br>    Properties:<br>      AvailabilityZone: ap-northeast-1a<br>      CidrBlock: !Ref PublicSubnetACidrBlock<br>      VpcId: !Ref VpcId<br>  PublicRouteTable:<br>    Type: AWS::EC2::RouteTable<br>    Properties:<br>      VpcId: !Ref VpcId<br>  PublicRoute:<br>    Type: AWS::EC2::Route<br>    Properties:<br>      RouteTableId: !Ref PublicRouteTable<br>      DestinationCidrBlock: 0.0.0.0/0<br>      GatewayId: !Sub ${InternetGatewayId}<br>  PublicSubnetARoute:<br>    Type: AWS::EC2::SubnetRouteTableAssociation<br>    Properties:<br>      RouteTableId: !Ref PublicRouteTable<br>      SubnetId: !Ref RedashALBPublicSubnetA<br>  ####################################################################################################<br>  #### Re:dash EC2 Instance<br>  ####################################################################################################<br>  RedashInstance:<br>    Type: AWS::EC2::Instance<br>    Properties:<br>      LaunchTemplate:<br>        LaunchTemplateId: !Ref RedashInstanceLaunchTemplate<br>        Version: !GetAtt RedashInstanceLaunchTemplate.LatestVersionNumber<br>      SubnetId: !Ref RedashInstancePrivateSubnetA<br>  RedashInstanceLaunchTemplate:<br>    Type: AWS::EC2::LaunchTemplate<br>    Properties:<br>      LaunchTemplateName: redash-isntance-lt<br>      LaunchTemplateData:<br>        SecurityGroupIds:<br>          - !Ref RedashInstanceSecurityGroup<br>        ImageId: ami-060741a96307668be<br>        InstanceType: t2.small<br>  RedashInstanceSecurityGroup:<br>    Type: AWS::EC2::SecurityGroup<br>    Properties:<br>      GroupDescription: This Security Group is used for Re:dash Instance<br>      GroupName: redash-instance-sg<br>      SecurityGroupIngress:<br>          - IpProtocol: tcp<br>            FromPort: 80<br>            ToPort: 80<br>            SourceSecurityGroupId: !Ref RedashALBSecurityGroup<br>      VpcId: !Ref VpcId<br></code></pre><br><p>From marcin's comment; I try the template below; but it did not work well; ec2 status check shows '1/2 failed'</p><br><pre><code>AWSTemplateFormatVersion: '2010-09-09'<br>Description: This template is used for creating redash analysis foundation<br>Resources:<br>  ####################################################################################################<br>  #### NetWork Setting<br>  ####################################################################################################<br><br>  RedashInstancePrivateSubnetA:<br>    Type: AWS::EC2::Subnet<br>    Properties:<br>      AvailabilityZone: ap-northeast-1a<br>      CidrBlock: 172.18.0.0/24<br>      VpcId: &lt;VPCID&gt;<br>      Tags:<br>        - Key: Name<br>          Value: Private<br><br>  PrivateSubnetARoute:<br>    Type: AWS::EC2::SubnetRouteTableAssociation<br>    Properties:<br>      RouteTableId: !Ref PrivateSubnetRouteTable<br>      SubnetId: !Ref RedashInstancePrivateSubnetA<br><br><br>  PrivateSubnetRouteTable:<br>    Type: AWS::EC2::RouteTable<br>    Properties:<br>        VpcId: &lt;VPCID&gt;<br><br>  NATGatewayForPrivateSubnetA:<br>    Type: AWS::EC2::NatGateway<br>    Properties:<br>      AllocationId: !GetAtt NATGatewayAEIP.AllocationId<br>      SubnetId: !Ref RedashALBPublicSubnetA<br><br>  NATGatewayAEIP:<br>    Type: AWS::EC2::EIP<br>    Properties:<br>      Domain: vpc<br><br>  PrivateARoute:<br>    Type: AWS::EC2::Route<br>    Properties:<br>      RouteTableId: !Ref PrivateSubnetRouteTable<br>      DestinationCidrBlock: 0.0.0.0/0<br>      NatGatewayId: !Ref NATGatewayForPrivateSubnetA<br><br>  RedashALBPublicSubnetA:<br>    Type: AWS::EC2::Subnet<br>    Properties:<br>      AvailabilityZone: ap-northeast-1a<br>      CidrBlock: 172.18.2.0/24<br>      VpcId: &lt;VPCID&gt;<br>      MapPublicIpOnLaunch: true<br>      Tags:<br>        - Key: Name<br>          Value: Public<br><br>  PublicRouteTable:<br>    Type: AWS::EC2::RouteTable<br>    Properties:<br>      VpcId: &lt;VPCID&gt;<br><br>  PublicRoute:<br>    Type: AWS::EC2::Route<br>    Properties:<br>      RouteTableId: !Ref PublicRouteTable<br>      DestinationCidrBlock: 0.0.0.0/0<br>      GatewayId: &lt;INTERNETGATEWAYID&gt;<br><br>  PublicSubnetARoute:<br>    Type: AWS::EC2::SubnetRouteTableAssociation<br>    Properties:<br>      RouteTableId: !Ref PublicRouteTable<br>      SubnetId: !Ref RedashALBPublicSubnetA<br>  ####################################################################################################<br>  #### Re:dash EC2 Instance<br>  ####################################################################################################<br>  RedashInstance:<br>    Type: AWS::EC2::Instance<br>    Properties:<br>      LaunchTemplate:<br>        LaunchTemplateId: !Ref RedashInstanceLaunchTemplate<br>        Version: !GetAtt RedashInstanceLaunchTemplate.LatestVersionNumber<br>      SubnetId: !Ref RedashInstancePrivateSubnetA<br><br>  RedashInstanceLaunchTemplate:<br>    Type: AWS::EC2::LaunchTemplate<br>    Properties:<br>      LaunchTemplateName: redash-isntance-lt<br>      LaunchTemplateData:<br>        SecurityGroupIds:<br>          - !Ref RedashInstanceSecurityGroup<br>        ImageId: ami-060741a96307668be<br>        InstanceType: t2.small<br><br>  RedashInstanceSecurityGroup:<br>    Type: AWS::EC2::SecurityGroup<br>    Properties:<br>      GroupDescription: This Security Group is used for Re:dash Instance<br>      GroupName: redash-instance-sg<br>      SecurityGroupIngress:<br>          - IpProtocol: tcp<br>            FromPort: 80<br>            ToPort: 80<br>            CidrIp: 0.0.0.0/0<br>            #SourceSecurityGroupId: !Ref RedashALBSecurityGroup<br>      VpcId: &lt;VPCID&gt;<br></code></pre><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>To use DynamoDB or Redis on our service</h3><p>I am implementing a new service wherein I am planning to use Redis or DynamoDB but not sure which one to pick.<br>The working would be something like:</p><br><ol><br><li>Get data from front-end store it in DB.</li><br><li>If the front end sends one more request; then forward the data stored in the database and delete it later.</li><br><li>Else; if the front end does not send one more request we can delete the data stored.</li><br></ol><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>How to Pull list of AWS Config custom Rules using Advanced query</h3><p>I am trying to get a list of all the AWS custom rules with resource type IAM USER and IAM ROLES on my AWS account.</p><br><pre><code>    SELECT<br>  resourceType;<br>  resourceName;<br>  configuration.configRuleList.configRuleName<br>WHERE<br>  resourceType = 'AWS::IAM::User'<br></code></pre><br><p>I have been trying the above Advanced query but no luck. Any other ideas ????</p><br>
0.0,0.0,0.6666666666666666,0.0,1.0,0.0,0.0,<h3>AWS EBS - How to pull environment name into .ebextensions script</h3><p>I have a grails app that I deploy to AWS Elastic Beanstalk through Jenkins. I want to add a splunk forwarder to my project so I can keep track of my logs outside of AWS and set up easy notifications.</p><br><p>The problem is; I have multiple environments of the app running (dev; pre-prod; prod; etc); which is fine because you can just change the environment name for the forwarded and be able to easily sort through that in Splunk.</p><br><p>However; the same .ebextensions file has to be used between all the environments; no I need a way to set the environment name to whatever AWS has the name as. Is there a way I can easily do this that I'm overlooking?</p><br><p>Start of the script:</p><br><pre><code>container_commands:<br>    01install-splunk:<br>        command: /usr/local/bin/install-splunk.sh<br>    02set-splunk-outputs:<br>        command: /usr/local/bin/set_splunk_outputs.sh<br>        env:<br>            SPLUNK_SERVER_HOST: &quot;splunk.host&quot;<br>    03add-inputs-to-splunk:<br>        command: /usr/local/bin/add-inputs-to-splunk.sh<br>        env:<br>            ENVIRONMENT_NAME: &quot;Development&quot;<br>        cwd: /root<br>        ignoreErrors: false <br></code></pre><br><p>That <code>ENVIRONMENT_NAME</code> variable I'm setting that's passed to the 3rd script is what I want to be able to change based on what environment is being deployed. Can I set this in Jenkins or pull it through AWS somehow?</p><br>
0.0,0.0,1.0,1.0,0.3333333333333333,0.0,0.0,<h3>Accessing S3 buckets with same access policy from EC2 instance with different results</h3><p>I have two different buckets in the same account and they both have identical access policies</p><br><p>First bucket policy is</p><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Id&quot;: &quot;Policy0&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Sid&quot;: &quot;Stmt0&quot;;<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Principal&quot;: {<br>                &quot;AWS&quot;: [<br>                    &quot;arn:aws:iam::&lt;account&gt;:role/MyEC2Role0&quot;;<br>                    &quot;arn:aws:iam::&lt;account&gt;:role/MyEC2Role1&quot;<br>                ]<br>            };<br>            &quot;Action&quot;: &quot;s3:*&quot;;<br>            &quot;Resource&quot;: [<br>                &quot;arn:aws:s3:::bucket-uploads&quot;;<br>                &quot;arn:aws:s3:::bucket-uploads/*&quot;<br>            ]<br>        }<br>    ]<br>}<br></code></pre><br><p>Second bucket policy is just the same but changing the bucket name</p><br><pre><code>{<br>    &quot;Version&quot;: &quot;2012-10-17&quot;;<br>    &quot;Id&quot;: &quot;Policy1&quot;;<br>    &quot;Statement&quot;: [<br>        {<br>            &quot;Sid&quot;: &quot;Stmt1&quot;;<br>            &quot;Effect&quot;: &quot;Allow&quot;;<br>            &quot;Principal&quot;: {<br>                &quot;AWS&quot;: [<br>                    &quot;arn:aws:iam::&lt;account&gt;:role/MyEC2Role0&quot;;<br>                    &quot;arn:aws:iam::&lt;account&gt;:role/MyEC2Role1&quot;<br>                ]<br>            };<br>            &quot;Action&quot;: &quot;s3:*&quot;;<br>            &quot;Resource&quot;: [<br>                &quot;arn:aws:s3:::bucket-exchange&quot;;<br>                &quot;arn:aws:s3:::bucket-exchange/*&quot;<br>            ]<br>        }<br>    ]<br>}<br></code></pre><br><p>I access both buckets from an EC2 instance running Win16 Server. The instance role is <code>MyEC2Role1</code> and it has assigned the policy <code>AmazonS3FullAccess</code> and it belongs to same account as well.</p><br><p>When I run the AWS CLI command <code>aws s3 ls s3://bucket-uploads/</code>; it's successful and lists the bucket's contents. However when I run <code>aws s3 ls s3://bucket-exchange/</code>; it raises following error</p><br><pre><code>An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied<br></code></pre><br><p>I've compared buckets properties and policies once and again and I haven't found any difference; I've searched at AWS Documentation for troubleshot for that error; I've also googled for a long; I've recreated the bucket and anything worked.</p><br><p>Has anyone had the same problem? Any idea about a solution or anythin to try?</p><br><p>Thanks.</p><br>
0.0,0.0,1.0,0.0,0.3333333333333333,0.6666666666666666,0.0,<h3>Docker does not set dynamic environment variables</h3><p>We are using AWS Secrets Manager to put every secret to single key:</p><br><pre><code>{<br>  &quot;containerDefinitions&quot;: [<br>    &quot;secrets&quot;: [<br>      {<br>        &quot;valueFrom&quot;: &quot;arn:aws:secretsmanager:eu-west-1:1234:secret:path/to/secret-7cHzY2&quot;;<br>        &quot;name&quot;: &quot;AWS_SECRETS&quot;<br>      }<br>    ];<br>  ]<br>}<br></code></pre><br><p>that produces following <code>env</code> key:</p><br><pre><code>AWS_SECRETS: {&quot;SECRET_1&quot;: &quot;VALUE_1&quot;; &quot;SECRET_2&quot;: &quot;VALUE_2&quot;}<br></code></pre><br><p><strong>My goal</strong> is to have <code>echo $SECRET_1 # output: VALUE_1</code> inside docker container</p><br><hr /><br><p>For that I have create file that Docker executes:</p><br><blockquote><br><p>docker-secrets-extractor.sh</p><br></blockquote><br><pre><code>#!/bin/bash<br><br># Export JSON formatted AWS secrets to ENV variables<br>if [ ! -z &quot;${AWS_SECRETS}&quot; ]; then<br>    export $(echo &quot;${AWS_SECRETS}&quot; | jq -r &quot;to_entries|map(\&quot;\(.key)=\(.value|tostring)\&quot;)|.[]&quot;)<br>else<br>    echo &quot;No AWS_SECRETS variable&quot;<br>fi<br><br>env<br><br>exec &quot;$@&quot;<br></code></pre><br><p>And in <code>Dockerfile</code></p><br><pre><code>RUN apt-get update &amp;&amp; apt-get install jq<br><br>ADD ./scripts/docker-init/docker-secrets-extractor.sh /docker-secrets-extractor.sh<br><br>ENTRYPOINT [&quot;/docker-secrets-extractor.sh&quot;]<br>CMD [&quot;/bin/bash&quot;; &quot;/start.sh&quot;]<br></code></pre><br><hr /><br><p>Problem is that inside <code>docker-secrets-extractor.sh</code> output of <code>env</code> has all values exported; but when doing <code>docker exec -it fooBar bash</code> and then doing <code>env</code>; no values are present.</p><br><p>What am I doing wrong? Maybe there is better approach?</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>How can I return to the aws rekognition previous version</h3><p>I use AWS Rekognition API for comparing to faces on different photos which works greatly but from some date to today now the API recognize also faces with mask (like the once use for covid) but this is not desired behavior in my application.</p><br><p>So; how can I return to the previous version of the API.</p><br><p>I'm using boto3 to execute the service like this:</p><br><pre><code>   import boto3<br>    rekognition = boto3.client('rekognition'; 'us-west-2')<br>    response = rekognition.compare_faces(<br>                    SourceImage={<br>                        'Bytes': credencial_data<br>                    };<br>                    TargetImage={<br>                        'Bytes': captura_data<br>                    };<br>                    SimilarityThreshold=threshold;<br>                )<br></code></pre><br><p>Thank you for the help.</p><br>
0.0,0.0,0.0,0.0,1.0,1.0,0.0,<h3>How to capture request id inside a lambda function in a chalice app</h3><p>I am having a chalice app which exposes some urls backed by api gateway and lambda function.<br>What I want is to log the request id with each and every log msg for debugging purposes.<br>Below is how my code looks like.</p><br><pre><code>@app.route('/instrument'; methods=['GET'])<br>@logging_and_error_handling()<br>def get_instrument_value():<br> // some code<br> // logger.log() Here I want to put the request id.<br></code></pre><br><p>I can see that request id is getting logged in the cloudwatch by default. I want to use the same request id. How do I capture it inside my method ?</p><br><p><a href="https://i.stack.imgur.com/c5Fyz.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/c5Fyz.png" alt="enter image description here" /></a></p><br>
0.0,0.0,0.6666666666666666,0.0,0.3333333333333333,0.6666666666666666,0.3333333333333333,<h3>How to connect Google sheet API on AWS EC2</h3><p>I am trying to use Google Sheets API to load data into EC2 using Python.<br>So; I tried this <a href="https://developers.google.com/sheets/api/quickstart/python" rel="nofollow noreferrer">quickstart</a>.</p><br><p>But I am stuck <a href="https://i.stack.imgur.com/nTgUB.png" rel="nofollow noreferrer">configuring OAuth client</a> to get credential.json file. I can't understand what drop-down type I should select.</p><br><p>Hope I was clear. Thanks in advance for your time.</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>AWS S3 life cycle transition to glacier deep archive</h3><p>Since the Minimum number of days for objects in Glacier deep archive is 180 days. In my case the data is transitioned to Glacier deep archive after 10 days. should the expiry be mentioned as 190 days ?</p><br><p>Basically the expiry days we mention in the AWS console s3 lifecycle policy - Does it mean from the time we moved it to Deep Glacier or from the time the object got created.</p><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>automate aws keys into credential file</h3><p>I am building a scrip to automatically take values from aws sts get-session-token and populate my ~/.aws/credentials file. I can independently run these tasks</p><br><ol><br><li>aws sts get-session-token --serial-number &lt;&gt; -token-code &lt;&gt;</li><br><li>manually take the values from the output and run &quot;aws configure set aws_access_key_id xxxxxx --profile abc &quot; etc...</li><br></ol><br><p>is there a way to get the output from step 1 as a variable and pass specific values to as inputs to step 2 ?</p><br><p>THanks</p><br>
0.0,0.0,0.0,0.3333333333333333,0.6666666666666666,1.0,0.0,<h3>AWS Lambda accessing S3 Bucket</h3><p>I am currently working on a project which is a Lambda function triggered by an API call via AWS API Gateway; which needs to access an S3 bucket.<br>Now I am using NodeJS and my code so far is:</p><br><pre class="lang-js prettyprint-override"><code>const AWS = require(&quot;aws-sdk&quot;);<br>const s3 = new AWS.S3({ apiVersion: '2006-03-01' });<br><br>const noteId = event.id;<br>const params = {<br>    Bucket: &quot;notebytes&quot;;<br>    Key: `/note/${noteId}.json`;<br>};<br>exports.handler = async (event) =&gt; {<br>    s3.getObject(params; function(err; data) {<br>        if(err){<br>            return {&quot;statusCode&quot;:404;&quot;body&quot;:`NOTE#${noteId} not found`};<br>        } else {<br>            return{&quot;statusCode&quot;:200};<br>        }<br>    });<br>    return {&quot;statusCode&quot;:400};<br>};<br></code></pre><br><p>When I don't have package &quot;aws&quot; on; it returns errors that there is no such package; which is fair. When I added a zip with node module aws; it returns the error that <code>AWS.S3 is not a constructor</code>. When I change the <code>require(&quot;aws&quot;)</code> to <code>require(&quot;aws-sdk&quot;)</code> it just times out. When I try adding the <code>aws-sdk</code> package; it is just too big for Lambda.</p><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>Python/AWS Parameter Store: updated SSM values</h3><p>Let's say I have a AWS SSM called <code>/config/db</code> withe following values:</p><br><pre><code>{<br>   &quot;host&quot;: &quot;localhost&quot;;<br>   &quot;port&quot;: &quot;3306&quot;<br>}<br></code></pre><br><p>now I need to add the following<br>item(s) to the same SSM</p><br><pre><code>{<br>  &quot;my_version&quot;: &quot;1.0&quot;<br>}<br></code></pre><br><p>How can I use the Python/boto3 package to archive this action?</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>Why are these query plans so different?</h3><p>We have two RDS environments with read replicas (test has one; production has three) and a conundrum: why does the same query; over the same data (19.5million rows for the view) + same indexes; take close to 1600msec or more in the production environment compared to the test environment's sub-millisecond response?</p><br><p>Both environments are using the latest RDS PostgreSQL (11.8); are using the same VM type of db.m5.xlarge; and to the best of my ability to check; are configured the same - apart from the number of read replicas.</p><br><p>I don't know what to look at (or into) to figure out why the query plans are <em>so</em> different.</p><br><p>Edit: The settings in both test and production containing the word <code>parallel</code> are</p><br><pre><code> enable_parallel_append                          on     <br> enable_parallel_hash                            on     <br> force_parallel_mode                             off    <br> max_parallel_maintenance_workers                2      <br> max_parallel_workers                            8      <br> max_parallel_workers_per_gather                 2      <br> min_parallel_index_scan_size                    512kB  <br> min_parallel_table_scan_size                    8MB    <br> parallel_leader_participation                   on     <br> parallel_setup_cost                             1000   <br> parallel_tuple_cost                             0.1    <br></code></pre><br><p>The query plan for test is</p><br><pre class="lang-sql prettyprint-override"><code>db=&gt; explain (analyze; buffers)<br>select columns<br>   from view<br>   where ( search_column like '342 KING ST C'||'%' ESCAPE '~' OR search_column like '342 KING STREET C' ||'%' ESCAPE '~' OR search_column like '342 KING SAINT C' ||'%' ESCAPE '~' )<br>   AND result_type in (1; 2; 3; 4)<br>     limit 10<br>;<br>                                                                                         QUERY PLAN                                                                                         <br>--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------<br> Limit  (cost=18.24..97.45 rows=10 width=152) (actual time=0.034..0.095 rows=9 loops=1)<br>   Buffers: shared hit=76<br>   -&gt;  Append  (cost=18.24..44543.84 rows=5621 width=152) (actual time=0.033..0.093 rows=9 loops=1)<br>         Buffers: shared hit=76<br>         -&gt;  Subquery Scan on &quot;*SELECT* 1&quot;  (cost=18.24..44464.47 rows=5449 width=151) (actual time=0.033..0.075 rows=9 loops=1)<br>               Buffers: shared hit=55<br>               -&gt;  Nested Loop Left Join  (cost=18.24..44409.98 rows=5449 width=219) (actual time=0.032..0.073 rows=9 loops=1)<br>                     Buffers: shared hit=55<br>                     -&gt;  Bitmap Heap Scan on _address_full_location a  (cost=17.80..21.82 rows=5449 width=110) (actual time=0.022..0.031 rows=9 loops=1)<br>                           Recheck Cond: ((address_display ~~ '342 KING ST C%'::text) OR (address_display ~~ '342 KING STREET C%'::text) OR (address_display ~~ '342 KING SAINT C%'::text))<br>                           Filter: ((address_display ~~ '342 KING ST C%'::text) OR (address_display ~~ '342 KING STREET C%'::text) OR (address_display ~~ '342 KING SAINT C%'::text))<br>                           Heap Blocks: exact=7<br>                           Buffers: shared hit=19<br>                           -&gt;  BitmapOr  (cost=17.80..17.80 rows=1 width=0) (actual time=0.017..0.017 rows=0 loops=1)<br>                                 Buffers: shared hit=12<br>                                 -&gt;  Bitmap Index Scan on _address_full_location_ix_address_search_isunit  (cost=0.00..4.57 rows=1 width=0) (actual time=0.009..0.009 rows=0 loops=1)<br>                                       Index Cond: ((address_display &gt;= '342 KING ST C'::text) AND (address_display &lt; '342 KING ST D'::text))<br>                                       Buffers: shared hit=4<br>                                 -&gt;  Bitmap Index Scan on _address_full_location_ix_address_search_isunit  (cost=0.00..4.57 rows=1 width=0) (actual time=0.006..0.006 rows=9 loops=1)<br>                                       Index Cond: ((address_display &gt;= '342 KING STREET C'::text) AND (address_display &lt; '342 KING STREET D'::text))<br>                                       Buffers: shared hit=4<br>                                 -&gt;  Bitmap Index Scan on _address_full_location_ix_address_search_isunit  (cost=0.00..4.57 rows=1 width=0) (actual time=0.002..0.002 rows=0 loops=1)<br>                                       Index Cond: ((address_display &gt;= '342 KING SAINT C'::text) AND (address_display &lt; '342 KING SAINT D'::text))<br>                                       Buffers: shared hit=4<br>                     -&gt;  Index Scan using _property_ix_property_id on _property p  (cost=0.43..8.14 rows=1 width=5) (actual time=0.003..0.003 rows=1 loops=9)<br>                           Index Cond: (a.property_id = property_id)<br>                           Buffers: shared hit=36<br>         -&gt;  Bitmap Heap Scan on _address_street_location st  (cost=13.43..17.45 rows=166 width=172) (actual time=0.006..0.006 rows=0 loops=1)<br>               Recheck Cond: ((street_display ~~ '342 KING ST C%'::text) OR (street_display ~~ '342 KING STREET C%'::text) OR (street_display ~~ '342 KING SAINT C%'::text))<br>               Filter: ((street_display ~~ '342 KING ST C%'::text) OR (street_display ~~ '342 KING STREET C%'::text) OR (street_display ~~ '342 KING SAINT C%'::text))<br>               Buffers: shared hit=9<br>               -&gt;  BitmapOr  (cost=13.43..13.43 rows=1 width=0) (actual time=0.006..0.006 rows=0 loops=1)<br>                     Buffers: shared hit=9<br>                     -&gt;  Bitmap Index Scan on _address_street_location_ix_street_search  (cost=0.00..4.43 rows=1 width=0) (actual time=0.003..0.003 rows=0 loops=1)<br>                           Index Cond: ((street_display &gt;= '342 KING ST C'::text) AND (street_display &lt; '342 KING ST D'::text))<br>                           Buffers: shared hit=3<br>                     -&gt;  Bitmap Index Scan on _address_street_location_ix_street_search  (cost=0.00..4.43 rows=1 width=0) (actual time=0.001..0.002 rows=0 loops=1)<br>                           Index Cond: ((street_display &gt;= '342 KING STREET C'::text) AND (street_display &lt; '342 KING STREET D'::text))<br>                           Buffers: shared hit=3<br>                     -&gt;  Bitmap Index Scan on _address_street_location_ix_street_search  (cost=0.00..4.43 rows=1 width=0) (actual time=0.001..0.002 rows=0 loops=1)<br>                           Index Cond: ((street_display &gt;= '342 KING SAINT C'::text) AND (street_display &lt; '342 KING SAINT D'::text))<br>                           Buffers: shared hit=3<br>         -&gt;  Bitmap Heap Scan on _address_suburb_location su  (cost=12.90..16.91 rows=5 width=158) (actual time=0.004..0.004 rows=0 loops=1)<br>               Recheck Cond: ((suburb_display ~~ '342 KING ST C%'::text) OR (suburb_display ~~ '342 KING STREET C%'::text) OR (suburb_display ~~ '342 KING SAINT C%'::text))<br>               Filter: ((suburb_display ~~ '342 KING ST C%'::text) OR (suburb_display ~~ '342 KING STREET C%'::text) OR (suburb_display ~~ '342 KING SAINT C%'::text))<br>               Buffers: shared hit=6<br>               -&gt;  BitmapOr  (cost=12.90..12.90 rows=1 width=0) (actual time=0.004..0.004 rows=0 loops=1)<br>                     Buffers: shared hit=6<br>                     -&gt;  Bitmap Index Scan on _address_suburb_location_ix_suburb_search  (cost=0.00..4.30 rows=1 width=0) (actual time=0.002..0.002 rows=0 loops=1)<br>                           Index Cond: ((suburb_display &gt;= '342 KING ST C'::text) AND (suburb_display &lt; '342 KING ST D'::text))<br>                           Buffers: shared hit=2<br>                     -&gt;  Bitmap Index Scan on _address_suburb_location_ix_suburb_search  (cost=0.00..4.30 rows=1 width=0) (actual time=0.001..0.001 rows=0 loops=1)<br>                           Index Cond: ((suburb_display &gt;= '342 KING STREET C'::text) AND (suburb_display &lt; '342 KING STREET D'::text))<br>                           Buffers: shared hit=2<br>                     -&gt;  Bitmap Index Scan on _address_suburb_location_ix_suburb_search  (cost=0.00..4.30 rows=1 width=0) (actual time=0.001..0.001 rows=0 loops=1)<br>                           Index Cond: ((suburb_display &gt;= '342 KING SAINT C'::text) AND (suburb_display &lt; '342 KING SAINT D'::text))<br>                           Buffers: shared hit=2<br>         -&gt;  Bitmap Heap Scan on _address_postcode_location pc  (cost=12.88..16.90 rows=1 width=147) (actual time=0.006..0.006 rows=0 loops=1)<br>               Recheck Cond: ((postcode_display ~~ '342 KING ST C%'::text) OR (postcode_display ~~ '342 KING STREET C%'::text) OR (postcode_display ~~ '342 KING SAINT C%'::text))<br>               Filter: ((postcode_display ~~ '342 KING ST C%'::text) OR (postcode_display ~~ '342 KING STREET C%'::text) OR (postcode_display ~~ '342 KING SAINT C%'::text))<br>               Buffers: shared hit=6<br>               -&gt;  BitmapOr  (cost=12.88..12.88 rows=1 width=0) (actual time=0.006..0.006 rows=0 loops=1)<br>                     Buffers: shared hit=6<br>                     -&gt;  Bitmap Index Scan on _address_postcode_location_ix_postcode_search  (cost=0.00..4.29 rows=1 width=0) (actual time=0.003..0.003 rows=0 loops=1)<br>                           Index Cond: ((postcode_display &gt;= '342 KING ST C'::text) AND (postcode_display &lt; '342 KING ST D'::text))<br>                           Buffers: shared hit=2<br>                     -&gt;  Bitmap Index Scan on _address_postcode_location_ix_postcode_search  (cost=0.00..4.29 rows=1 width=0) (actual time=0.001..0.001 rows=0 loops=1)<br>                           Index Cond: ((postcode_display &gt;= '342 KING STREET C'::text) AND (postcode_display &lt; '342 KING STREET D'::text))<br>                           Buffers: shared hit=2<br>                     -&gt;  Bitmap Index Scan on _address_postcode_location_ix_postcode_search  (cost=0.00..4.29 rows=1 width=0) (actual time=0.001..0.001 rows=0 loops=1)<br>                           Index Cond: ((postcode_display &gt;= '342 KING SAINT C'::text) AND (postcode_display &lt; '342 KING SAINT D'::text))<br>                           Buffers: shared hit=2<br> Planning Time: 3.427 ms<br> Execution Time: 0.174 ms<br>(74 rows)<br></code></pre><br><p>The production query plan is</p><br><pre class="lang-sql prettyprint-override"><code>db=&gt; explain (analyze; buffers)<br>select columns<br>   from view<br>   where ( search_column like '342 KING ST C'||'%' ESCAPE '~' OR search_column like '342 KING STREET C' ||'%' ESCAPE '~' OR search_column like '342 KING SAINT C' ||'%' ESCAPE '~' )<br>   AND result_type in (1; 2; 3; 4)<br>     limit 10<br>;<br>                                                                                         QUERY PLAN                                                                                         <br>--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------<br> Limit  (cost=1000.00..2053.45 rows=10 width=151) (actual time=1243.140..1680.998 rows=9 loops=1)<br>   Buffers: shared hit=15815 read=425564<br>   I/O Timings: read=835.908<br>   -&gt;  Gather  (cost=1000.00..603046.39 rows=5715 width=151) (actual time=1243.139..1682.255 rows=9 loops=1)<br>         Workers Planned: 2<br>         Workers Launched: 2<br>         Buffers: shared hit=15815 read=425564<br>         I/O Timings: read=835.908<br>         -&gt;  Parallel Append  (cost=0.00..601474.89 rows=5614 width=151) (actual time=1329.613..1677.727 rows=3 loops=3)<br>               Buffers: shared hit=15815 read=425564<br>               I/O Timings: read=835.908<br>               -&gt;  Subquery Scan on &quot;*SELECT* 1&quot;  (cost=0.43..581643.66 rows=5541 width=150) (actual time=1290.180..1638.292 rows=3 loops=3)<br>                     Buffers: shared hit=328 read=425564<br>                     I/O Timings: read=835.908<br>                     -&gt;  Nested Loop Left Join  (cost=0.43..581588.25 rows=2309 width=218) (actual time=1290.178..1638.288 rows=3 loops=3)<br>                           Buffers: shared hit=328 read=425564<br>                           I/O Timings: read=835.908<br>                           -&gt;  Parallel Seq Scan on _address_full_location a  (cost=0.00..562773.42 rows=2309 width=109) (actual time=1290.140..1638.218 rows=3 loops=3)<br>                                 Filter: ((address_display ~~ '342 KING ST C%'::text) OR (address_display ~~ '342 KING STREET C%'::text) OR (address_display ~~ '342 KING SAINT C%'::text))<br>                                 Rows Removed by Filter: 6258793<br>                                 Buffers: shared hit=290 read=425564<br>                                 I/O Timings: read=835.908<br>                           -&gt;  Index Scan using _property_ix_property_id on _property p  (cost=0.43..8.14 rows=1 width=5) (actual time=0.015..0.016 rows=1 loops=9)<br>                                 Index Cond: (a.property_id = property_id)<br>                                 Buffers: shared hit=38<br>               -&gt;  Parallel Seq Scan on _address_street_location st  (cost=0.00..19162.97 rows=70 width=172) (actual time=57.433..57.433 rows=0 loops=2)<br>                     Filter: ((street_display ~~ '342 KING ST C%'::text) OR (street_display ~~ '342 KING STREET C%'::text) OR (street_display ~~ '342 KING SAINT C%'::text))<br>                     Rows Removed by Filter: 280386<br>                     Buffers: shared hit=15074<br>               -&gt;  Parallel Seq Scan on _address_suburb_location su  (cost=0.00..535.86 rows=3 width=158) (actual time=2.798..2.798 rows=0 loops=1)<br>                     Filter: ((suburb_display ~~ '342 KING ST C%'::text) OR (suburb_display ~~ '342 KING STREET C%'::text) OR (suburb_display ~~ '342 KING SAINT C%'::text))<br>                     Rows Removed by Filter: 17472<br>                     Buffers: shared hit=356<br>               -&gt;  Parallel Seq Scan on _address_postcode_location pc  (cost=0.00..104.33 rows=1 width=147) (actual time=0.629..0.629 rows=0 loops=1)<br>                     Filter: ((postcode_display ~~ '342 KING ST C%'::text) OR (postcode_display ~~ '342 KING STREET C%'::text) OR (postcode_display ~~ '342 KING SAINT C%'::text))<br>                     Rows Removed by Filter: 4598<br>                     Buffers: shared hit=57<br> Planning Time: 2.846 ms<br> Execution Time: 1682.402 ms<br>(39 rows)<br></code></pre><br>
0.0,0.0,1.0,1.0,0.0,0.0,0.0,<h3>Terraform cloudwatch conflict with log types</h3><p>I am getting the following error while running <code>terraform plan</code> command:</p><br><pre><code>Error: aws_rds_cluster.my_rds_cluster: expected enabled_cloudwatch_logs_exports.0 to be one of [audit error general slowquery]; got postgresql<br></code></pre><br><p>I am using terraform-aws-provider version 2.24. I am trying to use cloudwatch with RDS Aurora PostgreSQL.</p><br><pre><code>aws rds modify-db-cluster \<br>    --db-cluster-identifier my-db-cluster \<br>    --cloudwatch-logs-export-configuration '{&quot;EnableLogTypes&quot;:[&quot;postgresql&quot;; &quot;upgrade&quot;]}'<br></code></pre><br>
1.0,0.0,0.6666666666666666,0.0,0.0,0.0,0.0,<h3>AWS Elasticsearch publishing wrong total request metric</h3><p>We have an AWS Elasticsearch cluster setup. However; our Error rate alarm goes off at regular intervals. The way we are trying to calculate our error rate is:</p><br><p>((sum(4xx) + sum(5xx))/sum(ElasticsearchRequests)) * 100</p><br><p>However; if you look at the screenshot below; at 7:15 <code>4xx</code> was <code>4</code>; however <code>ElasticsearchRequests</code> value is only <code>2</code>. Based on the metrics info on AWS Elasticsearch <a href="https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-managedomains-cloudwatchmetrics.html#es-managedomains-cloudwatchmetrics-cluster-metrics" rel="nofollow noreferrer">documentation</a> page; <code>ElasticsearchRequests</code> should be total number of requests; so it should clearly be greater than or equal to 4xx.</p><br><p>Can someone please help me understand in what I am doing wrong here?</p><br><p><a href="https://i.stack.imgur.com/sCh0J.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/sCh0J.png" alt="enter image description here" /></a></p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.6666666666666666,0.0,<h3>User: ARN is not authorized to perform: SNS:Publish on resource: ARN (AWS Integration error)</h3><p>I am trying to monitor an AWS Cloud environment using AquaSec. AquaSec helps you connect with your AWS environment by providing a CloudFormation template; which you would deploy at your AWS environment. Once it is deployed; you use it's ARN in AquaSec to connect/integrate both. Any alert in Aquasec would be sent to an SNS Topic which would send it further to a HTTPS endpoint.</p><br><p>This is the CloudFormation template file;</p><br><pre><code>{<br>&quot;AWSTemplateFormatVersion&quot;: &quot;2010-09-09&quot;;<br>&quot;Description&quot;: &quot;Aqua CSPM security scanner cross-account role&quot;;<br>&quot;Parameters&quot;: {<br>    &quot;ExternalId&quot;: {<br>        &quot;Type&quot;: &quot;String&quot;;<br>        &quot;Description&quot;: &quot;The external ID auto-generated from the Aqua Cloud dashboard. Do not change this value.&quot;;<br>        &quot;AllowedPattern&quot;: &quot;[-a-z0-9]*&quot;;<br>        &quot;ConstraintDescription&quot;: &quot;Must be lowercase or numbers; no spaces; dashes ok.&quot;<br>    }<br>};<br>&quot;Resources&quot;: {<br>    &quot;AquaCSPMRole&quot;: {<br>        &quot;Type&quot;: &quot;AWS::IAM::Role&quot;;<br>        &quot;Properties&quot;: {<br>            &quot;AssumeRolePolicyDocument&quot;: {<br>                &quot;Statement&quot;: [<br>                    {<br>                        &quot;Effect&quot;: &quot;Allow&quot;;<br>                        &quot;Principal&quot;: {<br>                            &quot;AWS&quot;: &quot;arn:aws:iam::057012691312:role/lambda-cloudsploit-api&quot;<br>                        };<br>                        &quot;Action&quot;: &quot;sts:AssumeRole&quot;;<br>                        &quot;Condition&quot;: {<br>                            &quot;StringEquals&quot;: {<br>                                &quot;sts:ExternalId&quot;: {<br>                                    &quot;Ref&quot;: &quot;ExternalId&quot;<br>                                }<br>                            };<br>                            &quot;IpAddress&quot;: {<br>                                &quot;aws:SourceIp&quot;: &quot;3.231.74.65/32&quot;<br>                            }<br>                        }<br>                    };<br>                    {<br>                        &quot;Effect&quot;: &quot;Allow&quot;;<br>                        &quot;Principal&quot;: {<br>                            &quot;AWS&quot;: &quot;arn:aws:iam::057012691312:role/lambda-cloudsploit-collector&quot;<br>                        };<br>                        &quot;Action&quot;: &quot;sts:AssumeRole&quot;;<br>                        &quot;Condition&quot;: {<br>                            &quot;StringEquals&quot;: {<br>                                &quot;sts:ExternalId&quot;: {<br>                                    &quot;Ref&quot;: &quot;ExternalId&quot;<br>                                }<br>                            };<br>                            &quot;IpAddress&quot;: {<br>                                &quot;aws:SourceIp&quot;: &quot;3.231.74.65/32&quot;<br>                            }<br>                        }<br>                    };<br>                    {<br>                        &quot;Effect&quot;: &quot;Allow&quot;;<br>                        &quot;Principal&quot;: {<br>                            &quot;AWS&quot;: &quot;arn:aws:iam::057012691312:role/lambda-cloudsploit-remediator&quot;<br>                        };<br>                        &quot;Action&quot;: &quot;sts:AssumeRole&quot;;<br>                        &quot;Condition&quot;: {<br>                            &quot;StringEquals&quot;: {<br>                                &quot;sts:ExternalId&quot;: {<br>                                    &quot;Ref&quot;: &quot;ExternalId&quot;<br>                                }<br>                            };<br>                            &quot;IpAddress&quot;: {<br>                                &quot;aws:SourceIp&quot;: &quot;3.231.74.65/32&quot;<br>                            }<br>                        }<br>                    };<br>                    {<br>                        &quot;Effect&quot;: &quot;Allow&quot;;<br>                        &quot;Principal&quot;: {<br>                            &quot;AWS&quot;: &quot;arn:aws:iam::057012691312:role/lambda-cloudsploit-tasks&quot;<br>                        };<br>                        &quot;Action&quot;: &quot;sts:AssumeRole&quot;;<br>                        &quot;Condition&quot;: {<br>                            &quot;StringEquals&quot;: {<br>                                &quot;sts:ExternalId&quot;: {<br>                                    &quot;Ref&quot;: &quot;ExternalId&quot;<br>                                }<br>                            };<br>                            &quot;IpAddress&quot;: {<br>                                &quot;aws:SourceIp&quot;: &quot;3.231.74.65/32&quot;<br>                            }<br>                        }<br>                    }<br>                ]<br>            };<br>            &quot;Policies&quot;: [<br>                {<br>                    &quot;PolicyName&quot;: &quot;aqua-cspm-supplemental-policy&quot;;<br>                    &quot;PolicyDocument&quot;: {<br>                        &quot;Version&quot;: &quot;2012-10-17&quot;;<br>                        &quot;Statement&quot;: [<br>                            {<br>                                &quot;Effect&quot;: &quot;Allow&quot;;<br>                                &quot;Action&quot;: [<br>                                    &quot;ses:DescribeActiveReceiptRuleSet&quot;;<br>                                    &quot;athena:GetWorkGroup&quot;;<br>                                    &quot;logs:DescribeLogGroups&quot;;<br>                                    &quot;logs:DescribeMetricFilters&quot;;<br>                                    &quot;elastictranscoder:ListPipelines&quot;;<br>                                    &quot;elasticfilesystem:DescribeFileSystems&quot;;<br>                                    &quot;servicequotas:ListServiceQuotas&quot;;<br>                                    &quot;ssm:ListAssociations&quot;;<br>                                    &quot;dlm:GetLifecyclePolicies&quot;;<br>                                    &quot;airflow:ListEnvironments&quot;;<br>                                    &quot;glue:GetSecurityConfigurations&quot;;<br>                                    &quot;devops-guru:ListNotificationChannels&quot;<br>                                ];<br>                                &quot;Resource&quot;: &quot;*&quot;<br>                            }<br>                        ]<br>                    }<br>                }<br>            ];<br>            &quot;ManagedPolicyArns&quot;: [<br>                &quot;arn:aws:iam::aws:policy/SecurityAudit&quot;<br>            ]<br>        }<br>    }<br>};<br>&quot;Outputs&quot;: {<br>    &quot;AquaCSPMeArn&quot;: {<br>        &quot;Description&quot;: &quot;The role ARN of the cross-account user. Copy this into Aqua Cloud.&quot;;<br>        &quot;Value&quot;: {<br>            &quot;Fn::GetAtt&quot;: [<br>                &quot;AquaCSPMRole&quot;;<br>                &quot;Arn&quot;<br>            ]<br>        }<br>    };<br>    &quot;StackVersion&quot;: {<br>        &quot;Description&quot;: &quot;The Aqua CSPM stack version.&quot;;<br>        &quot;Value&quot;: &quot;2.0&quot;<br>    }<br>}<br></code></pre><br><p>}</p><br><p>I am trying to setup an <strong>Amazon SNS Topic</strong>; use its ARN here at Aquasec to send out alerts. Once i create an SNS topic; and copy it's ARN at Aquasec; and try out a test notification - I keep getting the error;</p><br><blockquote><br><p>{<br>&quot;message&quot;: &quot;User: arn:aws:sts::057012691312:assumed-role/lambda-cloudsploit-api/cloudsploit-api is not authorized to perform: SNS:Publish on resource: arn:aws:sns:us-east-1:940386435759:Sample_Aqua_Integration&quot;;<br>&quot;code&quot;: &quot;AuthorizationError&quot;;<br>&quot;time&quot;: &quot;2021-04-19T22:15:54.463Z&quot;;<br>&quot;requestId&quot;: &quot;bc808944-3430-5683-aed1-d1bc376a70f5&quot;;<br>&quot;statusCode&quot;: 403;<br>&quot;retryable&quot;: false;<br>&quot;retryDelay&quot;: 50.2772842473471<br>}</p><br></blockquote><br><p>I have tried nearly every possible way - changed topic policy(various combinations of &quot;Principle&quot; field) in the SNS Topic; tried to give permissions in the specific IAM role. Nothing seems to work and I get the same error.<br>I feel it has to do something with the template file (in the '<em>assumeRole</em>' configuration) ?</p><br><p>Any suggestions on how and what to change/try?<br>Thanks</p><br>
0.0,0.6666666666666666,0.3333333333333333,0.0,1.0,0.0,0.0,<h3>Elastic Beanstalk &amp; EC2 Instance Connect: Can&#39;t connect</h3><p>I created an Elastic Beanstalk environment from Visual Studio and need to login to service the underlying ec2 vm.</p><br><p>I don't have an credentials for the server; so I wanted to use <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Connect-using-EC2-Instance-Connect.html?icmpid=docs_ec2_console" rel="nofollow noreferrer">EC2 Instance Connect</a>.</p><br><p>When I click connect; I get an <strong>error message:</strong></p><br><blockquote><br><p>We were unable to connect to your instance. Make sure that your instances network settings are configured correctly for EC2 Instance Connect. For more information; see <a href="https://docs.aws.amazon.com/console/ec2/instances/connect/ec2-instance-connect/network-access" rel="nofollow noreferrer">Task 1: Configure network access to an instance.</a></p><br></blockquote><br><p>Following the link; I found the instructions:</p><br><blockquote><br><ul><br><li>Ensure that the security group associated with your instance allows inbound SSH traffic on port 22 from your IP address or from your network.</li><br><li>(Amazon EC2 console browser-based client) We recommend that your instance allows inbound SSH traffic from the <a href="https://ip-ranges.amazonaws.com/ip-ranges.json" rel="nofollow noreferrer">recommended IP block published for the service</a>. Use the <code>EC2_INSTANCE_CONNECT</code> filter for the service parameter to get the IP address ranges in the EC2 Instance Connect subset.</li><br></ul><br></blockquote><br><p><strong>How do I connect to the Elastic Beanstalk underlying EC2 via EC2 Instance Connect?</strong></p><br><p><em>What I've tried:</em></p><br><p>I created a new security group that contains my client IP address; but that didn't work.  Which makes sense; as it's the EC2 Instance Connect app running in the Console making the SSH connection; not my local machine.</p><br><p>Also looked at the the ip ranges json file (<a href="https://ip-ranges.amazonaws.com/ip-ranges.json" rel="nofollow noreferrer">https://ip-ranges.amazonaws.com/ip-ranges.json</a>); but not sure what to do with that.</p><br><p><a href="https://i.stack.imgur.com/jmuKU.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/jmuKU.png" alt="enter image description here" /></a></p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.6666666666666666,0.0,<h3>Best approach for migrating Maven development projects into AWS Code Pipeline</h3><p>We are trying to migrate several of our Java/Maven projects into AWS Code Pipeline and we could not find a good and reasonable migration approach (our current architecture is to use AWS for production). Specifically we are interested in several things:</p><br><ol><br><li><p>How to cache Maven dependencies so that build tasks do not download the same packages all over again.<br>There are several approaches possible; for example:<br /><br>a) Use Code Artifact; but then the Maven projects will be connected to a specific AWS subscription.<br /><br>b) Use S3 buckets; but then 3PP modules (Maven Wagons) will need to be used.<br /><br>c) Use EC2 instance for building.<br /><br>d) Use Docker container created specifically for build purposes.</p><br></li><br><li><p>It is not really clear if Jenkins or Code Pipeline is recommended as a CI/CD product in AWS. We could see some examples that Code Pipeline is used with Jenkins. What is the purpose of such a setup.</p><br></li><br></ol><br><p>Thank you;</p><br>
0.0,0.0,0.3333333333333333,0.0,0.0,1.0,0.0,<h3>Python Json Output From AWS Not Removing Single Quotes</h3><p>I am kind of new to Python and playing around with classes. I created a class to access aws ec2 instances to stop and start along with current state. Don't know how else to explain this other than showing the issue.</p><br><pre><code>#!/usr/bin/python<br>import boto3<br>import json<br>import datetime<br>from botocore.exceptions import ClientError<br><br>class Ec2(object):<br>    def __init__(self; instance_id; region):<br>        self.instances = [instance_id]<br>        self.region = 'us-gov-west-1'<br>        self.state = ''<br><br>        if region is not None:<br>            self.region = region<br>        self.ec2 = boto3.client('ec2'; region_name=region)<br>        self._setState()<br><br>    def _setState(self):<br>        def convert_timestamp(datetime_key):<br>            if isinstance(datetime_key; (datetime.date; datetime.datetime)):<br>                return datetime_key.timestamp()<br><br>        status = self.ec2.describe_instances(InstanceIds=self.instances)<br><br>        # Find the current status of the instance<br>        dump = json.dumps(status; default=convert_timestamp)<br>        body = json.loads(dump)<br>        self.state = body[&quot;Reservations&quot;][0][&quot;Instances&quot;][0][&quot;State&quot;][&quot;Name&quot;]<br><br><br>    def getState(self):<br>        self._setState()<br>        return self.state<br>    ...<br></code></pre><br><p>So if in the Python3 interpretor I run this:</p><br><pre><code>from ec2 import Ec2<br>ssm_1 = Ec2('i-12345abcdef';'us-gov-west-1')<br>ssm_1.getState()<br>Python 3.8.0 (default; Oct 28 2019; 16:14:01)<br>[GCC 8.3.0] on linux<br>Type &quot;help&quot;; &quot;copyright&quot;; &quot;credits&quot; or &quot;license&quot; for more information.<br>&gt;&gt;&gt; from ec2 import Ec2<br>&gt;&gt;&gt; ssm_1 = Ec2('i-12345abcdef';'us-gov-west-1')<br>&gt;&gt;&gt; ssm_1.getState()<br>'stopped'<br>&gt;&gt;&gt;<br></code></pre><br><p>If I change the method _setState in the class to:</p><br><pre><code>def _setState(self):<br>        def convert_timestamp(datetime_key):<br>            if isinstance(datetime_key; (datetime.date; datetime.datetime)):<br>                return datetime_key.timestamp()<br><br>        status = self.ec2.describe_instances(InstanceIds=self.instances)<br><br>        # Find the current status of the instance<br>        dump = json.dumps(status; default=convert_timestamp)<br>        body = json.loads(dump)<br>        self.state = body[&quot;Reservations&quot;][0][&quot;Instances&quot;][0][&quot;State&quot;][&quot;Name&quot;]<br>        self.state = self.state.replace(&quot;'&quot;; &quot;&quot;)<br></code></pre><br><p>then rerun above the single quotes do not get removed! It appears that these quotes are not part of the actual string; validated by changing the last line to:</p><br><pre><code>        self.state = self.state[1:-1]<br><br>&gt;&gt;&gt; ssm_1.getState()<br>'toppe'<br></code></pre><br><p>so it removed the first and last letters not the single quotes; s &amp; d</p><br><p>BUT if I change _setState to:</p><br><pre><code>    def _setState(self):<br>        def convert_timestamp(datetime_key):<br>            if isinstance(datetime_key; (datetime.date; datetime.datetime)):<br>                return datetime_key.timestamp()<br><br>        status = self.ec2.describe_instances(InstanceIds=self.instances)<br><br>        # Find the current status of the instance<br>        dump = json.dumps(status; default=convert_timestamp)<br>        body = json.loads(dump)<br>        self.state = body[&quot;Reservations&quot;][0][&quot;Instances&quot;][0][&quot;State&quot;][&quot;Name&quot;]<br></code></pre><br><p>Now run:</p><br><pre><code>&gt;&gt;&gt; from ec2 import Ec2<br>&gt;&gt;&gt; ssm_1 = Ec2('i-12345abcdef';'us-gov-west-1')<br>&gt;&gt;&gt; ssm_1.getState().replace(&quot;'&quot;; &quot;&quot;)<br>stopped<br>&gt;&gt;&gt;<br></code></pre><br><p>the single quotes are now gone!</p><br><p>So it appears in the class the string does not actually include the single quotes but outside the class it does. Why? Thanks in advance.</p><br>
0.0,0.0,0.6666666666666666,0.0,1.0,0.3333333333333333,0.0,<h3>Python boto3 (AWS EC2) list Nested JSON Data</h3><p>I use <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html" rel="nofollow noreferrer">boto3</a> to get a list of all instances like this.</p><br><pre><code>id: i-fa512784; zone: us-east-1; state: running; name: redis; env: staging-db; app: php<br>id: i-fa112784; zone: us-east-1; state: running; name: redis; env: production; app: php<br></code></pre><br><p>I would like to create a single string for all values per instance. I.e. each of instances should have own string with own keys. My goal is to put this data into Prometheus.</p><br><p>I have got stuck on parsing nested <code>&quot;Tags&quot;: [</code> to get all values and output all of them into one string</p><br><p>My code</p><br><pre><code>#!/usr/bin/python3<br><br>import boto3.utils<br>boto3.setup_default_session(profile_name='profile')<br>client = boto3.client('ec2')<br><br>response = client.describe_instances(<br>   MaxResults=10;<br>)<br><br>for r in response['Reservations']:<br>    for i in r['Instances']:<br>        for tags in i['Tags']:<br>        print ('id:';i['InstanceId']; 'zone:';i['Placement']['AvailabilityZone']; 'state:';i['State']['Name'])<br></code></pre><br><p>Thank you in advance</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.0,<h3>How to get cloud formation template for an already deployed AWS resource</h3><p>If I create a resource from AWS GUI ; lets say a S3 bucket or an AWS glue Job ; is it possible to use AWS CLI to get the CF template for such resources. Is there any way to reverse engineer the CF templates ?</p><br><p>Thanks</p><br>
0.0,0.0,0.0,1.0,0.6666666666666666,0.6666666666666666,0.0,<h3>Using Lambda to add Json to DynamoDB</h3><p>I am trying to load this big Json file (over 8k transactions) with the structure below into DynamoDB using the Lambda function.</p><br><pre><code>{<br>    &quot;transactions&quot;: [<br>        {<br>            &quot;customerId&quot;: &quot;abc&quot;;<br>            &quot;transactionId&quot;: &quot;123&quot;;<br>            &quot;transactionDate&quot;: &quot;2020-09-01&quot;;<br>            &quot;merchantId&quot;: &quot;1234&quot;;<br>            &quot;categoryId&quot;: &quot;3&quot;;<br>            &quot;amount&quot;: &quot;5&quot;;<br>            &quot;description&quot;: &quot;McDonalds&quot;<br>        };<br>        {<br>            &quot;customerId&quot;: &quot;def&quot;;<br>            &quot;transactionId&quot;: &quot;456&quot;;<br>            &quot;transactionDate&quot;: &quot;2020-09-01&quot;;<br>            &quot;merchantId&quot;: &quot;45678&quot;;<br>            &quot;categoryId&quot;: &quot;2&quot;;<br>            &quot;amount&quot;: &quot;-11.70&quot;;<br>            &quot;description&quot;: &quot;Tescos&quot;<br>        };<br>        {<br>            &quot;customerId&quot;: &quot;jkl&quot;;<br>            &quot;transactionId&quot;: &quot;gah&quot;;<br>            &quot;transactionDate&quot;: &quot;2020-09-01&quot;;<br>            &quot;merchantId&quot;: &quot;9081&quot;;<br>            &quot;categoryId&quot;: &quot;3&quot;;<br>            &quot;amount&quot;: &quot;-139.00&quot;;<br>            &quot;description&quot;: &quot;Amazon&quot;<br>        };<br>    ...<br></code></pre><br><p>The lambda function I am trying to use is going to be triggered upon uploading the Json file into the S3 bucket. That should then automatically load data into DynamoDB. The lambda function currently has the following code:</p><br><pre><code>import json<br>s3_client = boto3.client('s3')<br>dynamodb = boto3.resource('dynamodb')<br><br>def lambda_handler(event; context):<br>    bucket = event['Records'][0]['s3']['bucket']['name']<br>    json_file_name = event['Records'][0]['s3']['object']['key']<br>    print(bucket)<br>    print(json_file_name)<br>    print(str(event))<br>    json_object = s3_client.get_object(Bucket=bucket;Key=json_file_name)<br>    jsonFileReader = json_object ['Body'].read()<br>    jsonDict = json.loads(jsonFileReader)<br>    table = dynamodb.Table('CustomerEvents')<br>    table.put_item(Item=jsonDict)<br>    return 'Hello from Lambda'<br></code></pre><br><p>This works fine if I try to upload one unique transaction into DynamoDB; i.e; if the structure of the file is simply the below:</p><br><pre><code>{<br>            &quot;customerId&quot;: &quot;abc&quot;;<br>            &quot;transactionId&quot;: &quot;123&quot;;<br>            &quot;transactionDate&quot;: &quot;2020-09-01&quot;;<br>            &quot;merchantId&quot;: &quot;1234&quot;;<br>            &quot;categoryId&quot;: &quot;3&quot;;<br>            &quot;amount&quot;: &quot;5&quot;;<br>            &quot;description&quot;: &quot;McDonalds&quot;<br> }<br></code></pre><br><p>How can I go about tweaking the lambda function to load all the transactions (&gt; 8k) into DynamoDB as per above?</p><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>AWS Update Secret Json value with adding an additional column</h3><p>My question is similar to what is told <a href="https://stackoverflow.com/q/63772533/740756">here</a>. But my requirements are different. So I will explain it.</p><br><p>I use my secrets (which is a json containing user name/password etc) to authenticate users to login to sftp server. I would like to add an additional field to this existing json value based on the LastAccessedDate.</p><br><p>So the requirement is to find all secrets which have not been accessed within last 'x' days and add a status field to indicate this user is Inactive. The moment I access the secret value programmatically; the Last Accessed Date is updated with date time. So I would like to do it without updating that field. Below is what I have written in powershell script so far.</p><br><pre><code>$filterValue=&quot;users&quot;<br>$threshholdDays=10<br><br>$secrets = aws secretsmanager list-secrets --filters Key=name;Values=$filterValue | ConvertFrom-Json<br><br>foreach ($secret in $secrets.SecretList) {<br>  $secretName = $secret.Name<br><br>  $lastAccessedDate = if($secret.LastAccessedDate -eq $null) {[DateTime]::Today.AddDays(-91)} else {$secret.LastAccessedDate}<br>  $lastAccessedDate = (Get-Date -Date $lastAccessedDate).ToUniversalTime()<br>    <br>  $elapsedDays = [int]([DateTime]::UtcNow - $lastAccessedDate).TotalDays<br>  <br>  if($elapsedDays -gt $threshholdDays){<br>    Write-Host &quot;'$secretName' is last accessed on '$lastAccessedDate'. This user is being inactivated.&quot;<br>    $secretString=&quot;{\&quot;UserName\&quot;:\&quot;my-username\&quot;;\&quot;Password\&quot;:\&quot;my-password\&quot;;\&quot;UserRole\&quot;:\&quot;arn:aws:iam::###:role/acct-managed/my-role\&quot;;\&quot;UserFolder\&quot;:\&quot;/my-s3-bucket/user-folder\&quot;; **&quot;Status&quot;: &quot;inactive&quot;** &lt;#Expect this value to be added#&gt; }<br><br>    #Update the secret value and add **Status: 'inactive'** without updating the LastAccessedDate<br>    <br>    #aws secretsmanager put-secret-value --secret-id $secretName --secret-string $secretString<br>  }<br><br></code></pre><br>
0.0,0.0,0.0,0.0,0.0,0.6666666666666666,1.0,<h3>Cant connect to AWS IoT Core via MQTT using AWSIoTPythonSDK</h3><p>I have followed the AWS tutorial step by step. <a href="https://aws.amazon.com/premiumsupport/knowledge-center/iot-core-publish-mqtt-messages-python/" rel="nofollow noreferrer">https://aws.amazon.com/premiumsupport/knowledge-center/iot-core-publish-mqtt-messages-python/</a></p><br><p>I have created the open-ended policy with the *; registered a thing and attached it to the policy; generated; downloaded; and activated the certificates. I have tried to connect and publish to a subscription using both the AWS IoT SDK for Python v2 and the original sdk but neither work. The code I'm using is straight from AWS's demo example connection code but they just wont connect.</p><br><p>While using the AWS IoT SDK for Python v2 I get this error message:</p><br><pre><code>RuntimeError: 1038 (AWS_IO_FILE_VALIDATION_FAILURE): A file was read and the input did not match the expected value<br></code></pre><br><p>While using the original SDK I get this error message:</p><br><pre><code>TimeoutError: [Errno 60] Operation timed out<br></code></pre><br><p>The python code I'm using:</p><br><pre><code># Copyright Amazon.com; Inc. or its affiliates. All Rights Reserved.<br># SPDX-License-Identifier: MIT-0<br><br>import time as t<br>import json<br>import AWSIoTPythonSDK.MQTTLib as AWSIoTPyMQTT<br><br># Define ENDPOINT; CLIENT_ID; PATH_TO_CERT; PATH_TO_KEY; PATH_TO_ROOT; MESSAGE; TOPIC; and RANGE<br>ENDPOINT = &quot;XXXXX-ats.iot.ap-southeast-2.amazonaws.com&quot;<br>CLIENT_ID = &quot;testDevice&quot;<br>PATH_TO_CERT = &quot;certs/XXXX-certificate.pem.crt&quot;<br>PATH_TO_KEY = &quot;certs/XXXX-private.pem.key&quot;<br>PATH_TO_ROOT = &quot;certs/root.pem&quot;<br>MESSAGE = &quot;Hello World&quot;<br>TOPIC = &quot;test/testing&quot;<br>RANGE = 20<br><br>myAWSIoTMQTTClient = AWSIoTPyMQTT.AWSIoTMQTTClient(CLIENT_ID)<br>myAWSIoTMQTTClient.configureEndpoint(ENDPOINT; 8883)<br>myAWSIoTMQTTClient.configureCredentials(PATH_TO_ROOT; PATH_TO_KEY; PATH_TO_CERT)<br><br>myAWSIoTMQTTClient.connect()<br>print('Begin Publish')<br>for i in range (RANGE):<br>    data = &quot;{} [{}]&quot;.format(MESSAGE; i+1)<br>    message = {&quot;message&quot; : data}<br>    myAWSIoTMQTTClient.publish(TOPIC; json.dumps(message); 1) <br>    print(&quot;Published: '&quot; + json.dumps(message) + &quot;' to the topic: &quot; + &quot;'test/testing'&quot;)<br>    t.sleep(0.1)<br>print('Publish End')<br>myAWSIoTMQTTClient.disconnect()<br></code></pre><br><p>(I censored the endpoint and the certificate ID)</p><br><p>(I'm using a macbook air and on a public school network)</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>How to setup amazon timestream in php?</h3><p>I have found the documentation for it <a href="https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.TimestreamWrite.TimestreamWriteClient.html" rel="nofollow noreferrer">here</a>. I have PHP SDK installed. Now when I go through the documents there is not so much in detail about the PHP one. I have the following questions:</p><br><ol><br><li>Here how can I specify the $client</li><br></ol><br><blockquote><br><pre><code>$result = $client-&gt;createDatabase([<br>    'DatabaseName' =&gt; '&lt;string&gt;'; // REQUIRED<br>    'KmsKeyId' =&gt; '&lt;string&gt;';<br>    'Tags' =&gt; [<br>        [<br>            'Key' =&gt; '&lt;string&gt;'; // REQUIRED<br>            'Value' =&gt; '&lt;string&gt;'; // REQUIRED<br>        ];<br>        // ...<br>    ];<br>]);<br></code></pre><br></blockquote><br><ol start="2"><br><li>Is there any good documents or videos regarding the timestream in PHP from where I can get some help.</li><br></ol><br>
0.0,0.0,0.0,0.0,0.0,0.3333333333333333,1.0,<h3>Goautodial :&quot;Logging in to your phone. Please wait...&quot;</h3><p>i have installed <code>GoautodialV4</code> from scratch on AWS;  i configured and login as agent; when try to login <code>Dialer</code>; it stuck in <code>Logging in to your phone. Please wait...</code> <a href="https://i.stack.imgur.com/tqMXA.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/tqMXA.png" alt="enter image description here" /></a></p><br><p>and when i check the <code>Kamailio</code> status</p><br><blockquote><br><p>ERROR: tls [tls_util.h:42]: tls_err_ret(): TLS accept:error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown</p><br></blockquote><br><blockquote><br><p>ERROR:  [core/tcp_read.c:1352]: tcp_read_req(): ERROR: tcp_read_req: error reading - c: 0x7f450ac215b0 r: 0x7f450ac21630</p><br></blockquote><br><p><a href="https://i.stack.imgur.com/GFiXm.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/GFiXm.png" alt="enter image description here" /></a></p><br><p>Regards !</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>AWS DynamoDB Partition Key Design</h3><p>I read <a href="https://stackoverflow.com/questions/48219867/whats-the-recommended-index-schema-for-dynamo-for-a-typical-crud-application/48222907#48222907">this answer</a>; which clarified a lot of things; but I'm still confused about how I should go about designing my primary key.</p><br><p>First off I want to clarify the idea of WCUs.<br>I get that WCU is the write capacity of max 1kb per second. Does it mean that if writing a piece of data takes 0.25 seconds; I would need 4 of those to be billed 1 WCU? Or each time I write something it consumes 1 WCU; but I could also write X times within 1 second  and still be billed 1 WCU?</p><br><p><strong>Usage</strong></p><br><p>I want to create a table that stores the form data for a set of gyms (95% will be waivers; the rest will be incidents reports).<br>Most of the time; each forms will be accessed directly via its unique ID. I also want to query  the forms by date; form; userId; etc..</p><br><p>We can assume an average of 50k forms per gym</p><br><p><strong>Options</strong></p><br><ul><br><li><p>First option is straight forward: having the formId be the partition key.<br>What I don't like about this option is that scan operations will always filter out 90% of the data (i.e. the forms from other gyms); which isn't good for RCUs.</p><br></li><br><li><p>Second option is that I would make the gymId the partition key; and add a sort key for the date; formId; userId. To implement this option I would need to know more about the implications of having 50k records on one partition key.</p><br></li><br><li><p>Third option is to have one table per gyms and have the formId as partition key. This seems to be like the best option for now; but I don't really like the idea of having a a large number of tables doing the same thing in my account.</p><br></li><br></ul><br><p>Is there another option? Which one of the three is better?</p><br><p>Edit:<br>I'm assuming another option would be <a href="https://stackoverflow.com/questions/8961333/amazon-simpledb-vs-amazon-dynamodb?rq=1">SimpleDB</a>?</p><br>
0.0,0.0,0.0,0.0,0.0,0.3333333333333333,0.6666666666666666,<h3>AWS Amplify Video Livestreaming and Video on Demand -obs</h3><p>When I enter amplify video setup-obs command run successfully. But not created OBS directory. How can I resolve this matter?</p><br>
0.0,0.0,0.0,0.6666666666666666,0.3333333333333333,1.0,0.0,<h3>How to implement a Lambda trigger to fire once on a global dynamoDb table</h3><p>I have a dynamoDb table that is setup as global (2019 version) between two regions.</p><br><p>I have a lambda function assigned as a trigger on the table.<br>When a record is inserted into; say; the east version of the table then the east version of the lambda is triggered. The record is then replicated to the west version of the table and the west version of the lambda is triggered.</p><br><p>I want one lambda triggered. But I also want both triggers to be enabled in case one region goes down.</p><br><p>How can I achieve this?<br>I would rather not make my trigger logic idempotent.</p><br>
0.0,0.0,0.6666666666666666,0.0,1.0,0.0,0.0,<h3>AWS Elasticbeanstalk deployment suddenly failing</h3><p>I have a play application written in Scala that I deploy using elastic beanstalk. Up until now this has worked fine; but a few days ago new deployments started failing. The error message in <code>eb-activity.log</code> that I get is:</p><br><pre><code>[2020-11-25T20:54:29.150Z] INFO  [3127]  - [Application deployment givinga-1.8.1-20201125b@152/AddonsBefore] : Starting activity...<br>[2020-11-25T20:54:29.150Z] INFO  [3127]  - [Application deployment givinga-1.8.1-20201125b@152/AddonsBefore/ConfigCWLAgent] : Starting activity...<br>[2020-11-25T20:54:29.150Z] INFO  [3127]  - [Application deployment givinga-1.8.1-20201125b@152/AddonsBefore/ConfigCWLAgent/10-config.sh] : Starting activity...<br>[2020-11-25T20:54:58.963Z] INFO  [3127]  - [Application deployment givinga-1.8.1-20201125b@152/AddonsBefore/ConfigCWLAgent/10-config.sh] : Activity execution failed; because:  (ElasticBeanstalk::ExternalInvocationError)<br>caused by:  (Executor::NonZeroExitStatus)<br><br><br>[2020-11-25T20:54:58.964Z] INFO  [3127]  - [Application deployment givinga-1.8.1-20201125b@152/AddonsBefore/ConfigCWLAgent/10-config.sh] : Activity failed.<br>[2020-11-25T20:54:58.964Z] INFO  [3127]  - [Application deployment givinga-1.8.1-20201125b@152/AddonsBefore/ConfigCWLAgent] : Activity failed.<br></code></pre><br><p>Deploying to other test environments works; here is the relevant log line when it works:</p><br><pre><code>[2020-11-25T23:19:51.549Z] INFO  [3058]  - [Application deployment givinga-1.8.1-20201126a@482/AddonsBefore/ConfigCWLAgent] : Starting activity...<br>[2020-11-25T23:19:51.549Z] INFO  [3058]  - [Application deployment givinga-1.8.1-20201126a@482/AddonsBefore/ConfigCWLAgent/10-config.sh] : Starting activity...<br>[2020-11-25T23:19:53.910Z] INFO  [3058]  - [Application deployment givinga-1.8.1-20201126a@482/AddonsBefore/ConfigCWLAgent/10-config.sh] : Completed activity. Result:<br>  Starting awslogs: [  OK  ]<br>  Enabled log streaming.<br>[2020-11-25T23:19:53.910Z] INFO  [3058]  - [Application deployment givinga-1.8.1-20201126a@482/AddonsBefore/ConfigCWLAgent] : Completed activity. Result:<br>  Successfully execute hooks in directory /opt/elasticbeanstalk/addons/logstreaming/hooks/config.<br></code></pre><br><p>So my question is; what is the log streaming doing here? What could cause it to fail? There doesn't seem to be a way for me to delete this addon; or even to configure it.</p><br>
0.0,0.0,0.0,1.0,1.0,0.0,0.0,<h3>Storing the Cassandra data using Stateful set</h3><p>I have been reading about the ways in which I can push the database on kubernetes. Initially; I attached the data to the docker image and deployed the <code>service</code> and <code>deployment</code> files. But the issue was that when the container/pod restarts the data gets lost.<br>I; then; came across the concept of persistent volume claim. I found (<a href="https://www.magalix.com/blog/kubernetes-and-database" rel="nofollow noreferrer">https://www.magalix.com/blog/kubernetes-and-database</a>) and (<a href="https://kubernetes.io/docs/tutorials/stateful-application/cassandra/" rel="nofollow noreferrer">https://kubernetes.io/docs/tutorials/stateful-application/cassandra/</a>) very useful. I have few questions regarding them though:</p><br><p>PVC:</p><br><pre><code>apiVersion: v1<br>kind: PersistentVolumeClaim<br>metadata:<br>name: mysql-pv-claim<br>spec:<br>storageClassName: manual<br>accessModes:<br>- ReadWriteOnce<br>resources:<br>requests:<br>storage: 20Gi<br></code></pre><br><p>PV:</p><br><pre><code>apiVersion: v1<br>kind: PersistentVolume<br>metadata:<br>name: mysql-pv-volume<br>labels:<br>type: local<br>spec:<br>storageClassName: manual<br>capacity:<br>storage: 20Gi<br>accessModes:<br>- ReadWriteOnce<br>hostPath:<br>path: &quot;/mnt/data&quot;<br></code></pre><br><ol><br><li>How does the PVC get the storage from PV in the cluster? If I am running my service using Amazon cloud; what are the steps for the same; if any.</li><br></ol><br>
0.0,0.6666666666666666,0.3333333333333333,0.0,0.3333333333333333,0.0,0.0,<h3>How to use AWS ACM Private with Docker Node.js + nginx proxy?</h3><p>i'm trying to use the private CA generated on AWS ACM but i dont know what to do with Certificate.pem and CertificateChain.pem that is generated.</p><br><p>edit: See my answer below.</p><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.0,<h3>Is it possible to reference an AWS Lambda from itself?</h3><p>I apologize if this question is unclear in any way - I will do my best to add detail if it is difficult to understand. I have an AWS Lambda; from which I would like to access the tags for that same lambda. I have found the listTags method for AWS Lambda; which appears to be what I am looking for. It can be called as follows:</p><br><pre><code> var params = {<br>  Resource: &quot;arn:aws:lambda:us-west-2:123456789012:function:my-function&quot;<br> };<br> lambda.listTags(params; function(err; data) {<br>   if (err) console.log(err; err.stack); // an error occurred<br>   else     console.log(data);           // successful response<br> });<br></code></pre><br><p>However; in order to use this function; we have to create a new instance of the lambda using the lambda constructor:</p><br><pre><code>var lambda = new AWS.Lambda({apiVersion: '2015-03-31'});<br></code></pre><br><p>I don't think that this is what I want to do. Instead; I want to have access to the tags for this particular lambda whenever the lambda is run. So; if I invoke the lambda; I want that invocation to be able to look and see that the lambda; itself; has a tag with the key &quot;environment&quot; and value &quot;production;&quot; for example. I wouldn't think I would want to construct a new instance from within it... of itself.</p><br><p>Surely there has to be a way to do this? I may be missing something obvious. I've tried the code I've provided above using the context object in place of the lambda; but to no avail.</p><br>
0.0,0.0,0.0,0.0,0.0,0.0,1.0,<h3>Is it possible to match a Qualtrics variable to the data recorded by mTurk?</h3><p>I realized I forgot to record mTurk ID's directly in my Qualtrics survey I ran and I need to send some workers bonus payments.</p><br><p>Is there anyway I can match the data collected by Qualtrics (Times; IPAddress; Longitude Latitude) to the Review Results from mTurk?</p><br><p>Similar to: <a href="https://stackoverflow.com/questions/18475749/match-ids-between-mturk-and-qualtrics">match ids between mturk and qualtrics</a></p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.6666666666666666,<h3>Does sending an email through aws require only AccessKey and SecretKey?</h3><p>im using this dependency <a href="https://github.com/daniel-zahariev/php-aws-ses" rel="nofollow noreferrer">https://github.com/daniel-zahariev/php-aws-ses</a> to send email through AWS and i don't see where can I set host; username; ports and password. is there a way to set it does anyone know how to usethis php-aws-ses?</p><br>
0.0,0.6666666666666666,1.0,0.6666666666666666,0.0,0.0,0.0,<h3>opsmanager application not able to connect to opsmanager database</h3><p>I have installed opsmanager database in one instance and application in other instance<br>Taken points from <a href="https://docs.opsmanager.mongodb.com/v4.4/tutorial/install-simple-test-deployment/" rel="nofollow noreferrer">this</a> link</p><br><ol><br><li><p>In application database server when i <code>netstat -nltp</code> it shows 127.0.0.1:27017 running mongod</p><br></li><br><li><p>After installing application in other instance I edit the <code>mongo.mongoUri</code> as <code>mongodb://db_instance_publicip:27017</code> in <code>/opt/mongodb/mms/conf/conf-mms.properties</code></p><br></li><br><li><p>When I start by sudo service mongodb-mms start is shows below error</p><br><blockquote><br><p>uri=mongodb://db_instance_publicip:27017/?maxPoolSize=150} Error: Timed out after 30000 ms while waiting to connect.......</p><br></blockquote><br></li><br></ol><br><p><strong>Error: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN; servers=[{address=db_instance_publicip:27017; type=UNKNOWN; state=CONNECTING; exception={com.mongodb.MongoSocketOpenException: Exception opening socket}; caused by {java.net.ConnectException: Connection refused (Connection refused)}}]</strong></p><br>
0.0,0.0,0.0,1.0,0.6666666666666666,0.0,0.0,<h3>Do EBS volume need to be formatted before it can be attached to a container</h3><p>I'm wondering if one needs to format an EBS volume (e.g. via <code>mkfs.xfs</code>) before it can be used by a container running in kubernetes. I've examples where in people just create the volumes and have it mounted by kubernetes. Does kubernetes format the volume before using it first?</p><br>
0.0,0.5,0.0,0.5,0.0,0.5,0.0,<h3>Content Upload Issue from wordpress w3totalcache to Amazon S3 storage via Cloudfront</h3><p>I'm facing content upload issue from wordpress w3totalcache to amazon S3 via Cloudfront; test is failing in wordpress admin console. Please help me to fix this issue.</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.5,<h3>Problem with apache camel reading a file from s3</h3><p>I am implementing a route that takes a file from s3 reads it and deletes it. The problem is that after deleting it continues to search for the file and goes into error; I don't understand why the route does not stop.</p><br><p>This is my from</p><br><pre><code>var uri = &quot;aws-s3://$s3BucketName?amazonS3Client=#s3Client&amp;fileName=$fileName&amp;deleteAfterRead=true&quot;<br>from(uri)<br></code></pre><br><blockquote><br><p>2021-02-23 23:53:23.226  WARN 2452 --- [ftp-bucket/test] o.a.camel.component.aws.s3.S3Consumer    : Consumer S3Consumer[aws-s3://bucket/test?amazonS3Client=%23s3Client&amp;deleteAfterRead=true&amp;fileName=testAlex.CSV] failed polling endpoint: aws-s3://bucket/test?amazonS3Client=%23s3Client&amp;deleteAfterRead=true&amp;fileName=testAlex.CSV. Will try again at next poll. Caused by: [com.amazonaws.services.s3.model.AmazonS3Exception - The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: ; S3 Extended Request ID: /=)]<br>com.amazonaws.services.s3.model.AmazonS3Exception: The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: ; S3 Extended Request ID: /=)<br>at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1742) ~[aws-java-sdk-core-1.11.714.jar:na]<br>at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1371) ~[aws-java-sdk-core-1.11.714.jar:na]<br>at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1347) ~[aws-java-sdk-core-1.11.714.jar:na]<br>at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1127) ~[aws-java-sdk-core-1.11.714.jar:na]<br>at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:784) ~[aws-java-sdk-core-1.11.714.jar:na]<br>at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:752) ~[aws-java-sdk-core-1.11.714.jar:na]<br>at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726) ~[aws-java-sdk-core-1.11.714.jar:na]<br>at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686) ~[aws-java-sdk-core-1.11.714.jar:na]<br>at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668) ~[aws-java-sdk-core-1.11.714.jar:na]<br>at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532) ~[aws-java-sdk-core-1.11.714.jar:na]<br>at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512) ~[aws-java-sdk-core-1.11.714.jar:na]<br>at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5052) ~[aws-java-sdk-s3-1.11.714.jar:na]<br>at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4998) ~[aws-java-sdk-s3-1.11.714.jar:na]<br>at com.amazonaws.services.s3.AmazonS3Client.getObject(AmazonS3Client.java:1486) ~[aws-java-sdk-s3-1.11.714.jar:na]<br>at org.apache.camel.component.aws.s3.S3Consumer.poll(S3Consumer.java:74) ~[camel-aws-s3-3.2.0.jar:3.2.0]<br>at org.apache.camel.support.ScheduledPollConsumer.doRun(ScheduledPollConsumer.java:187) ~[camel-support-3.2.0.jar:3.2.0]<br>at org.apache.camel.support.ScheduledPollConsumer.run(ScheduledPollConsumer.java:106) ~[camel-support-3.2.0.jar:3.2.0]<br>at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[na:na]<br>at java.base/java.util.concurrent.FutureTask.runAndReset$$$capture(FutureTask.java:305) ~[na:na]<br>at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java) ~[na:na]<br>at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) ~[na:na]<br>at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[na:na]<br>at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[na:na]<br>at java.base/java.lang.Thread.run(Thread.java:834) ~[na:na]</p><br></blockquote><br>
0.0,0.0,0.6666666666666666,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>AWS CLI: &quot;cannot import name UnrewindableBodyError&quot;</h3><p>I got below error when running 'aws' on a ec2 instance. Any clues what this problem is about?</p><br><pre class="lang-none prettyprint-override"><code>&gt;&gt;aws configure<br><br>Traceback (most recent call last):<br>  File &quot;/bin/aws&quot;; line 19; in &lt;module&gt;<br>    import awscli.clidriver<br>  File &quot;/usr/lib/python2.7/site-packages/awscli/clidriver.py&quot;; line 17; in &lt;module&gt;<br>    import botocore.session<br>  File &quot;/usr/lib/python2.7/site-packages/botocore/session.py&quot;; line 27; in &lt;module&gt;<br>    import botocore.configloader<br>  File &quot;/usr/lib/python2.7/site-packages/botocore/configloader.py&quot;; line 19; in &lt;module&gt;<br>    from botocore.compat import six<br>  File &quot;/usr/lib/python2.7/site-packages/botocore/compat.py&quot;; line 26; in &lt;module&gt;<br>    from urllib3 import exceptions<br>  File &quot;/usr/lib/python2.7/site-packages/urllib3/__init__.py&quot;; line 10; in &lt;module&gt;<br>    from .connectionpool import (<br>  File &quot;/usr/lib/python2.7/site-packages/urllib3/connectionpool.py&quot;; line 31; in &lt;module&gt;<br>    from .connection import (<br>  File &quot;/usr/lib/python2.7/site-packages/urllib3/connection.py&quot;; line 45; in &lt;module&gt;<br>    from .util.ssl_ import (<br>  File &quot;/usr/lib/python2.7/site-packages/urllib3/util/__init__.py&quot;; line 4; in &lt;module&gt;<br>    from .request import make_headers<br>  File &quot;/usr/lib/python2.7/site-packages/urllib3/util/request.py&quot;; line 5; in &lt;module&gt;<br>    from ..exceptions import UnrewindableBodyError<br>ImportError: cannot import name UnrewindableBodyError<br></code></pre><br>
0.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.3333333333333333,<h3>AWS Elastic Beanstalk - What&#39;s the difference between the Java platform and the Tomcat platform?</h3><p>Our programmers are currently switching our code over to Springboot. They gave me a .war file and if I deploy it in Elastic Beanstalk and select the &quot;Tomcat&quot; platform it will come up and work. If I select the &quot;Java&quot; platform our site will not work.</p><br><p>I want to understand what is happening in the background. When I select &quot;Tomcat&quot; is Elastic Beanstalk spinning up and configuring tomcat automatically for me and installing our .war to use it?</p><br><p>What about when Java is selected? What will be the servlet container and how is it set up? Is it trying to set up Nginx when Java is selected?</p><br>
0.0,0.0,0.0,0.0,0.3333333333333333,0.6666666666666666,0.0,<h3>Http Request not being ran unsure what the issue is</h3><p>Been debugging all night; but I can't figure out why my http request is not being called</p><br><pre><code>return getWordDefinition(queryUrl).then(function(responseMsg) {<br>    //Perform some other business logic<br>};<br>  function(err) {<br>      console.log('Error Getting Definition: ' + err);<br>});     <br></code></pre><br><p>The function I'm trying to call:</p><br><pre><code>function getWordDefinition(queryUrl){<br>    <br>    const options = {<br>    method: &quot;GET&quot;<br>  };<br><br>    return new Promise(function(resolve; reject) {<br>        const request = https.request(queryUrl; options; function(response) {<br><br>        console.log(&quot;STATUS CODE: &quot; + response.statusCode);  // &lt;------Not being called<br><br>        var data;<br>        response.on(&quot;data&quot;; function(chunk) {<br>          if (!data) {<br>            data = chunk;<br>          } else {<br>            data += chunk;<br>          }<br>        });<br>    <br>        response.on(&quot;end&quot;; function() {<br>          console.log(&quot;Data: &quot; + JSON.stringify(data));<br>          resolve(&quot;Finished getting data&quot;);<br>        });<br>        <br>        request.on('error'; (e) =&gt; {<br>          reject(&quot;ERROR ON REQUEST: &quot; + e.message);<br>        });<br>        <br>        request.end();<br>        <br>      });<br>    });<br>  <br>}<br></code></pre><br><p>This code is inside my <code>AWS lambda function</code> which seems to be timing out.<br>I'm logging the status code for the http request; but it's never being called in my function.<br>What am I doing incorrectly?</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Is there a way to create a certificate without CA (multi-account registration) with cdk?</h3><p>I understood the way to create a certificate through the AWS console. But is it possible to do that by CDK (java\python)?</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>Deactivate versioning on s3 buckets</h3><p>I activated versioning on my bucket. I used :</p><br><pre><code>aws s3api put-bucket-versioning --bucket my_bucket --versioning-configuration Status=Enabled --endpoint-url https://XXXXXXXXX<br></code></pre><br><p>Now; I would like to deactivate that versioning on that bucket. So; I have 2 questions :</p><br><ul><br><li>Do I need to use : <code>aws s3api put-bucket-versioning --bucket my_bucket --versioning-configuration Status=Suspended --endpoint-url https://XXXXXXXXX</code></li><br></ul><br><p>or do I need to edit the ./lifecycle.json file at the &quot;Status&quot; line ?</p><br><pre><code>{<br>    &quot;Rules&quot;: [<br>        {<br>            &quot;ID&quot;: &quot;Delete old versions after 90 days&quot;;<br>            &quot;Status&quot;: &quot;Enabled&quot;;   -&gt; Suspended <br>            &quot;Prefix&quot;: &quot;&quot;;<br>            &quot;NoncurrentVersionExpiration&quot;: {<br>                &quot;NoncurrentDays&quot;: 90<br>              }<br>        }<br>    ]<br>}<br></code></pre><br><p>Probably; both solution work?</p><br><ul><br><li>My second question is : will the versions be deleted in the bucket? I mean; is the version with <code>&quot;IsLatest&quot;: true;</code> that will be saved?</li><br></ul><br><p>Bests</p><br>
0.0,0.0,0.0,0.6666666666666666,0.0,0.6666666666666666,0.0,<h3>How to store a large; very frequently accessed list on a server using Django?</h3><p>So I'm creating a video streaming application using Django. I'm using Cassandra as my database to hold all user content; and video content. Eventually once I'm finished; the app is going to run in the Amazon cloud.</p><br><p>Each video held in the database has about 10 (give or take) columns in it; ranging from large text files to small floats. In my app; I have a function that goes through every video entry in the database and saves each one as a dictionary (matching column names to column values as key names and values); and then creates a large list of all of the dictionaries.</p><br><p>The database is constantly changing (every time any user watches something it changes); so the function has to run every time the page loads in order to have the most up-to-date video files. This is leading to a clear issue; in that if the list becomes very large - say 100;000+ videos - then the program is having to search through and modify 100;000+ items in a list every time the page loads.</p><br><p>My question is; how do I go about having a list of videos on the server that is constantly being updated and retrieved; without having to query the database every time the page loads; or run through the list every time the page loads?</p><br>
0.0,0.0,0.6666666666666666,0.0,0.6666666666666666,0.0,0.3333333333333333,<h3>AWS: Your system is not supported by certbot-auto anymore</h3><p>Trying to renew letsencript on Amazon Linux 2 using certbot and I get the following message:</p><br><blockquote><br><p>Your system is not supported by certbot-auto anymore. Certbot cannot<br>be installed.</p><br></blockquote><br><p>I am totally lost and I do not know what to do. I cannot find any exhaustive documentation that gives a solution.</p><br>
0.0,0.0,0.0,0.0,0.0,0.3333333333333333,1.0,<h3>AWS GameLift ThrottlingException: Rate exceeded when attempting to retrieve multiple PlayerSessions via JS SDK</h3><p>I'm trying to retrieve ALL the data on all the Fleets in our GameLift; to be displayed on a React-powered CMS site.</p><br><p>Currently our page retrieves the Fleets (via <code>listFleets</code>); the Fleet Attributes (via <code>describeFleetAttributes</code>); and Game Sessions (via <code>describeGameSessions</code>).</p><br><p>Here's the code for all of that:</p><br><pre><code>requestGameLiftData = async () =&gt; {<br>    const gamelift = new AWS.GameLift();<br>    try {<br>        const { FleetIds } = await new Promise((resolve; reject) =&gt; { // Get Fleet IDs<br>            gamelift.listFleets({}; function(err; data) {<br>                if (err) { reject(&quot;Fleet id error&quot;); } <br>                else { resolve(data); }<br>            });<br>        });<br><br>        const { FleetAttributes } = await new Promise((resolve; reject) =&gt; { // Get Fleet Attributes by IDs<br>            gamelift.describeFleetAttributes(<br>                { FleetIds: FleetIds };<br>                function(err; data) {<br>                    if (err) { reject(&quot;Fleet attributes error&quot;); } <br>                    else { resolve(data); }<br>                }<br>            );<br>        });<br><br>        await new Promise((resolve; reject) =&gt; { // Save Fleet Attributes to state<br>            this.setState(<br>                { fleetList: [...FleetAttributes] };<br>                () =&gt; { resolve(); }<br>            );<br>        });<br><br>        const instancePromiseArr = [];<br>        const gameSessionPromiseArr = [];<br>        const playerSessionPromiseArr = [];<br>        const { fleetList } = this.state;<br>        for (let fleet of fleetList) {<br>            instancePromiseArr.push(this.getFleetInstances(fleet; gamelift));<br>            gameSessionPromiseArr.push(this.getFleetGameSessions(fleet; gamelift));<br>        }<br><br>        let instanceData; // Get all Instances of every Fleet<br>        try { instanceData = await Promise.all(instancePromiseArr); }<br>        catch (err) { throw new Error(&quot;Fleet instances error&quot;); }<br><br>        let gameSessionData; // Get all Game Sessions of every Fleet<br>        try { gameSessionData = await Promise.all(gameSessionPromiseArr); }<br>        catch (err) { throw new Error(&quot;Fleet game session error&quot;); }<br><br>        fleetList.forEach((fleet; index) =&gt; { // Nesting game sessions and instances inside their respective fleets<br>            fleet[&quot;Instances&quot;] = instanceData[index].Instances;<br>            fleet[&quot;GameSessions&quot;] = gameSessionData[index].GameSessions;        <br>        });<br>  <br>        await new Promise((resolve; reject) =&gt; {<br>            this.setState(<br>                { fleetList: [...fleetList] };<br>                () =&gt; { resolve(); }<br>            );<br>        });<br><br>        this.setState({isFetched: true});<br>    } catch (error) { this.setState({isFetched: true}); }<br>};<br><br>getFleetInstances = (fleet; gamelift) =&gt; {<br>    return new Promise((resolve; reject) =&gt; {<br>        gamelift.describeInstances(<br>            { FleetId: fleet.FleetId };<br>            function(err; data) {<br>                if (err) { reject(&quot;Fleet instances error&quot;); } <br>                else { resolve(data); }<br>            }<br>        );<br>    });<br>};<br><br>getFleetGameSessions = (fleet; gamelift) =&gt; {<br>    return new Promise((resolve; reject) =&gt; {<br>        gamelift.describeGameSessions(<br>            { FleetId: fleet.FleetId };<br>            function(err; data) {<br>                if (err) { reject(&quot;Fleet game sessions error&quot;); } <br>                else { resolve(data); }<br>            }<br>        );<br>    });<br>};<br></code></pre><br><p>Now I have to get the Player Sessions. To that end; I added the following:</p><br><pre><code>let playerSessionData; // Before the &quot;Nesting&quot;<br>try { playerSessionData = await Promise.all(playerSessionPromiseArr); }<br>catch (err) { throw new Error(&quot;Player session error&quot;); }<br><br>getPlayersInSession = (gameSession; gamelift) =&gt; {<br>    return new Promise((resolve; reject) =&gt; { // Function to get Player sessions; outside of requestGameLiftData <br>          gamelift.describePlayerSessions(<br>              { GameSessionId: gameSession.GameSessionId };<br>              function(err; data) {<br>                  if (err) { reject(&quot;Fleet player sessions error&quot;); } <br>                  else { resolve(data); }<br>              }<br>          );<br>    });<br>};<br></code></pre><br><p>And then modified the <code>Get all Game Sessions</code> portion to the following:</p><br><pre><code>try {<br>    gameSessionData = await Promise.all(gameSessionPromiseArr);<br>    for (const gameSessionItem of gameSessionData) {<br>        for (const data of gameSessionItem.GameSessions) {<br>            playerSessionPromiseArr.push(this.getPlayersInSession(data; gamelift; delay));<br>        }<br>    }<br>} catch (err) { throw new Error(&quot;Fleet game session error&quot;); }<br></code></pre><br><p>And nested it in:</p><br><pre><code>fleetList.forEach((fleet; index) =&gt; { // Nesting game sessions and instances inside their respective fleets<br>    fleet[&quot;Instances&quot;] = instanceData[index].Instances;    <br>    for (const gameSession of gameSessionData[index].GameSessions) {<br>        gameSession['PlayerSessions'] = [];<br>        for (const playerSessions of playerSessionData) {<br>            if (playerSessions.PlayerSessions.length &gt; 0) {<br>                for (const playerSessionItem of playerSessions.PlayerSessions) {<br>                    if (playerSessionItem.GameSessionId === gameSession.GameSessionId) {<br>                      gameSession['PlayerSessions'].push(playerSessionItem);<br>                    }<br>                }<br>            }<br>        }<br>    }    <br>    fleet[&quot;GameSessions&quot;] = gameSessionData[index].GameSessions;        <br>});<br></code></pre><br><p>This works... sometimes. Most of the times; I get a <code>ThrottlingException: Rate exceeded</code> and <code>400 Bad Request</code>. This doesn't happen in other regions with significantly less fleets; so I thought it was related to the sheer number of requests made at once (as of this writing; 8 for the fleets; 8 for GameSessions; and no less than 28 for the PlayerSessions). So I tried adding a delay:</p><br><pre><code>for (const gameSessionItem of gameSessionData) {<br>    let delay = 0;<br>    for (const data of gameSessionItem.GameSessions) {<br>        delay += 50;<br>        playerSessionPromiseArr.push(this.getPlayersInSession(data; gamelift; delay));<br>    }<br>}<br><br>getPlayersInSession = (gameSession; gamelift; delay) =&gt; {<br>    return new Promise(resolve =&gt; setTimeout(resolve; delay)).then(() =&gt; {<br>        return new Promise((resolve; reject) =&gt; {<br>            gamelift.describePlayerSessions(<br>                { GameSessionId: gameSession.GameSessionId};<br>                function(err; data) {<br>                    if (err) { reject(&quot;Fleet player sessions error&quot;); }<br>                    else { resolve(data); }<br>                }<br>            );<br>        });<br>    });<br>};<br></code></pre><br><p>Which didn't work; of course. Is there anything I'm missing? Or is there another approach to this; to get all the data in one sitting without making too many requests?</p><br>
0.0,1.0,0.0,0.0,0.0,0.3333333333333333,0.0,<h3>API Gateway: selecting backend using consistent hash on a request parameter</h3><p>Does API gateway support using consistent hash on a request parameter to select the upstream backend to route the request to?</p><br><p>Something like:<br><a href="https://www.nginx.com/resources/wiki/modules/consistent_hash/" rel="nofollow noreferrer">https://www.nginx.com/resources/wiki/modules/consistent_hash/</a></p><br><p>I want all requests matching a certain criteria; identified through the hash of request;  to be sent to same backend server. For example;  if I have 5 different backend servers and have 1 million end users send requests wit 5 different IDs in the url; then I want each backend server to process requests with just 1 ID.</p><br>
0.0,0.0,0.3333333333333333,0.0,0.0,1.0,0.0,<h3>AWS Amplify lambda function unable to access a mutation</h3><p>Following <a href="https://docs.amplify.aws/lib/graphqlapi/graphql-from-nodejs/q/platform/js" rel="nofollow noreferrer">this documentation</a> to build a Lambda function to do some admin-type stuff for an Amplify-based app.  I have Cognito log in set up for the client side; and that all works. I've added the function (transcript lost) and then modified it:</p><br><pre><code>&gt; amplify update function<br>? Select which capability you want to update: Lambda function (serverless function)<br>? Select the Lambda function you want to update categoryAdder<br>? Do you want to update the Lambda function permissions to access other resources in this<br>project? Yes<br>? Select the category api<br>Api category has a resource called todoexample<br>? Select the operations you want to permit for todoexample create; read; update; delete<br><br>You can access the following resource attributes as environment variables from your Lambda<br> function<br>        API_TODOEXAMPLE_GRAPHQLAPIENDPOINTOUTPUT<br>        API_TODOEXAMPLE_GRAPHQLAPIIDOUTPUT<br>? Do you want to invoke this function on a recurring schedule? No<br>? Do you want to configure Lambda layers for this function? No<br>? Do you want to edit the local lambda function now? No<br></code></pre><br><p>Here is the GraphQL schema:</p><br><pre><code>type Todo @model<br>@auth(rules: [{ allow: owner; operations: [create; delete; update] }])<br>{<br>    id: ID!<br>    name: String!<br>    description: String<br>    category: Category<br>}<br><br>type Category @model<br>@auth (rules: [<br>    { allow: private; operations: [read] };<br>    { allow: private; provider: iam }<br>])<br>{<br>    id: ID!<br>    category: String!<br>}<br><br>type Mutation<br>{<br>    addCategory(category: String!): Category @function(name: &quot;categoryAdder-${env}&quot;)<br>}<br></code></pre><br><p>The idea is that only the Lambda function can run the <code>addCategory</code> mutation.</p><br><p>Here's the code:</p><br><pre class="lang-js prettyprint-override"><code>/* Amplify Params - DO NOT EDIT<br>    API_TODOEXAMPLE_GRAPHQLAPIENDPOINTOUTPUT<br>    API_TODOEXAMPLE_GRAPHQLAPIIDOUTPUT<br>    ENV<br>    REGION<br>Amplify Params - DO NOT EDIT */<br><br>const https = require('https')<br>const urlParse = require('url').URL<br>const appsyncUrl = process.env.API_TODOEXAMPLE_GRAPHQLAPIENDPOINTOUTPUT<br>const region = process.env.REGION<br>const endpoint = new urlParse(appsyncUrl).hostname.toString()<br>const AWS = require('aws-sdk')<br>require('es6-promise').polyfill()<br>require('isomorphic-fetch')<br><br>exports.handler = async (event) =&gt; {<br>  const mutation = `<br>  mutation addCategory($category: String!) {<br>    addCategory(category: $category) {<br>      id<br>      category<br>    }<br>  }<br>`<br><br>  AWS.config.update({<br>    region;<br>    credentials: new AWS.Credentials(<br>      process.env.AWS_ACCESS_KEY_ID;<br>      process.env.AWS_SECRET_ACCESS_KEY;<br>      process.env.AWS_SESSION_TOKEN<br>    );<br>  })<br>  const credentials = AWS.config.credentials<br><br>  const req = new AWS.HttpRequest(appsyncUrl; region)<br>  req.method = 'POST'<br>  req.path = '/graphql'<br>  req.headers.host = endpoint<br>  req.headers['Content-Type'] = 'application/json'<br>  req.body = JSON.stringify({<br>    query: mutation;<br>    operationName: 'addCategory';<br>    variables: { category: 'Cats' };<br>  })<br><br>  const signer = new AWS.Signers.V4(req; 'appsync'; true)<br>  signer.addAuthorization(credentials; AWS.util.date.getDate())<br><br>  console.log(`About to do promise...`)<br>  const data = await new Promise((resolve; reject) =&gt; {<br>    console.log(`About to do https.request...`)<br>    const httpRequest = https.request({ ...req; host: endpoint }; (result) =&gt; {<br>      console.log(`In https.request callback`)<br>      result.on('data'; (data) =&gt; {<br>        console.log(`In result.on callback; data is`; data.toString())<br>        resolve(JSON.parse(data.toString()))<br>      })<br>    })<br><br>    console.log(`About to do httpRequest.write`)<br>    httpRequest.write(req.body)<br>    httpRequest.end()<br>  })<br>  console.log(`Returning data`; data)<br><br>  return {<br>    statusCode: 200;<br>    body: data;<br>  }<br>}<br></code></pre><br><p>The error I get is &quot;Not Authorized to access addCategory on type Mutation&quot;.</p><br><p>Any clues appreciated!</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>[AWS][Cognito][Advanced Security]How to recieve Adaptive authentication notification messages?</h3><p>How to recieve [Adaptive authentication notification messages]??</p><br><p>I have already set up the ARN[Notification message customization] and I can receive the verification code with the email address using the domain.</p><br><p>Thank you for your cooperation.</p><br>
0.0,1.0,0.0,0.6666666666666666,0.0,0.0,0.0,<h3>Serve SPA multi-tenants through AWS CloudFront from a single S3 bucket</h3><p>Given the following considerations:</p><br><ul><br><li>Single S3 bucket containing static Frontend SPA files.</li><br><li>Frontend is being served through CloudFront; where each tenant has their own CloudFront distribution (tenantA.domain.com; ... tenantZ.domain.com).</li><br><li>Each tenant has their own configurations (which can be fetched from a Configuration service resolving the domain).</li><br><li>Each CloudFront needs to inject such configurations in the Frontend at run time.</li><br></ul><br><p><a href="https://i.stack.imgur.com/Ao0w8.jpg" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/Ao0w8.jpg" alt="Consider the diagram below." /></a></p><br><hr /><br><p>I am thinking of a Lambda function; that queries the &quot;Configuration service&quot; (possibly caching the response); and then setting globally scoped variables (e.g. <code>window.config1</code>) for the SPA to use. Is such a scenario possible through CloudFront? Is there a more common/standardized way?</p><br>
0.3333333333333333,0.0,0.0,0.6666666666666666,0.0,1.0,0.0,<h3>Is there an way to get file size on basis of number of lines in java?</h3><p>Basically using java and aws SDK multipart upload for file; And we require those parted files to stored as new key(different objects) i.e instead of merging these part files internally by aws ; I want them to be stored as different objects in the same bucket.</p><br><p>Now the issue at hand is if we store the part files as objects in aws these partition have data corruption as the part_1 will be completing whenever the part size is done irrespective of data till then</p><br><p>Example:<br>Main_file ==<br>r1_Data_1|r1_Data_2|r1_Data_3|r1_Data_4|r1_Data_5|r1_Data_6|<br>r2_Data_1|r2_Data_2|r2_Data_3|r2_Data_4|r2_Data_5|r2_Data_6|<br>r3_Data_1|r3_Data_2|r3_Data_3|r3_Data_4|r3_Data_5|r3_Data_6|<br>r4_Data_1|r4_Data_2|r4_Data_3|r4_Data_4|r4_Data_5|r4_Data_6|</p><br><p>Expected: --<br>part_1 ::<br>r1_Data_1|r1_Data_2|r1_Data_3|r1_Data_4|r1_Data_5|r1_Data_6|<br>r2_Data_1|r2_Data_2|r2_Data_3|r2_Data_4|r2_Data_5|r2_Data_6|</p><br><p>part_2 ::<br>r3_Data_1|r3_Data_2|r3_Data_3|r3_Data_4|r3_Data_5|r3_Data_6|<br>r4_Data_1|r4_Data_2|r4_Data_3|r4_Data_4|r4_Data_5|r4_Data_6|</p><br><p>Actual : --<br>part1::<br>r1_Data_1|r1_Data_2|r1_Data_3|r1_Data_4|r1_Data_5|r1_Data_6|<br>r2_Data_1|r2_Data_2|r2_Data_3|r2_Data_4|r2_Data_5|r2_Data_6|<br>r3_Data_1|r3_Data_2|</p><br><p>part2::<br>r3_Data_3|r3_Data_4|r3_Data_5|r3_Data_6|<br>r4_Data_1|r4_Data_2|r4_Data_3|r4_Data_4|r4_Data_5|r4_Data_6|</p><br><p>Possible Solution :<br>1.&gt; Reading line by line until line after which the next line would exceed the batch size. and setting batch size the same as size that would be covering the last accepted line.</p><br>
0.0,0.0,0.6666666666666666,0.0,1.0,0.0,0.0,<h3>Pod execution role is not found in auth config or does not have all required permissions. How can I debug?</h3><h3>Objective</h3><br><p>I want o be able to deploy AWS EKS using Fargate. I have successfully made the deployment work with a <code>node_group</code>. However; when I shifted to using Fargate; it seems that the pods are all stuck in the pending state.</p><br><h3>How my current code looks like</h3><br><p>I am provisioning using Terraform (not necessarily looking for a Terraform answer). This is how I create my EKS Cluster:</p><br><pre class="lang-sh prettyprint-override"><code>module &quot;eks_cluster&quot; {<br>  source                            = &quot;terraform-aws-modules/eks/aws&quot;<br>  version                           = &quot;13.2.1&quot;<br>  cluster_name                      = &quot;${var.project_name}-${var.env_name}&quot;<br>  cluster_version                   = var.cluster_version<br>  vpc_id                            = var.vpc_id<br>  cluster_enabled_log_types         = [&quot;api&quot;; &quot;audit&quot;; &quot;authenticator&quot;; &quot;controllerManager&quot;; &quot;scheduler&quot;]<br>  enable_irsa                       = true<br>  subnets                           = concat(var.private_subnet_ids; var.public_subnet_ids)<br>  create_fargate_pod_execution_role = true<br>  write_kubeconfig                  = false<br>  fargate_pod_execution_role_name   = &quot;${var.project_name}-role&quot;<br>  # Assigning worker groups<br>  node_groups = {<br>    my_nodes = {<br>      desired_capacity = 1<br>      max_capacity     = 1<br>      min_capacity     = 1<br>      instance_type    = var.nodes_instance_type<br>      subnets          = var.private_subnet_ids<br>    }<br>  }<br>}<br></code></pre><br><p>And this is how I provision the Fargate profile:</p><br><pre class="lang-sh prettyprint-override"><code>//#  Create EKS Fargate profile<br>resource &quot;aws_eks_fargate_profile&quot; &quot;fargate_profile&quot; {<br>  cluster_name           = module.eks_cluster.cluster_id<br>  fargate_profile_name   = &quot;${var.project_name}-fargate-profile-${var.env_name}&quot;<br>  pod_execution_role_arn = aws_iam_role.fargate_iam_role.arn<br>  subnet_ids             = var.private_subnet_ids<br><br>  selector {<br>    namespace = var.project_name<br>  }<br>}<br></code></pre><br><p>And this is how I created and attach the required policies:</p><br><pre class="lang-sh prettyprint-override"><code>//# Create IAM Role for Fargate Profile<br>resource &quot;aws_iam_role&quot; &quot;fargate_iam_role&quot; {<br>  name                  = &quot;${var.project_name}-fargate-role-${var.env_name}&quot;<br>  force_detach_policies = true<br>  assume_role_policy    = jsonencode({<br>    Statement = [{<br>      Action    = &quot;sts:AssumeRole&quot;<br>      Effect    = &quot;Allow&quot;<br>      Principal = {<br>        Service = &quot;eks-fargate-pods.amazonaws.com&quot;<br>      }<br>    }]<br>    Version   = &quot;2012-10-17&quot;<br>  })<br>}<br><br># Attach IAM Policy for Fargate<br>resource &quot;aws_iam_role_policy_attachment&quot; &quot;fargate_pod_execution&quot; {<br>  role       = aws_iam_role.fargate_iam_role.name<br>  policy_arn = &quot;arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy&quot;<br>}<br></code></pre><br><h3>What I have tried but seems not to work</h3><br><p>Running <code>kubectl describe pod</code> I get:</p><br><pre><code>Events:<br>  Type     Reason            Age   From               Message<br>  ----     ------            ----  ----               -------<br>  Warning  FailedScheduling  14s   fargate-scheduler  Misconfigured Fargate Profile: fargate profile fargate-airflow-fargate-profile-dev blocked for new launches due to: Pod execution role is not found in auth config or does not have all required permissions for launching fargate pods.<br></code></pre><br><h3>Other things I have tried but without success</h3><br><p>I have tried mapping the role via the module's feature like:</p><br><pre class="lang-sh prettyprint-override"><code>module &quot;eks_cluster&quot; {<br>  source                            = &quot;terraform-aws-modules/eks/aws&quot;<br>  version                           = &quot;13.2.1&quot;<br>  cluster_name                      = &quot;${var.project_name}-${var.env_name}&quot;<br>  cluster_version                   = var.cluster_version<br>  vpc_id                            = var.vpc_id<br>  cluster_enabled_log_types         = [&quot;api&quot;; &quot;audit&quot;; &quot;authenticator&quot;; &quot;controllerManager&quot;; &quot;scheduler&quot;]<br>  enable_irsa                       = true<br>  subnets                           = concat(var.private_subnet_ids; var.public_subnet_ids)<br>  create_fargate_pod_execution_role = true<br>  write_kubeconfig                  = false<br>  fargate_pod_execution_role_name   = &quot;${var.project_name}-role&quot;<br>  # Assigning worker groups<br>  node_groups = {<br>    my_nodes = {<br>      desired_capacity = 1<br>      max_capacity     = 1<br>      min_capacity     = 1<br>      instance_type    = var.nodes_instance_type<br>      subnets          = var.private_subnet_ids<br>    }<br>  }<br># Trying to map role<br>  map_roles = [<br>    {<br>      rolearn  = aws_eks_fargate_profile.airflow.arn<br>      username = aws_eks_fargate_profile.airflow.fargate_profile_name<br>      groups   = [&quot;system:*&quot;]<br>    }<br>  ]<br>}<br></code></pre><br><p>But my attempt was not successful. How can I debug this issue? And what is the cause behind it?</p><br>
0.0,0.0,0.3333333333333333,1.0,1.0,0.0,0.0,<h3>Managing volume rollbacks in K8s using persistent volumes</h3><p>I have a kubernetes deployment managed by a helm chart that I am planning an upgrade of. The app has 2 persistent volumes attached which are are EBS volumes in AWS. If the deployment goes wrong and needs rolling back I might also need to roll back the EBS volumes. How would one manage that in K8s? I can easily create the volume manually in AWS from my snapshot I've taken pre deployment but for the deployment to use it would I need to edit the pv yaml file to point to my new volume ID? Or would I need to create a new PV using the volume ID and a new PVC and then edit my deployment to use that claim name?</p><br>
0.0,1.0,0.3333333333333333,0.0,0.6666666666666666,0.0,0.0,<h3>Create nlb-ip loadbalancers in kubernetes created in AWS through Kops</h3><p>I have a Kubernetes cluster created through the Kops tool. And I have a requirement to expose my service using a network load balancer. And the target groups should be based on IP based. I have found the answer using the annotation mentioned in the site <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.1/guide/service/nlb_ip_mode/" rel="nofollow noreferrer">https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.1/guide/service/nlb_ip_mode/</a>.</p><br><p>This seems to work only when we have the cluster created through EKS. Since I'm using a kops tool could you please help me in installing <em>alb load balancer controller</em> which is one of the requirements to create a nlb-IP loadbalancers?</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>Why is queue visibility timeout is recommended to be six times function timeout plus batch window?</h3><p>From <a href="https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html</a></p><br><blockquote><br><p>Set your queue visibility timeout to 6 times your function timeout; plus the value of <code>MaximumBatchingWindowInSeconds</code></p><br></blockquote><br><p>Why can't the queue visibility timeout be equal to the function timeout? Let's say the function has a timeout of 30 seconds; so is the queue visibility. The function picks up the message and 30s passed; the message has not been deleted; now it's visible for other functions/consumers. Then why does it have to be 6 times? And what role does maxium batching window plays in queue visiblity timeout?</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.6666666666666666,0.3333333333333333,<h3>SNS published messages not reaching SQS</h3><p>I've a encrypted SQS queue and SNS topic by custom managed KMS key. Currently I'm using a similar kind of SQS policy stated in the below link where it is working fine <a href="https://stackoverflow.com/a/38755387/9896523">SQS Policy</a></p><br><p>But if i use the below SQS policy it's not working. I don't want to have Principal as '*' due to security reasons. Can someone explain me why is this happening</p><br><pre><code>    {<br>  &quot;Version&quot;:&quot;2012-10-17&quot;;<br>  &quot;Statement&quot;:[<br>    {<br>      &quot;Sid&quot;:&quot;MySQSPolicy001&quot;;<br>      &quot;Effect&quot;:&quot;Allow&quot;;<br>      &quot;Principal&quot;:{<br>    &quot;AWS&quot;: &quot;arn:aws:iam::123456789012:root&quot;<br>  };<br>      &quot;Action&quot;:&quot;sqs:SendMessage&quot;;<br>      &quot;Resource&quot;:&quot;arn:aws:sqs:us-east-1:123456789012:MyQueue&quot;<br>    }<br>  ]<br>}<br></code></pre><br>
0.0,0.0,0.0,0.0,1.0,0.0,0.0,<h3>aws bottlerocketOS container runtime</h3><p>I'm trying to test docker container runtime on a aws bottlerocketOS instance. The instance is created on ec2. Instance userdata has docker service enabled as mentioned <a href="https://github.com/bottlerocket-os/bottlerocket/blob/develop/sources/models/src/aws-dev/override-defaults.toml" rel="nofollow noreferrer">here</a>. After enabling host-container.admin and ssh into the server; docker seem to not exist. Also as per docs there is support for <a href="https://github.com/bottlerocket-os/bottlerocket/blob/develop/packages/docker-engine/docker.service" rel="nofollow noreferrer">docker service</a>. I guess i'm missing something; really appreciate your time in here.</p><br>
0.0,0.0,0.3333333333333333,0.0,0.0,1.0,0.0,<h3>VS 2019 AWS Serverless.Template Parse Error</h3><p>Afternoon;<br>Trying to use aws toolkit and servless.template to create a lambda that triggers on a schedule and inserts data into an sqs bucket. All is good except for the &quot;Events&quot; line. I am getting an error that says &quot;Parse error on line 24&quot;. I have checked; double checked; and verfied the squiggle bracket placement but no go. According to aws documentation; I should be able to establish the events within the lambda definition.</p><br><p>Any help is much appreciated! Here is my code. Parameterized as the app will be deployed via azure across multiple environments.</p><br><pre><code>{<br>&quot;AWSTemplateFormatVersion&quot;: &quot;2010-09-09&quot;;<br>  &quot;Transform&quot;: &quot;AWS::Serverless-2016-10-31&quot;;<br>  &quot;Description&quot;: &quot;An AWS Serverless Application.&quot;;<br>  &quot;Parameters&quot;:{<br>    &quot;AwsRole&quot; : { &quot;Description&quot; : &quot;Role for lambda expression&quot;; &quot;Type&quot;: &quot;String&quot;};<br>    &quot;SecurityGroup&quot; : { &quot;Description&quot; : &quot;List of secuirty group ids; comma delimited&quot;; &quot;Type&quot; : &quot;List&lt;String&gt;&quot; };<br>    &quot;SubNets&quot; : { &quot;Description&quot; : &quot;List of subnet ids; comma delimited&quot;; &quot;Type&quot; : &quot;List&lt;String&gt;&quot; };<br>    &quot;Schedule&quot; : {&quot;Description&quot; : &quot;Cron schedule&quot;; &quot;Type&quot; : &quot;String&quot;}<br>  };<br>  &quot;Resources&quot;: {<br>    &quot;PopulateQueue&quot;: {<br>      &quot;Type&quot;: &quot;AWS::Serverless::Function&quot;;<br>      &quot;Properties&quot;: {<br>        &quot;Handler&quot;: &quot;SendToQueue::SendToQueue.PopulateSqs::RunPopulate&quot;;<br>        &quot;Runtime&quot;: &quot;dotnetcore3.1&quot;;<br>        &quot;CodeUri&quot;: &quot;&quot;;<br>        &quot;MemorySize&quot;: 256;<br>        &quot;Role&quot; : {&quot;Ref&quot; : &quot;AwsRole&quot;};<br>        &quot;VpcConfig&quot; : {           <br>            &quot;SecurityGroupIds&quot; : { &quot;Ref&quot; : &quot;SecurityGroup&quot; } ;<br>            &quot;SubnetIds&quot; : { &quot;Ref&quot; : &quot;SubNets&quot;}<br>         }   <br>error here---&gt;&quot;Events&quot;: {<br>          &quot;Trigger&quot;:{<br>            &quot;Type&quot;: &quot;Schedule&quot;<br>            &quot;Properties&quot; : {<br>                &quot;Schedule&quot; : { &quot;Ref&quot; : &quot;Schedule&quot; };<br>                &quot;Name&quot; : &quot;PopulateSqsSchedule&quot;;<br>                &quot;Description&quot; : &quot;Schedule for sending items to sqs&quot;;<br>                &quot;Enabled&quot; : false<br>            }<br>          }<br>            <br>        }<br>      }<br>    };<br>    &quot;SendQueue&quot;: {<br>        &quot;Type&quot;: &quot;AWS::SQS::Queue&quot;;<br>        &quot;Properties&quot;: {<br>            &quot;ContentBasedDeduplication&quot;: true;<br>            &quot;MessageRetentionPeriod&quot;: 1200;<br>            &quot;QueueName&quot;: &quot;SendQueue&quot;<br>        }<br>    }<br><br>    }<br>  }<br>}<br></code></pre><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>Amazon S3 aws: Deleting folders older than 5 days</h3><p>I need to delete folders older than 5 days that are in a folder in a bucket. I tried with the aws console by setting a rule that deletes the old folders of the &quot;xyz&quot; folder: I used the prefix &quot;xyz /&quot;. Does not work! Can someone help me?</p><br><p>Thankyou!<br>Antonio</p><br>
0.0,1.0,0.0,0.0,0.0,0.0,0.0,<h3>fetch call to API with absolute URLs</h3><p>I was recently developing a website where the set up was: a static React bundle handling FE logic (served from Cloudfront); and a Docker container running a Flask application listening on Elastic Beanstalk.</p><br><p>In the React code; i was making a <code>fetch()</code> call with an absolute path to the API backend (see; <code>fetch(https://mywebsitehere.com/api/cool_endpoint</code>.</p><br><p>For some reason; in production and hitting the AWS server (through some redirect behaviors between Cloudfront and EB); I kept getting <code>ERR_CONNECTION_REFUSED</code>! I had no idea why. What was weird was that when I hit <code>https://mywebsitehere.com/api/cool_endpoint</code> directly through the browser; my API worked just fine.</p><br><p>Eventually I figured out that the solution was to add <code>www</code> to the absolute URL. Everything worked; but I still have no idea why.</p><br><p>It works; but I would like to understand what's going on. Does chrome automatically fill in that <code>www</code>? Did the lack of the <code>www</code> in the original cause the fetch machinery to fail to resolve; and thus cause an <code>ERR_CONNECTION_REFUSED</code>?</p><br><p>Let me know what y'all think!</p><br>
0.0,0.0,0.0,0.0,0.6666666666666666,1.0,0.0,<h3>Make multiple instances of a lambda function write to the same document</h3><p>I am not entirely sure if this is even possible so I was hoping someone could steer me in the correct direction. I have an AWS project that is triggered when files are placed in my AWS bucket. The project processes the file and creates an output document.</p><br><p>While the project is processing the file; I send certain email notifications such as &quot;File ____ has been processed successfully&quot; or &quot;File ____ has failed&quot;. Instead of getting all of these individual emails; is it possible to just write all of this to some document and after all files are done processing; email the document to myself?</p><br><p>I am not sure that this is possible though because how will each instance of the lambda function have access to the same file?</p><br><p>Any ideas are greatly appreciated!</p><br>
0.6666666666666666,0.0,0.3333333333333333,0.0,0.3333333333333333,0.0,0.0,<h3>Connect with the cluster</h3><p>I'm trying to create an EMR X5.xlarge and I get all the time the same error.<br>I tried:</p><br><ul><br><li>changing the key;</li><br><li>changing my amazon account .</li><br></ul><br><p>The same problem persist all the time when I try to create my EMR cluster. I succeed to create an EC2; but when I try the EMR I get this error. Any help?</p><br><p><a href="https://i.stack.imgur.com/kJ7up.jpg" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/kJ7up.jpg" alt="error" /></a></p><br>
1.0,0.0,0.3333333333333333,0.0,0.3333333333333333,0.0,0.0,<h3>Understanding EMR autoscaling</h3><p>I have the following code; which is working fine:</p><br><pre><code>def emr_client():<br>    config = get_aws_config()<br>    return boto3.client(<br>        'emr';<br>        region_name=config['aws_region'];<br>        aws_access_key_id=config['aws_access_key_id'];<br>        aws_secret_access_key=config['aws_secret_access_key']<br>    )<br><br><br>response = emr_client().run_job_flow(<br>        Name=cluster_name;<br>        ReleaseLabel=&quot;**********&quot;;<br>        Instances={<br>            'InstanceGroups': build_instance_groups_config(num_core_nodes; num_task_nodes; spots;<br>                                                           instance_type='r4.4xlarge');<br>            'Ec2KeyName': '**********';<br>            'KeepJobFlowAliveWhenNoSteps': True;<br>            'TerminationProtected': False;<br>            'EmrManagedMasterSecurityGroup': '**********';<br>            'EmrManagedSlaveSecurityGroup': '**********';<br>            'Ec2SubnetId': '**********';<br>        };<br>        VisibleToAllUsers=True;<br>        JobFlowRole='EMR_EC2_DefaultRole';<br>        ServiceRole='EMR_DefaultRole';<br>        Applications=[<br>            {'Name': 'Hadoop'};<br>            {'Name': 'Spark'};<br>            {'Name': 'Ganglia'};<br>            {'Name': 'Zeppelin'}<br>        ];<br>        Tags=tags;<br>        Configurations=[<br>            {<br>                &quot;Classification&quot;: &quot;yarn-site&quot;;<br>                &quot;Properties&quot;: {&quot;yarn.nodemanager.vmem-check-enabled&quot;: &quot;false&quot;}<br>            }<br>        ]<br>    )<br></code></pre><br><p>Now; I want to change it a little bit; in order to use auto scaling. Here is the relevant documentation: <a href="https://docs.aws.amazon.com/emr/latest/APIReference/API_RunJobFlow.html" rel="nofollow noreferrer">RunJobFlow</a></p><br><p>I see there something that's called  <code>&quot;AutoScalingRole&quot;: &quot;string&quot;</code>; as well as  <code>&quot;AutoScalingPolicy&quot;</code> which is quite a large object; but I don't see any example showing how to populate it; and haven't found such on google either.</p><br><ol><br><li><p>Could anybody provide me with an example?</p><br></li><br><li><p>Also; after I make the changes; how can I check that the autoscaling is really working?</p><br></li><br></ol><br>
0.0,1.0,0.0,0.0,0.0,0.0,0.0,<h3>configure base domain url (site.com without www for example) to api gateway; and make it work from within the browser</h3><p>I'm trying to create the shortest URL for unsubscribing messages. It should look like -</p><br><p>site.com/f3v4g1</p><br><p>This should result in calling an AWS Lambda function through AWS API Gateway.<br>For that to happen; I need to set an A record in the DNS (in my case GoDaddy) and point to the API Gateway.<br>The API Gateway has a custom domain name configured pointing to xxxnxxx.cloudfront.net.</p><br><p>Problem is that the A record requires a static IP.</p><br><p>Another try I did; with GoDaddy Forwarding option; but that will also not work; as it forwards the request to xxxnxxx.cloudfront.net; and it fails with a forbidden error; probably because the host is incorrect (it is expected to be executed from site.com).</p><br><p>Any idea how it can be done? Any simple AWS solution for that - that doesn't require a server with a static IP ;).</p><br><p>Thanks</p><br>
0.5,0.0,0.5,0.0,0.0,0.5,0.5,<h3>Error creating IAM Role - Value of property PolicyDocument must be an object</h3><p>I am creating an IAM role in order to send logs from a stack to kinesis stream in another stack.<br>When I add permission policy; it fails with the error :</p><br><blockquote><br><p>&quot;Value of property PolicyDocument must be an object&quot;.</p><br></blockquote><br><p>This is my cloudformation.template.yml :</p><br><pre><code>  KinesisRole:<br>    Type: AWS::IAM::Role<br>    Properties:<br>      RoleName: {'Fn::Sub': 'Kinesis-Role-${AWS::Region}'}<br>      AssumeRolePolicyDocument:<br>        Statement:<br>        - Effect: Allow<br>          Principal:<br>            Service: [logs.amazonaws.com]<br>          Action: ['sts:AssumeRole']<br>      Policies:<br>      - PolicyName: KinesisPolicy<br>        PolicyDocument:<br>        - Version: '2017-10-17'<br>          Statement:<br>          - Action: ['kinesis:PutRecord']<br>            Effect: Allow<br>            Resource: '*'<br></code></pre><br>
0.0,0.0,0.3333333333333333,0.0,0.3333333333333333,0.3333333333333333,1.0,<h3>Is there a way of using Selenium in python in a website which is hosted on Amazon AWS AppStream 2.0?</h3><p>I used to run a script to capture some data from a website but recently the website migrated to Amazon AWS AppStream 2.0; and I'm not able to click on anything using my selenium tool.</p><br><p>I've heard about SikuliX to automate but is there any other solution using python?</p><br><p>brgds</p><br>
1.0,0.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>Which one is more performant in redshift - Truncate followed with Insert Into or Drop and Create Table As?</h3><p>I have been working on AWS Redshift and kind of curious about which of the data loading (full reload) method is more performant.</p><br><p>Approach 1 (Using Truncate):</p><br><ol><br><li>Truncate the existing table</li><br><li>Load the data using Insert Into Select statement</li><br></ol><br><p>Approach 2 (Using Drop and Create):</p><br><ol><br><li>Drop the existing table</li><br><li>Load the data using Create Table As Select statement</li><br></ol><br><p>We have been using both in our ETL; but I am interested in understanding what's happening behind the scene on AWS side.</p><br><p>In my opinion - Drop and Create Table As statement should be more performant as it reduces the overhead of scanning/handling associated data blocks for table needed in Insert Into statement.<br>Moreover; truncate in AWS Redshift does not reseed identity columns - <a href="https://stackoverflow.com/questions/64530857/redshift-truncate-table-and-reset-identity">Redshift Truncate table and reset Identity?</a></p><br><p>Please share your thoughts.</p><br>
0.0,1.0,0.0,0.0,0.3333333333333333,0.0,0.0,<h3>AWS Application Load Balancer sends request to unhealthy target groups / instances</h3><p>In my AWS VPC I have an application load balancer and two target groups. Each target group has only 1 instances in different az with health check setup.</p><br><p>Now when one of my instance in a target group goes down AWS correctly marks its as unhealthy. But my application load balancer still keeps sending requests to it; causing my application to work intermittently.</p><br><p><a href="https://i.stack.imgur.com/OkolX.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/OkolX.png" alt="enter image description here" /></a></p><br><p><em>But in the documentation it's clearly mentioned that unhealthy instances are ignored by the load balancer.</em></p><br><p>I checked all the LB and TG configuration none suggest that they shouldn't be ignored.</p><br><blockquote><br><p>It monitors the health of its registered targets; and routes traffic<br>only to the healthy targets.</p><br></blockquote><br><p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a></p><br>
0.0,0.0,0.3333333333333333,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>Running AWS services in bash script creates an error when trying to run the second command; am I doing something wrong?</h3><p>So I have a bash script that runs some AWS commands to spin up an ECS service; I then want to attach some autoscaling policies to it but I am getting an error. I'm not sure if it is anything to do with the AWS service CLI or I am running it incorrectly in bash; syntax wise it seems fine according to the AWS docs.</p><br><p>Here's the code:</p><br><pre><code>aws ecs create-service \<br>      --cluster $1 \<br>      --service-name $2 \<br>      --task-definition $3:$4 \<br>      --desired-count $DESIRED_COUNT \<br>      --launch-type FARGATE \<br>      --platform-version LATEST \<br>      --network-configuration &quot;awsvpcConfiguration={subnets=[${SUBNETS[0]}${SUBNETS[1]}${SUBNETS[2]}];securityGroups=[$SECURITY_GROUPS];assignPublicIp=ENABLED}&quot; \<br>      --service-registries registryArn=$SERVICE_DISCOVERY_ARN \<br>      --load-balancers targetGroupArn=$BACKEND_TARGETGROUP;containerName=$BACKEND_CONTAINERNAME;containerPort=$BACKEND_CONTAINERPORT \<br>      --profile $PROFILE<br><br>    echo &quot;============ Now creating $2 service auto-scaling policies ============&quot;<br>    aws application-autoscaling register-scalable-target \<br>      --service-namespace ecs \<br>      --scalable-dimension ecs:service:DesiredCount \<br>      --resource-id &quot;service/$CLUSTER/$BACKEND_SERVICE&quot; \<br>      --min-capacity $MINIMUM_APP_COUNT --max-capacity $MAXIMUM_APP_COUNT<br>    <br>    aws application-autoscaling put-scaling-policy <br>      --service-namespace ecs \<br>      --scalable-dimension ecs:service:DesiredCount \<br>      --resource-id &quot;service/$CLUSTER/$BACKEND_SERVICE&quot; \<br>      --policy-name &quot;${CLUSTER}_scale_up&quot; --policy-type StepScaling \<br>      --step-scaling-policy-configuration file://stepScalingUpPolicy.json<br>    <br>    aws application-autoscaling put-scaling-policy <br>      --service-namespace ecs \<br>      --scalable-dimension ecs:service:DesiredCount \<br>      --resource-id &quot;service/$CLUSTER/$BACKEND_SERVICE&quot; \<br>      --policy-name &quot;${CLUSTER}_scale_down&quot; --policy-type StepScaling \<br>      --step-scaling-policy-configuration file://stepScalingDownPolicy.json<br></code></pre><br><p>Image is of error on my pipeline after running the first aws service.<br>[1]: <a href="https://i.stack.imgur.com/5PxBW.png" rel="nofollow noreferrer">https://i.stack.imgur.com/5PxBW.png</a></p><br>
0.0,0.0,0.0,0.0,1.0,0.6666666666666666,0.0,<h3>lambda quits before completion and does not await</h3><p>I'm trying to create a lambda function which resized images with <code>sharp</code>; however it quits before completing the task. I've used <code>await</code> be seems it doesn't wait!</p><br><p>This is my code:</p><br><pre><code><br>    const fileName = decodeURIComponent(event.Records[0].s3.object.key).replace(/\+/g; ' ')<br>    const bucket = event.Records[0].s3.bucket.name<br>    const keyName = fileName.split(&quot;.&quot;).slice(0; -1).join(&quot;.&quot;)<br>    const imageName = keyName.split(&quot;/&quot;).slice(-1).join()<br>    const imageFormat = fileName.split(&quot;.&quot;).slice(-1).pop()<br><br>    console.log(&quot;Getting object from S3&quot;)<br>    const imageData = await s3.getObject({ Bucket: bucket; Key: fileName }).promise()<br>    const imagePath = `/tmp/${imageName}.${imageFormat}`<br><br>    console.log('creating image file')<br>    fs.createWriteStream(imagePath)<br><br>    console.log(&quot;Writing buffer on the file&quot;)<br>    fs.writeFile(imagePath; imageData.Body; async () =&gt; {<br>        const myMap = new Map();<br>        await Promise.all(<br>            sizes.map(async (size) =&gt; {<br>                const newImageName = `${imageName}_${size.name}.${imageFormat}`<br>                const output = `/tmp/${newImageName}`<br><br>                console.log('Going to resize image')<br>                await sharp(imagePath).resize({ width: size.width; height: size.height }).toFile(output)<br>                console.log(&quot;Resized image&quot;)<br>                myMap.set(size.name; { newImagePath: output; newImageName: newImageName })<br>            })<br>        )<br><br>        console.log(&quot;map&quot;; myMap)<br>        await Promise.all(<br>            Array.from(myMap).map(async ([key; value]) =&gt; {<br>                const params = {<br>                    Bucket: bucket;<br>                    Key: `images/${key}/${value.newImageName}`;<br>                    Body: value.newImagePath;<br>                    ContentType: `image/${imageFormat}`<br>                }<br>        <br>                console.log(&quot;Uploading to S3&quot;)<br>                await s3.putObject(params).promise()<br>            })<br>        )<br>    })<br></code></pre><br><p>And this is AWS cloudwatch logs:</p><br><pre><code>START RequestId: 9199e286-c850-40ef-bc6e-e6541741bda7 Version: $LATEST<br>2021-04-26T05:52:45.150Z    9199e286-c850-40ef-bc6e-e6541741bda7    INFO    Env variable is detected:  true<br>2021-04-26T05:52:45.150Z    9199e286-c850-40ef-bc6e-e6541741bda7    INFO    image format is:  jpg<br>2021-04-26T05:52:45.150Z    9199e286-c850-40ef-bc6e-e6541741bda7    INFO    Getting object from S3<br>2021-04-26T05:52:45.328Z    9199e286-c850-40ef-bc6e-e6541741bda7    INFO    creating image file<br>2021-04-26T05:52:45.328Z    9199e286-c850-40ef-bc6e-e6541741bda7    INFO    Writing buffer on the file<br>2021-04-26T05:52:45.348Z    9199e286-c850-40ef-bc6e-e6541741bda7    INFO    Going to resize image<br>2021-04-26T05:52:45.370Z    9199e286-c850-40ef-bc6e-e6541741bda7    INFO    Going to resize image<br>END RequestId: 9199e286-c850-40ef-bc6e-e6541741bda7<br>REPORT RequestId: 9199e286-c850-40ef-bc6e-e6541741bda7  Duration: 549.10 ms Billed Duration: 550 ms Memory Size: 512 MB Max Memory Used: 134 MB Init Duration: 760.73 ms    <br></code></pre><br><p>Any ideas?</p><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.6666666666666666,0.0,<h3>using AWS EC2 macOS for gitlab CI / CD</h3><p>since AWS announced that they have now finally MacOS machines in their portfolio; and they are advertise it that was setup for customers to use it for their iOS CI / CD; I want to try that as well. Since I'm very new to the AWS ecosystem; I'm not really aware of what AWS provides overall which I could use for that.<br>I saw that they provide the macOS in a EC2 and also as a on demand service.</p><br><p>Status Quo:<br>I host my Repository in GitLab<br>I have a gitlab CI where I run the iOS pipeline through a curl in azure pipelines. (you pay for a agent per month and my experience with their stability is very bad)</p><br><p>What I want to achieve:<br>I host my Repository in GitLab<br>...<br>...<br>Run the iOS Pipeline on an AWS EC2 macOS instance on demand.</p><br><p>I already had a look into a lot of how to's but I always end up that I was not able to choose a macOS instance.</p><br>
0.0,1.0,0.0,0.0,0.6666666666666666,0.3333333333333333,0.0,<h3>Node Express Proxy - Getting Forbidden when deployed on aws lambda to call another api that is using api gateway</h3><p>I am trying to create a AWS lambda proxy using http-proxy-middleware below is my code. When I try in my localhost it is properly working but when I deploy it to aws lambda I am receiving Forbidden error response from the API_URL which is a api gateway endpoint.</p><br><pre><code>const morgan = require(&quot;morgan&quot;);<br>    const { createProxyMiddleware } = require(&quot;http-proxy-middleware&quot;);<br>    app.use(morgan(&quot;dev&quot;));<br>    app.use(<br>      &quot;/&quot;;<br>      createProxyMiddleware({<br>        target: process.env.API_URL;<br>        changeOrigin: true;<br>      })<br>    );<br></code></pre><br><p><a href="https://i.stack.imgur.com/UPnrZ.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/UPnrZ.png" alt="enter image description here" /></a></p><br>
0.0,0.0,0.0,0.0,1.0,1.0,0.0,<h3>Uploaded file must be a non-empty zip (Service: AWSLambdaInternal; Status Code: 400; Error Code: InvalidParameterValueException;</h3><p>I have a simple Express/Node.js application with the following files with no subdirectories (other than .serverless and node_modules).</p><br><ul><br><li>.serverless</li><br><li>node_modules</li><br><li>app.js</li><br><li>lambda.js</li><br><li>package.json</li><br><li>package-lock.json</li><br><li>serverless.yml</li><br></ul><br><p>AWS throws this error - <strong>An error occurred: ApiLambdaFunction - Uploaded file must be a non-empty zip (Service: AWSLambdaInternal; Status Code: 400; Error Code: InvalidParameterValueException; Request ID: 5779d4c3-beaa-4bc2-b525-c1e89a82635a; Proxy: null).</strong></p><br><p>The .zip file was not empty when I accessed the S3 bucket to which the code gets uploaded. I was able to see the .zip folder with all its contents in there; However; the <code>serverless deploy</code> operation fails from AWS CLI; throwing the above exception. Any help to zero in on the problem would be greatly appreciated. Thanks!</p><br><p>Following is the code.</p><br><ul><br><li>app.js</li><br></ul><br><pre><code>const express = require(&quot;express&quot;)<br>const app = express();<br><br>app.use(express.json());<br>app.use(express.urlencoded({extended: false}));<br><br>app.get(&quot;/&quot;; (req; res) =&gt; {<br>    res.status(200).send(&quot;Root endpoint&quot;)<br>})<br><br>app.post(&quot;/postReq&quot;; (req; res) =&gt; {<br>    res.status(200).send(req.body);<br>})<br><br>app.listen(8080; () =&gt; console.log(&quot;App started.&quot;))<br><br></code></pre><br><ul><br><li>lambda.js</li><br></ul><br><pre><code>'use strict'<br>const awsServerlessExpress = require('aws-serverless-express')<br>const app = require('./app')<br>const server = awsServerlessExpress.createServer(app)<br><br>exports.handler = (event; context) =&gt; { awsServerlessExpress.proxy(server; event; context) }<br></code></pre><br><ul><br><li>package.json</li><br></ul><br><pre><code>{<br>  &quot;name&quot;: &quot;my-lambda&quot;;<br>  &quot;version&quot;: &quot;1.0.0&quot;;<br>  &quot;description&quot;: &quot;Lambda Function&quot;;<br>  &quot;main&quot;: &quot;app.js&quot;;<br>  &quot;scripts&quot;: {<br>    &quot;start&quot;: &quot;node app.js&quot;;<br>    &quot;deploy&quot;: &quot;serverless deploy&quot;<br>  };<br>  &quot;author&quot;: &quot;Aseem Savio&quot;;<br>  &quot;license&quot;: &quot;ISC&quot;;<br>  &quot;dependencies&quot;: {<br>    &quot;aws-serverless-express&quot;: &quot;^3.4.0&quot;;<br>    &quot;express&quot;: &quot;^4.17.1&quot;<br>  }; <br>  &quot;devDependencies&quot;: {<br>    &quot;serverless&quot;: &quot;^2.21.1&quot;<br>  }<br>}<br><br></code></pre><br><ul><br><li>serverless.yml</li><br></ul><br><pre><code>service: my-lambda-function<br><br>provider: <br>  name: aws<br>  runtime: nodejs10.x<br>  memorySize: 512<br>  timeout: 15<br>  stage: production<br>  region: ap-south-1<br><br>functions:<br>  api:<br>    handler: lambda.handler<br>    events:<br>      - http: ANY {proxy+}<br>      - http: ANY /<br></code></pre><br><p>The following is the console logs.</p><br><pre><code>aseemsavio@aseem-ubuntu:~/projects/Node.js/my-lambda$ npm run deploy<br><br>&gt; my-lambda@1.0.0 deploy<br>&gt; serverless deploy<br><br>Serverless: Configuration warning at 'functions.api.events[0].http': value 'ANY {proxy+}' does not satisfy pattern /^(?:\*|(GET|POST|PUT|PATCH|OPTIONS|HEAD|DELETE|ANY) (\/\S*))$/i<br>Serverless:  <br>Serverless: Learn more about configuration validation here: http://slss.io/configuration-validation<br>Serverless:  <br>Serverless: Deprecation warning: Starting with next major version; default value of provider.lambdaHashingVersion will be equal to &quot;20201221&quot;<br>            More Info: https://www.serverless.com/framework/docs/deprecations/#LAMBDA_HASHING_VERSION_V2<br>Serverless: Deprecation warning: Starting with next major version; API Gateway naming will be changed from &quot;{stage}-{service}&quot; to &quot;{service}-{stage}&quot;.<br>            Set &quot;provider.apiGateway.shouldStartNameWithService&quot; to &quot;true&quot; to adapt to the new behavior now.<br>            More Info: https://www.serverless.com/framework/docs/deprecations/#AWS_API_GATEWAY_NAME_STARTING_WITH_SERVICE<br>Serverless: Packaging service...<br>Serverless: Excluding development dependencies...<br>Serverless: Uploading CloudFormation file to S3...<br>Serverless: Uploading artifacts...<br>Serverless: Uploading service my-lambda.zip file to S3 (917.12 KB)...<br>Serverless: Validating template...<br>Serverless: Updating Stack...<br>Serverless: Checking Stack update progress...<br>................<br>Serverless: Operation failed!<br>Serverless: View the full error output: https://ap-south-1.console.aws.amazon.com/cloudformation/home?region=ap-south-1#/stack/detail?stackId=arn%3Aaws%3Acloudformation%3Aap-south-1%3A617584887932%3Astack%2Fmy-lambda-production%2F6d6f6ed0-607d-11eb-a05e-02c8952955f8<br> <br>  Serverless Error ---------------------------------------<br> <br>  An error occurred: ApiLambdaFunction - Uploaded file must be a non-empty zip (Service: AWSLambdaInternal; Status Code: 400; Error Code: InvalidParameterValueException; Request ID: 5779d4c3-beaa-4bc2-b525-c1e89a82635a; Proxy: null).<br> <br>  Get Support --------------------------------------------<br>     Docs:          docs.serverless.com<br>     Bugs:          github.com/serverless/serverless/issues<br>     Issues:        forum.serverless.com<br> <br>  Your Environment Information ---------------------------<br>     Operating System:          linux<br>     Node Version:              15.6.0<br>     Framework Version:         2.21.1 (local)<br>     Plugin Version:            4.4.2<br>     SDK Version:               2.3.2<br>     Components Version:        3.5.1<br> <br>npm ERR! code 1<br>npm ERR! path /home/aseemsavio/projects/Node.js/my-lambda<br>npm ERR! command failed<br>npm ERR! command sh -c serverless deploy<br><br>npm ERR! A complete log of this run can be found in:<br>npm ERR!     /home/aseemsavio/.npm/_logs/2021-01-27T09_38_04_670Z-debug.log<br>aseemsavio@aseem-ubuntu:~/projects/Node.js/my-lambda$ <br></code></pre><br><p>The full logs from the debug file are as follows:</p><br><pre><code>0 verbose cli [ '/usr/local/bin/node'; '/usr/local/bin/npm'; 'run'; 'deploy' ]<br>1 info using npm@7.4.0<br>2 info using node@v15.6.0<br>3 timing config:load:defaults Completed in 1ms<br>4 timing config:load:file:/usr/local/lib/node_modules/npm/npmrc Completed in 0ms<br>5 timing config:load:builtin Completed in 0ms<br>6 timing config:load:cli Completed in 1ms<br>7 timing config:load:env Completed in 1ms<br>8 timing config:load:file:/home/aseemsavio/projects/Node.js/my-lambda/.npmrc Completed in 0ms<br>9 timing config:load:project Completed in 0ms<br>10 timing config:load:file:/home/aseemsavio/.npmrc Completed in 0ms<br>11 timing config:load:user Completed in 0ms<br>12 timing config:load:file:/usr/local/etc/npmrc Completed in 1ms<br>13 timing config:load:global Completed in 1ms<br>14 timing config:load:cafile Completed in 0ms<br>15 timing config:load:validate Completed in 0ms<br>16 timing config:load:setUserAgent Completed in 0ms<br>17 timing config:load:setEnvs Completed in 1ms<br>18 timing config:load Completed in 5ms<br>19 verbose npm-session 39866318e36c1c25<br>20 timing npm:load Completed in 11ms<br>21 timing command:run-script Completed in 49470ms<br>22 verbose stack Error: command failed<br>22 verbose stack     at ChildProcess.&lt;anonymous&gt; (/usr/local/lib/node_modules/npm/node_modules/@npmcli/promise-spawn/index.js:64:27)<br>22 verbose stack     at ChildProcess.emit (node:events:379:20)<br>22 verbose stack     at maybeClose (node:internal/child_process:1065:16)<br>22 verbose stack     at Process.ChildProcess._handle.onexit (node:internal/child_process:296:5)<br>23 verbose pkgid my-lambda@1.0.0<br>24 verbose cwd /home/aseemsavio/projects/Node.js/my-lambda<br>25 verbose Linux 5.8.0-38-generic<br>26 verbose argv &quot;/usr/local/bin/node&quot; &quot;/usr/local/bin/npm&quot; &quot;run&quot; &quot;deploy&quot;<br>27 verbose node v15.6.0<br>28 verbose npm  v7.4.0<br>29 error code 1<br>30 error path /home/aseemsavio/projects/Node.js/my-lambda<br>31 error command failed<br>32 error command sh -c serverless deploy<br>33 verbose exit 1<br><br></code></pre><br>
0.0,0.0,0.0,1.0,0.0,0.5,0.0,<h3>Get an object from S3 bucket using partial key</h3><p>I'm working on an AWS Lambda function using Node.js 12.x. What is the best way to get an object from the S3 bucket if I only know the unique part of its key but I don't know the full key?</p><br><p><strong>Full object key</strong> &gt; <code>2020/11/15/abc-123-xyz</code></p><br><p><strong>The partial key I have</strong> &gt; <code>abc-123-xyz</code> (this part of the key is unique)</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>Guarantee a single execution of a state machine</h3><p>I have an S3 file upload triggering state machine execution. The problem is I don't want to have more than one instance of a State Machine at a time. Can I queue S3 events and process them sequentially.</p><br><p>State Machine takes few hours to complete so it doesn't look that I can queue S3 events with SQS FIFO and Lambda. Any ideas?</p><br>
0.0,0.0,0.0,0.3333333333333333,0.0,1.0,0.0,<h3>How to store the JSON data to CSV file in S3 using AWS Lambda (Node JS)?</h3><p>Used AWS Lambda to extract the data from DynamoDB which is in JSON format now need to use this data to send an email using AWS SES with csv file as attachment. Need to fill the csv file with the extracted data. How can I accomplish the task of storing the extracted data to csv file in AWS Lambda with NodeJS ? Any help will be appreciated. The code used is attached below.</p><br><p><div class="snippet" data-lang="js" data-hide="false" data-console="true" data-babel="false"><br><br><div class="snippet-code"><br><br><pre class="snippet-code-html lang-html prettyprint-override"><code>const AWS = require("aws-sdk");<br>const dynamo = new AWS.DynamoDB.DocumentClient();<br>//const converter = require('json-2-csv');<br>var licenseStatus = "trial";<br><br>var params = {<br>  TableName: process.env.TABLE_NAME;<br>  ProjectionExpression: 'email;firstName;lastName;organization;jobTitle;licenseStatus';<br>  <br>  FilterExpression : 'licenseStatus =:licenseStatus';<br>  ExpressionAttributeValues : {<br>    ':licenseStatus': licenseStatus<br>  }<br>};<br><br>const getAllRecords = async () =&gt; {<br>  const scanResult = await dynamo<br>    .scan( params)<br>    .promise();<br><br>  return scanResult;<br>};<br><br>exports.handler = async (event) =&gt; {<br>  const data = await getAllRecords();<br>  <br><br>    const response = {<br>    statusCode: 200;<br>    //body: JSON.stringify(data.Items[2].firstName);<br>    body : JSON.stringify(data)<br>  };<br><br>   return response;<br><br>    <br>  <br><br>  <br><br><br>};</code></pre><br><br></div><br><br></div><br><br></p><br>
0.0,0.0,0.0,1.0,0.0,1.0,0.0,<h3>Creating a @Bean out of a DynamoDB Table Bad Practice?</h3><p>I am using the below piece of code to create config beans in Spring Boot framework</p><br><p>AWS allows two methods of storing items; as  I understand it. First is by using the DynamoDBMapper.save() method and second being by getting the Table directly and then using the PutItem method on the table.</p><br><p>I was solely using the DynamoDBMapper.save() method first in my project but then I needed to save a JSON datatype which could only be done using the Table.putItem() because of which I have found the need to create now both beans - one for the mapper and second for the table which I use in my @Repository class to then put items into the table.</p><br><pre><code>@Configuration<br>public class DynamoDBConfig<br>{<br>    @Value(&quot;${amazon.access.key}&quot;)<br>    private String awsAccessKey;<br><br>    @Value(&quot;${amazon.access.secret-key}&quot;)<br>    private String awsSecretKey;<br><br>    @Value(&quot;${amazon.region}&quot;)<br>    private String awsRegion;<br><br>    @Value(&quot;${amazon.end-point.url}&quot;)<br>    private String awsDynamoDBEndPoint;<br><br><br>    /*client builder*/<br>    public AmazonDynamoDB amazonDynamoDBClient()<br>    {<br>        return AmazonDynamoDBClientBuilder<br>                .standard()<br>                .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(awsDynamoDBEndPoint;awsRegion))<br>                .withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(awsAccessKey; awsSecretKey)))<br>                .build();<br>    }<br><br>    /*uses client builder to return DB mapper*/<br>    @Bean<br>    public DynamoDBMapper mapper()<br>    {<br>        return new DynamoDBMapper(amazonDynamoDBClient());<br>    }<br><br>    @Bean<br>    @Qualifier(&quot;TABLENAME&quot;)<br>    public Table getTable()<br>    {<br>        DynamoDB dynamoDB = new DynamoDB(amazonDynamoDBClient());<br><br>        return dynamoDB.getTable(&quot;TABLENAME&quot;);<br>    }<br><br>}<br></code></pre><br><p>My question is:</p><br><p>Is using both methods of saving items or creating a bean out of the Table object bad practice? Should I be instead creating the table instance in my Repository using the AmazonDynamoDB client each time I want to store an object?</p><br>
0.0,1.0,0.0,0.0,0.6666666666666666,0.6666666666666666,0.3333333333333333,<h3>Returning binary file (pdf) from an AWS Lambda via AWS API Gateway</h3><p>There are a couple of questions that are similar; but none of the answers have so far worked for me.</p><br><p>I have an AWS Lambda function behind an AWS API Gateway powered by Serverless; the Lambda should be returning a PDF document via:</p><br><pre class="lang-js prettyprint-override"><code>let responseObj = {<br>      statusCode: 200;<br>      isBase64Encoded: true;<br>      headers: {<br>        'Content-type': 'application/pdf';<br>        // 'accept-ranges': 'bytes';<br>        'Content-Disposition': 'attachment; filename=' + pdfName + '.pdf'<br>      };<br>      body: pdfBuffer &amp;&amp; pdfBuffer.toString('base64')<br>    }<br>    return responseObj;<br></code></pre><br><p>when I do an <code>console.log()</code> to AWS CloudWatch of pdfBuffer (before base64; it indeed looks like PDF data:</p><br><pre><code>%PDF-1.4<br>%<br>1 0 obj<br>&lt;&lt;/Creator (Chromium)<br>/Producer (Skia/PDF m90)<br>...<br></code></pre><br><p>Yet when I look in postman; I see in my body:</p><br><blockquote><br><p>JVBERi0xLjQKJdPr6eEKMSAwIG9iago8PC9DcmVhdG9yIChDaHJvbWl1bSkKL1Byb2R1Y2VyIChTa2lhL1...</p><br></blockquote><br><p>So it's obviously not returning a binary file (my pdf).</p><br><p>Looking at API Gateway; it's been suggested you set <strong>Binary Media Types</strong> to contain <code>*/*</code>.</p><br><p>Now; my API gateway has two end points; when I set it to <code>*/*</code>; the PDF serving endpoint does indeed correctly serve my PDF; however I have another endpoint that takes in a body of JSON; and when <code>*/*</code> is set under <strong>Binary Media Types</strong>; it malforms/base64 encodes the JSON input making my CSV endpoint useless.</p><br><p>Setting <strong>Binary Media Types</strong> to contain <code>application/pdf</code> allows my CSV serving endpoint to work; but my PDF endpoint reverts back to serving up junk data; even when manually setting the <strong>Accepts</strong> header in postman to <code>application/pdf</code>.</p><br><p>So leaving <strong>Binary Media Types</strong> as <code>application/pdf</code>; I turn to Resources within the API Gateway UI settings:</p><br><p><a href="https://i.stack.imgur.com/odAz2.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/odAz2.png" alt="Screenshot of API Gateway UI showing the Resources tab selected." /></a></p><br><p>Here i'm a little unsure which to edit.  It seems i have two options in the sidebar; one for GET and one for OPTIONS:</p><br><p><a href="https://i.stack.imgur.com/J2jn4.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/J2jn4.png" alt="Screenshot of API Gateway UI showing &quot;GET - Method Execution&quot; options" /></a></p><br><p><a href="https://i.stack.imgur.com/BZx1k.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/BZx1k.png" alt="Screenshot of API Gateway UI showing &quot;OPTIONS - Method Execution&quot; options" /></a></p><br><p>The <strong>OPTIONS - Method Execution</strong> allows me to edit the <strong>Integration Response</strong> whereas the <strong>GET - Method Execution</strong> does not.</p><br><p>When I edit the <strong>Integration Response</strong> option; and I set <code>Content Handling</code> to <code>Convert to binary (if needed)</code>; there appears to be no change in what is returned to me via Postman.</p><br><p><a href="https://i.stack.imgur.com/3mTyN.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/3mTyN.png" alt="Screenshot of API Gateway UI showing Integration Response selected from OPTIONS - Method Execution" /></a></p><br><p>There must be a step or something I am missing.  Setting <strong>Binary Media Types</strong> to contain <code>*/*</code> seems like a broken answer.  There must be a way to allow certain endpoints to return binary data (like a pdf file) whilst allowing other endpoints to return or accept non binary data.</p><br>
0.0,0.0,1.0,0.3333333333333333,1.0,0.0,0.0,<h3>Windows Authentication (AD Authentication) to AWS Elastic Beanstalk</h3><p>I have an ASP.net MVC Web application with on-premises Active Directory authentication; which I want to move to AWS PaaS service. My SQL database for the ASP.Net MVC web application will remain on-premises.</p><br><p>I did some research and found that AWS ECS is a good feature for containerization. But I am not looking for IaaS approach.</p><br><p>I am mainly looking for PaaS approach to migrate my on-premises application.</p><br><ol><br><li><p>For the AWS Elastic Beanstalk Website; I am not finding an option to enable on-premises Active Directory Authentication. Is it possible?</p><br></li><br><li><p>Also can I connect to on-premises SQL server from AWS Elastic Beanstalk website using Windows Authentication/AD Authentication?</p><br></li><br></ol><br>
0.0,0.0,0.6666666666666666,0.0,1.0,0.0,0.0,<h3>Where can I view service account created by `eksctl`?</h3><p>I create a EKS cluster in AWS and use this command to create a service account <code>eksctl create iamserviceaccount --name alb-ingress-controller --cluster $componentName --attach-policy-arn $serviceRoleArn --approve --override-existing-serviceaccounts</code>.<br>The output of the command is:</p><br><pre><code>[]  using region ap-southeast-2<br>[]  1 existing iamserviceaccount(s) (default/alb-ingress-controller) will be excluded<br>[]  1 iamserviceaccount (default/alb-ingress-controller) was excluded (based on the include/exclude rules)<br>[!]  metadata of serviceaccounts that exist in Kubernetes will be updated; as --override-existing-serviceaccounts was set<br>[]  no tasks<br></code></pre><br><p>I am not sure whether it is created successfully or not.</p><br><p>I use this command <code>eksctl get iamserviceaccount</code> to verify the result but get an error response:</p><br><p><code>Error: getting iamserviceaccounts: no output &quot;Role1&quot; in stack &quot;eksctl-monitor-addon-iamserviceaccount-default-alb-ingress-controller&quot;</code></p><br><p>I also tried to run <code>kubectl get serviceaccount</code> but I got the error: <code>Error from server (NotFound): serviceaccounts &quot;alb-ingress-controller&quot; not found</code>.</p><br><p>Does this mean the service account failed to create? Where can I view the service account in AWS console? or where can I view the error?</p><br>
0.0,0.5,0.5,0.5,0.0,0.0,1.0,<h3>Alexa; AWS: cannot access to Appsync graphQL DB from Alexa skill lamba function; JWT Token is missing</h3><p>We want to use Alexa-Skill to handle requests for an amazon-user on a mobile device (Tablet). So; if the lambda-function is called by Alexa the request contains the access-token. But i want to access the appsync client; to send a subscription to the appsync-client (mobile). Unfortunately it needs the JWT token to authenticate to appsyncclient with cognito.</p><br><p><strong>The Question is; how can i get the JWT for authentication?</strong></p><br><p><a href="https://i.stack.imgur.com/bNbDS.png" rel="nofollow noreferrer">account linking for Alexa</a></p><br><pre><code>function getAppSyncClient() {<br>    const token = getToken();<br>    console.log( &quot;AuthType=&quot;; token);<br> <br>    const option = {<br>        disableOffline: true;<br>        url: process.env.API_AMPLIFYDATASOURCE_GRAPHQLAPIENDPOINTOUTPUT;<br>        region: process.env.REGION;<br>        auth: {<br>            type: AUTH_TYPE.AMAZON_COGNITO_USER_POOLS;<br>            jwtToken: token  &lt;==== this does not exist!!!<br>        };<br>        defaultOptions: {<br>            query: {<br>                fetchPolicy: 'network-only';<br>                errorPolicy: 'all';<br>            };<br>        }<br>    }<br> <br>    console.log( &quot;option=&quot;; option)<br> <br>    return new AWSAppSyncClient(option);<br>}<br></code></pre><br>
0.0,0.0,0.0,0.0,0.0,0.3333333333333333,1.0,<h3>How to send contact form data using php to aws workmail email address</h3><p>I am having trouble sending my contact form data to my desired email address. I have an email address through AWS WorkMail at <code>user@mydomain.com</code>; and I have a user on my ubuntu with the same email address <code>user@mydomain.com</code><br>. My Ubuntu machine thinks I am trying to send the form data to the local user <code>user</code> at <code>user@mydomain.com</code>. Does anyone know how I can properly configure it so the mail is sent externally and not to the internal user?</p><br><p>When I send mail to my AWS WorkMail email; it works just fine; and I can send mail back. But when I submit my form data; its sent to the local user on the ubuntu; and not the external email address.</p><br><p>Here is the html code:</p><br><pre><code>&lt;form id=&quot;contact-form&quot; action=&quot;action.php&quot; method=&quot;post&quot;&gt;<br>  <br> &lt;div class=&quot;desktop-contact&quot;&gt;<br>      &lt;h3&gt;Contact Form&lt;/h3&gt;<br> &lt;/div&gt;<br><br> &lt;label for=&quot;fname&quot;&gt;* First Name&lt;/label&gt;<br> &lt;input type=&quot;text&quot; id=&quot;fname&quot; name=&quot;firstName&quot; placeholder=&quot;Your name...&quot; required&gt;<br><br> &lt;label for=&quot;lname&quot;&gt;* Last Name&lt;/label&gt;<br> &lt;input type=&quot;text&quot; id=&quot;lname&quot; name=&quot;lastName&quot; placeholder=&quot;Your last name...&quot; required&gt;<br><br> &lt;label for=&quot;email&quot;&gt;* Email&lt;/label&gt;<br> &lt;input type=&quot;email&quot; id=&quot;email&quot; name=&quot;myEmail&quot; placeholder=&quot;Enter Email&quot; required&gt;<br><br> &lt;label for=&quot;message&quot;&gt;* Message&lt;/label&gt;<br> &lt;textarea id=&quot;message&quot; name=&quot;myMessage&quot; placeholder=&quot;Write something...&quot; required&gt;&lt;/textarea&gt;<br><br> &lt;input type=&quot;submit&quot; id=&quot;submit&quot; name=&quot;submit&quot; value=&quot;Send Message&quot;&gt;<br><br> &lt;/form&gt;<br></code></pre><br><p>Here is the php code:</p><br><pre><code>&lt;?php<br><br>if (isset($_POST['submit'])) {<br>    $fName = $_POST['firstName'];<br>    $lName = $_POST['lastName'];<br>    $mailFrom = $_POST['myEmail'];<br>    $message = $_POST['myMessage'];<br><br>    $mailTo = &quot;user@mydomain.com&quot;;<br>    $headers = &quot;From: &quot; . $mailFrom . &quot;.\n\n&quot; . $message;<br>    $txt = &quot;You have received an e-mail from &quot; . $fName . &quot; &quot; . $lName . &quot;.\n\n&quot; . $message;<br><br><br>    if (mail($mailTo; $txt; $headers)) {<br>        echo &quot;Message accepted&quot;;<br>        header(&quot;Location: contacts.php?mailsent&quot;);<br>    } else {<br><br>        echo &quot;Error: Message not accepted&quot;;<br>    }<br>}<br></code></pre><br><p>postfix - main.cf:</p><br><pre><code>mydestination = $myhostname; mydomain.com; ip-someipaddress.us-west-2.compute.internal; localhost.us-west-2.compute.internal; localhost <br><br></code></pre><br><p>php.ini:</p><br><pre><code>[mail function]<br><br>For Unix only.  You may supply arguments as well (default: &quot;sendmail -t -i&quot;).<br>sendmail_path = /usr/sbin/sendmail -t -i <br><br></code></pre><br>
0.0,0.0,0.0,0.0,0.3333333333333333,0.0,1.0,<h3>How to publish to AWS Shadow from lambda</h3><p>This is the code Iam using;</p><br><pre><code>var AWS = require('aws-sdk');<br><br>var iotdata = new AWS.IotData({<br>  endpoint: '###########.iot.ap-south-1.amazonaws.com'<br>});<br><br>exports.handler = async (event) =&gt; {<br><br>  var params = {<br>  payload: Buffer.from('...') || 'STRING_VALUE'  <br>  encoded on your behalf */; /* required */<br>  thingName: 'ESP32'; /* required */<br>  //shadowName: 'STRING_VALUE'<br> };<br> iotdata.updateThingShadow(params; function(err; data) {<br>  if (err) console.log(err; err.stack); // an error occurred<br>  else     console.log(data);           // successful response<br> });<br>  <br>};<br><br></code></pre><br><p>I referred to this from the given link;<br><a href="https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/IotData.html#updateThingShadow-property" rel="nofollow noreferrer">https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/IotData.html#updateThingShadow-property</a></p><br><p>But I cant update my shadow.<br>I also dont understand in which format I should include the payload.</p><br><p>I am running this on AWS Lambda function in Nodejs 12.x environment. I also dont receive any errors in cloudwatch . Cloudwatch tells me execution result has succeeded? Can you help me?</p><br><p>I have already given permission for Lambda to updateShadow.</p><br>
0.0,0.0,1.0,0.6666666666666666,0.0,0.6666666666666666,0.0,<h3>SignatureDoesNotMatch - Browser-Based Upload using HTTP POST (Using AWS Signature Version 4) Java</h3><p>I'm trying to upload file to Amazon S3 directly from browser.  but got the error message  SignatureDoesNotMatch The request signature we calculated does not match the signature you provided. Check your key and signing method</p><br><p>Here is the Controller</p><br><pre><code><br>@Controller<br>@RequestMapping(&quot;aws&quot;)<br>public class AwsS3Controller {<br><br>   /*<br>   * get 20210320T000000Z date<br>  */<br>   private String getTimeStamp() {<br>       DateFormat dateFormat = new SimpleDateFormat(&quot;yyyyMMdd'T'HHmmss'Z'&quot;);<br>       dateFormat.setTimeZone(TimeZone.getTimeZone(&quot;GMT+11&quot;));//server timezone<br>       return dateFormat.format(new Date());     <br>   }<br>   <br>  /*<br>   * get 20210320 date<br>  */<br>   private String getDate() {<br>       DateFormat dateFormat = new SimpleDateFormat(&quot;yyyyMMdd&quot;);<br>       dateFormat.setTimeZone(TimeZone.getTimeZone(&quot;GMT+11&quot;));<br>       return dateFormat.format(new Date());       <br>   }<br>   <br>  <br>  /*<br>   * HmacSHA256<br>  */<br>   private byte[] HmacSHA256(String data; byte[] key) throws Exception {<br>        String algorithm=&quot;HmacSHA256&quot;;<br>        Mac mac = Mac.getInstance(algorithm);<br>        mac.init(new SecretKeySpec(key; algorithm));<br>        return mac.doFinal(data.getBytes(&quot;UTF-8&quot;));<br>    }<br><br>  /*<br>   * get SignatureKey<br>  */<br>    private String getSignatureKey(String key; String dateStamp; String regionName; String serviceName;String stringToSign) throws Exception {<br>        byte[] kSecret = (&quot;AWS4&quot; + key).getBytes(&quot;UTF-8&quot;);<br>        byte[] kDate = HmacSHA256(dateStamp; kSecret);<br>        byte[] kRegion = HmacSHA256(regionName; kDate);<br>        byte[] kService = HmacSHA256(serviceName; kRegion);<br>        byte[] kSigning = HmacSHA256(&quot;aws4_request&quot;; kService);<br>        byte[] signature = HmacSHA256(stringToSign;kSigning);<br>        return Hex.encodeToString(signature);<br>    }<br>    <br>    <br>  /*<br>   *  get upload html page<br>  */<br>    @RequestMapping(&quot;/uploadfile&quot;)<br>     public String getSignatureV4(Model model) throws Exception {<br>        String secretAccessKey= &quot;GD43wv3xFD5464EDBD00eez14grfetrytfd&quot;; <br>        String dateStamp= getDate(); <br>        String regionName = &quot;ap-southeast-2&quot;; <br>        String serviceName =&quot;s3&quot;;<br>        String stringToSign = &quot;eyAiZXhwaXJhdGlvbiI6ICIyMDE1LTEyLTMwVDEyOjAwOjAwLjAwMFoiLA0KICAiY29uZGl0aW9ucyI6IFsNCiAgICB7ImJ1Y2tldCI6ICJzaWd2NGV4YW1wbGVidWNrZXQifSwNCiAgICBbInN0YXJ0cy13aXRoIiwgIiRrZXkiLCAidXNlci91c2VyMS8iXSwNCiAgICB7ImFjbCI6ICJwdWJsaWMtcmVhZCJ9LA0KICAgIHsic3VjY2Vzc19hY3Rpb25fcmVkaXJlY3QiOiAiaHR0cDovL3NpZ3Y0ZXhhbXBsZWJ1Y2tldC5zMy5hbWF6b25hd3MuY29tL3N1Y2Nlc3NmdWxfdXBsb2FkLmh0bWwifSwNCiAgICBbInN0YXJ0cy13aXRoIiwgIiRDb250ZW50LVR5cGUiLCAiaW1hZ2UvIl0sDQogICAgeyJ4LWFtei1tZXRhLXV1aWQiOiAiMTQzNjUxMjM2NTEyNzQifSwNCiAgICB7IngtYW16LXNlcnZlci1zaWRlLWVuY3J5cHRpb24iOiAiQUVTMjU2In0sDQogICAgWyJzdGFydHMtd2l0aCIsICIkeC1hbXotbWV0YS10YWciLCAiIl0sDQoNCiAgICB7IngtYW16LWNyZWRlbnRpYWwiOiAiQUtJQUlPU0ZPRE5ON0VYQU1QTEUvMjAxNTEyMjkvdXMtZWFzdC0xL3MzL2F3czRfcmVxdWVzdCJ9LA0KICAgIHsieC1hbXotYWxnb3JpdGhtIjogIkFXUzQtSE1BQy1TSEEyNTYifSwNCiAgICB7IngtYW16LWRhdGUiOiAiMjAxNTEyMjlUMDAwMDAwWiIgfQ0KICBdDQp9&quot;;<br><br>        String signature = getSignatureKey(secretAccessKey; dateStamp; regionName; serviceName; stringToSign);<br>        <br>        model.addAttribute(&quot;signature&quot;;signature);  <br>        String zmzDate = getTimeStamp(); <br>        <br>        model.addAttribute(&quot;zmzDate&quot;; zmzDate);<br>        model.addAttribute(&quot;dateStamp&quot;; dateStamp);<br>        model.addAttribute(&quot;policy&quot;; stringToSign);<br>        return &quot;uploads3File&quot;;  <br>        <br>    }<br>}<br>    <br><br></code></pre><br><p>Here is the form of uploads3File.html(based thymeleaf)</p><br><pre><code>&lt;form action=&quot;http://bucketName.s3.amazonaws.com/&quot; method=&quot;post&quot;  enctype=&quot;multipart/form-data&quot;&gt;<br>    <br>  &lt;input type=&quot;input&quot; name=&quot;key&quot; value=&quot;user/user1/dddd.jpg&quot; /&gt;<br><br>  &lt;input type=&quot;input&quot; name=&quot;acl&quot; value=&quot;public-read&quot; /&gt;<br><br>  &lt;input    type=&quot;input&quot; name=&quot;success_action_redirect&quot;  value=&quot;http://bucketName.s3.amazonaws.com/successful_upload.html&quot; /&gt;<br><br>  &lt;input type=&quot;input&quot; name=&quot;Content-Type&quot; value=&quot;image/jpeg&quot; /&gt;<br><br>  &lt;!-- set  uuid equals 14365123651274--&gt;<br>  &lt;input type=&quot;hidden&quot; name=&quot;x-amz-meta-uuid&quot; value=&quot;14365123651274&quot; /&gt;<br><br> &lt;!-- set  my aws  AWSAccessKeyId --&gt;<br>  &lt;input type=&quot;input&quot; name=&quot;AWSAccessKeyId&quot; value=&quot;KSETUDF825OPUE599&quot; /&gt;<br><br>  &lt;input type=&quot;input&quot; name=&quot;x-amz-server-side-encryption&quot; value=&quot;AES256&quot; /&gt;<br><br>  &lt;!-- value is AWSAccessKeyId + dateStamp + region + s3 + aws4_request  --&gt;<br>  &lt;input    type=&quot;text&quot;    name=&quot;X-Amz-Credential&quot;  th:value=&quot;|KSETUDF825OPUE599/${dateStamp}/ap-southeast-2/s3/aws4_request|&quot;/&gt;<br><br>  &lt;input type=&quot;text&quot; name=&quot;X-Amz-Algorithm&quot; value=&quot;AWS4-HMAC-SHA256&quot; /&gt;<br> <br>  &lt;!-- get zmzDate value from controller --&gt;<br>  &lt;input type=&quot;text&quot; name=&quot;X-Amz-Date&quot; th:value=&quot;${zmzDate}&quot; /&gt;<br><br>  &lt;input type=&quot;input&quot; name=&quot;x-amz-meta-tag&quot; value=&quot;&quot; /&gt;<br><br>  &lt;!-- get policy value from controller --&gt;<br>  &lt;input    type=&quot;input&quot;    name=&quot;Policy&quot;    th:value=&quot;${policy}&quot; /&gt;<br><br> &lt;!-- get signature value from controller --&gt;<br>  &lt;input type=&quot;input&quot; name=&quot;Signature&quot; th:value=&quot;${signature}&quot; /&gt;<br><br>  &lt;input type=&quot;file&quot; name=&quot;file&quot; /&gt;<br><br>  &lt;input type=&quot;submit&quot; name=&quot;submit&quot; value=&quot;Upload to Amazon S3&quot; /&gt;<br><br>&lt;/form&gt;<br>                                <br><br></code></pre><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.0,<h3>How can I ref a variable in ImportValue in cloudformation?</h3><p>I have a cloudformation template and need to import a value based on passed in parameter. Below is the code. But I can't combine <code>!ImportValue</code> and <code>!Ref</code>. How can I use the <code>EnvironmentName</code> in <code>ImportValue</code> function?</p><br><pre><code>Parameters:<br><br>    EnvironmentName:<br>        Description: An environment name <br>        Type: String<br><br>...<br><br>VpcConfig:<br>        SecurityGroupIds:<br>          - !ImportValue # how can I reference EnvironmentName<br>...<br><br></code></pre><br>
0.0,0.3333333333333333,0.0,1.0,0.0,1.0,0.0,<h3>Unable to connect to AWS RDS using Sequelize ORM</h3><p>I am working on an application which uses the Sequelize ORM to connect to AWS RDS. I have my connection set up as such:</p><br><h3>Connection</h3><br><pre class="lang-js prettyprint-override"><code>import {Sequelize} from 'sequelize-typescript';<br><br><br><br>// Instantiate new Sequelize instance!<br>export const sequelize = new Sequelize({<br>  &quot;username&quot;: &quot;AWS RDS USER&quot;;<br>  &quot;password&quot;: &quot;AWS RDS PASS&quot;;<br>  &quot;database&quot;: &quot;postgres&quot;;<br>  &quot;host&quot;:     &quot;******.******.us-east-1.rds.amazonaws.com&quot;;<br><br>  dialect: 'postgres';<br>  storage: ':memory:';<br>});<br><br></code></pre><br><p>I also have defined a <strong>model</strong> to represent the database table which is defined as such:</p><br><h3>Model</h3><br><pre class="lang-js prettyprint-override"><code>import {Table; Column; Model; CreatedAt; UpdatedAt} from 'sequelize-typescript';<br><br>@Table<br>export class FeedItem extends Model&lt;FeedItem&gt; {<br>  @Column<br>  public caption!: string;<br><br>  @Column<br>  public url!: string;<br><br>  @Column<br>  @CreatedAt<br>  public createdAt: Date = new Date();<br><br>  @Column<br>  @UpdatedAt<br>  public updatedAt: Date = new Date();<br>}<br></code></pre><br><p>and exported as such:</p><br><pre><code>import { FeedItem } from './feed/models/FeedItem';<br><br><br>export const V0MODELS = [ FeedItem ];<br></code></pre><br><p>Then within my <em>server.ts</em> I import my sequelize connection and model and attempt to connect to my AWS RDS as such:</p><br><h3>server.ts</h3><br><pre><code>import express from 'express';<br>import { sequelize } from './sequelize';<br><br>import { IndexRouter } from './controllers/v0/index.router';<br><br>import { V0MODELS } from './controllers/v0/model.index';<br><br>(async () =&gt; {<br>  <br>  try {<br>    await sequelize.authenticate();<br>    console.log('Connection has been established successfully.');<br>    await sequelize.addModels(V0MODELS);<br>    await sequelize.sync({ force: true; logging: console.log });<br>  } catch (error) {<br>    console.error(error);<br>  }<br>  <br>  <br>  const app = express();<br>  const port = process.env.PORT || 8080; // default port to listen<br>  <br>  app.use(express.json());<br><br>  //CORS Should be restricted<br>  app.use(function(req; res; next) {<br>    res.header(&quot;Access-Control-Allow-Origin&quot;; &quot;http://localhost:8100&quot;);<br>    res.header(&quot;Access-Control-Allow-Headers&quot;; &quot;Origin; X-Requested-With; Content-Type; Accept; Authorization&quot;);<br>    next();<br>  });<br><br>  app.use('/api/v0/'; IndexRouter)<br><br>  // Root URI call<br>  app.get( &quot;/&quot;; async ( req; res ) =&gt; {<br>    res.send( &quot;/api/v0/&quot; );<br>  } );<br>  <br><br>  // Start the Server<br>  app.listen( port; () =&gt; {<br>      console.log( `server running http://localhost:${ port }` );<br>      console.log( `press CTRL+C to stop server` );<br>  } );<br>})();<br><br></code></pre><br><p>When I run the program no connection is established; and the server fails to start. When I remove the <code>sequelize.sync</code> method; the server will start but my tables are not created. No error is caught by the <code>catch</code> block so I do not suspect there is an error. Currently I do believe this is connection issue dealing with postgres and AWS; but I cannot seem to pinned it down. All feedback and direction are appreciated.</p><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.0,0.0,<h3>EKS uses 99 cpu and stuck the machine</h3><p>I have an EKS cluster on AWS with a few pods. I assigned each pod a CPU <code>limit</code> and <code>request</code> fields.</p><br><p>The EKS cluster launches the pods in one node and everything is ok until there is a peak in the traffic.</p><br><p>When the traffic is high; the EKS lets the pods to use all the CPU available. In AWS monitors I can see the CPU is on ~99%; which cause to the EC2 to stuck; and the node status is <code>NotReady</code>.</p><br><p>How do I solve that? I can't limit the pods' CPU to total of 90% because the EKS will launch there a pod with <code>limit</code> of ~10%; and the problem will happen again.</p><br><p>How do I prevent the K8s from giving all the CPU to the pods and stuck the machine?</p><br>
0.0,0.0,0.0,1.0,0.0,0.6666666666666666,0.0,<h3>Premature end of Content-Length delimited message body</h3><p>I debugged the code the error that I am getting is after the <code>obj.close()</code> statement. I just need the last modified date of the S3 object.</p><br><pre><code>AmazonS3 s3 = AmazonS3ClientBuilder<br>              .standard()<br>              .withCredentials(new DefaultAWSCredentialsProviderChain())<br>              .withRegion(Regions.US_EAST_1)<br>              .build();<br>    S3Object obj = null;<br>    try {<br>        obj = s3.getObject(&quot;bucketName&quot;;&quot;abc/1.txt&quot;);<br>    <br>        obj.getObjectContent();<br>        Date date = obj.getObjectMetadata().getLastModified();<br>    <br>       System.out.println(date);<br>    }finally {<br>        if(obj!=null) {<br>            obj.close();<br>        }   <br>    }<br></code></pre><br><p>Error:</p><br><pre><code>org.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 12; received: 0<br>    at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:180)<br>    at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:200)<br>    at org.apache.http.impl.io.ContentLengthInputStream.close(ContentLengthInputStream.java:103)<br>    at org.apache.http.impl.execchain.ResponseEntityProxy.streamClosed(ResponseEntityProxy.java:140)<br>    at org.apache.http.conn.EofSensorInputStream.checkClose(EofSensorInputStream.java:228)<br>    at org.apache.http.conn.EofSensorInputStream.close(EofSensorInputStream.java:174)<br>    at com.amazonaws.internal.SdkFilterInputStream.close(SdkFilterInputStream.java:99)<br>    at com.amazonaws.event.ProgressInputStream.close(ProgressInputStream.java:211)<br>    at com.amazonaws.util.IOUtils.closeQuietly(IOUtils.java:70)<br>    at com.amazonaws.services.s3.internal.S3AbortableInputStream.close(S3AbortableInputStream.java:185)<br>    at com.amazonaws.internal.SdkFilterInputStream.close(SdkFilterInputStream.java:99)<br>    at com.amazonaws.services.s3.model.S3ObjectInputStream.close(S3ObjectInputStream.java:136)<br>    at com.amazonaws.internal.SdkFilterInputStream.close(SdkFilterInputStream.java:99)<br>    at com.amazonaws.internal.SdkFilterInputStream.close(SdkFilterInputStream.java:99)<br>    at com.amazonaws.event.ProgressInputStream.close(ProgressInputStream.java:211)<br>    at java.base/java.io.FilterInputStream.close(FilterInputStream.java:180)<br>    at com.amazonaws.internal.SdkFilterInputStream.close(SdkFilterInputStream.java:99)<br>    at com.amazonaws.services.s3.model.S3ObjectInputStream.close(S3ObjectInputStream.java:136)<br>    at com.amazonaws.services.s3.model.S3Object.close(S3Object.java:225)<br></code></pre><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.0,<h3>How to fix The provider provider.aws does not support data source</h3><p>I am trying to get the <code>id</code> of the <code>default</code> security group created when creating a vpc using <code>terraform</code>.</p><br><p>Here's what I tried:</p><br><pre><code>data &quot;aws_default_security_group&quot; &quot;default&quot; {<br>  vpc_id = module.ecs_vpc.vpc_id<br>}<br></code></pre><br><p>but I am getting this error:</p><br><blockquote><br><p>The provider provider.aws does not support data source<br>&quot;aws_default_security_group&quot;</p><br></blockquote><br><p>can someone help me on this?</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Allow developers to create AWS Lambda or SAM without granting Administrator access</h3><p>It seems to be impossible to allow developers to create Lambdas and create or maintain SAM Applications in AWS without essentially having AdministratorAccess policies attached to their developer's role. AWS documents a suggested IAM setup where everyone is simply Administrator; or <em>only</em> has IAMFullAccess; or a even more specific set of permissions containing &quot;iam:AttachRolePolicy&quot; which all boils down to still having enough access to grant the AdministratorAccess permission to anyone at will with just 1 API call.</p><br><p>Besides creating a new AWS Account for each SAM or Lambda deployment there doesn't seem to be any secure way to manage this; but I really hope I'm missing something obvious. Perhaps someone knows of a combination of tags; permission boundaries and IAM Paths that would alleviate this?</p><br><p>The documentation I refer to: <a href="https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-permissions.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-permissions.html</a> which opens with:</p><br><blockquote><br><p>There are three main options for granting a user permission to manage<br>serverless applications. Each option provides users with different<br>levels of access control.</p><br><ol><br><li>Grant administrator permissions.</li><br><li>Attach necessary AWS managed policies.</li><br><li>Grant specific AWS Identity and Access Management (IAM) permissions.</li><br></ol><br></blockquote><br><p>Further down; a sample application is used to specify slightly more specific permissions:</p><br><blockquote><br><p>For example; the following AWS managed policies are sufficient to<br>deploy the sample Hello World application:</p><br><ul><br><li>AWSCloudFormationFullAccess</li><br><li>IAMFullAccess</li><br><li>AWSLambda_FullAccess</li><br><li>AmazonAPIGatewayAdministrator</li><br><li>AmazonS3FullAccess</li><br><li>AmazonEC2ContainerRegistryFullAccess</li><br></ul><br></blockquote><br><p>And at the end of the document an AWS IAM Policy document describes a set of permissions which is rather lengthy; but contains the mentioned &quot;iam:AttachRolePolicy&quot; permission with a wildcard resource for roles it may be applied on.</p><br>
0.0,1.0,0.0,0.0,0.0,1.0,0.0,<h3>Swagger-ui does not work with Quarkus + AWS</h3><p>I have a problem similar to the one mentioned here (<a href="https://stackoverflow.com/questions/66316912/quarkus-swagger-ui-not-working-on-kubernetes">Quarkus Swagger UI not working on Kubernetes</a>); if not the same.<br>We have some microservices implemented in Quarkus; and we are trying to configure Swagger-ui.<br>Locally swagger works perfectly; once deployed in AWS; swagger is not working anymore!<br>When we try to access it; it is returning an Error 302 and then it redirects to a reduced path (it removes the name of the micro service from the path) and ends up throwing an Error 404.<br>So we have configured our property:</p><br><pre><code>quarkus.swagger-ui.path=/api/swagger-ui<br>quarkus.swagger-ui.always-include=true<br></code></pre><br><p>It is being invoked as follows:</p><br><pre><code>https://some_url/name_of_microservice/api/swagger-ui<br></code></pre><br><p>This returns a 302 and ends up redirecting to:</p><br><pre><code>https://some_url/api/swagger-ui/<br></code></pre><br><p>Which ends up launching 404.<br>As you can see; when redirecting it removes the name of the microservice from the path.</p><br><p><strong>Log Cloudwatch</strong>:<br>22:17:33 INFO  traceId=; spanId=; sampled= [io.qu.ht.access-log] (vert.x-eventloop-thread-0) xxx.xx.xx.xxx - - 01/Jun/2021:22:17:33 +0000 &quot;GET /api/swagger-ui HTTP/1.1&quot; 302</p><br><p>Any suggestions would be welcome.</p><br>
0.0,0.6666666666666666,0.0,0.6666666666666666,0.6666666666666666,0.0,0.0,<h3>How do I resolve timeout issue while connecting a AWS Lambda with EFS</h3><p>I have ensure that the Lambda is in the same VPC as the EFS. Lambda has the same Security Group as the mounts. The mounts have all the permissions. However; when I run the Lambda function; it times out with following message  -<br>org.apache.axis2.AxisFault: The host did not accept the connection within timeout of 30000 ms.</p><br><p>The time out for lambda is set at 5 minutes.</p><br>
0.0,0.0,1.0,0.0,1.0,0.0,0.0,<h3>Kubernetes HPA : Send HPA logs as events to aws cloudwatch</h3><p>I am working on EKS cluster in AWS. We have an application; which is memory intensive. Because of that; I have added an HPA; which has 60% memory utilization. We would like to have only this info in cloudwatch; so we can scale our servers accordingly.  I tried container insights; but it's an overkill.</p><br><p><strong>Is there any way to get Kubernetes HPA logs in cloudwatch as events?</strong></p><br><p>HPA :</p><br><pre><code>apiVersion: autoscaling/v2beta2<br>kind: HorizontalPodAutoscaler<br>metadata:<br>  name: resize-hpa<br>spec:<br>  scaleTargetRef:<br>    apiVersion: apps/v1beta1<br>    kind: Deployment<br>    name: magento-prod-deployment<br>  minReplicas: 2<br>  maxReplicas: 5<br>  metrics:<br>  - type: Resource<br>    resource:<br>      name: memory <br>      target:<br>        type: Utilization <br>        averageUtilization: 60<br></code></pre><br><p>Versions :</p><br><pre><code> kubectl version<br>Client Version: version.Info{Major:&quot;1&quot;; Minor:&quot;18&quot;; GitVersion:&quot;v1.18.8&quot;; GitCommit:&quot;9f2892aab98fe339f3bd70e3c470144299398ace&quot;; GitTreeState:&quot;clean&quot;; BuildDate:&quot;2020-08-26T20:32:49Z&quot;; GoVersion:&quot;go1.13.15&quot;; Compiler:&quot;gc&quot;; Platform:&quot;linux/amd64&quot;}<br>Server Version: version.Info{Major:&quot;1&quot;; Minor:&quot;15+&quot;; GitVersion:&quot;v1.15.11-eks-065dce&quot;; GitCommit:&quot;065dcecfcd2a91bd68a17ee0b5e895088430bd05&quot;; GitTreeState:&quot;clean&quot;; BuildDate:&quot;2020-07-16T01:44:47Z&quot;; GoVersion:&quot;go1.12.17&quot;; Compiler:&quot;gc&quot;; Platform:&quot;linux/amd64&quot;}<br></code></pre><br><p>EKS cluster is running on 1.15</p><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.6666666666666666,0.0,<h3>No matches for kind &quot;ExternalMetric&quot; in version &quot;metrics.aws/v1alpha1&quot;</h3><p>I'm getting an error when attempting to deploy an external metric monitor for AWS SQS. I'm attempting to deploy this in EKS 1.15. Currently it works in my EKS 1.14 environment.</p><br><p><code>unable to recognize &quot;app-deployment.yml&quot;: no matches for kind &quot;ExternalMetric&quot; in version &quot;metrics.aws/v1alpha1&quot;</code></p><br><p>Here is my yaml file. I'm not sure where I goofed in here especially since it works in another cluster but of a different version.</p><br><pre class="lang-yaml prettyprint-override"><code>apiVersion: metrics.aws/v1alpha1<br>kind: ExternalMetric<br>metadata:<br>  name: app-offline-queue-length<br>  namespace: myNamespace<br>spec:<br>  name: app-offline-queue-length<br>  resource:<br>    resource: &quot;deployment&quot;<br>  queries:<br>    - id: sqs_realtime_length<br>      metricStat:<br>        metric:<br>          namespace: &quot;AWS/SQS&quot;<br>          metricName: &quot;ApproximateNumberOfMessagesVisible&quot;<br>          dimensions:<br>            - name: QueueName<br>              value: &quot;app-myname-offline&quot;<br>        period: 300<br>        stat: Average<br>        unit: Count<br>      returnData: true<br></code></pre><br><p>Any advice would be greatly appreciated</p><br>
0.0,0.0,0.6666666666666666,1.0,0.0,0.0,0.0,<h3>AWS S3 Bucket Policy Access Denied Tag Condition EC2</h3><p>I've read up and down the documentation; and cannot seem to download files from s3 bucket with bucket policy using wget from instance. I can however download using the s3 cp commands.</p><br><p>Ive tried most if not all global conditions but none seem to allow the wget download.<br>Examples are aws:PrincipalTag; aws:ResourceTag; aws:PrincipalArn for instance profile role attached to instance.</p><br><p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html</a><br><a href="https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazons3.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazons3.html</a></p><br><pre><code>{<br>        &quot;Effect&quot;: &quot;Allow&quot;;<br>        &quot;Principal&quot;: {<br>            &quot;AWS&quot;: &quot;*&quot;<br>        };<br>        &quot;Action&quot;: [<br>            &quot;s3:GetObject&quot;;<br>            &quot;s3:List*&quot;<br>        ];<br>        &quot;Resource&quot;: [<br>            &quot;arn:aws:s3:::my-config-folder/*&quot;;<br>            &quot;arn:aws:s3:::my-config-folder&quot;<br>        ];<br>        &quot;Condition&quot;: {<br>            &quot;StringEquals&quot;: {<br>                &quot;aws:ResourceTag/ROLE&quot;:&quot;TEST&quot;<br>            };<br>            &quot;Bool&quot;: {<br>                &quot;aws:SecureTransport&quot;: &quot;true&quot;<br>            }<br>        }<br>    };<br><br>&quot;Condition&quot;: {<br>            &quot;ArnLike&quot;: {<br>                &quot;aws:PrincipalArn&quot;: [<br>                    &quot;arn:aws:iam::123456789456:role/instance-role-*&quot;;<br>                    &quot;arn:aws:iam::123456789456:role/instance-profile-*&quot;;<br>                    &quot;arn:aws:iam::123456789456:role/service-role-*&quot;;<br>                    &quot;arn:aws:iam::123456789456:role/instance-role-blue-dev-env&quot;;<br>                    &quot;arn:aws:iam::123456789456:instance-profile/instance-profile-dev&quot;;<br>                    &quot;arn:aws:iam::123456789456:role/instance-role-dev&quot;;<br>                    &quot;arn:aws:iam::123456789456:role/instance-profile-*&quot;<br>                ]<br>            }<br>        }<br>    };<br><br><br>&quot;Condition&quot;: {<br>            &quot;ArnLike&quot;: {<br>                &quot;ec2:SourceInstanceARN&quot;: [<br>                    &quot;arn:aws:iam::123456789456:role/instance-role-*&quot;;<br>                    &quot;arn:aws:iam::123456789456:role/instance-profile-*&quot;;<br>                    &quot;arn:aws:iam::123456789456:role/service-role-*&quot;;<br>                    &quot;arn:aws:iam::123456789456:role/instance-role-blue-dev-env&quot;;<br>                    &quot;arn:aws:iam::123456789456:instance-profile/instance-profile-dev&quot;;<br>                    &quot;arn:aws:iam::123456789456:role/instance-role-dev&quot;;<br>                    &quot;arn:aws:iam::123456789456:role/instance-profile-*&quot;<br>                ]<br>            }<br>        }<br>    };<br></code></pre><br><p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html</a><br><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#container_definition_image" rel="nofollow noreferrer">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#container_definition_image</a></p><br><pre><code>&quot;command&quot;: [];<br>  &quot;linuxParameters&quot;: null;<br>  &quot;cpu&quot;: 10;<br>  &quot;environment&quot;: [<br>    {<br>      &quot;name&quot;: &quot;APP_CONFIG_URL&quot;;<br>      &quot;value&quot;: &quot;https://my-config-folder.s3-us-west-2.amazonaws.com/test/qa/app/testapp/appconfig.properties&quot;<br>    };<br>    {<br>      &quot;name&quot;: &quot;DIRECT_MEMORY_SIZE&quot;;<br>      &quot;value&quot;: &quot;512m&quot;<br>    }<br></code></pre><br>
0.0,0.0,0.0,0.0,1.0,0.6666666666666666,0.3333333333333333,<h3>AWS Lambda uses old Variables instead of reinitialize new Variables</h3><p>I am Deploying an AWS Lambda Function that does numeric calculations; but when I deploy the function it runs correctly for the first time; then it takes the previous values and recalculates everything based on previous values. (I am calculating percentage with this function; in the first execution it calculates 50% which is correct but when I run this function again; it shows 0%; also when I try to run it thrice it shows -50%; and so on.) This is the major problem I am facing; it should reinitialize the variables from scratch. When I try to run the same function in my local machine in VS Code or in CMD this thing does not happen. Language: NodeJS 12.x and the same thing happen with NodeJS 14.x. This thing happens in Lambda Console and in API Gateway. Also; this function is triggered with API Gateway same thing and the same results are shown in console and postman. All the cache options in API Gateway are turned off.</p><br><p>My Code works like this:</p><br><pre><code>exports.handler = function(event; context; callback) {<br>    var foo = 39600;<br>    var bar = 86400;<br>    var result = await MyLambdaFunction (foo; bar);<br><br>    callback(null; result);<br>}<br><br>async function MyLambdaFunction (foo; bar) {<br>    myPercentage = (foo/bar)*100;<br>    return myPercentage;<br>}<br></code></pre><br><p>foo and bar values are passed; but lambda takes previous results and returns those.</p><br><p>Thanks</p><br>
0.0,1.0,0.3333333333333333,0.0,1.0,0.0,0.0,<h3>Can&#39;t login to ECR using VPC endpoint</h3><p><strong>SDK version number</strong><br><code>aws-cli/1.18.147 Python/2.7.18 Linux/4.14.203-156.332.amzn2.x86_64 botocore/1.18.6</code></p><br><p><strong>Platform/OS/Hardware/Device</strong><br><code>Amazon Linux 2</code></p><br><p><strong>Describe the bug</strong><br>Can't login to ECR using VPC endpoint.<br>I created VPC Endpoints like <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html#ecr-setting-up-vpc-create" rel="nofollow noreferrer">here</a> for each service name with the default policies:</p><br><pre><code>com.amazonaws.us-east-1.ecr.api<br>com.amazonaws.us-east-1.ecr.dkr<br>com.amazonaws.us-east-1.s3<br></code></pre><br><p>I run successfully:</p><br><pre class="lang-sh prettyprint-override"><code>aws --region us-east-1 ecr get-login-password<br>aws --region us-east-1 ecr get-login --no-include-email | bash # this runs docker login successfully<br>aws --region us-east-1 ecr create-repository --repository-name test/alpine2 --endpoint-url https://api.ecr.us-east-1.amazonaws.com<br><br>## using curl - https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html#registry_auth<br>TOKEN=$(aws --region us-east-1 ecr get-authorization-token --output text --query 'authorizationData[].authorizationToken')<br>curl -i -H &quot;Authorization: Basic $TOKEN&quot; https://%ACCOUNT_ID%.dkr.ecr.us-east-1.amazonaws.com/v2/test/alpine2/tags/list<br></code></pre><br><p>But if I try to login to ECR using VPC endpoint I can't.</p><br><pre class="lang-sh prettyprint-override"><code>docker login -u AWS -p $(aws --region us-east-1 ecr get-login-password) api.ecr.us-east-1.amazonaws.com<br>WARNING! Using --password via the CLI is insecure. Use --password-stdin.<br>Error response from daemon: login attempt to https://api.ecr.us-east-1.amazonaws.com/v2/ failed with status: 403 Forbidden<br><br># using curl<br>curl -i -H &quot;Authorization: Basic $TOKEN&quot; https://api.ecr.us-east-1.amazonaws.com/v2/test/alpine2/tags/list <br>HTTP/1.1 403 Forbidden<br>x-amzn-RequestId: f7c5f001-ce49-4cb9-ae9a-8301701db80d<br>Date: Tue; 1 Dec 2020 13:53:07 GMT<br>Content-Length: 7402<br><br>&lt;IncompleteSignatureException&gt;<br>  &lt;Message&gt;'%LONG_TOKEN_FROM_PREVIOUS_COMMAND%' not a valid key=value pair (missing equal-sign) in Authorization header: 'Basic %LONG_TOKEN_FROM_PREVIOUS_COMMAND%'.&lt;/Message&gt;<br></code></pre><br><p><strong>Additional info</strong><br>I run EC2 instance in private VPC. I can successfully push image to ECR using its external name.<br>It means that I have perms to work with ECR. The problem is in VPC endpoint; may be.<br>Please; help.</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.6666666666666666,<h3>Dropzone not changing method to PUT</h3><p>I'm setting up my Dropzone elements in the following fashion:</p><br><pre><code>function createDropZone(componentId) {<br>return new Dropzone(`#${componentId}`; {<br>  url: &quot;/store/signup&quot;;<br>  method: &quot;put&quot;;<br>  maxFiles: 1;<br>  maxFilesize: 5;<br>  maxThumbnailFilesize: 5;<br>  acceptedFiles: &quot;.jpg; .jpeg; .png&quot;;<br>  addRemoveLinks: true;<br>  autoQueue: false;<br>  dictRemoveFile: &quot;&quot;;<br>  dictCancelUpload: &quot;&quot;;<br>  thumbnailWidth: null;<br>  thumbnailHeight: null;<br>  init: function() {<br>    this.on(&quot;thumbnail&quot;; function(file; dataUrl) {<br>      $('.dz-image').last().find('img').attr({width: '100%'; height: 'auto'});<br>    });<br>    this.on(&quot;success&quot;; function(file) {<br>      $('.dz-image').css({&quot;width&quot;:&quot;100%&quot;; &quot;height&quot;:&quot;auto&quot;});<br>    });<br>    this.on(&quot;addedfile&quot;; function(file) {<br>      if (this.files.length &gt; 1) { this.removeFile(this.files[0]); }<br>    })<br>  }<br>})<br></code></pre><br><p>The endpoint I upload to works with <code>PUT</code> not <code>POST</code>. Yet even when I set the method it remains as <code>POST</code>; any idea why this is?</p><br><p><a href="https://i.stack.imgur.com/WLvjR.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/WLvjR.png" alt="enter image description here" /></a></p><br>
0.0,0.3333333333333333,0.0,0.0,0.0,0.0,1.0,<h3>How to send email to AWS SES from GCP vm instance</h3><p>I have a requirement where GCP VM instance which is behind a squid proxy and NAT gateway should be able to send emails to AWS SES.</p><br><p>SES: email-smtp.eu-west-1.amazonaws.com</p><br><p>as far as I know; GCP has port 25 blocked and after referring to few articles on stackoverflow; it seems squid does not support SMTP as well.</p><br><p>can someone please help if they have worked on similar thing before?.</p><br><p>Thanks<br>Max</p><br>
1.0,0.0,0.0,1.0,0.0,0.3333333333333333,0.0,<h3>How can i extract information quickly from 130;000+ Json files located in S3?</h3><p>i have an S3 was over 130k Json Files which i need to calculate numbers based on data in the json files (for example calculate the number of gender of Speakers). i am currently using s3 Paginator and JSON.load to read each file and extract information form. but it take a very long time to process such a large number of file (2-3 files per second). how can i speed up the process? please provide working code examples if possible. Thank you<br>here is some of my code:</p><br><pre><code>client = boto3.client('s3')<br>paginator = client.get_paginator('list_objects_v2')<br>result = paginator.paginate(Bucket='bucket-name';StartAfter='') <br><br>for page in result:<br>    if &quot;Contents&quot; in page:<br>        for key in page[ &quot;Contents&quot; ]:<br>            keyString = key[ &quot;Key&quot; ]<br>                s3 = boto3.resource('s3')<br>                content_object = s3.Bucket('bucket-name').Object(str(keyString))<br>                    file_content = content_object.get()['Body'].read().decode('utf-8')<br>                    json_content = json.loads(file_content)<br>                    x = (json_content['dict-name'])<br></code></pre><br>
0.0,0.0,0.0,0.0,0.6666666666666666,0.3333333333333333,1.0,<h3>What would be the pricing for using Amazon SES from a lambda function?</h3><p>I am configuring Amazon SES to send emails; through a lambda function. I am trying to understand the pricing model for doing this.<br>As per the AWS docs; the pricing is as follows:</p><br><blockquote><br><p><strong>Sending email from an application hosted in Amazon EC2</strong><br /><br>$0 for the first 62;000 emails you send each month; and $0.10 for every 1;000 emails you send after that.</p><br></blockquote><br><blockquote><br><p><strong>Sending email from an email client or other software package</strong><br /><br>$0.10 for every 1;000 emails you send.</p><br></blockquote><br><p>Where do lambda functions fit into this pricing scenario? While I do understand AWS would host lambda functions on some variant of a virtual instance; will the pricing model of an EC2 instance apply while invoking SES through lambdas as well?</p><br><p>Thanks in advance!</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.3333333333333333,<h3>Appsync graphQL error: Variable &#39;input&#39; has coerced Null value for NonNull type &#39;ID!&#39;; despite console logging id</h3><p>Im using react native and aws Appsync; I am creating a chat application and have an input box for creating messages. When I try to create a message it gives me this error:</p><br><pre><code>    Object {<br>  &quot;data&quot;: null;<br>  &quot;errors&quot;: Array [<br>    Object {<br>      &quot;locations&quot;: Array [<br>        Object {<br>          &quot;column&quot;: 24;<br>          &quot;line&quot;: 1;<br>          &quot;sourceName&quot;: null;<br>        };<br>      ];<br>      &quot;message&quot;: &quot;Variable 'input' has coerced Null value for NonNull type 'ID!'&quot;;<br>      &quot;path&quot;: null;<br>    };<br>  ];<br>}<br></code></pre><br><p>Here is my input component:</p><br><pre><code>type props = {<br>    chatRoomId: any<br>}<br><br>const Input = ({chatRoomId}: props) =&gt; {<br>    const [userId; setUserId] = useState(null);<br>    const [message; setMessage] = useState('');<br><br>    useEffect(() =&gt; {<br>        const fetchUser = async () =&gt; {<br>            const userInfo = await Auth.currentAuthenticatedUser();<br>            setUserId(userInfo.attributes.sub);<br>        }<br>        fetchUser();<br>        console.log(&quot;userID1:&quot;; userId)<br>    }; [])<br><br>    console.log(&quot;userID2:&quot;; userId)<br><br>    const onSendPress = async () =&gt; {<br>        try {<br>            console.log(&quot;userID3:&quot;; userId)<br>            await API.graphql(<br>                graphqlOperation(<br>                    createMessage; {<br>                        input: {<br>                            content: message;<br>                            userID: userId;<br>                            chatRoomID: chatRoomId<br>                        }<br>                    }<br>                )<br>            )<br>        } catch (e) {<br>            console.log(e);<br>        }<br>        setMessage('');<br>    }<br>    return (<br>        &lt;View style={styles.container}&gt;<br>            &lt;TextInput<br>                placeholder='Type your message...'<br>                value={message}<br>                onChangeText={setMessage}<br>                style={styles.input}<br>            /&gt;<br>            &lt;View style={styles.iconContainer}&gt;<br>                &lt;TouchableOpacity onPress={onSendPress}&gt;<br>                    &lt;Ionicons name='ios-send' color='#FFF' size={25} style={styles.icon}/&gt;<br>                &lt;/TouchableOpacity&gt;<br>            &lt;/View&gt;<br>        &lt;/View&gt;<br>    )<br>}<br>export default Input;<br></code></pre><br><p>When I am console logging the ID I am getting the correct ID; however when I am sending it doesnt seem to use the ID.<br>My graphql schema is:</p><br><pre><code>    type User @model {<br>  id: ID!<br>  name: String!<br>  dob: String!<br>  imageUri: String<br>  chatRoomUser: [ChatRoomUser] @connection(keyName: &quot;byUser&quot;; fields: [&quot;id&quot;])<br>}<br><br>type ChatRoomUser<br>@model<br>@key(name: &quot;byUser&quot;; fields: [&quot;userID&quot;; &quot;chatRoomID&quot;])<br>@key(name: &quot;byChatRoom&quot;; fields: [&quot;chatRoomID&quot;; &quot;userID&quot;]) {<br>  id: ID!<br>  userID: ID!<br>  chatRoomID: ID!<br>  user: User @connection(fields: [&quot;userID&quot;])<br>  chatRoom: ChatRoom @connection(fields: [&quot;chatRoomID&quot;])<br>}<br><br>type ChatRoom @model {<br>  id: ID!<br>  chatRoomUsers: [ChatRoomUser] @connection(keyName: &quot;byChatRoom&quot;; fields: [&quot;id&quot;])<br>  messages: [Message]  @connection(keyName: &quot;byChatRoom&quot;; fields: [&quot;id&quot;])<br>  lastMessageID: ID!<br>  lastMessage: Message @connection(fields: [&quot;lastMessageID&quot;])<br>}<br><br>type Message<br>@model<br>@key(<br>  name: &quot;byChatRoom&quot;;<br>  fields: [&quot;chatRoomID&quot;; &quot;createdAt&quot;];<br>  queryField: &quot;messagesByChatRoom&quot;) {<br>  id: ID!<br>  createdAt: String!<br>  content: String!<br>  userID: ID!<br>  chatRoomID: ID!<br>  user: User @connection(fields: [&quot;userID&quot;])<br>  chatRoom: ChatRoom @connection(fields: [&quot;chatRoomID&quot;])<br>}<br></code></pre><br><p>I was loosely following a tutorial from github here: <a href="https://github.com/Savinvadim1312/WhatsappClone" rel="nofollow noreferrer">https://github.com/Savinvadim1312/WhatsappClone</a></p><br><p>And so if anyone can help point me to anything I can change it would be great! Thanks!</p><br>
0.0,0.6666666666666666,0.3333333333333333,0.3333333333333333,0.3333333333333333,1.0,0.6666666666666666,<h3>Need Assistance Hosting on AWS</h3><p>So Ive just finished working on my first big personal project; bought a domain name; created an AWS account; watched a lot of AWS tutorials; but I still cant figure out how to host my web app on AWS. The whole AWS thing is a mystery to me. No tutorial online seems to teach exactly what I need.</p><br><p>What Im trying to do is this:</p><br><p>Host my dynamic web app on a secure https connection.<br>Host the web app using the personalized domain name I purchased.<br>Link my git repo to AWS so I can easily commit and push changes when needed.<br>Please assist me by pointing me to a resource that can help me achieve the above 3 tasks.</p><br><p>For now; the web app is still hosted on Herokus free service; feel free to take a look at the application; and provide some feedback if you can.</p><br><p>Link to web app:<a href="http://biggie-beta.herokuapp.com/" rel="nofollow noreferrer">my web app</a></p><br>
0.0,0.0,1.0,0.0,0.0,0.3333333333333333,0.0,<h3>How Can I SignUp new User with AWS cognito with Postman without using hosted UI</h3><p>Basically I want the below flow in the application .</p><br><p>I have created one user pool in the cognito and configure it.</p><br><p>I want to integrate cognito authentication and authorization with below flow.</p><br><ol><br><li>Register new user with by using cognito signUp api via postman (I dont want to use hosted UI) .</li><br></ol><br><p>once user is successfully registered in cognito.</p><br><ol start="2"><br><li><p>User will call the cognito login api via postman - On successful login cognito will return access_token.</p><br></li><br><li><p>I will use that access token in all subsequent requests to make sure the user is authenticated and authorized .</p><br></li><br></ol><br><p>The main thing here is I do not want to use that hosted UI given by cognito .I want to achieve this via api calls .</p><br><p>I am not sure for achieving this what I need to . You can tell me if any more steps needed before the first step I wrote like authorize my app or anything like that.</p><br><p>I understood I need to authorize my app before it uses the signup api  but I am not sure about exact flow and process or in which manner I need to perform the steps .</p><br><p>Please guide..</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.0,<h3>AWS Codeguru review recommendation is not working</h3><p>I have setup AWS code guru reviewer in my profile with a Github branch. I have created the pull request which is associated with Codeguru reviewer; but recommendation is 0 and Metered lines of code also 0. Please suggest if there is anything wrong I have done .</p><br>
0.0,0.0,0.0,0.0,1.0,0.6666666666666666,0.3333333333333333,<h3>Increase number of worker_connections on Beanstalk nodejs environment</h3><p>I am trying to increase the number of possible <code>worker_connections</code> of my nginx on my Beanstalk nodejs server.<br /><br>I followed the documentation and created a <code>proxy.config</code> file in my <code>.ebextensions</code> folder at the root of my project.</p><br><pre><code>files:<br>  /etc/nginx/conf.d/proxy.conf:<br>    mode: &quot;000644&quot;<br>    owner: root<br>    group: root<br>    content: |<br><br>      worker_rlimit_nofile 65536;<br>      events {<br>        worker_connections  50000;<br>      }<br></code></pre><br><p>I re-deployed my project but still get this error<br><code>[alert] 19144#0: 1024 worker_connections are not enough</code>.</p><br><p><strong>EDIT:</strong><br>I was looking at the documentation for Amazon Linux 1; so here is my new problem: <a href="https://stackoverflow.com/questions/66343544/increasing-worker-connections-of-nginx-on-beanstalk-nodejs-environment">Increasing worker_connections of nginx on Beanstalk nodejs environment</a></p><br>
0.0,0.0,1.0,0.0,0.0,1.0,0.0,<h3>How to use saml2aws similar functionality on Nodejs app</h3><p>I am looking for suggestions on how to authenticate NodeJS server my AWS account using ADFS with Username and password followed by MFA Authentication to get a temporary access key and secret key for further actions.</p><br><p>For reference; I am trying to achieve what saml2aws do on the command-line interface. Here instead of a command-line interface; I would like to do it on my NodeJS server.</p><br><p>Any suggestions will be highly appreciated.</p><br><p>So far I have the below code with me; It does half job for me like it enters ADFS creds and submits the request; and then triggers a notification to approve on my Microsoft authenticator app. But my code doesn't wait for that authenticator response. It just gets the body of the webpage where it waits for the authenticator to approve. I need some way to make it wait for the authenticator response; which will give me a SAML assertion response after ADFS credit submission.</p><br><pre><code>require(&quot;dotenv&quot;).config();<br>const AWS = require(&quot;aws-sdk&quot;);<br>const sts = new AWS.STS();<br>const request = require(&quot;request&quot;).defaults({ jar: true });<br>const url = require(&quot;url&quot;);<br>const JSSoup = require(&quot;jssoup&quot;).default;<br>const os = require(&quot;os&quot;);<br>const path = require(&quot;path&quot;);<br>const fs = require(&quot;fs&quot;);<br>const ini = require(&quot;ini&quot;);<br><br>const HOME = os.homedir();<br>const CONFIG_FILE = path.join(HOME; &quot;.aws&quot;; &quot;credentials&quot;);<br><br>let IDP_URL = process.env.IDP_URL;<br>let IDP_USER = process.env.IDP_USER;<br>let IDP_PASS = process.env.IDP_PASS;<br>let AWS_PROFILE = process.env.AWS_PROFILE;<br>console.log(IDP_USER);<br>function base64encode(data) {<br>  return Buffer.from(data; &quot;utf8&quot;).toString(&quot;base64&quot;);<br>}<br><br>function base64decode(data) {<br>  return Buffer.from(data; &quot;base64&quot;).toString(&quot;utf8&quot;);<br>}<br><br>function httpGet(url) {<br>  const options = {<br>    url;<br>  };<br>  return new Promise((resolve; reject) =&gt; {<br>    request.get(options; (error; response; body) =&gt; {<br>      if (error) {<br>        reject(error);<br>      } else {<br>        resolve({ response; body });<br>      }<br>    });<br>  });<br>}<br><br>function httpPost(url; form) {<br>  const options = {<br>    url;<br>    form;<br>    followAllRedirects: true;<br>  };<br>  return new Promise((resolve; reject) =&gt; {<br>    request.post(options; (error; response; body) =&gt; {<br>      if (error) {<br>        reject(error);<br>      } else {<br>        resolve({ response; body });<br>      }<br>    });<br>  });<br>}<br><br>function getLoginData(body) {<br>  const soup = new JSSoup(body);<br>  const forms = soup.findAll(&quot;form&quot;);<br>  const form = forms.find((form) =&gt; form.attrs.id === &quot;loginForm&quot;);<br><br>  if (!form) {<br>    throw new Error(&quot;LOGIN_FORM_NOT_FOUND&quot;);<br>  }<br><br>  const action = url.resolve(IDP_URL; form.attrs.action);<br>  const inputs = {};<br><br>  for (const input of form.findAll(&quot;input&quot;)) {<br>    const name = input.attrs.name || &quot;&quot;;<br>    const value = input.attrs.value || &quot;&quot;;<br>    const namelc = name.toLowerCase();<br><br>    if (namelc.includes(&quot;user&quot;)) {<br>      inputs[name] = IDP_USER;<br>    } else if (namelc.includes(&quot;email&quot;)) {<br>      inputs[name] = IDP_USER;<br>    } else if (namelc.includes(&quot;pass&quot;)) {<br>      inputs[name] = IDP_PASS;<br>    } else {<br>      inputs[name] = value;<br>    }<br>  }<br><br>  return { action; inputs };<br>}<br><br>function getSAMLAssertion(body) {<br>  const soup = new JSSoup(body);<br>  const inputs = soup.findAll(&quot;input&quot;);<br>  const saml = inputs.find((input) =&gt; input.attrs.name === &quot;SAMLResponse&quot;);<br><br>  if (!saml) {<br>    throw new Error(&quot;SAML_ASSERTION_NOT_FOUND&quot;);<br>  }<br><br>  return base64decode(saml.attrs.value);<br>}<br><br>function getSAMLRoles(saml) {<br>  const soup = new JSSoup(saml);<br>  const roles = soup<br>    .findAll(&quot;AttributeValue&quot;)<br>    .filter((value) =&gt; {<br>      return (<br>        value.parent &amp;&amp;<br>        value.parent.name === &quot;Attribute&quot; &amp;&amp;<br>        value.parent.attrs &amp;&amp;<br>        value.parent.attrs.Name ===<br>          &quot;https://aws.amazon.com/SAML/Attributes/Role&quot;<br>      );<br>    })<br>    .map((value) =&gt; {<br>      const [provider; role] = (value.text || &quot;&quot;).split(&quot;;&quot;);<br>      return { provider; role };<br>    });<br><br>  if (!roles[0]) {<br>    throw new Error(&quot;SAML_ROLE_NOT_FOUND&quot;);<br>  }<br><br>  return roles;<br>}<br><br>async function getSTSToken(provider; role; assertion) {<br>  const params = {<br>    DurationSeconds: 3600;<br>    PrincipalArn: provider;<br>    RoleArn: role;<br>    SAMLAssertion: base64encode(assertion);<br>  };<br>  return new Promise((resolve; reject) =&gt; {<br>    sts.assumeRoleWithSAML(params; (error; data) =&gt; {<br>      if (error) {<br>        reject(error);<br>      } else {<br>        resolve(data);<br>      }<br>    });<br>  });<br>}<br><br>function saveSTSToken(filename; profile; sts) {<br>  const readFile = () =&gt; {<br>    try {<br>      return ini.decode(fs.readFileSync(filename; &quot;utf-8&quot;));<br>    } catch (e) {<br>      return {};<br>    }<br>  };<br><br>  const writeFile = (config) =&gt; {<br>    fs.writeFileSync(filename; ini.encode(config; { whitespace: true }));<br>  };<br><br>  const config = readFile();<br>  const section = config[profile] || {};<br>  const credentials = sts.Credentials || {};<br><br>  section.aws_access_key_id = credentials.AccessKeyId;<br>  section.aws_secret_access_key = credentials.SecretAccessKey;<br>  section.aws_session_token = credentials.SessionToken;<br><br>  config[profile] = section;<br>  writeFile(config);<br>}<br><br>function checkUsage() {<br>  if (!IDP_URL) {<br>    throw new Error(&quot;IDP_URL not set!&quot;);<br>  }<br>  if (!IDP_USER) {<br>    throw new Error(&quot;IDP_USER not set!&quot;);<br>  }<br>  if (!IDP_PASS) {<br>    throw new Error(&quot;IDP_PASS not set!&quot;);<br>  }<br>  if (!AWS_PROFILE) {<br>    throw new Error(&quot;AWS_PROFILE not set!&quot;);<br>  }<br>}<br><br>(async function main() {<br>  try {<br>    console.log(`aws-saml-session started.`);<br>    checkUsage();<br><br>    console.log(`Logging into SAML provider...`);<br>    const resp1 = await httpGet(IDP_URL);<br>    const data = getLoginData(resp1.body);<br>    // This is the place its not waiting and getting me body of the page <br>    //where it needs to wait and get me SAML response<br>    ```const resp2 = await httpPost(data.action; data.inputs);<br>    const saml = getSAMLAssertion(resp2.body);```<br><br>    const roles = getSAMLRoles(saml);<br><br>    const { provider; role } = roles[0];<br><br>    console.log(`Assuming role: ${role}...`);<br>    const sts = await getSTSToken(provider; role; saml);<br><br>    console.log(`Saving credentials: ${AWS_PROFILE}...`);<br>    saveSTSToken(CONFIG_FILE; AWS_PROFILE; sts);<br><br>    console.log(&quot;Done.&quot;);<br>  } catch (e) {<br>    console.log(&quot;ERROR:&quot;; e.message);<br>  }<br>})();<br></code></pre><br>
0.0,0.0,0.3333333333333333,0.0,0.3333333333333333,1.0,0.0,<h3>AWS CloudFormation pseudo parameters are incorrect locally</h3><p>I am working on AWS Serverless application using SAM. My <em>template.yaml</em> has this line for defining an S3 bucket name:</p><br><pre><code>BucketName: !Sub ${AWS::StackName}-visit-attachments-${AWS::AccountID}<br></code></pre><br><p>When I deploy to AWS using <code>sam deploy</code> the variables are substituted correctly.<br>But when I execute the lambda function locally; the resulting string is <code>local-visit-attachments-123456789012</code>. I can't find where the <code>local</code> and <code>123456789012</code> are coming from since I don't have them anywhere in the configs.</p><br><p>How can I make it so it uses the same values locally as when I deploy to AWS?</p><br>
0.0,0.6666666666666666,0.3333333333333333,0.0,0.0,0.0,0.0,<h3>No hosted zone found with ID error when deploying</h3><p>I am using aws-cdk to create my infrastructure. I'm using using the following code to lookup my hosted zone:</p><br><pre><code>const zone = route53.HostedZone.fromLookup(this; 'Zone'; { domainName: domainName });<br></code></pre><br><p>When I deploy; I get the following error</p><br><pre><code>Failed to create resource. No hosted zone found with ID:xxxxxxxxxxx<br></code></pre><br><p>I have only one hosted zone in route-53 and the ID which the above error is displaying isn't the ID of my Hosted zone. Not sure where it is getting this value from.</p><br>
0.0,1.0,0.0,0.3333333333333333,0.0,0.3333333333333333,0.0,<h3>AWS S3 / Cloudfront deployment - certain paths aren&#39;t updating on static website</h3><p>I'm fairly new to AWS and web dev - I have a simple static website at <a href="https://iveyfintechclub.ca" rel="nofollow noreferrer">https://iveyfintechclub.ca</a></p><br><p>I recently refactored the code and made some changes to the project organization. I basically wiped the S3 bucket and reuploaded all new files and folders.</p><br><p>On CloudFront; I have <code>object caching</code> set to <code>use origin cache headers</code>:</p><br><p><a href="https://i.stack.imgur.com/A9nY1.png" rel="nofollow noreferrer">my CloudFront distribution behavior config</a></p><br><p>I also did an invalidation with /*.</p><br><p>On S3; I've set the metadata <code>Cache-Control</code> to <code>max-age=0</code> for all files.</p><br><p>Two problems are still eluding me:</p><br><ol><br><li>The old bucket had a blank index.html which redirected to a<br>nested HTML file. The new bucket has index.html as the landing page.<br>When I attempt to visit the root URL; I get a 404 error as it still<br>attempts to reach the old nested HTML path. This doesn't happen in incognito mode (browser cache issue).</li><br></ol><br><p><s>2. On the new landing page; I have a script file which is getting a 404<br>error as its looking for the file on its old path. Inspecting the HTML shows that the new path is in the client. This is happening in incognito mode too. All other resources are loading properly with<br>new paths; just this one is failing.</s></p><br><p>I'm wondering if I just have to wait longer or if I'm still missing a configuration.</p><br>
0.0,0.0,1.0,0.0,1.0,0.0,0.0,<h3>Can you restrict an IAM instance profile to specific Linux accounts?</h3><p>When associating an EC2 instance with a IAM role via &quot;aws ec2 associate-iam-instance-profile&quot; it seems that all Linux users on the instance can make API calls via those credentials. Is there a way to leverage the IAM instance profile but restrict access to specific users within the OS?</p><br>
1.0,0.0,0.3333333333333333,0.0,0.0,0.3333333333333333,0.0,<h3>how to setup multiple automated workflows on aws glue</h3><p>We're trying to use AWS Glue for ETL operations in our nodejs project. The workflow will be like below</p><br><ol><br><li>user uploads csv file</li><br><li>data transformation from XYZ format to ABC format(mapping and changing field names)</li><br><li>download transformed csv file to local system</li><br></ol><br><p>Note that; this flow should happen programmatically(creating crawlers; job triggers should be done programmatically not using the console). I don't know why documentation and other articles always show how to create crawlers; create jobs from glue console?</p><br><p>I believe that we have to create lambda functions and triggers. but not quite sure how to achieve this end to end flow. can anyone please help me. Thanks</p><br>
0.0,0.0,0.0,0.0,0.0,0.0,1.0,<h3>How to remotely debug an iot device using google cloud platform?</h3><p>Imagine having an iot device in a building miles away from you. The device encountered a problem and I want to debug remotely. Is there an place in gcp where I can do that ? (With aws; I can use <a href="https://docs.aws.amazon.com/iot/latest/developerguide/secure-tunneling.html" rel="nofollow noreferrer">secure tunneling</a> to establish a secure tunnel and debug the device remotely)</p><br>
0.0,0.0,1.0,0.6666666666666666,0.0,0.6666666666666666,0.0,<h3>AWS s3 upload error: InvalidIdentityPoolConfigurationException: Missing credentials in config</h3><p>I'm trying to set up an upload to an s3 bucket using this tutorial: <a href="https://medium.com/@shresthshruti09/uploading-files-in-aws-s3-bucket-through-javascript-sdk-with-progress-bar-d2a4b3ee77b5" rel="nofollow noreferrer">https://medium.com/@shresthshruti09/uploading-files-in-aws-s3-bucket-through-javascript-sdk-with-progress-bar-d2a4b3ee77b5</a></p><br><p>I have everything set up the way its supposed to be but when I try the upload I get this error:<br><code>error: InvalidIdentityPoolConfigurationException: Missing credentials in config</code></p><br><p>The code looks like this:</p><br><pre><code>//Bucket Configurations<br>var bucketName = &quot;name-of-bucket&quot;;<br>var bucketRegion = &quot;us-east-2&quot;;<br>var IdentityPoolId = &quot;[Identity Pool Id]&quot;;<br>AWS.config.update({<br>    region: bucketRegion;<br>    credentials: new AWS.CognitoIdentityCredentials({<br>        IdentityPoolId: IdentityPoolId<br>    })<br>});<br><br>var s3 = new AWS.S3({<br>    apiVersion: '2006-03-01';<br>    params: {Bucket: bucketName}<br>});<br>function s3upload() {<br>    var files = document.getElementById('screenshot').files;<br>    if (files) {<br>        var file = files[0];<br>        var fileName = file.name;<br>        var filePath = 'name-of-bucket/' + fileName;<br>        var fileUrl = 'https://' + bucketRegion + '.amazonaws.com/name-of-bucket/feedback-screenshots/' +  filePath;<br>        s3.upload({<br>            Key: filePath;<br>            Body: file;<br>            ACL: 'public-read'<br>        }; function(err; data) {<br>            if(err) {<br>                console.log('error:'; err);<br>            } else {<br>                alert('Successfully Uploaded!');<br>            }<br>        }).on('httpUploadProgress'; function (progress) {<br>            var uploaded = parseInt((progress.loaded * 100) / progress.total);<br>            $(&quot;progress&quot;).attr('value'; uploaded);<br>        });<br>    }<br>};<br></code></pre><br>
0.0,1.0,0.0,0.0,1.0,0.0,0.0,<h3>Why do EC2 instances need a public IP when placed in front of an ALB?</h3><p>As I have been researching; there are two ways of placing EC2 instances in front of ALBs in a given availability zone:</p><br><ol><br><li><p>Placing them in the same public subnets referenced by ALB in that availability zone + giving them a public IP</p><br></li><br><li><p>Placing them in private subnets + creating a NAT Gateway a public subnet referenced by ALB in that availability zone</p><br></li><br></ol><br><p>I would like to be able to place ALBs in front of EC2 instances without public IP; for example; an EC2 instance without a public IP in a public subnet referenced by an ALB</p><br><p>When I try to do this the ALB says my EC2 instance is unhealthy; but when I give the intance a public IP the ALB almost instantly says its healthy</p><br><p>Why is this the case? This does not seem to be addressed until now in stack overflow (<a href="https://stackoverflow.com/questions/22541895/amazon-elb-for-ec2-instances-in-private-subnet-in-vpc">this</a> seem to be the only answer until now)</p><br>
0.0,0.0,1.0,0.0,0.0,0.6666666666666666,0.0,<h3>AWS SAM template validation error detected during SAM Deploy. Pseudo-param ${AWS::AccountId} not accepted</h3><p>While attempting SAM Deploy; I'm getting the following reject:</p><br><pre><code>1 validation error detected: Value 'arn:aws:iam::${AWS::AccountId}:role/Lambda-Exec-And-CloudWatch' at 'role' failed to satisfy constraint: Member must satisfy regular expression pattern: arn:(aws[a-zA-Z-]*)?:iam::\d{12}:role/?[a-zA-Z_0-9+=;.@\-_/]+ (Service:AWSLambdaInternal; Status Code: 400; Error Code: ValidationException;<br></code></pre><br><p>This occurs when I replace my account ID in the arn role construction for a Lambda function with the pseudo-parameter:  <strong>${AWS::AccountId}</strong>.</p><br><p>This is the Role property of the Function in the YAML template where that happens:</p><br><pre><code>Role:<br>        arn:aws:iam::${AWS::AccountId}:role/Lambda-Exec-And-CloudWatch<br></code></pre><br><p>The expected regex expression seems to only accept hard-coded account ID.</p><br><p>If it helps; I'm using Cloud9 as my IDE.</p><br><p><strong>What am I missing?</strong></p><br><p>Thanks!</p><br>
0.0,0.3333333333333333,0.0,0.0,1.0,0.3333333333333333,0.0,<h3>How do I add a custom 404 page for my Elastic Beanstalk application</h3><p>I have a php elastic beanstalk application set up and I want it to use my custom 404.html page if there is a 404 error. My proxy server is set to apache. Normally I would just define this line: ErrorDocument 404 /404.php in a .htaccess file or in the apache configs. I have tried putting the following in a apache.config in the .ebextensions directory:</p><br><pre><code>files:<br>  &quot;/etc/httpd/conf.d/update.conf&quot;:<br>    mode: &quot;000644&quot;<br>    owner: root<br>    group: root<br>    encoding: plain<br>    content: |<br>      ErrorDocument 404 /404.html<br></code></pre><br><p>This didn't seem to do anything. If anyone could advise please let me know.</p><br>
0.0,0.6666666666666666,0.0,0.0,1.0,0.0,0.0,<h3>When I tried to connect aws ec2 machine getting connection closed</h3><pre><code>$ ssh -v -i &quot;infra.pem&quot; ubuntu@13.233.128.254 -p 22<br>OpenSSH_8.3p1; OpenSSL 1.1.1g  21 Apr 2020<br>debug1: Reading configuration data /etc/ssh/ssh_config<br>debug1: Connecting to 13.233.128.254 [13.233.128.254] port 22.<br>debug1: Connection established.<br>load pubkey &quot;infra.pem&quot;: invalid format<br>debug1: identity file infra.pem type -1<br>debug1: identity file infra.pem-cert type -1<br>debug1: Local version string SSH-2.0-OpenSSH_8.3<br>debug1: Remote protocol version 2.0; remote software version OpenSSH_7.6p1 Ubuntu-4ubuntu0.3<br>debug1: match: OpenSSH_7.6p1 Ubuntu-4ubuntu0.3 pat OpenSSH_7.0*;OpenSSH_7.1*;OpenSSH_7.2*;OpenSSH_7.3*;OpenSSH_7.4*;OpenSSH_7.5*;OpenSSH_7.6*;OpenSSH_7.7* compat 0x04000002<br>debug1: Authenticating to 13.233.128.254:22 as 'ubuntu'<br>debug1: SSH2_MSG_KEXINIT sent<br>debug1: SSH2_MSG_KEXINIT received<br>debug1: kex: algorithm: curve25519-sha256<br>debug1: kex: host key algorithm: ecdsa-sha2-nistp256<br>debug1: kex: server-&gt;client cipher: chacha20-poly1305@openssh.com MAC: &lt;implicit&gt; compression: none<br>debug1: kex: client-&gt;server cipher: chacha20-poly1305@openssh.com MAC: &lt;implicit&gt; compression: none<br>debug1: expecting SSH2_MSG_KEX_ECDH_REPLY<br>debug1: Server host key: ecdsa-sha2-nistp256 SHA256:0NftNS5VQtaGmmOsUOZzxdYwnQ/hSwF2e7SDizNp6Ew<br>The authenticity of host '13.233.128.254 (13.233.128.254)' can't be established.<br>ECDSA key fingerprint is SHA256:0NftNS5VQtaGmmOsUOZzxdYwnQ/hSwF2e7SDizNp6Ew.<br>Are you sure you want to continue connecting (yes/no/[fingerprint])? yes<br>Warning: Permanently added '13.233.128.254' (ECDSA) to the list of known hosts.<br>debug1: rekey out after 134217728 blocks<br>debug1: SSH2_MSG_NEWKEYS sent<br>debug1: expecting SSH2_MSG_NEWKEYS<br>debug1: SSH2_MSG_NEWKEYS received<br>debug1: rekey in after 134217728 blocks<br>debug1: Will attempt key: infra.pem  explicit<br>debug1: SSH2_MSG_EXT_INFO received<br>debug1: kex_input_ext_info: server-sig-algs=&lt;ssh-ed25519;ssh-rsa;rsa-sha2-256;rsa-sha2-512;ssh-dss;ecdsa-sha2-nistp256;ecdsa-sha2-nistp384;ecdsa-sha2-nistp521&gt;<br>debug1: SSH2_MSG_SERVICE_ACCEPT received<br>debug1: Authentications that can continue: publickey<br>debug1: Next authentication method: publickey<br>debug1: Trying private key: infra.pem<br>Connection closed by 13.233.128.254 port 22<br><br><br></code></pre><br><p>''<br>While trying to connect the aws ec2 getting the above issue &amp; I have checked SG as well which contains allowed all traffic with 0.0.0.0/0&quot;.Plase help me to fix this issue.Thanks.<br>''</p><br>
0.0,0.0,0.6666666666666666,0.0,1.0,0.0,0.0,<h3>Your current user or role does not have access to Kubernetes objects on this EKS cluster - EKS</h3><p>I'm trying to create an EKS Cluster with Managed Node Group. I'm being thrown this error when the Managed node group gets created. <strong>Your current user or role does not have access to Kubernetes objects on this EKS cluster. This may be due to the current user or role not having Kubernetes RBAC permissions to describe cluster resources or not having an entry in the clusters auth config map.</strong>. I have setup all the necessary IAM roles and policies needed for the EKS Cluster to run. Any help will be appreciated.</p><br><p>EKS Version: 1.18</p><br>
0.0,0.0,1.0,0.0,0.3333333333333333,0.6666666666666666,0.0,<h3>AWS API gateway for K8s using Cognito with JWT</h3><p>I have AWS K8s cluster(EKS) and I want to use AWS API gateway to protect endpoints and separate authorization logic from microservices. I need to have 2 authentication schemas:</p><br><ol><br><li>Send login/password and get JWT</li><br><li>OAuth2</li><br></ol><br><p>There is an integration between API gateway and K8s cluster via <a href="https://aws.amazon.com/blogs/containers/api-gateway-as-an-ingress-controller-for-eks/" rel="nofollow noreferrer">ALB Ingress Controller</a>. It looks fine. Then I need to authenticate somehow. AWS provides Cognito as a service to manage users and the possibility to have your own identity provider. I know that we can integrate API gateway authorizer with Cognito; but I can't understand the following things:</p><br><ol><br><li>How to integrate Cognito with already existed LDAP for example? (SAML?)</li><br><li>Can I use my own already created OAuth2 authentication endpoint?</li><br><li>How Can I authenticate with login/password and retrieve JWT using API gateway+Cognito?</li><br></ol><br>
0.0,0.6666666666666666,0.0,0.0,0.0,1.0,0.0,<h3>aws api-gateway documentation with CDK issues</h3><p>I am attempting to document my API through CDK; but am encountering an error:</p><br><blockquote><br><p>Invalid Documentation version specified (Service: AmazonApiGateway;<br>Status Code: 400; Error Code: BadRequestException.</p><br></blockquote><br><p>The line number indicates that the error is wher I set documentationVersion in the _initApi() function.<br>The code is like this:</p><br><pre><code>  private _apiVersion  = &quot;v1&quot;;<br>  private _api:apiGateway.RestApi;<br>  private _auth:apiGateway.CfnAuthorizer;<br>  private _documentVersion:apiGateway.CfnDocumentationVersion;<br><br>  constructor (scope: cdk.Construct; id: string; props?: cdk.StackProps) {<br>    super(scope; id);<br><br>    this._initApi();<br>    this._initAuth();<br>    this._initValidators();<br>    this._initDoc();<br>  }<br><br><br>  private _initDoc() {<br>    this._documentVersion = new apiGateway.CfnDocumentationVersion(this; 'docVersion1'; {<br>      documentationVersion: this._apiVersion;<br>      restApiId: this._api.restApiId;<br>      description: 'this is a test of documentation'<br>    });<br>  }    <br><br> private _initApi()  {<br>    const api = new apiGateway.RestApi(this; this.TAG; <br>    { <br>      restApiName: this.TAG;<br>      description: 'xxxx';<br>      deploy: true;<br>      deployOptions: {<br>        // documentationVersion: this._documentVersion.documentationVersion;<br>        // documentationVersion: this._apiVersion;<br>        documentationVersion: 'v1';<br>        loggingLevel: apiGateway.MethodLoggingLevel.INFO;<br>        dataTraceEnabled: true<br>      };<br>     ...<br></code></pre><br><p>inside my _addMethod function:</p><br><pre><code>public addMethod(resource: apiGateway.Resource; method: string; lambda: lambda.IFunction; options?: any) <br> {<br>   ...<br>   this._documentMethod(resource; method; 'test description');<br>   ...<br> }<br><br> private _documentMethod(resource: apiGateway.Resource; method: string; description: string) {<br>    const docpart = new apiGateway.CfnDocumentationPart(this; resource.path + '-' + method; {<br>      location: {<br>        type: 'METHOD';<br>        method: method;<br>        path: resource.path<br>        // path: books.path<br>      };<br>      properties: JSON.stringify({<br>        &quot;status&quot;: &quot;successful&quot;;<br>        &quot;code&quot;: 200;<br>        &quot;message&quot;: &quot;Get method was succcessful&quot;<br>      });<br>      restApiId: this._api.restApiId<br>    });<br>    this._documentVersion.addDependsOn(docpart);<br>  }<br></code></pre><br>
0.0,1.0,0.5,0.0,0.0,0.5,0.0,<h3>AWS replace EC2 with CloudFront</h3><p>I have a domain <code>dev-www.myexample.de</code> hosted via AWS EC2 and a Load Balancer. I want to replace this configuration with an S3 bucket + CloudFront and keep same domain name.</p><br><p>The only possible way I could find was destroying my EC2 instance and Route 53 and then deploy my CloudFront and domain and this works fine.</p><br><p>However; I want to show a <code>503</code> page for all traffic that comes while my EC2 instance is down and before my CloudFront and S3 bucket are ready to use. Any idea how I can do that?</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Create alert whenever a new resource is created without tagging AWS Cloud using promethus</h3><p>Can any one suggest any approach or reference link how to create alert in prometheus whenevera new resource is created without tagging in aws cloud.</p><br>
0.0,0.0,0.0,0.0,1.0,0.6666666666666666,0.0,<h3>How to run .py files as cronjob in aws?</h3><p>I have a Django application which it's deployed to Amazon Elastic Beanstalk(Python 3.7 running on 64bit Amazon Linux 2/3.1.1). I have been trying to run a .py file as a <code>cronjob</code> that works at 4 a.m every day in AWS and I have created a <code>.config</code> file into <code>.ebextensions</code> for that such as below.</p><br><p><strong>cron.config file:</strong></p><br><pre><code>files:<br>    &quot;/etc/cron.d/cron_process&quot;:<br>        mode: &quot;000644&quot;<br>        owner: root<br>        group: root<br>        content: |<br>            0 4 * * * root /usr/local/bin/task_process.sh<br><br>    &quot;/usr/local/bin/task_process.sh&quot;:<br>        mode: &quot;000755&quot;<br>        owner: root<br>        group: root<br>        content: |<br>            #!/bin/bash<br><br>            date &gt; /tmp/date<br>            source /var/app/venv/staging-LQM1lest/bin/activate<br>            cd /var/app/current<br>            python Bot/run_spiders.py<br>            exit 0<br><br>commands:<br>    remove_old_cron:<br>        command: &quot;rm -f /etc/cron.d/cron_process.bak&quot;<br></code></pre><br><p><strong>run_spiders.py file:</strong></p><br><pre><code>from first_bot.start import startallSpiders<br>from .models import Spiders<br>import importlib<br><br>test = Spiders.objects.get(id=1)<br>test.spider_name = &quot;nigdehalk&quot;<br>test.save()<br></code></pre><br><p>I'm trying to change an attribute of one of my objects for testing but it didn't work. Am I missing something? How can I create this cronjob?</p><br>
0.0,1.0,0.0,0.0,1.0,0.0,0.0,<h3>Lambda Edge Origin Request Trigger 502</h3><p>I'm getting started with Amazon AWS and integrating JAMStack in their system; following <a href="https://pages.awscloud.com/Deploying-JAMStack-Applications-Using-Amazon-S3-CloudFront-and-LambdaEdge_2020_0515-NET_OD.html" rel="nofollow noreferrer">their official tutorial</a> (which includes <a href="https://www.youtube.com/watch?v=pFean2aj_8I" rel="nofollow noreferrer">this video</a>) of a example project.</p><br><p>Everything went smoothly until the creation of a Lambda@Edge Function.</p><br><p>I just created the function and set to trigger at Origin Request from CloudFront. It's just this:</p><br><pre><code>exports.handler = async (event) =&gt; {<br>    const REQUEST = event.Records[0].cf.request;<br>    const URI = REQUEST.uri;<br><br>    return REQUEST;<br>};<br></code></pre><br><p>When I go to the root my CloudFront domain; I get my &quot;Hello World&quot; from index.html; which is in a Bucket. However when I go to subpaths; like /404 or non-existing path I get 502 and not 404.html from /404 nor 404.html for non-existing pages.</p><br><p>The role is &quot;Basic Lambda@Edge permissions (for CloudFront trigger)&quot;.</p><br><hr /><br><p>I get this:</p><br><blockquote><br><p>502 ERROR The request could not be satisfied. The Lambda function<br>result failed validation: The body is not a string; is not an object;<br>or exceeds the maximum size. See Limits in the Amazon CloudFront<br>Developer Guide. We can't connect to the server for this app or<br>website at this time. There might be too much traffic or a<br>configuration error. Try again later; or contact the app or website<br>owner. If you provide content to customers through CloudFront; you can<br>find steps to troubleshoot and help prevent this error by reviewing<br>the CloudFront documentation.</p><br></blockquote><br>
0.0,0.0,1.0,0.0,1.0,0.0,0.0,<h3>How to monitor the JVM of containers and alert on single container usage on AWS?</h3><p>I've been playing around with a SpringBoot application; the Micrometer faade; the Statsd implementation for Micrometer and the AWS OpenTelemetry distro deployed on ECS/Fargate. So far; I've been able to export many different metrics (JVM; tomcat; data source; etc) to CloudWatch; adding the cluster name; the service name and the task ID as dimensions.</p><br><p>My problem now is that I don't know how to handle that information. In a production deployment I may have more than one container and I may need to scale them out/in. This makes impossible (or at least I don't know how to do it) to create a dashboard as I need to select the task IDs up front. Another problem is that there is no way to add a filter in the dashboard that just shows the list of available task IDs so I can select the one I want to monitor at that moment to remove the noise from the other ones. Something QuickSight can do.</p><br><p>Am I better off just moving to something like Prometheus/Grafana for this? How do people handle monitoring of containers; specially Java applications?</p><br><p>AWS gives you the option to alarm you based on ECS metrics but at the service level (so I guess either based on the average or max of CPU usage for example) but that isn't enough when you have a workload that is not evenly spread across your instances. Is alerting not possible when at the container level (something like alert me when the service is at 60% CPU or a single container is at 80% for example)?</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>How to connect AWS Glue Job to Salesforce</h3><p>I am following questions like <a href="https://stackoverflow.com/questions/58205999/how-can-i-use-an-external-python-library-in-aws-glue">this</a>  and blogs like t<a href="https://docs.aws.amazon.com/glue/latest/dg/reduced-start-times-spark-etl-jobs.html" rel="nofollow noreferrer">his one</a>  but I cannot make the connection work due to (I think) library versions conflict.</p><br><p>I want to use <a href="https://pypi.org/project/simple-salesforce/#files" rel="nofollow noreferrer">this library</a>; which afaik is the most used and referenced one; to connect from a aws glue job to Salesforce. This code works on my local machine; but on glue I get the following message when I use &quot;python shell&quot; configuration: <code>ERROR: botocore 1.12.232 has requirement urllib3&lt;1.26;&gt;=1.20; python_version &gt;= &quot;3.4&quot;; but you'll have urllib3 1.26.2 which is incompatible.</code></p><br><p>Or; if I use &quot;spark&quot; option: <code>Traceback (most recent call last): File &quot;/tmp/bp-etl-crm-sparkV2&quot;; line 1; in &lt;module&gt; from simple_salesforce import Salesforce File &quot;/tmp/simple_salesforce-1.10.1-py2.py3-none-any.whl/simple_salesforce/__init__.py&quot;; line 4; in &lt;module&gt; from .api import Salesforce; SFType File &quot;/tmp/simple_salesforce-1.10.1-py2.py3-none-any.whl/simple_salesforce/api.py&quot;; line 18; in &lt;module&gt; from .login import SalesforceLogin File &quot;/tmp/simple_salesforce-1.10.1-py2.py3-none-any.whl/simple_salesforce/login.py&quot;; line 16; in &lt;module&gt; from authlib.jose import jwt ModuleNotFoundError: No module named 'authlib'</code></p><br><p>The code is as simple as a connection and a query; which again; I've tested and the credentials and connection do work when I try from a local console and not aws glue:</p><br><pre><code>from simple_salesforce import Salesforce<br><br>def main():<br>    print(&quot;INIT&quot;)<br>    sf = Salesforce(username='username'; password='pw'; security_token='securitytoken'; domain='test')<br>    res_bulk = sf.bulk.Account.query('SELECT Id; Name FROM Table')<br>    print(res_bulk)<br><br>if __name__ == &quot;__main__&quot;:<br>    main()<br></code></pre><br><p>What have I tried so far:</p><br><ul><br><li><p>As I said; I have tried to both configure the job as python shell; with Glue 1.0; and Spark with Glue 2.0. Both fail due to dependencies problems.</p><br></li><br><li><p>I have tried downgrading the simple-salesforce version. So far none have worked; it keeps throwing <code>ERROR: botocore 1.12.232 has requirement urllib3&lt;1.26;&gt;=1.20; python_version &gt;= &quot;3.4&quot;; but you'll have urllib3 1.26.2 which is incompatible.</code></p><br></li><br><li><p>I have tried getting urllib version lower than 1.26.2; uploading it to S3; and adding it to the list of libraries to be used by my code. This has not worked so far; but I am not sure why; since I do not know what does Glue do when ordered to use a certain version of a library it is designed to use regardless of what you do; like urllib.</p><br></li><br></ul><br><p>Any ideas as to what could I be doing wrong; or what else could I try to make it work.</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>When updating cloudformation stack validationissue is coming</h3><p>When trying to update cloudformation stack below issue is coming. I found this error in cloudtrail logs.</p><br><p>&quot;errorCode&quot;: &quot;ValidationException&quot;;<br>&quot;errorMessage&quot;: &quot;No updates are to be performed.&quot;</p><br><p>what could be the reason ? i am trying to update parameters in CFT stck</p><br>
0.0,0.6666666666666666,0.3333333333333333,0.0,0.0,0.3333333333333333,0.0,<h3>How to enable CORS with AWS SAM</h3><p>I'm trying to enable CORS in my AWS SAM app. Here is the snippet from my <code>template.yaml</code>:</p><br><pre><code>Globals:<br>  Api:<br>    Cors:<br>      AllowMethods: &quot;'*'&quot;<br>      AllowHeaders: &quot;'*'&quot;<br>      AllowOrigin: &quot;'*'&quot;<br><br>Resources:<br>  MyApi:<br>    Type: AWS::Serverless::Api<br>    Properties:<br>      StageName: prod<br>      Auth:<br>        Authorizers:<br>          MyCognitoAuthorizer: ...<br><br>  getByIdFunc:<br>    Type: AWS::Serverless::Function<br>    Properties:<br>      Handler: src/handler.handle<br>      Events:<br>        ApiEvent:<br>          Type: Api<br>          Properties:<br>            Path: /{id}<br>            Method: GET<br>            RestApiId: !Ref MyApi<br></code></pre><br><p>According to this <a href="https://stackoverflow.com/questions/50229563/using-cors-with-aws-sam">Using CORS with AWS SAM</a> and that <a href="https://github.com/aws/serverless-application-model/issues/373" rel="nofollow noreferrer">https://github.com/aws/serverless-application-model/issues/373</a>; the cors config should work but unfortunately no header is set on the API response; as seen below.</p><br><pre><code>&lt; HTTP/2 200 <br>&lt; content-type: application/json<br>&lt; content-length: 770<br>&lt; date: Tue; 13 Apr 2021 19:55:31 GMT<br>&lt; x-amzn-requestid: ...<br>&lt; x-amz-apigw-id: ...<br>&lt; x-amzn-trace-id: Root=1-...-...;Sampled=0<br>&lt; x-cache: Miss from cloudfront<br>&lt; via: 1.1 ...cloudfront.net (CloudFront)<br>&lt; x-amz-cf-pop: FRA2-C2<br>&lt; x-amz-cf-id: ...==<br>&lt; <br>* Connection #0 to host ....execute-api.eu-central-1.amazonaws.com left intact<br>[{&quot;model&quot;: ..}]<br></code></pre><br><p>I also tried adding the cors config to the API definition (MyApi) itself like its stated in the <a href="https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-property-api-corsconfiguration.html" rel="nofollow noreferrer">offical docs here</a>; but without success.</p><br><p>I could add the header in the response by myself but i rather have it in the template file.</p><br>
0.0,0.3333333333333333,0.3333333333333333,0.0,0.0,0.3333333333333333,0.0,<h3>Invalid policy document. Policy syntax is wrong</h3><p>I have a rest API resource as such;</p><br><pre><code>TempApi:<br>    Type: AWS::ApiGateway::RestApi<br>    Properties:<br>      Name: !Sub ${Environment}-temp-api<br>      EndpointConfiguration:<br>        Types:<br>          - PRIVATE<br>        VpcEndpointIds:<br>          - vpce-0cfefxxxxxxxxxxxx<br>      Policy: !Sub |<br>        {<br>          &quot;Version&quot;: &quot;2012-10-17&quot;;<br>          &quot;Statement&quot;: [<br>            {<br>              &quot;Effect&quot;: &quot;Allow&quot;<br>              &quot;Principal&quot;: &quot;*&quot;<br>              &quot;Action&quot;: &quot;execute-api:Invoke&quot;<br>              &quot;Resource&quot;: &quot;execute-api:/*&quot;<br>            };<br>            {<br>              &quot;Effect&quot;: &quot;Deny&quot;<br>              &quot;Principal&quot;: &quot;*&quot;<br>              &quot;Action&quot;: &quot;execute-api:Invoke&quot;<br>              &quot;Resource&quot;: &quot;execute-api:/*&quot;<br>              &quot;Condition&quot;: {<br>                &quot;StringNotEquals&quot;: {<br>                  &quot;aws:sourceVpce&quot;: !FindInMap [Environments; !Ref Environment; VPCEndpointAPI]<br>                }<br>              }<br>            }<br>          ]<br>        }<br></code></pre><br><p>Upon deploying I receive the following error:</p><br><pre><code>Invalid policy document. Please check the policy syntax and ensure that Principals are valid.<br>(Service: AmazonApiGateway; Status Code: 400; Error Code: BadRequestException)<br></code></pre><br><p>Any help in identifying what's wrong with the policy document will be greatly appreciated.</p><br><p>Thanks;</p><br><p>Paras</p><br>
0.0,1.0,0.6666666666666666,0.0,0.0,0.6666666666666666,0.0,<h3>How to destroy the additional tgw route table created by terraform transit-gateway module</h3><p>I have created tgw using the official transit-gateway module and I am using the default route table; Iam also seeing that the module has created an additional route table which I am not able to remove via tf code.</p><br><p><div class="snippet" data-lang="js" data-hide="false" data-console="true" data-babel="false"><br><br><div class="snippet-code"><br><br><pre class="snippet-code-html lang-html prettyprint-override"><code>module "transit-gateway" {<br>  source          = "terraform-aws-modules/transit-gateway/aws"<br>  version         = "1.4.0"<br>  name            = var.tgw<br>  amazon_side_asn = 64532<br><br>  enable_auto_accept_shared_attachments = true<br>  vpc_attachments = {<br>    vpc = {<br>      vpc_id                                          = module.vpc.vpc_id<br>      subnet_ids                                      = [module.vpc.private_subnets[0]]<br>      dns_support                                     = true<br>      ipv6_support                                    = false<br>      transit_gateway_default_route_table_association = true<br>      transit_gateway_default_route_table_propagation = true<br>    }<br>  }<br><br>  ram_allow_external_principals = true<br>  ram_principals                = [123456789; 0987654321]<br><br>  tags = {<br>    Environment = "${var.env}"<br>    Automated   = "Terraform"<br>    Owner       = "${var.owner}"<br>    Project     = "${var.project}"<br>  }<br>}</code></pre><br><br></div><br><br></div><br><br></p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>AWS Glue Python Job not creating new Data Catalog partitions</h3><p>I created a AWS Glue Job using Glue Studio.<br>It takes data from a Glue Data Catalog; does some transformations; and writes to a different Data Catalog.</p><br><p>When configuring the target node; I enabled the option to create new partitions after running:</p><br><p><a href="https://i.stack.imgur.com/g7X5e.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/g7X5e.png" alt="enter image description here" /></a></p><br><p>The job runs successfully; data is written to S3 with proper partition folder structure; but no new partitions are created in the actual Data Catalog table - I still have to run a Glue Crawler to create them.</p><br><p>The code in the generated script that is responsible for partition creation is this (last two lines of the job):</p><br><pre><code>DataSink0 = glueContext.write_dynamic_frame.from_catalog(frame = Transform4; database = &quot;tick_test&quot;; table_name = &quot;test_obj&quot;; transformation_ctx = &quot;DataSink0&quot;; additional_options = {&quot;updateBehavior&quot;:&quot;LOG&quot;;&quot;partitionKeys&quot;:[&quot;date&quot;;&quot;provider&quot;];&quot;enableUpdateCatalog&quot;:True})<br>job.commit()<br></code></pre><br><p>What am I doing wrong? Why are new partitions not being created? How do I avoid having to run a crawler to have the data available in Athena?</p><br><p>I am using Glue 2.0 - PySpark 2.4</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.3333333333333333,<h3>How to set the from number on AWS SNS (Using PHP v3 SDK)</h3><h1>Setup</h1><br><p>I've download the <a href="https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/getting-started_installation.html" rel="nofollow noreferrer">aws.zip file for AWS V3 SDK for PHP</a> and extracted it into my project's folder and connected it's auto-loader into my auto-loader. I've also gone though all of the steps to create an AWS account; along with setting up SNS including acquiring a phone number to use. All of the examples work has shown in the sanity check.</p><br><p>There is a little dark corner that you should know about. You need to create a <code>credentials</code> file in your <code>~/.aws/</code> directory. If you are using this from a php-fpm context; that home directory might be your <code>/var/www/</code> directory so you should put your credential files under <code>/var/www/.aws/</code>.</p><br><p>More information on the configuration file can be found here ... <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html" rel="nofollow noreferrer">AWS Command Line Interface - Configuration and credential file settings</a>.</p><br><h1>Sanity Check</h1><br><p>Following the examples from the <a href="https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/sns-examples-sending-sms.html#publish-to-a-text-message-sms-message" rel="nofollow noreferrer">AWS SNS Documentation - Publish to a Text Message (SMS Message)</a>.</p><br><p><strong>Test-AWS-SNS.php</strong></p><br><pre><code>require 'vendor/autoload.php';<br><br>use Aws\Sns\SnsClient; <br>use Aws\Exception\AwsException;<br><br>$SnSclient = new SnsClient([<br>    'profile' =&gt; 'default';<br>    'region' =&gt; 'us-east-1';<br>    'version' =&gt; '2010-03-31'<br>]);<br><br>$message = 'This message is sent from a Amazon SNS code sample.';<br>$phone = '+1AAALLL####';<br><br>try {<br>    $result = $SnSclient-&gt;publish([<br>        'Message' =&gt; $message;<br>        'PhoneNumber' =&gt; $phone;<br>    ]);<br>    var_dump($result);<br>} catch (AwsException $e) {<br>    // output error message if fails<br>    error_log($e-&gt;getMessage());<br>} <br></code></pre><br><p>(Notes: <code>+1AAALLL####</code> actually points to my cell phone number; but I'm obviously not going to put that here.)</p><br><h1>The Problem</h1><br><p>The problem is that there is no documentation for how to send a text message from one of the numbers that I own from AWS' SNS Long Code list. It always comes from the first phone number in the list and I can't find any documentation on how to change the number to another one. Some help here would be great.</p><br><h1>Prior research</h1><br><p>Obviously; I've been all over the documentation and searched Stack Overflow as well.<br><a href="https://stackoverflow.com/questions/46568343/aws-sns-invalid-parameter-phone-number">This one is pretty close; as it sets a SenderID</a>. Their documentation<br><a href="https://docs.aws.amazon.com/sns/latest/dg/sms_publish-to-phone.html" rel="nofollow noreferrer">mentions how to do it as an optional number 9 next to the sender ID from above</a><br>But I do know that it is possible; because they added the feature on <a href="https://aws.amazon.com/about-aws/whats-new/2020/10/amazon-sns-now-supports-selecting-the-origination-number-when-sending-sms-messages/" rel="nofollow noreferrer">Oct 23; 2020 - Amazon SNS now supports selecting the origination number when sending SMS messages</a> -- They just haven't put any code samples.</p><br><p>Help me Obi-Wan Kenobi; you're my only hope.</p><br>
0.0,0.0,0.0,0.5,0.0,1.0,0.0,<h3>Host React js page (with API) on AWS s3 bucket</h3><p>I am pretty new to AWS but have been learning a lot. I am trying to host a full stack React js application on AWS. I hosted my MySQL database on an RDS instance and hosted my Node / express API server on Elastic beanstalk. When I run my react app on the console; these all work correctly and communicate as intended. I ran npm build on my react app and added the build files to an s3 bucket; but when I click the object URL; a blank page renders and the console is full of 400; 403; and 404 errors. I have only been able to find documentation on how to upload a static react app to an s3 bucket. However; I am wondering if there is something different I must do since my react app has fetch requests to my elastic beanstalk server. Possibly some sort of ports / traffic configuration? However; I doubt this is the issue because the basic elements in my render method are not even showing up. I am unable to find an answer. Does anyone have any ideas? Thanks so much!!</p><br>
0.0,0.0,0.5,0.0,1.0,1.0,0.0,<h3>InvalidSignatureException: Credential should be scoped to correct service: execute-api</h3><p>i am implementing an AWS Lambda Function (call it the frontend function) call that calls another AWS Lambda function (call it the backend function) from its code. I use serverless. I used serverless.yml to add an event path and method for my function; using an existing API i created for the frontend function. The frontend function works fine. Calling the backend f() only works locally using serverless-offline; with the local URL given by SLS-Offline (http://localhost:3002/). When i replace that url with the AWS endpoint url for the backend f(); it fails with this error: &quot;InvalidSignatureException: Credential should be scoped to correct service: 'execute-api'&quot;. The functions have auth=none. I've added all the possible permissions to both functions; doesn't help. Is there a place where i can specify the &quot;correct service&quot; when i invoke my backend f()? I couldn't find any documentation on this. I am using Slack's Bolt JS SDK and my code is in JS. Here is my invocation code; inside of my frontend handler:</p><br><pre><code>module.exports.handler = (event; context) =&gt; {<br>  console.log(&quot; Bolt app is running!&quot;);<br>  awsServerlessExpress.proxy(server; event; context);<br><br>const lambdaBackend = new Lambda({<br>  apiVersion: &quot;latest&quot;;<br>  region: &quot;us-east-1&quot;;<br>  // endpoint needs to be set only if it deviates from the default; e.g. in a dev environment<br>  // process.env.SOME_VARIABLE could be set in e.g. serverless.yml for provider.environment or function.environment<br>  endpoint: lambda2_api_url<br>  <br>});<br><br>  const params = {<br>          InvocationType: 'Event'; // async invocation<br>          FunctionName: 'serverless-function-dev-backend';<br>          Payload: JSON.stringify(event)<br>        };<br>  lambdaBackend.invoke(params).promise();<br><br><br>};<br></code></pre><br><p>Any guidance is greatly appreciated.</p><br>
0.0,0.0,1.0,0.0,0.0,0.6666666666666666,0.0,<h3>Unable to get user attributes in the cognitoUser object after authenticating using aws-amplify</h3><p>I have an Angular 11 project in which I'm using AWS Amplify (<a href="https://www.npmjs.com/package/aws-amplify" rel="nofollow noreferrer">aws-amplify</a> v3.3.26) and I'm using Cognito user pools to manage users. I've set up the hosted UI and I've not used any custom attributes. Everything is working and I've selected all the attributes for read and write permissions in the user attributes in the user pool.</p><br><p>I'm using the following code to get the cognitoUser object:-</p><br><pre><code>Auth.currentAuthenticatedUser()<br>      .then((currentUser) =&gt; {<br>        console.log('The currentUser =&gt; '; currentUser);<br>      <br>        Auth.currentUserInfo()<br>          .then((res) =&gt; {<br>            console.log('Here are the current user info! =&gt;'; res);<br>          })<br>          .catch((err) =&gt; {<br>            console.log('Current user info failed to fetch'; err);<br>          });<br>       <br>      })<br>      .catch((err) =&gt; {<br>        console.log(err);<br>      });<br>    return;<br>  }<br></code></pre><br><p>According to the <a href="https://docs.amplify.aws/lib/auth/manageusers/q/platform/js#retrieve-attributes-for-current-authenticated-user" rel="nofollow noreferrer">AWS Amplify docs</a> I should be able to get the attributes right within the cognitoUser object. But I'm not getting it.</p><br><p>This is all I get (also note that session is null; not sure what that means):-<br><a href="https://i.stack.imgur.com/b6CMT.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/b6CMT.png" alt="enter image description here" /></a></p><br><p>As you can see there is no &quot;attributes&quot; key in the object.</p><br><p>I also noticed that there is a <code>Auth.currentUserInfo()</code> method that is available and my console log of that just results in an empty object.</p><br><p>And as you can see here; I have set all the required permissions for all the attributes in the user pool client definition and I don't have any custom attributes:-<br><a href="https://i.stack.imgur.com/BL48P.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/BL48P.png" alt="enter image description here" /></a></p><br><p>Currently I am storing name and email as required attributes and all users have that. But I can't access it. I was able to see the email deeply nested within the cognitoUser object (<code>cognitoUser.signInUserSession.idToken.payload.email</code>) but still can't find the name attribute. I would like to get the entire attributes object listing all available attributes of the current user.</p><br><p>What am I missing?</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,1.0,<h3>How to stream a S3 bucket video in node.js</h3><p>Hy ;I'm using the direct s3 bucket object video url to access videos in my application ;I'm using node.js as a server language ;but instead of streaming the video its downloading the video which is causing a lag; how can I stream the video instead of downloading it ;please help ....<br>thanks in advance</p><br>
0.0,0.0,0.0,0.0,0.0,1.0,0.6666666666666666,<h3>amazon-ivs-player with Angular</h3><p>I am trying to use the npm version of the amazon-ivs-player with Angular.</p><br><p>It seems to have been designed for use with webpack.. but of course I am stuck with the angular-cli.</p><br><p>I am following this example:<br><a href="https://github.com/aws-samples/amazon-ivs-player-web-sample/blob/master/samples/npm-sdk/npm-sdk.ts" rel="nofollow noreferrer">https://github.com/aws-samples/amazon-ivs-player-web-sample/blob/master/samples/npm-sdk/npm-sdk.ts</a></p><br><p>Angular-cli does not seem to like the two import statement for the wasm files:<br>import wasmBinaryPath from 'amazon-ivs-player/dist/assets/amazon-ivs-wasmworker.min.wasm'<br>import wasmWorkerPath from 'amazon-ivs-player/dist/assets/amazon-ivs-wasmworker.min.js';</p><br><p>I get these errors when trying to compile the application:</p><br><pre><code>ERROR in ./src/app/components/player2/video-player.component.ts 48:43-57<br>&quot;export 'default' (imported as 'wasmBinaryPath') was not found in 'amazon-ivs-player/dist/assets/amazon-ivs-wasmworker.min.wasm'<br>ERROR in ./node_modules/amazon-ivs-player/dist/assets/amazon-ivs-wasmworker.min.wasm<br>Module not found: Error: Can't resolve 'a' in 'C:\Users\james\demo\ui\node_modules\amazon-ivs-player\dist\assets'<br><br>ERROR in ./node_modules/amazon-ivs-player/dist/assets/amazon-ivs-wasmworker.min.wasm<br>WebAssembly module is included in initial chunk.<br>This is not allowed; because WebAssembly download and compilation must happen asynchronous.<br>Add an async splitpoint (i. e. import()) somewhere between your entrypoint and the WebAssembly module:<br>multi (webpack)-dev-server/client?http://0.0.0.0:0/sockjs-node&amp;sockPath=/sockjs-node ./src/main.ts --&gt; ./src/main.ts --&gt; ./src/app/app.module.ts --&gt; ./src/app/components/player2/video-player.component.ts --&gt; ./node_modules/amazon-ivs-player/dist/assets/amazon-ivs-wasmworker.min.wasm<br></code></pre><br><p>Is there anyone that could shed any light on this?</p><br><p>Maybe there is another way to use wasm with the Angular framework?</p><br><p>If I cant use npm then maybe I have to use the script tags but then I dont think I can interact with the Angular code in the application.</p><br><p>Kind regards<br>J</p><br><pre><code>// vjs-player.component.ts<br>import { Component; ElementRef; Input; OnDestroy; OnInit; ViewChild; ViewEncapsulation } from '@angular/core';<br>import videojs from 'video.js';<br><br>import {<br>    registerIVSTech;<br>    registerIVSQualityPlugin;<br>    VideoJSQualityPlugin;<br>    VideoJSIVSTech;<br>    VideoJSEvents;<br>    create;<br>    ErrorType;<br>    isPlayerSupported;<br>    MediaPlayer;<br>    PlayerError;<br>    PlayerEventType;<br>    PlayerState;<br>    Quality;<br>    TextCue;<br>    TextMetadataCue<br>} from 'amazon-ivs-player';<br><br>// We use the TypeScript compiler (TSC) to check types; it doesn't know what this WASM module is; so let's ignore the error it throws (TS2307).<br>// @ts-ignore<br>import wasmBinaryPath from 'amazon-ivs-player/dist/assets/amazon-ivs-wasmworker.min.wasm';<br>import wasmWorkerPath from 'amazon-ivs-player/dist/assets/amazon-ivs-wasmworker.min.js';<br><br><br>@Component({<br>    selector: 'app-vjs-player';<br>    template: `<br>  &lt;video #target class=&quot;video-js&quot; width=960 height=540 controls muted playsinline preload=&quot;none&quot;&gt;&lt;/video&gt;`;<br>    styleUrls: [<br>        './video-player.component.css'<br>    ];<br>    encapsulation: ViewEncapsulation.None;<br>})<br>export class VideoPlayerComponent implements OnInit; OnDestroy {<br>    @ViewChild('target'; { static: true }) target: ElementRef;<br>    // see options: https://github.com/videojs/video.js/blob/maintutorial-options.html<br>    @Input() options: {<br>        fluid: boolean;<br>        aspectRatio: string;<br>        autoplay: boolean;<br>        sources: {<br>            src: string;<br>            type: string;<br>        }[];<br>    };<br>    player: videojs.Player;<br>    url: string;<br>        constructor(<br>        private elementRef: ElementRef;<br>    ) { }<br><br><br>    ngOnInit() {<br>    }<br><br>    ngAfterViewInit() {<br>        const createAbsolutePath = (assetPath: string) =&gt; new URL(assetPath; document.URL).toString();<br>        <br>        const player = videojs('videojs-player'; {<br>            techOrder: [&quot;AmazonIVS&quot;]<br>        });<br>        //registerIVSTech(videojs; options);<br><br>        // Set up IVS playback tech and quality plugin<br>        registerIVSTech(videojs; {<br>            wasmWorker: createAbsolutePath(wasmWorkerPath);<br>            wasmBinary: createAbsolutePath(wasmBinaryPath);<br>        });<br>        registerIVSQualityPlugin(videojs);<br><br>    }<br><br>    ngOnDestroy() {<br>    }<br>}<br></code></pre><br><p>Any pointers really appreciated.</p><br>
0.0,0.0,0.6666666666666666,0.0,0.6666666666666666,0.0,0.0,<h3>AWS lambda rollback while publishing</h3><p>i am a Lambda Learner using c# and visualstudio (aws toolkit)</p><br><p>when i try to publish a simple api(lambda application c# without test - webapi)<br>i am getting this below error</p><br><pre><code>2020-12-05 00:31:08 UTC+0530    Lambdaapp2  ROLLBACK_COMPLETE   -<br>2020-12-05 00:31:07 UTC+0530    AspNetCoreFunctionRole  DELETE_COMPLETE -<br>2020-12-05 00:31:06 UTC+0530    Bucket  DELETE_COMPLETE -<br>2020-12-05 00:31:06 UTC+0530    AspNetCoreFunctionRole  DELETE_IN_PROGRESS  -<br>2020-12-05 00:30:57 UTC+0530    Lambdaapp2  ROLLBACK_IN_PROGRESS    The following resource(s) failed to create: [AspNetCoreFunctionRole; Bucket]. Rollback requested by user.<br>2020-12-05 00:30:56 UTC+0530    AspNetCoreFunctionRole  CREATE_FAILED   Resource creation cancelled<br>2020-12-05 00:30:56 UTC+0530    Bucket  CREATE_FAILED   lamaws already exists<br>2020-12-05 00:30:56 UTC+0530    AspNetCoreFunctionRole  CREATE_IN_PROGRESS  Resource creation Initiated<br>2020-12-05 00:30:55 UTC+0530    AspNetCoreFunctionRole  CREATE_IN_PROGRESS  -<br>2020-12-05 00:30:55 UTC+0530    Bucket  CREATE_IN_PROGRESS  -<br>2020-12-05 00:30:52 UTC+0530    Lambdaapp2  CREATE_IN_PROGRESS  User Initiated<br>2020-12-05 00:30:45 UTC+0530    Lambdaapp2  REVIEW_IN_PROGRESS  User Initiated<br></code></pre><br>
0.0,1.0,0.0,1.0,0.0,0.6666666666666666,0.0,<h3>AWS CloudFront signed cookie - change filename</h3><p>I use AWS CloudFront signed cookies to download files from S3 in a secure way. In S3 bucket I've got objects with keys as UUID (instead of default filename). Due to that; when I download file; it has UUID in a name (wihtout extension).</p><br><p>Is it possible to change that name during download or during generating cookis details?<br>I'm genereting it using AWS SDK for Java (CloudFrontCookieSigner.CookiesForCannedPolicy).</p><br>
0.0,1.0,0.0,0.0,0.0,0.0,0.0,<h3>How do I ignore cookies for some Cloudfront files?</h3><p>Cloudfront is caching every file per session cookie. Which means a slow initial load.</p><br><p>Is there any way to tell cloudfront to cache some files for all customers/sessions; e.g. images; css files?</p><br><p>Is there a header that says &quot;cache-for-all&quot; or some behaviour that says &quot;cache these file types for all&quot;?</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,1.0,0.0,<h3>Suppress AWS CDK output to stdout</h3><p>When I run <code>cdk deploy *</code> the process prints to console something like that:</p><br><blockquote><br><p>  PrefixRandomMainStackPrefixRandomCmkStackB7461D3C<br>(Prefix-Random-Cmk-Stack) (no changes)</p><br><p>Outputs:<br>PrefixRandomMainStackPrefixRandomCmkStackB7461D3C.ExportsOutputFnGetAttPrefixRandomCmk6E87F415ArnD999CC18<br>= arn:aws:kms:some-region-2:12345678910:key/123-123-123-123-9aadccac4deb</p><br><p>Stack ARN: PrefixRandomMainStackPrefixRandomRandomApiStackD1546C35<br>(Prefix-Random-RandomApi-Stack)<br>PrefixRandomMainStackPrefixRandomRandomApiStackD1546C35<br>(Prefix-Random-RandomApi-Stack): deploying...</p><br><p>  PrefixRandomMainStackPrefixRandomRandomApiStackD1546C35<br>(Prefix-Random-RandomApi-Stack) (no changes)</p><br></blockquote><br><p>etc.</p><br><p><em><strong>Is there a way to suppress all this output?</strong></em></p><br>
0.0,0.0,0.0,1.0,0.0,1.0,0.0,<h3>S3 file upload with presigned url returns status (canceled)</h3><p>I am trying to upload a file to Amazon S3 using the angular client. I have generated Presigned URL using the NodeJs application server. While uploading a file to presigned URL; but it fails to upload a file from the client; gets (canceled).</p><br><p>I tried to upload file in formats: <strong>buffer</strong>; <strong>base64</strong>; <strong>formData</strong> and raw <strong>File</strong></p><br><p>This works if I try uploading with the postman to the generated URL on binary form</p><br><p>Generating <strong>Presigned URL</strong> using <strong>NodeJs</strong></p><br><pre><code>      const s3 = new AWS.S3({<br>         accessKeyId: AWS_ACCESS_KEY;<br>         secretAccessKey: AWS_SECRET_ACCESS_KEY;<br>         region: 'eu-central-1';<br>      });<br>      const signedUrlExpireSeconds = 60 * 5;<br>      const presignedS3Url = s3.getSignedUrl('putObject'; {<br>         Bucket: process.env.bucket;<br>         Key: './test3Public.pdf';<br>         Expires: signedUrlExpireSeconds;<br>         ACL: 'public-read';<br>         ContentType: 'application/pdf';<br>      });<br></code></pre><br><p>HTML</p><br><pre><code>&lt;input<br>      type=&quot;file&quot;<br>      (change)=&quot;onFileSelected($event)&quot;<br>      accept=&quot;application/pdf; .docx&quot;<br>   /&gt;<br></code></pre><br><p>Component.ts</p><br><pre><code>onFileSelected(event: any) {<br>      this.fileToUpload = &lt;File&gt;event.target.files[0];<br>     <br>      this.fileUpload<br>         .generatePresignedURL(this.fileToUpload.name)<br>         .pipe(first())<br>         .subscribe(<br>            (data) =&gt; {<br>               this.fileUpload<br>                  .uploadfileAWSS3(this.fileToUpload; data.presignedS3Url)<br>                  .pipe(first())<br>                  .subscribe(<br>                     (data) =&gt; {<br>                        console.log('uploaded'; data);<br>                     };<br>                     (error) =&gt; {<br>                        console.log('error'; error);<br>                     }<br>                  );<br>            };<br>            (error) =&gt; {<br>               console.log('error'; error);<br>            }<br>         );<br>   }<br></code></pre><br><p>format that im sending a file:</p><br><p><a href="https://i.stack.imgur.com/PYi9e.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/PYi9e.png" alt="enter image description here" /></a></p><br><p><strong>Angular 11</strong> Service</p><br><pre><code> uploadfileAWSS3(file; fileuploadurl) {<br>      const req = new HttpRequest('PUT'; fileuploadurl; file);<br>      return this.http.request(req);<br>   }<br></code></pre><br><p><a href="https://i.stack.imgur.com/SZ03N.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/SZ03N.png" alt="enter image description here" /></a></p><br><p>Could you help me where is the problem that the upload gets canceled?</p><br>
0.0,0.0,0.6666666666666666,0.6666666666666666,0.0,0.0,0.0,<h3>Misleading error message when creating an RDS DB Subnet Group in AWS</h3><p>I am using restricted roles for deploying resources via Cloudformation and have come across an error that threw me for a loop for some time.</p><br><p>I have a minimal role that I am using to create the resource. It only has the following allowed actions (On all resources)</p><br><pre><code>rds:CreateDBSubnetGroup<br>ec2:DescribeVpcs<br>ec2:DescribeSubnets<br>ec2:DescribeAvailabilityZones<br></code></pre><br><p>With the above actions; I am able to successfully create a DB Subnet Group with the following aws cli command:</p><br><pre><code>aws rds create-db-subnet-group \<br>    --db-subnet-group-name testing \<br>    --db-subnet-group-description testing \<br>    --subnet-ids &quot;subnet-abc&quot; &quot;subnet-def&quot;<br></code></pre><br><p>However; if I leave out any one of those ec2 actions; then I get a rather misleading error when running the same command.</p><br><blockquote><br><p>An error occurred (InvalidParameterValue) when calling the CreateDBSubnetGroup operation: Missing necessary credentials. Please check <a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAM.ServiceLinkedRoles.html" rel="nofollow noreferrer">http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAM.ServiceLinkedRoles.html</a></p><br></blockquote><br><p>This threw me down a long rabbit-hole of looking at service linked roles; which made no difference (Adding the action <code>iam:CreateServiceLinkedRole</code> did not stop the error). I eventually found the ec2 actions to add via trial and error and looking at CloudTrail. The error about using RDS with IAM Service Linked roles was completely misleading.</p><br><p>My question really is... Is my assessment of this scenario accurate; or am I missing something fundamental here?</p><br><p>It would not be so bad if it were not for the fact that I then ran in to the same error when creating the DB instance. Naturally I no longer trusted this error; so did not look at service linked roles. Spent a day searching down that rabbit hole that led nowhere - only to find that in this instance it WAS the service linked roles (Easy fix).</p><br>
1.0,0.0,0.0,0.0,0.0,0.3333333333333333,0.3333333333333333,<h3>AWS Lex and Slack integration not working</h3><p>Bot is working fine in Lex console; but I am not getting any response via Slack.</p><br><p>I followed <a href="https://docs.aws.amazon.com/lex/latest/dg/slack-bot-association.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/lex/latest/dg/slack-bot-association.html</a> for Slack integration</p><br>
0.0,1.0,0.0,1.0,0.0,0.0,0.0,<h3>Route 53 Alias record not pointing to S3 static hosted website</h3><p>I have bought a domain <a href="http://sandesh.link" rel="nofollow noreferrer">sandesh.link</a> from route 53; created a publicly accessible S3 bucket with the same name; created a hosted zone and created an alias record pointing to the S3 static hosted website.</p><br><p>When accessing the domain; it shows that the domain is still parked at gandi.</p><br><p>I have updated the nameservers of the domain to the ones given in the hosted zone nameservers(can be seen at whois). I've also verified the domain through email.</p><br><p>How can I get this working?</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>AWS Glue Streaming Job could not connect to Kafka</h3><p>I am trying to implement Glue ETL Job with self managed Kafka cluster (on EC2 instances). Cluster is working; as I checked with simple poducer and consumer scripts on my local machine.<br>The problem is that in Glue Job I get the following warning in logs:</p><br><pre><code>Feb 11; 2021; 5:31:06 PM 21/02/11 11:31:06 WARN NetworkClient: [Consumer clientId=consumer-1; groupId=spark-kafka-source-3e37027f-1b50-4aac-bbbf-68c11f4db9bd-1062155183-driver-0] Connection to node -1 could not be established. Broker may not be available.<br>Feb 11; 2021; 5:33:17 PM 21/02/11 11:33:17 WARN NetworkClient: [Consumer clientId=consumer-1; groupId=spark-kafka-source-3e37027f-1b50-4aac-bbbf-68c11f4db9bd-1062155183-driver-0] Connection to node -3 could not be established. Broker may not be available.<br>Feb 11; 2021; 5:35:28 PM 21/02/11 11:35:28 WARN NetworkClient: [Consumer clientId=consumer-1; groupId=spark-kafka-source-3e37027f-1b50-4aac-bbbf-68c11f4db9bd-1062155183-driver-0] Connection to node -2 could not be established. Broker may not be available.<br>Feb 11; 2021; 5:37:39 PM 21/02/11 11:37:39 WARN NetworkClient: [Consumer clientId=consumer-1; groupId=spark-kafka-source-3e37027f-1b50-4aac-bbbf-68c11f4db9bd-1062155183-driver-0] Connection to node -1 could not be established. Broker may not be available.<br>Feb 11; 2021; 5:39:50 PM 21/02/11 11:39:50 WARN NetworkClient: [Consumer clientId=consumer-1; groupId=spark-kafka-source-3e37027f-1b50-4aac-bbbf-68c11f4db9bd-1062155183-driver-0] Connection to node -3 could not be established. Broker may not be available.<br></code></pre><br><p>I checked SGs for EC2 instances; settings of the connection in Glue; but it still is not working</p><br>
0.0,0.3333333333333333,0.0,0.6666666666666666,0.0,0.0,1.0,<h3>AWS data transfer Estimates in a distributed set up</h3><p>I would like to understand how we can estimate the data transfer costs.</p><br><p>let me explain the set up;</p><br><p>I have a rest endpoint for accessing data from our caches for multiple users in multiple regions on the cloud.</p><br><p>the set up consists of cassandra; hazelcast caches for data storage. the added complexity is in having the source of the data to cassandra from components in on-premise server</p><br><p><strong>Cassandra Set up:</strong></p><br><p>cassandra nodes spread across the AZs. these are in two regions (UK and HK). streaming services from US; ME on premise servers access the data but only when the data is not present in our Hazelcast caches. the <strong>UK cassandra instance replicates data to HK instance for data consistency</strong></p><br><p><strong>HZ set up:</strong><br>HZ caches are set up in 5 regions as a local cache. these caches sync up using a bidirectional sync. when a data is not found in the cache to serve a rest call; it initiates a gprc call from a service to pull the data to pull the missing data</p><br><p>my method of estimating data transfer is</p><br><p>for api; payload * number of requests in a day</p><br><p>How do I estimate the data transfer for cassandra replication ( includes the gossip ) and Hazelcast Replication across regions ?</p><br>
0.0,1.0,0.0,0.0,0.3333333333333333,0.0,0.0,<h3>Connecting to Kubernetes cluster on AWS internal network</h3><p>I have two Kubernetes clusters in AWS; each in it's own VPC.</p><br><ul><br><li>Cluster1 in VPC1</li><br><li>Cluster2 in VPC2</li><br></ul><br><p><a href="https://i.stack.imgur.com/MfdKQ.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/MfdKQ.png" alt="enter image description here" /></a></p><br><p>I want to do http(s) requests from cluster1 into cluster2 through a VPC peering. The VPC peering is setup and I can ping hosts from Cluster1 to hosts in Cluster2 currently.</p><br><p>How can I create a service that I can connect to from Cluster1 in Cluster2. I have experience setting up services using external ELBs and the like; but not for traffic internally in this above scenario.</p><br>
0.0,0.0,0.0,0.0,1.0,0.6666666666666666,0.0,<h3>Deploying nextjs/typescript app(express server) on aws beanstalk</h3><p>I deployed one on heroku(with github pipeline) and it works perfect; but I can't do it on aws beanstalk. If it's not possible to do it with github; I'll be satisfied enough to do it with uploading local file bundle. Here's the code I upload on heroku; but can't upload on beanstalk. <a href="https://github.com/wiktorkujawa/cinema-manager" rel="nofollow noreferrer">https://github.com/wiktorkujawa/cinema-manager</a></p><br><p>Update: I did it with local source bundle and it works.</p><br>
0.0,0.0,0.3333333333333333,0.0,0.0,1.0,0.0,<h3>terraform infrastructure runs locally ; building and deploying it on aws codepipeline gives error</h3><p>I have created my aws infrastructure using terraform . the infrastructure includes creating elastic beanstalk apps ; application load balancer ; s3 ; dynamodb ; vpc-subnets and vpc-endpoints.</p><br><p>the aws infrastructure runs locally using the terraform commands as shown below:</p><br><pre><code>terraform init<br>terraform plan -var-file=&quot;terraform.tfvars&quot;<br>terraform apply -auto-approve -var-file=&quot;terraform.tfvars&quot;<br></code></pre><br><p>The terraform.tfvars contains the variables like region ; instance type ; access key etc .</p><br><p>I want to automate the build and deploy process of this terraform infrastructure using the aws codepipeline .<br>How can I achieve this task ? What steps to follow ? Where to save the terraform.tfvars file ? What roles to specify in the specific codebuild role . What about the manual process of auto-approve ?</p><br><p><strong>MY APPROACH</strong> :The entire process of codecommit/github ; codebuild ; codedeploy ie (codepipeline) is carried out through aws console ; I started with github as source ; it is working (the github repo includes my terraform code for building aws infrastructure) then for codebuild ; I need to specify the env variables and the buildspec.yml file ; this is the problem ; Iocally I had a terraform.tfvars to do the job but here I need to do it in the buildspec.yml file .</p><br><p><strong>QUESTIONS</strong> :I am unaware how to specify my terraform.tfvars credentials in the buildspec.yml file and what env variables to specify? I also know we need to specify roles in the codebuild project but how to effectively specify them ? How to also Store the Terraform state in s3 ?</p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.0,1.0,<h3>Amazon SES CloudWatch cost estimation</h3><p>I am working for a client that wants to send emails making use of Amazon SES and wants to be able to see the &quot;open&quot; and &quot;click&quot; metrics from the SES emails. The client has asked me about the price of such service.</p><br><p>My calculation (which assumes that 100;000 emails have been sent in a month) is as follows:</p><br><ul><br><li><p>Total number of metrics: 100;000 emails * 2 metrics (open &amp; click) per email = 200;000 metrics</p><br></li><br><li><p>Total number of API (PutMetricData) requests: 200;000 (because it equals the number of metrics)</p><br></li><br><li><p>First 10;000 personalised metrics at 0.30 USD per metric = 3;000 USD</p><br></li><br><li><p>Rest of personalised metrics at 0.10 USD per metric = 190;000*0.1 = 19;000 USD</p><br></li><br><li><p>First 1;000;000 API requests = 0 USD per month</p><br></li><br><li><p><strong>Total cost</strong> = 3000 + 19000 + 0 = <strong>22;000 USD per month</strong></p><br></li><br></ul><br><p>Do you guys think my calculation is sound? If not; why? What is the right way to calculate the cost of tracking open and click metrics in CloudWatch?</p><br>
0.0,0.0,0.0,1.0,0.6666666666666666,1.0,0.0,<h3>Test Execution Error in Lambda RDS Data API for Javascript</h3><p>I am trying to connect the AWS Lambda with SQL query to an AWS RDS (MySQL) using the Data API and return the query result for a user with a particular <code>id</code>.</p><br><p>This is how the <strong>handler</strong> looks like:</p><br><pre><code>'use strict';<br>const AWS = require('aws-sdk')<br>const RDS = new AWS.RDSDataService({ endpoint: '******.cluster-*********.us-west-2.rds.amazonaws.com' })<br>    <br>module.exports.fetchById = async (event; context; callback) =&gt; {<br><br>  const req_id = event.pathParameters.id;<br><br>  try {<br>    const params = {<br>      resourceArn: 'arn:aws:rds:us-west-2:***********';<br>      secretArn: 'arn:aws:secretsmanager********';<br>      sql: `SELECT * FROM user WHERE user_id = :id`;<br>      database: '*********';<br>      includeResultMetadata: true;<br>      parameters: [<br>        { id: req_id };<br>      ]<br>    }<br><br>    const db_res = await rdsDataService.executeStatement(params).promise();<br><br>    const response = {<br>      body: JSON.stringify({<br>        message: 'Data fetched!!';<br>        data: db_res.records<br>      })<br>    };<br>    callback(null; response);<br>  } catch (error) {<br>    console.log('Error Received'; error)<br>  }<br>};<br></code></pre><br><p><strong>serverless.yml</strong></p><br><pre><code>functions:<br>  fetchByIdId:<br>    handler: handler.fetchById<br>    events:<br>      - http:<br>          path: user/{id}<br>          method: get<br>          authorizer:<br>            name: cognito-authorizer<br>            arn: arn:aws:***********<br>          request:<br>            parameters:<br>              paths:<br>                id: true<br></code></pre><br><p>Few issues that I need to work upon:</p><br><ol><br><li><p>If I instantiate like:</p><br><p><code>const RDS = new AWS.RDSDataService({ endpoint: '******.cluster-*********.us-west-2.rds.amazonaws.com' })</code></p><br></li><br></ol><br><p>by including an endpoint cluster as a parameter; the handler function does not execute at all. It just keeps throwing:</p><br><p><code>{&quot;errorMessage&quot;: &quot;2020-10-30T07:31:12.258Z c4b4ca2d-3cbb-4733-8cfe-0c7aad228c29 Task timed out after 6.01 seconds&quot;}</code>.</p><br><p>Tried increasing the timeout also but it didn't made any difference &amp; the error still perists.<br>But if endpoint is removed &amp; only used like:</p><br><pre><code>const RDS = new AWS.RDSDataService()<br></code></pre><br><p>; the function does not throw timeout error; but these two new issues are faced:</p><br><ol><br><li><p>The <code>id</code> is required. I passed the required config to the yml file; but it<br /><br>doesn't seem to mark it as required. If the http endpoint is executed as<br /><br><code>/user/</code>; it does not throw any error.</p><br></li><br><li><p>I need to perform input data validation/sanitization for the request<br>parameters. On executing the endpoint <code>/user/123</code>; it throws an error:<br><code>INFO Error Received UnexpectedParameter: Unexpected key 'id' found in  params.parameters[0]</code>.</p><br></li><br></ol><br><p>I read out in the documentation but could not find any particular clue to complete the same.</p><br><p>Any help to resolve this is appreciated.</p><br>
0.0,0.0,0.0,0.0,0.6666666666666666,1.0,0.0,<h3>Lambda connected to appsync always returns Lambda:Unhandled errorType no matter the custom Exception</h3><p>I have a chat lambda that stores messages into dynamodb. If the user is not authorized to add the message; I need to return a custom error exception with unique errorType and message to my client.</p><br><p>I have setup my custom error using the documentation at <a href="https://docs.aws.amazon.com/appsync/latest/devguide/resolver-mapping-template-reference-lambda.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/appsync/latest/devguide/resolver-mapping-template-reference-lambda.html</a></p><br><p>i.e.:</p><br><pre><code>export default class ForbiddenError extends Error {<br>    /**<br>     * ForbiddenError constructor<br>     * @param {string} name<br>     * @param {string} message<br>     */<br>    constructor(name = null; message = null) {<br>        message = message || 'You are forbidden from performing this action.'<br>        super(message)<br>        this.name = name || 'Forbidden'<br>    }<br>}<br></code></pre><br><p>and then I throw the error inside my app via:</p><br><p><code>throw new ForbiddenError()</code></p><br><p>When I test my lambda locally; everything works nicely; the code encounters an error. I catch it and call</p><br><p><code>context.fail(error)</code></p><br><p>Even when I test my lambda using a testing lambda in the AWS console; I get a beautiful response containing both the message and the error type:</p><br><pre><code>class ForbiddenError extends Error {<br>    constructor(message) {<br>        super(message)<br>        this.name = 'ForbiddenError'<br>    }<br>}<br><br>exports.handler = (event; context; callback) =&gt; {<br>    throw new ForbiddenError('You are forbidden from performing this action.')<br>    <br>    return context.fail('test');<br>};<br></code></pre><br><p>and the error:</p><br><pre><code>{<br>  &quot;errorType&quot;: &quot;ForbiddenError&quot;;<br>  &quot;errorMessage&quot;: &quot;You are forbidden from performing this action.&quot;;<br>  &quot;trace&quot;: [<br>    &quot;ForbiddenError: You are forbidden from performing this action.&quot;;<br>    &quot;    at Runtime.exports.handler (/var/task/index.js:9:21)&quot;;<br>    &quot;    at Runtime.handleOnce (/var/runtime/Runtime.js:66:25)&quot;<br>  ]<br>}<br></code></pre><br><p>but when I call my lambda using appsync; suddenly only the message is passed into the error; but the errorType is always the same: <code>Lambda:Unhandled</code> i.e.:</p><br><pre><code>{<br>          &quot;graphQLErrors&quot;: [<br>                    {<br>                              &quot;path&quot;: [<br>                                        &quot;storeStreamChatMessage&quot;<br>                              ];<br>                              &quot;data&quot;: null;<br>                              &quot;errorType&quot;: &quot;Lambda:Unhandled&quot;;<br>                              &quot;errorInfo&quot;: null;<br>                              &quot;locations&quot;: [<br>                                        {<br>                                                  &quot;line&quot;: 2;<br>                                                  &quot;column&quot;: 3;<br>                                                  &quot;sourceName&quot;: null<br>                                        }<br>                              ];<br>                              &quot;message&quot;: &quot;You are forbidden from performing this action.&quot;<br>                    }<br>          ];<br>          &quot;networkError&quot;: null;<br>          &quot;message&quot;: &quot;GraphQL error: You are forbidden from performing this action.&quot;<br>}<br></code></pre><br><p>The response mapping template is the same is in the documentation:</p><br><pre><code>  ResponseMappingTemplate: |<br>    #if($ctx.error)<br>      $util.error($ctx.error.message; $ctx.error.type)<br>    #end<br><br>    $util.toJson($context.result)<br></code></pre><br><p>I was trying to change the $ctx.error.type to $ctx.error.errorType; but then the errorType is returned as &quot;Custom lambda error&quot; and not &quot;Forbidden&quot;.</p><br><p>I have tried using both the context.fail() and callback methods from the lambda exports.handler; both work nicely in the console but both only return the message when called from the appsync resolver.</p><br><p>I have tested that the context.fail(error) is indeed called in the lambda and that the exception is caught via .catch() statement before finally calling the context.fail(error)</p><br><p>Even cloudwatch displays the error with the message and errorType when the main lambda is called; so I'm suspecting an error in exepction and context.fail() returning</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>ERROR: XX000: Materialized view could not be created on redshift</h3><pre><code>Hi everyone;<br><br>I am trying to create a materialized view and command throwing the below error. Are there any restrictions on redshift materialized view? Kindly assist me here. <br><br>**ERROR: XX000: Materialized view could not be created.**<br></code></pre><br><p>CREATE  MATERIALIZED VIEW tbcdbv.tbc_delivery_aggregator_MV1<br>--BACKUP NO AUTO REFRESH NO<br>AS<br>SELECT<br>a.store_number as restid;<br>COALESCE(A.dw_restid; B.dw_restid) AS dw_restid<br>; COALESCE(A.dw_day; B.dw_day) AS dw_day<br>;COALESCE(A.fiscalweekno; B.fiscalweekno) AS fiscalweekno<br>; COALESCE(A.order_submission_date::date ; B.order_submission_date::date ) AS business_date<br>; COALESCE(A.delivery_id ; B.dd_order_number ) AS delivery_id<br>; COALESCE(A.client_order_id ; B.client_order_id ) AS client_order_id<br>; COALESCE(A.external_pos_id ; B.client_order_id ) AS external_pos_id<br>; COALESCE(A.order_status; B.order_status) AS order_status<br>; store_name<br>; street_address<br>; city<br>; country<br>; COALESCE(A.week ; B.week) AS week<br>; CASE<br>WHEN COALESCE(A.IS_CONSUMER_PICKUP; B.IS_CONSUMER_PICKUP) = FALSE THEN 'Delivery'<br>WHEN COALESCE(A.IS_CONSUMER_PICKUP; B.IS_CONSUMER_PICKUP) = TRUE THEN 'Pick Up'<br>ELSE 'Not Listed'<br>END AS retrieval<br>; is_scheduled_order<br>; COALESCE(A.is_dashpass_order; B.is_dashpass ) AS is_dashpass_order<br>; order_scheduled_time<br>; A.count_of_skus AS count_of_skus<br>; COALESCE(A.food_subtotal_local; B.food_subtotal_local) AS food_subtotal<br>; COALESCE(A.tax_subtotal_local; B.tax_subtotal_local) AS tax_subtotal<br>; A.tax_remitted_by_doordash_local AS tax_remitted_by_aggregator_local<br>; COALESCE(A.commission_rate / 100.00; B.commission_rate / 100.00) AS commission_rate<br>; COALESCE(A.commission; B.commission) AS commission<br>; COALESCE(A.marketing_fee; B.marketing_fee) AS marketing_fee<br>; COALESCE(A.currency; B.currency) AS currency<br>; b.store_charge<br>; b.store_refund<br>; b.net_payout<br>; c.aggregator_id<br>; aggregator_name<br>; markup_percentage<br>; store_confirmed_time<br>; dasher_arrived_at_store_time<br>; pickup_time<br>; COALESCE(A.delivered_time::timestamp ; B.delivered_time::timestamp )as  delivered_time<br>; customer_rating<br>; customer_comments<br>; COALESCE(A.order_submission_date_timestamp::timestamp ; B.order_submission_date_timestamp::timestamp )as  order_submission_date_timestamp<br>; COALESCE(A.original_file_name; B.original_file_name ) AS original_file_name<br>; COALESCE(A.created_date::timestamp ; B.created_date::timestamp )as  created_date<br>; COALESCE(A.updated_date::timestamp ; B.updated_date::timestamp )as  updated_date<br>FROM<br>tbcdb.doordash_weekly_sales_summary a<br>FULL OUTER JOIN tbcdb.doordash_weekly_payment b ON<br>a.delivery_id = b.dd_order_number<br>AND a.dw_restid = b.dw_restid<br>AND a.dw_day = b.dw_day<br>left outer join tbcdb.tbc_delivery_markup c<br>on a.dw_restid = c.dw_restid<br>inner join tbcdb.tbc_aggregator_dim e<br>on c.aggregator_id = e.aggregator_id<br>inner join tbcdb.align_dim d<br>on a.store_number = d.restid<br>where tbc_opr_brand ='Y' and status_cd in ('OPN';'CLT')<br>and c.aggregator_id=6&quot;</p><br>
1.0,0.0,0.0,0.0,0.0,0.0,0.0,<h3>How to create hyperparameter tuning job with custom docker container on Amazon Sagemaker?</h3><p>I created a custom docker container to run Catboost on Amazon Sagemaker; followed this demo (<a href="https://github.com/aws-samples/sagemaker-byo-catboost-container-demo/blob/master/Catboost_container_for_SageMaker.ipynb" rel="nofollow noreferrer">https://github.com/aws-samples/sagemaker-byo-catboost-container-demo/blob/master/Catboost_container_for_SageMaker.ipynb</a>). I now want to do hyperparameter tuning with this custom container; but this is not a built-in or pre-built Sagemaker container; so I am not sure if I could or how to create hyperparameter tuning job on Sagemaker with a custom container. I didn't find any official documentation or official examples about using custom docker container to do HYT.</p><br><p>So my question is: how to create hyperparameter tuning with a custom container on Amazon Sagemaker?</p><br>
0.0,0.0,0.5,0.0,1.0,0.5,0.0,<h3>CloudFormation template stuck at CREATE_IN_PROGRESS when creating ECS service</h3><p>I'm creating a ECS service in CloudFormation.</p><br><p>I receive no error; it just will sit at the CREATE_IN_PROGRESS on the <code>logical ID = Service</code> phase..</p><br><p>Here's my CF template (ECS cluster &amp; some other stuff above but cut out due to relevance).</p><br><pre><code>  TaskDefinition:<br>    Type: 'AWS::ECS::TaskDefinition'<br>    Properties:<br>      Family: flink<br>      Memory: 2048<br>      Cpu: 512<br>      NetworkMode: awsvpc<br>      RequiresCompatibilities:<br>        - FARGATE <br>      ContainerDefinitions:<br>        - Name: flink-jobmanager<br>          Image: ACCOUNT_ID.dkr.ecr.us-west-1.amazonaws.com/teststack-flink:latest<br>          Essential: true<br>          PortMappings:<br>            - ContainerPort: 8081<br>              HostPort: 8081<br>          LogConfiguration:<br>            LogDriver: awslogs<br>            Options:<br>              awslogs-group: ecs/flink-stream<br>              awslogs-region: !Ref AWS::Region<br>              awslogs-stream-prefix: ecs<br>          Command:<br>            - jobmanager<br>        - Name: flink-taskmanager<br>          Image: ACCOUNT_ID.dkr.ecr.us-west-1.amazonaws.com/teststack-flink:latest<br>          Essential: true<br>          Command:<br>            - taskmanager<br>      ExecutionRoleArn: !Sub arn:aws:iam::${AWS::AccountId}:role/ecsTaskExecutionRole<br>      Volumes: []<br>      TaskRoleArn: !Sub arn:aws:iam::${AWS::AccountId}:role/ecsTaskExecutionRole<br>      Tags:<br>        -<br>          Key: EnvironmentStage<br>          Value: !Ref EnvironmentStage<br><br>  Service:<br>    Type: 'AWS::ECS::Service'<br>    Properties:<br>      ServiceName: !Join [''; [!Ref EnvironmentStage; '-'; !Ref 'AWS::StackName']]<br>      Cluster: !Join [''; ['arn:aws:ecs:'; !Ref 'AWS::Region'; ':'; !Ref 'AWS::AccountId'; ':cluster/'; !Ref ECSCluster]]<br>      LaunchType: FARGATE<br>      DeploymentConfiguration:<br>        MaximumPercent: 200<br>        MinimumHealthyPercent: 75<br>      TaskDefinition: !Join [''; ['arn:aws:ecs:'; !Ref 'AWS::Region'; ':'; !Ref 'AWS::AccountId'; ':task-definition/'; !Ref TaskDefinition]]<br>      # TaskDefinition: !Ref TaskDefinition<br>      DesiredCount: 1<br>      DeploymentController:<br>        Type: ECS<br>      EnableECSManagedTags: true<br>      PropagateTags: TASK_DEFINITION<br>      SchedulingStrategy: REPLICA<br>      NetworkConfiguration:<br>        AwsvpcConfiguration:<br>          AssignPublicIp: ENABLED<br>          SecurityGroups:<br>            - !Ref FlinkSecurityGroup<br>          Subnets:<br>            - subnet-466da11c<br>            - subnet-6fe65509<br>      Tags:<br>        -<br>          Key: EnvironmentStage<br>          Value: !Ref EnvironmentStage<br></code></pre><br><p>The containers both deploy to the cluster when I set it up manually</p><br>
0.0,0.0,0.5,0.0,0.5,0.5,0.0,<h3>AWS CLI Command Issue</h3><p>I want to get a count of total instances in my account so I ran the below command:</p><br><blockquote><br><p>get-discovered-resource-counts</p><br></blockquote><br><p>But its getting me the below error:</p><br><blockquote><br><p>'get-discovered-resource-counts' is not recognized as an internal or external command; operable program or batch file.</p><br></blockquote><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.0,0.0,<h3>How to determine active tasks count when using AWS to auto scale-in web server</h3><p>When auto scale-in web server using AWS; I'll use a metric named &quot;ActiveTasksCount&quot; and check if the metric value is 0 and current EC2 instance state is DRAINING.<br>My question is do I need to set the metric in my web server? If not; how to determine the real active tasks count when several requests come in; and how to determine if a task is completed?</p><br>
0.0,0.3333333333333333,0.0,0.0,0.3333333333333333,1.0,0.0,<h3>Add Content Handling Parameter to AWS PROXY</h3><p>I asked this on the <a href="https://forum.serverless.com/t/add-content-handling-parameter-to-aws-proxy/13585" rel="nofollow noreferrer">Serverless forum</a> a few days ago and didn't receive a reply; so thought I would try here.</p><br><p>I am building a serverless function which returns a PDF. In my config for my event I have (modified from the typescript starter):</p><br><pre><code>export default {<br>  handler: `${__dirname.split(process.cwd())[1].substring(1)}/handler.main`;<br>  events: [<br>{<br>  http: {<br>    method: &quot;post&quot;;<br>    path: &quot;generateForm&quot;;<br>    request: {<br>      contentHandling: &lt;const&gt;&quot;CONVERT_TO_BINARY&quot;;<br>      schema: {<br>        &quot;application/json&quot;: schema;<br>      };<br>    };<br>  };<br>};<br>  ];<br>};<br></code></pre><br><p>When I deploy I get the error:</p><br><pre><code>Warning! Youre using the AWS_PROXY in combination with a request configuration in your function generateForm. Only the request.parameters; request.schema configs are available in conjunction with AWS_PROXY. Serverless will remove this configuration automatically before deployment.<br></code></pre><br><p>However when testing it seems to correctly return the binary file. Can I safely ignore this error?</p><br>
0.0,1.0,0.0,0.0,0.6666666666666666,0.0,0.0,<h3>HTTP Push Stream in Ingress Nginx</h3><p>I've my kubernetes cluster deployed on EKS and it is exposed via ingress-nginx (NLB). I want to use HTTP Push Stream with ingress nginx. Is it possible to do so.</p><br><ol><br><li>I've tried including this in servers snippet but it didn't worked.</li><br><li>HTTP Push Stream Link: <a href="https://www.nginx.com/resources/wiki/modules/push_stream/" rel="nofollow noreferrer">https://www.nginx.com/resources/wiki/modules/push_stream/</a></li><br></ol><br>
0.0,0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,0.3333333333333333,<h3>How to use Restricted Data Token (RDT) in Amazon SP Api for Java</h3><p>I am using <a href="https://github.com/amzn/selling-partner-api-docs/blob/main/guides/en-US/developer-guide/SellingPartnerApiDeveloperGuide.md#connecting-to-the-selling-partner-api-using-a-generated-java-sdk" rel="nofollow noreferrer">Amazon SP api with generated Java SDK</a>. My test-application is checking the orders and also the delivery address. According to Amazon; this should be a <a href="https://github.com/amzn/selling-partner-api-docs/blob/main/guides/en-US/use-case-guides/tokens-api-use-case-guide/tokens-API-use-case-guide-2021-03-01.md#restricted-operations" rel="nofollow noreferrer">restricted operation</a>. But at the moment; I can get the information without the usage of RDT. I tried to use RDT as shown below but it is not working.</p><br><pre><code>// Build orders-api<br>OrdersV0Api ordersApi = new OrdersV0Api.Builder()<br>              .awsAuthenticationCredentials(awsAuthenticationCredentials)<br>              .lwaAuthorizationCredentials(lwaAuthorizationCredentials)<br>              .awsAuthenticationCredentialsProvider(awsAuthenticationCredentialsProvider)<br>              .endpoint(&quot;https://sellingpartnerapi-eu.amazon.com&quot;)<br>              .build();<br>    <br>List&lt;String&gt; orderStatuses = new ArrayList&lt;String&gt;();<br>orderStatuses.add(OrderStatusEnum.UNSHIPPED.getValue());<br>    <br>Calendar cal = Calendar.getInstance();<br>cal.add(Calendar.DATE; -3);<br>    <br>SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd'T'HH:mm:ssXX&quot;);<br>sdf.setTimeZone(TimeZone.getTimeZone(&quot;UTC&quot;));<br>String tmp = sdf.format(new Date());<br>    <br>try {<br>    // Get orders<br>    GetOrdersResponse respOrders = ordersApi.getOrders(Marketplaces.getAllMarketplaces(); sdf.format(cal.getTime()); null; null; null; orderStatuses; null; null; null; null; 100; null; null; null);<br>    OrderList orders = respOrders.getPayload().getOrders();<br>    Order ord = orders.get(0);<br>        <br>    // Restricted Data Token<br>    RestrictedResource rr = new RestrictedResource();<br>    rr.setMethod(MethodEnum.GET);<br>    rr.setPath(&quot;/orders/v0/orders/&quot; + ord.getAmazonOrderId() + &quot;/address&quot;);<br><br>    CreateRestrictedDataTokenRequest req = new CreateRestrictedDataTokenRequest();<br>    req.addRestrictedResourcesItem(rr);<br>    <br>    TokensApi tokensApi = new TokensApi.Builder()<br>              .awsAuthenticationCredentials(awsAuthenticationCredentials)<br>              .lwaAuthorizationCredentials(lwaAuthorizationCredentials)<br>              .awsAuthenticationCredentialsProvider(awsAuthenticationCredentialsProvider)<br>              .endpoint(&quot;https://sellingpartnerapi-eu.amazon.com&quot;)<br>              .build(); <br><br>    CreateRestrictedDataTokenResponse resp3 = tokensApi.createRestrictedDataToken(req);<br>    String token = resp3.getRestrictedDataToken();<br><br>    // Add token to Header ?<br>    ordersApi.getApiClient().addDefaultHeader(&quot;x-amz-access-token&quot;; token); <br>        <br>    // Get delivery adress<br>    GetOrderAddressResponse respAdress = ordersApi.getOrderAddress(ord.getAmazonOrderId());<br>    Address adr = respAdress.getPayload().getShippingAddress();<br>        <br>} catch (ApiException e) {<br>        e.printStackTrace();<br>} <br></code></pre><br><p>When I execute this; I am getting this error:<br><em>&quot;message&quot;: &quot;The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.</em></p><br><p>It seems like only setting x-amz-access-token is not the right operation to add the RDP. But I also can not see any other operation on ordersApi or ordersApi.getApiClient that seems to be the right one.<br>And also it is unclear to me; why it is working when I just completely not use tokens-api at all.</p><br>
0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,1.0,0.0,<h3>502 Error after trading info between AWS Lambda functions</h3><p>I'm using two Lambda functions with different API Gateway resources to communicate with each other. Basically the architecture is like this :</p><br><p>api-gitlab-launcher ------requestId &amp; pipelineId in headers----------&gt;</p><br><p>&lt;---response status 200 if correct information or else 404-----------database-manager</p><br><p>The code itself is this :</p><br><p><em>First Lambda function : <code>api-gitlab-launcher</code></em></p><br><pre><code>    api.post('/api-gitlab-launcher/create'; async (lambdaRequest; lambdaResponse; next) =&gt; {<br>        config = {<br>            method: 'POST';<br>            url: &quot;API_GATEWAY_URL/v1-test/database-manager/create-request&quot;;<br>            headers: {<br>                &quot;request-id&quot;: requestNb;<br>                &quot;pipeline-id&quot;: pipelineId<br>            }<br>        }<br>        console.log(&quot;find error&quot;) //appears in console log<br>        await axios(config)<br>            .then((lambdaResponse) =&gt; {<br>                    if (lambdaResponse.status == 200) {<br>                        console.log(&quot;1 request sent to database&quot;) // doesn't appear in console log<br>                    }<br>                }<br>            )<br>        lambdaResponse.json({requestId: requestNb})<br><br>    }<br>)<br></code></pre><br><p>The second one is below :</p><br><p><em>Second Lambda function : <code>database-manager</code></em></p><br><pre><code>api.post('/database-manager/create-request'; async (lambdaRequest; lambdaResponse; callback) =&gt; {<br>    await provideConnectionInfo()<br>    let connection = mysql.createConnection({<br>            host: mySQLHost;<br>            user: mySQLUser;<br>            password: mySQLPassword;<br>            database: mySQLDatabase<br>        }<br>    )<br>    requestNb = lambdaRequest.headers['request-id']<br>    pipelineId = lambdaRequest.headers['pipeline-id']<br><br>    connection.connect(function (err) {<br>        if (err) throw err<br>        let query = `INSERT INTO ec2_request VALUES (${requestNb};${pipelineId})`<br>        connection.connect(function (err) {<br>    if (err) throw err<br>    let query = `INSERT INTO ec2_request VALUES (${requestNb};${pipelineId})`<br>    connection.query(query; function (err; result) {<br>        if (err) {<br>            let response = {<br>                &quot;statusCode&quot; : 404;<br>                &quot;headers&quot; : {} ;<br>                &quot;body&quot;: JSON.stringify({message : &quot;Record not inserted - Duplicate&quot;});<br>                &quot;isBase64Encoded&quot;: false<br>            }<br>            return response<br>        } else {<br>            console.log(&quot;1 record inserted&quot;)<br>            let response = {<br>                &quot;statusCode&quot; : 200;<br>                &quot;headers&quot; : {} ;<br>                &quot;body&quot;: JSON.stringify({message : &quot;Record inserted&quot;});<br>                &quot;isBase64Encoded&quot;: false<br>            }<br>            return response<br>        }<br><br>    })<br>   })<br>})<br></code></pre><br><p>I also provide screenshots from the API Gateway :<br><a href="https://i.stack.imgur.com/3PmUM.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/3PmUM.png" alt="api-gitlab-launcher API" /></a></p><br><p><a href="https://i.stack.imgur.com/IPaMM.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/IPaMM.png" alt="database-manager API" /></a></p><br><p>My gut tells me it's related to the Lambda proxy configuration... But I'm not sure. Does someone see something else?</p><br>
0.6666666666666666,0.0,0.0,0.0,0.0,0.3333333333333333,0.0,<h3>Which URL do we have to POST to for the GetHLSStreamingSessionURL API?</h3><p>I was trying to get the Streaming URL of one of my AWS Kinesis Video Streams; but to get that URL I need to use the GetHLSStreamingSessionURL API as per the documentation. The problem is; I don't know where to POST The JSON.<br>The URL for the documentation:<br><a href="https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/API_reader_GetHLSStreamingSessionURL.html" rel="nofollow noreferrer">https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/API_reader_GetHLSStreamingSessionURL.html</a></p><br>
1.0,0.0,1.0,0.0,0.0,0.0,0.0,<h3>Error when creating GlueSecurityConfiguration using Cloudformation script</h3><p>I am trying to create the glue security configuration using cloudformation script but I am getting the following error:<br>Property validation failure: [Value of property {/EncryptionConfiguration/S3Encryptions} does not match type {Array}]</p><br><p>What is the right way to give the S3encryption?</p><br><pre><code>AWSTemplateFormatVersion: 2010-09-09<br>Description: Script creates resources for GlueSecurityConfiguration<br>    <br>Resources:<br>  GlueSecurityConfiguration:<br>    Type: AWS::Glue::SecurityConfiguration<br>    Properties: <br>      EncryptionConfiguration: <br>        S3Encryptions:<br>            KmsKeyArn: !Ref KMSArn<br>            S3EncryptionMode: SSE-KMS<br>        JobBookmarksEncryption:<br>          KmsKeyArn: !Ref KMSArn<br>        CloudWatchEncryption:<br>          KmsKeyArn: !Ref KMSArn<br>      Name: !Sub '${SystemValue}-${SubSystemValue}'<br></code></pre><br>
0.6666666666666666,0.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>must match query not working as expected in Elasticsearch</h3><p>I've created my index below using Kibana which connected to my AWS ES domain:</p><br><pre><code>PUT sals_poc_test_20210217-7<br>{<br>    &quot;settings&quot; : {<br>      &quot;index&quot; : {<br>        &quot;number_of_shards&quot; : 10;<br>        &quot;number_of_replicas&quot; : 1;<br>        &quot;max_result_window&quot;: 50000;<br>        &quot;max_rescore_window&quot;: 50000<br>      }<br>    };<br>    &quot;mappings&quot;: {<br>      &quot;properties&quot;: {<br>        &quot;identifier&quot;: {<br>          &quot;type&quot;: &quot;keyword&quot;<br>        };<br>        &quot;CLASS_NAME&quot;: {<br>          &quot;type&quot;: &quot;keyword&quot;<br>        };<br>        &quot;CLIENT_ID&quot;: {<br>          &quot;type&quot;: &quot;keyword&quot;<br>        }<br>      }<br>    }<br>}<br></code></pre><br><p>then I've indexed 100 documents; using below command returns all 100 documents:</p><br><pre><code>POST /sals_poc_test_20210217-7/_search<br>{<br>  &quot;query&quot;: {<br>    &quot;match&quot;: {<br>      &quot;_index&quot;: &quot;sals_poc_test_20210217-7&quot;<br>    }<br>  }<br>}<br></code></pre><br><p>two sample documents are below:</p><br><pre><code>{<br>        &quot;_index&quot; : &quot;sals_poc_test_20210217-7&quot;;<br>        &quot;_type&quot; : &quot;_doc&quot;;<br>        &quot;_id&quot; : &quot;cd0a3723-106b-4aea-b916-161e5563290f&quot;;<br>        &quot;_score&quot; : 1.0;<br>        &quot;_source&quot; : {<br>          &quot;identifier&quot; : &quot;xweeqkrz&quot;;<br>          &quot;class_name&quot; : &quot;/Sample_class_name_1&quot;;<br>          &quot;client_id&quot; : &quot;random_str&quot;<br>        }<br>      };<br>{<br>        &quot;_index&quot; : &quot;sals_poc_test_20210217-7&quot;;<br>        &quot;_type&quot; : &quot;_doc&quot;;<br>        &quot;_id&quot; : &quot;cd0a3723-106b-4aea-b916-161e556329ab&quot;;<br>        &quot;_score&quot; : 1.0;<br>        &quot;_source&quot; : {<br>          &quot;identifier&quot; : &quot;xweeqkra&quot;;<br>          &quot;class_name&quot; : &quot;/Sample_class_name_2&quot;;<br>          &quot;client_id&quot; : &quot;random_str_2&quot;<br>        }<br>      }<br></code></pre><br><p>but when I wanted to search by <code>CLASS_NAME</code> by below command:</p><br><pre><code>POST /sals_poc_test_20210217-7/_search<br>{<br>  &quot;size&quot;: 200;<br>  &quot;query&quot;: { <br>    &quot;bool&quot;: { <br>      &quot;must&quot;: [ <br>        { &quot;match&quot;: { &quot;CLASS_NAME&quot;: &quot;/Sample_class_name_1&quot;}}<br>      ]<br>    }<br>  }<br>}<br></code></pre><br><p>Not only the documents that match this <code>class_name</code> returned; but also other ones.</p><br><p>Anyone could shed any light into this case please?</p><br><p>I'm suspecting the way I wrote my search query is problematic. But cannot figure out why.</p><br><p>Thanks!</p><br>
0.0,0.0,0.0,1.0,0.0,0.3333333333333333,0.0,<h3>Does AWS DocumentDB support pymongo .hint at all?</h3><p>I have a large but simple collection in DocumentDB (3.6) and need to quickly and efficiently retrieve all document ids matching a simple regex pattern.</p><br><p>On the cli I can search for these with a regex and hint and seem to get good results via explain.</p><br><p>When I try to transfer this to a call from pymongo I get an error saying the index cannot be found... which is odd as its _id I'm hinting for and I can clearly see it defined on the cli.</p><br><p>Am I going crazy or is there no way to hint in pymongo when using DocumentDB?</p><br><p>How could I work around this issue if not possible? Does it make sense to go as far as calling the mongo cli from a system call within python (seems excessive...)?</p><br>
0.0,1.0,0.6666666666666666,0.0,0.0,0.3333333333333333,0.0,<h3>Automatically create a subnet for each AWS availability zone in Terraform</h3><p>Is there a better way to optime the code below so I don't have to ask for availability zone again and again instead can do it in once. as the region is variable so I cant define hardcoded availability zone. can you guys please I want my public subnets to be /24</p><br><pre><code>provider &quot;aws&quot; {<br>    region = var.region<br>}<br><br>resource &quot;aws_vpc&quot; &quot;app_vpc&quot; {<br>  cidr_block           = var.vpc_cidr<br>  enable_dns_support   = true<br>  enable_dns_hostnames = true<br>  tags = {<br>    Name = var.vpc_name<br>  }<br>}<br><br><br>data &quot;aws_availability_zones&quot; &quot;available&quot; {<br>  state = &quot;available&quot;<br>}<br><br>#provision public subnet<br>resource &quot;aws_subnet&quot; &quot;public_subnet_01&quot; {<br>  vpc_id     = aws_vpc.app_vpc.id<br>  cidr_block = var.public_subnet_01<br>  availability_zone = data.aws_availability_zones.available.names[0]<br>  tags = {<br>    Name = &quot;public_subnet_01&quot;<br>  }<br>  depends_on = [aws_vpc_dhcp_options_association.dns_resolver]<br>}<br>resource &quot;aws_subnet&quot; &quot;public_subnet_02&quot; {<br>  vpc_id     = aws_vpc.app_vpc.id<br>  cidr_block = var.public_subnet_02<br>  availability_zone = data.aws_availability_zones.available.names[1]<br>  tags = {<br>    Name = &quot;public_subnet_02&quot;<br>  }<br>  depends_on = [aws_vpc_dhcp_options_association.dns_resolver]<br>}<br>resource &quot;aws_subnet&quot; &quot;public_subnet_03&quot; {<br>  vpc_id     = aws_vpc.app_vpc.id<br>  cidr_block = var.public_subnet_03<br>  availability_zone = data.aws_availability_zones.available.names[2]<br>  tags = {<br>    Name = &quot;public_subnet_03&quot;<br>  }<br>  depends_on = [aws_vpc_dhcp_options_association.dns_resolver]<br>}<br></code></pre><br>
0.0,0.0,0.0,1.0,0.5,0.5,0.0,<h3>Limiting batch size for event_source_mapping concerned with missing records</h3><p>I am currently have an event source mapping that is reading from dynamodb stream and invoking a lambda.</p><br><p>I'm concerned with limiting the batch size for my event_source_mapping. I'm worried that the event_source_mapping will miss records due to my synchronous process. I want to limit my batch size to 5; and my concurrent batches per shard to one.</p><br><pre><code>Batch size: 5<br>Batch window: None<br>Concurrent batches per shard: 1<br>Last processing result: OK<br>Maximum age of record: 604800<br>On-failure destination:<br>{<br>  &quot;onFailure&quot;: {<br>    &quot;destination&quot;: &quot;arn:aws:sns:us-east-1:#############:&lt;&lt;topicname&gt;&gt;&gt;&quot;<br>  }<br>}<br>Retry attempts: 1<br>Split batch on error: No<br></code></pre><br><p>Because my event source mapping starts at the latest position; do I have to worry that while the event source mapping is handling processing the current batches I could miss events in the dynamodb stream?</p><br><pre><code>resource &quot;aws_lambda_event_source_mapping&quot; &quot;history&quot; {<br>  event_source_arn  = &quot;${data.aws_dynamodb_table.auditlogDynamoDB.stream_arn}&quot;<br>  function_name     = &quot;${module.cx-clientcomm-history-lambda.lambda_function_arn}&quot;<br>  starting_position = &quot;LATEST&quot;<br>  maximum_retry_attempts = 1<br>  batch_size = 1<br>  maximum_record_age_in_seconds = 604800 // adding to explicitly set. QA plan was providing invalid value<br>  #(Optional) An Amazon SQS queue or Amazon SNS topic destination for failed records. <br>  destination_config {<br>    on_failure {<br>      destination_arn = &quot;${data.aws_sns_topic.cx-clientcomm-sns-slack.arn}&quot;<br>    }<br>  }<br>}<br></code></pre><br>
0.0,0.0,0.0,1.0,0.0,1.0,0.0,<h3>Printing out to a php page using AWS PHP SDK [S3]</h3><p>I am learning to use the PHP SDK by AWS.</p><br><p>What I want to achieve is; that when some user requests a .php page on my site.<br><code>example.com/listbuckets.php</code></p><br><p>That page should return the buckets associated with my IAM role.</p><br><p>Here's my code:</p><br><pre><code>$s3Client = new S3Client([<br>    'profile' =&gt; 'default';<br>    'region' =&gt; 'eu-west-1';<br>    'version' =&gt; 'latest'<br>]);<br><br>//Listing all S3 Bucket<br>$buckets = $s3Client-&gt;listBuckets();<br>foreach ($buckets['Buckets'] as $bucket) {<br>    echo $bucket['Name'] . &quot;\n&quot;;<br>}<br><br>?&gt;<br><br><br></code></pre><br><p>With that code; via CLI or terminal I can successfully see my S3 buckets. But when I request the page via browser; it shows 500 Internal Server Error.</p><br><p>What I am missing on here?</p><br><p>Thanks in advance.</p><br><p>EDIT:<br>I tried passing my AWS Creds (key/secret) via:</p><br><pre><code>$credentials = new Aws\Credentials\Credentials('key'; 'secret');<br><br>$s3 = new Aws\S3\S3Client([<br>    'version'     =&gt; 'latest';<br>    'region'      =&gt; 'eu-west-1';<br>    'credentials' =&gt; $credentials<br>]);<br></code></pre><br><p>And I can't still resolve the php page if accesed via a web-browser; could this be an issue related to permissions/ownership? Even tho i can see my buckets if I execute via terminal<br><code>php listbuckets.php</code></p><br><p>EDIT2:<br>I passed a new parameter on</p><br><pre><code>$s3Client = new Aws\S3\S3Client([<br>    'version'     =&gt; 'latest';<br>    'region'      =&gt; 'eu-west-1';<br>    'credentials' =&gt; $credentials;<br>    'debug'       =&gt; true<br>]);<br></code></pre><br><p>And now the page is responding with this chunk of text:</p><br><blockquote><br><p>-&gt; Entering step init; name 'idempotency_auto_fill' --------------------------------------------------- command was set to array(3) { [&quot;instance&quot;]=&gt; string(32) &quot;044155c203a162626f000d004ff45d46&quot; [&quot;name&quot;]=&gt; string(11) &quot;ListBuckets&quot; [&quot;params&quot;]=&gt; array(2) { [&quot;@http&quot;]=&gt; array(1) { [&quot;debug&quot;]=&gt; resource(4) of type (stream) } [&quot;@context&quot;]=&gt; array(0) { } } } request was set to array(0) { } -&gt; Entering step init; name 's3.ssec' ------------------------------------- no changes -&gt; Entering step init; name 's3.source_file' -------------------------------------------- no changes -&gt; Entering step init; name 's3.save_as' ---------------------------------------- no changes -&gt; Entering step init; name 's3.location' ----------------------------------------- no changes -&gt; Entering step init; name 's3.auto_encode' -------------------------------------------- no changes -&gt; Entering step init; name 's3.head_object' -------------------------------------------- no changes -&gt; Entering step validate; name 'validation' -------------------------------------------- no changes -&gt; Entering step validate; name 'input_validation_middleware' ------------------------------------------------------------- no changes -&gt; Entering step build; name 'builder' -------------------------------------- request.instance was set to 00fq4442000004ffw2c46 request.method was set to GET request.headers was set to array(2) { [&quot;X-Amz-Security-Token&quot;]=&gt; string(7) &quot;[TOKEN]&quot; [&quot;Host&quot;]=&gt; array(1) { [0]=&gt; string(26) &quot;s3.eu-west-1.amazonaws.com&quot; } } request.scheme was set to https request.path was set to / -&gt; Entering step build; name 'ApiCallMonitoringMiddleware' ---------------------------------------------------------- no changes -&gt; Entering step build; name '' ------------------------------- request.instance changed from e33e626cc to 00dw0sq03c45d46 request.headers.User-Agent was set to array(1) { [0]=&gt; string(50) &quot;aws-sdk-php/3.178.4 OS/Linux/4.19.0-16-cloud-amd64&quot; } -&gt; Entering step build; name 'endpoint_parameter' ------------------------------------------------- no changes -&gt; Entering step build; name 'EndpointDiscoveryMiddleware' ---------------------------------------------------------- no changes -&gt; Entering step build; name 's3.checksum' ------------------------------------------ no changes -&gt; Entering step build; name 's3.content_type' ---------------------------------------------- no changes -&gt; Entering step build; name 's3.endpoint_middleware' ----------------------------------------------------- no changes -&gt; Entering step build; name 's3.bucket_endpoint_arn' ----------------------------------------------------- no changes -&gt; Entering step sign; name 'StreamRequestPayloadMiddleware' ------------------------------------------------------------ no changes -&gt; Entering step sign; name 'invocation-id' ------------------------------------------- request.instance changed from 000003a1626c9002f900025d46 to 0e55j00003a1626cy004ff4r3d46 request.headers.aws-sdk-invocation-id was set to array(1) { [0]=&gt; string(32) &quot;321593e23171d701cdwae9&quot; } -&gt; Entering step sign; name 'retry' ----------------------------------- request.instance changed from dw000qdqd04ff45d46 to 0400411626c4210004445d46 request.headers.aws-sdk-retry was set to array(1) { [0]=&gt; string(3) &quot;0/0&quot; } -&gt; Entering step sign; name 'signer' ------------------------------------ request.instance changed from 02z00004ss2004ff45d46 to 01626d000045d46 request.headers.x-amz-content-sha256 was set to array(1) { [0]=&gt; string(64) &quot;ec44298f9b934ca491b7855&quot; } request.headers.X-Amz-Date was set to array(1) { [0]=&gt; string(16) &quot;25T142413Z&quot; } request.headers.Authorization was set to array(1) { [0]=&gt; string(211) &quot;AWS4-HMAC-SHA256 Credential=[KEY]/20210415/eu-west-1/s3/aws4_request; SignedHeaders=host;x-amz-content-sha256;x-amz-date; Signature=[SIGNATURE] }</p><br></blockquote><br><p>I edited some of the values; but from Amazon; what's this error related to ?<br>Seems like a signature error</p><br><p>EDIT 3:<br>As I said; it seems like a signature error; I have compared the two chunks of code; one from the browser and the other from the terminal and the browser-petition just stops at.<br><code>Entering step sign; name 'signer' </code></p><br><p><strong>How could I sign (via browser) the same way I sign when I use my terminal from the ec2 instance?</strong></p><br><p>EDIT 4:<br>I managed and searched a lot; and found that I should be working with Pre-Signed URLS.<br>With the following code; I can grant a user a 20 minute lifespan link which will redirect him and show him all the available buckets.</p><br><pre><code>$s3Client = new Aws\S3\S3Client([<br>    'credentials' =&gt; $credentials;<br>    'region' =&gt; 'eu-west-1';<br>    'version' =&gt; '2006-03-01';<br>]);<br><br>$cmd = $s3Client-&gt;getCommand('ListBuckets'; [<br>    'Bucket' =&gt; '*'<br>    <br>]);<br><br>$request = $s3Client-&gt;createPresignedRequest($cmd; '+20 minutes');<br><br>// Get the actual presigned-url<br>$presignedUrl = (string)$request-&gt;getUri();<br>echo($presignedUrl);<br></code></pre><br><p>EDIT 5:<br>If someone would like to print the result of the url generated by the AWS PHP SDK; use this php code</p><br><pre><code>$result = file_get_contents($presignedUrl);<br>echo($result);<br><br></code></pre><br><p>Finally; I got that; so I am gonna post the answer!</p><br>
0.0,0.0,1.0,0.0,0.0,0.0,0.5,<h3>How can I get the list of roles mapped to an app user with the OneLogin API?</h3><p>I am using the Amazon Web Services (AWS) Multi Account connector to grant users federated access to AWS. I have manually assigned some users to certain IAM roles by going to the Users tab; selecting a user; navigating to Applications and setting the roles in the &quot;Edit Amazon Web Services (AWS) Multi Account login for X&quot; dialog.</p><br><p>Now I would like to use the <a href="https://developers.onelogin.com/api-docs/2/getting-started/dev-overview" rel="nofollow noreferrer">OneLogin API</a>s to retrieve these role assignments  is this possible? I have tried the APIs for Apps; Users; User Mappings; and Roles but I haven't found these assignments.</p><br>
0.6666666666666666,0.0,0.0,0.3333333333333333,0.0,0.0,0.0,<h3>Configure &quot;indices.memory.index_buffer_size&quot; on an Elasticsearch 7.7 AWS managed instance</h3><p>Is there a way you can set &quot;indices.memory.index_buffer_size&quot; setting for an AWS managed ES node in order to improve indexing as suggested in <a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/tune-for-indexing-speed.html" rel="nofollow noreferrer">Tune for indexing speed</a>?</p><br><p>Normally this configuration is set in <em>elasticsearch.yml</em> file; but you can't access this file on an managed instance.</p><br><p>I also checked Boto3 ElasticsearchService documentation; more specific <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/es.html#ElasticsearchService.Client.update_elasticsearch_domain_config" rel="nofollow noreferrer">update_elasticsearch_domain_config()</a> function; but the setting is not mentioned anywhere.</p><br><p>Thank you</p><br>
0.0,0.0,1.0,0.3333333333333333,0.0,0.3333333333333333,0.0,<h3>AWS KMS (Key Management Service)</h3><p>My team is working on a customer facing piece of hardware which will store various physical items. To gain access to this hardware; the customer will enter a pin - the pin that was created by a manager from an internal amazon dashboard. We are trying to determine the best way to encrypt these pins from the dashboard into our RDS and decrypt from the customer entry entry. Here are our current workflows but looking for guidance on if we're thinking about this;</p><br><p>From Dashboard (Pin Creation):<br>Managers enters pin &gt; Triggers api call into Gateway &gt; Triggers Lambda which triggers the createalias and encryptkey API for KMS and imports it into KMS.</p><br><p>From Hardware (Customer Entry)<br>Customer enters pin &gt; triggers API call into gateway &gt; triggers lambda which triggers the decrypt API into KMS &gt; <em><strong>once the password has been decrypted we need a way for this decryption to then trigger various workflows that will take place in our RDS eventually leading to a door on the locker open</strong></em></p><br><p>The question we have here is once the key has been decrypted within KMS at customer entry; what is the best way to then trigger the next RDS function? Can KMS make API calls out? Do I need a lambda function in between the KMS and RDS? Also want to ensure our thought process about the workflow above makes sense. Any advice is appreciated.</p><br>
1.0,0.0,0.0,0.0,0.0,0.3333333333333333,0.0,<h3>Which one is faster for querying Athena: pyathena or boto3?</h3><p>Which one is faster <code>pyathena</code> or <code>boto3</code> to query AWS Athena schemas using python script?</p><br><p>Currently I am using pyathena to query Athena schemas but it's quite slow and I know there is another option of boto3 but before starting need some experts advice.</p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.0,0.0,<h3>Designing a User Table with different Identity providers in DynamoDB</h3><p>The important attributes for the User Table are:</p><br><ul><br><li>unique id (uuid): to identify the user (and to be referenced later with other datasets for identity)</li><br><li>social media id: FB; Google; Apple</li><br><li>email id</li><br><li>mobile no</li><br></ul><br><p>User can login using either social media or mobile no or email. For this 3 cases; the searching needs to be done in the DynamoDB table for its existence.</p><br><p>So; what would be the ideal designing of this User table (with respect to keys) to ensure the performance as well as the purpose?</p><br><p>For better understanding of the Use Case:</p><br><p><strong>Use Case:</strong></p><br><ol><br><li>User can register with Social Media (any of the FB or Google or Apple) along with mandatory detail like mobile and email</li><br><li>User can login using Social Media (any of the FB or Google or Apple) or Mobile No or Email ID (using OTP)</li><br><li>The Data volume will be on the higher side.</li><br></ol><br><p>So; all this attributes to be stored alongside with the right design of the Primary Key to fetch information correctly during login.</p><br>
0.0,0.0,0.3333333333333333,1.0,0.0,0.0,0.0,<h3>How do I sync AWS S3 files using file paths in include option</h3><p>I have following directories structure in my S3 bucket; I want run multiple sync jobs in such a way that I could sync all folders that start with 1 in one job and those which start with 2 in another sync job.</p><br><p>I know I can sync using aws s3 sync s3://bucket my_local_dir for all individual sub-directories.</p><br><p>/Root</p><br><ul><br><li>X<br><ul><br><li>1</li><br><li>10</li><br><li>11</li><br><li>114</li><br><li>2</li><br><li>211</li><br><li>213</li><br></ul><br></li><br><li>Y</li><br></ul><br><p>Thanks in Advance.</p><br>
0.0,1.0,0.0,0.0,0.0,0.0,0.0,<h3>AWS API Gateway HTTP API custom domain with VPC Link to ALB</h3><p>I'm trying to set up an API using a REGIONAL custom domain that routes HTTP requests to ALB.<br>The domain is registered with another DNS provider so I will not be using Route53.</p><br><p>Concept:<br><code>/path/to/service</code> --&gt; ALB (Listener: <code>/path/*</code>) --&gt; ECS</p><br><p>I first tried with the original invoke URL <code>https://cuxxxxmvk0.execute-api.ap-east-1.amazonaws.com/stage/path/to/service</code>; it returns ALB context path error (It's expected because ALB gets <code>/stage/path/to/service</code> which doesn't hit any prefix).</p><br><p>Then I created a custom domain with API mapping (no base path) that maps to the stage; and try invoking it with the provided &quot;API Gateway domain name&quot; (The one generated by custom domain). <code>Full URL: https://d-yjexxxds3.execute-api.ap-east-1.amazonaws.com/path/to/service</code><br>However; it returns <code>{&quot;message&quot;:&quot;Not Found&quot;}</code></p><br><p><a href="https://i.stack.imgur.com/GGvTC.png" rel="nofollow noreferrer">API Gateway domain name</a></p><br><hr /><br><p>So my question here is:</p><br><p>How does the &quot;API Gateway domain name&quot; generated by custom domain works? Can I directly invoke the API with it?</p><br><p>Is it a must to CNAME it (i.e. <code>CAME api.mydomain.com d-yjexxxds3.execute-api.ap-east-1.amazonaws.com</code>)?</p><br><p>Can I make my final endpoint to be <code>api.mydomain.com/path/to/service</code> without the stage in path?</p><br>
0.0,0.6666666666666666,0.3333333333333333,0.0,0.0,1.0,0.0,<h3>AWS API Gateway integration with AWS Event Bridge(Cloudwatch Events) in Cloudformation Script</h3><p><em>Original Requirement</em>:</p><br><p>Create a route/path on AWS Api Gateway which connects API Gateway directly to AWS Event Bridge (Cloudwatch Events) and puts/pushes event on an event bus of it.</p><br><p>Was able to create it and executes just fine when done from AWS Console.</p><br><p><em>Actual Problem</em>:</p><br><p>When writing the AWS Cloudformation script for this API Gateway; it looks like this:</p><br><pre><code>EventsPostMethod:<br> Type: AWS::ApiGateway::Method<br> Properties:<br>   ResourceId:<br>     Ref: EventsResource<br> RestApiId:<br>   Ref: RestAPI<br> HttpMethod: POST<br> AuthorizationType: NONE<br> Integration:<br>   Type: AWS<br>   IntegrationHttpMethod: POST<br>   Uri:<br>     Fn::Sub: arn:aws:apigateway:${AWS::Region}:cloudwatchEvents:action/PutEvents<br>   RequestParameters:<br>     integration.request.header.X-Amz-Target: &quot;'AWSEvents.PutEvents'&quot;<br>   RequestTemplate:<br>     some-script-here...<br>   <br></code></pre><br><p>Notice the Uri value:</p><br><pre><code>&quot;arn:aws:apigateway:${AWS::Region}:cloudwatchEvents:action/PutEvents&quot;<br>arn:aws:apigateway:{region}:{subdomain.service|service}:path|action/{service_api}<br></code></pre><br><p>According to AWS <a href="https://docs.aws.amazon.com/apigateway/api-reference/resource/integration/#type" rel="nofollow noreferrer">Docs</a> the value of uri should be following:</p><br><blockquote><br><p>For AWS or AWS_PROXY integrations; the URI is of the form arn:aws:apigateway:{region}:{subdomain.service|service}:path|action/{service_api}. Here; {Region} is the API Gateway region (e.g.; us-east-1); {service} is the name of the integrated AWS service (e.g.; s3); and {subdomain} is a designated subdomain supported by certain AWS service for fast host-name lookup. action can be used for an AWS service action-based API; using an Action={name}&amp;{p1}={v1}&amp;p2={v2}... query string. The ensuing {service_api} refers to a supported action {name} plus any required input parameters. Alternatively; path can be used for an AWS service path-based API. The ensuing service_api refers to the path to an AWS service resource; including the region of the integrated AWS service; if applicable. For example; for integration with the S3 API of GetObject; the uri can be either arn:aws:apigateway:us-west-2:s3:action/GetObject&amp;Bucket={bucket}&amp;Key={key} or arn:aws:apigateway:us-west-2:s3:path/{bucket}/{key}</p><br></blockquote><br><p>You must have noticed that I replaced the <strong>service</strong> with <strong>cloudwatchEvents</strong> in the above mentioned uri.</p><br><p>Now; error Given by AWS Cloudformation Console during Publish of API Gateway:</p><br><blockquote><br><p>AWS Service of type cloudwatchEvents not supported (Service: AmazonApiGateway; Status Code: 400; Error Code: BadRequestException; Request ID: 07bae22c-d198-4595-8de9-6ea23763eff5; Proxy: null)</p><br></blockquote><br><p>Now I have tried replacing service with</p><br><ul><br><li>cloudwatch</li><br><li>eventBridge</li><br><li>cloudwatchEvent</li><br><li>event-bus</li><br></ul><br><p>This is the real problem. What should I place in service in uri so that it accepts ?</p><br><p><a href="https://i.stack.imgur.com/rA8oO.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/rA8oO.png" alt="enter image description here" /></a></p><br>
0.0,0.0,0.3333333333333333,0.0,1.0,0.0,0.0,<h3>Deploying Docker Compose to AWS ECS</h3><p>I've been trying to deploy my Docker Compose file to AWS ECS using Docker Compose CLI.</p><br><p>Steps:</p><br><pre><code>docker context create aws myproject<br>docker context use myproject<br>docker compose up<br></code></pre><br><p>My result is:</p><br><pre><code>FrontendService TaskFailedToStart: CannotPullContainerError: inspect image has been retried 1 time(s): failed to resolve ref &quot;docker.pkg.github.com/org/frontend/frontend:latest@sha256:360461ada5ee15cea9d058280eab4a2558c68cbbdc04fe14eaae789129ddb091&quot;: docker.pkg.github.com/org...<br></code></pre><br><p>My Docker Compose File:</p><br><pre><code>version: '3.3'<br>services:<br>  frontend:<br>    image: 'docker.pkg.github.com/org/frontend/frontend:latest'<br>    restart: always<br>    x-aws-pull_credentials: &quot;arn secret&quot;<br>    env_file:<br>      - ../frontend/.env.production<br>    ports:<br>      - 3000:3000<br></code></pre><br><p>Output of <code>docker compose convert</code>:</p><br><pre><code>AWSTemplateFormatVersion: 2010-09-09<br>Resources:<br>  CloudMap:<br>    Properties:<br>      Description: Service Map for Docker Compose project docker<br>      Name: docker.local<br>      Vpc: vpc-c7823fba<br>    Type: AWS::ServiceDiscovery::PrivateDnsNamespace<br>  Cluster:<br>    Properties:<br>      ClusterName: docker<br>      Tags:<br>      - Key: com.docker.compose.project<br>        Value: docker<br>    Type: AWS::ECS::Cluster<br>  Default3000Ingress:<br>    Properties:<br>      CidrIp: 0.0.0.0/0<br>      Description: frontend:3000/tcp on default network<br>      FromPort: 3000<br>      GroupId:<br>        Ref: DefaultNetwork<br>      IpProtocol: TCP<br>      ToPort: 3000<br>    Type: AWS::EC2::SecurityGroupIngress<br>  DefaultNetwork:<br>    Properties:<br>      GroupDescription: docker Security Group for default network<br>      Tags:<br>      - Key: com.docker.compose.project<br>        Value: docker<br>      - Key: com.docker.compose.network<br>        Value: docker_default<br>      VpcId: vpc-c7823fba<br>    Type: AWS::EC2::SecurityGroup<br>  DefaultNetworkIngress:<br>    Properties:<br>      Description: Allow communication within network default<br>      GroupId:<br>        Ref: DefaultNetwork<br>      IpProtocol: &quot;-1&quot;<br>      SourceSecurityGroupId:<br>        Ref: DefaultNetwork<br>    Type: AWS::EC2::SecurityGroupIngress<br>  FrontendService:<br>    DependsOn:<br>    - FrontendTCP3000Listener<br>    Properties:<br>      Cluster:<br>        Fn::GetAtt:<br>        - Cluster<br>        - Arn<br>      DeploymentConfiguration:<br>        MaximumPercent: 200<br>        MinimumHealthyPercent: 100<br>      DeploymentController:<br>        Type: ECS<br>      DesiredCount: 1<br>      LaunchType: FARGATE<br>      LoadBalancers:<br>      - ContainerName: frontend<br>        ContainerPort: 3000<br>        TargetGroupArn:<br>          Ref: FrontendTCP3000TargetGroup<br>      NetworkConfiguration:<br>        AwsvpcConfiguration:<br>          AssignPublicIp: ENABLED<br>          SecurityGroups:<br>          - Ref: DefaultNetwork<br>          Subnets:<br>          - subnet-a7d289a9<br>          - subnet-a56b0484<br>          - subnet-22df6b13<br>          - subnet-4c96f72a<br>          - subnet-3bd2b064<br>          - subnet-f486b7b9<br>      PlatformVersion: 1.4.0<br>      PropagateTags: SERVICE<br>      SchedulingStrategy: REPLICA<br>      ServiceRegistries:<br>      - RegistryArn:<br>          Fn::GetAtt:<br>          - FrontendServiceDiscoveryEntry<br>          - Arn<br>      Tags:<br>      - Key: com.docker.compose.project<br>        Value: docker<br>      - Key: com.docker.compose.service<br>        Value: frontend<br>      TaskDefinition:<br>        Ref: FrontendTaskDefinition<br>    Type: AWS::ECS::Service<br>  FrontendServiceDiscoveryEntry:<br>    Properties:<br>      Description: '&quot;frontend&quot; service discovery entry in Cloud Map'<br>      DnsConfig:<br>        DnsRecords:<br>        - TTL: 60<br>          Type: A<br>        RoutingPolicy: MULTIVALUE<br>      HealthCheckCustomConfig:<br>        FailureThreshold: 1<br>      Name: frontend<br>      NamespaceId:<br>        Ref: CloudMap<br>    Type: AWS::ServiceDiscovery::Service<br>  FrontendTCP3000Listener:<br>    Properties:<br>      DefaultActions:<br>      - ForwardConfig:<br>          TargetGroups:<br>          - TargetGroupArn:<br>              Ref: FrontendTCP3000TargetGroup<br>        Type: forward<br>      LoadBalancerArn:<br>        Ref: LoadBalancer<br>      Port: 3000<br>      Protocol: TCP<br>    Type: AWS::ElasticLoadBalancingV2::Listener<br>  FrontendTCP3000TargetGroup:<br>    Properties:<br>      Port: 3000<br>      Protocol: TCP<br>      Tags:<br>      - Key: com.docker.compose.project<br>        Value: docker<br>      TargetType: ip<br>      VpcId: vpc-c7823fba<br>    Type: AWS::ElasticLoadBalancingV2::TargetGroup<br>  FrontendTaskDefinition:<br>    Properties:<br>      ContainerDefinitions:<br>      - Command:<br>        - us-east-1.compute.internal<br>        - docker.local<br>        Essential: false<br>        Image: docker/ecs-searchdomain-sidecar:1.0<br>        LogConfiguration:<br>          LogDriver: awslogs<br>          Options:<br>            awslogs-group:<br>              Ref: LogGroup<br>            awslogs-region:<br>              Ref: AWS::Region<br>            awslogs-stream-prefix: docker<br>        Name: Frontend_ResolvConf_InitContainer<br>      - DependsOn:<br>        - Condition: SUCCESS<br>          ContainerName: Frontend_ResolvConf_InitContainer<br>        Environment:<br>        - Name: NEXT_PUBLIC_BACKEND_URL<br>          Value: '&quot;http://localhost:4000&quot;'<br>        - Name: NEXT_PUBLIC_SEGMENT_TOKEN<br>          Value: '&quot;8GVxkwMLOYo5w4XrDwEGtATBaGlUrruH&quot;'<br>        Essential: true<br>        Image: docker.pkg.github.com/org/frontend/frontend:latest@sha256:360461ada5ee15cea9d058280eab4a2558c68cbbdc04fe14eaae789129ddb091<br>        LinuxParameters: {}<br>        LogConfiguration:<br>          LogDriver: awslogs<br>          Options:<br>            awslogs-group:<br>              Ref: LogGroup<br>            awslogs-region:<br>              Ref: AWS::Region<br>            awslogs-stream-prefix: docker<br>        Name: frontend<br>        PortMappings:<br>        - ContainerPort: 3000<br>          HostPort: 3000<br>          Protocol: tcp<br>        RepositoryCredentials:<br>          CredentialsParameter: arn:aws<br>      Cpu: &quot;256&quot;<br>      ExecutionRoleArn:<br>        Ref: FrontendTaskExecutionRole<br>      Family: docker-frontend<br>      Memory: &quot;512&quot;<br>      NetworkMode: awsvpc<br>      RequiresCompatibilities:<br>      - FARGATE<br>    Type: AWS::ECS::TaskDefinition<br>  FrontendTaskExecutionRole:<br>    Properties:<br>      AssumeRolePolicyDocument:<br>        Statement:<br>        - Action:<br>          - sts:AssumeRole<br>          Condition: {}<br>          Effect: Allow<br>          Principal:<br>            Service: ecs-tasks.amazonaws.com<br>        Version: 2012-10-17<br>      ManagedPolicyArns:<br>      - arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy<br>      - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly<br>      Policies:<br>      - PolicyDocument:<br>          Statement:<br>          - Action:<br>            - secretsmanager:GetSecretValue<br>            - ssm:GetParameters<br>            - kms:Decrypt<br>            Condition: {}<br>            Effect: Allow<br>            Principal: {}<br>            Resource:<br>            - arn:aws:secretsmanager<br>        PolicyName: frontendGrantAccessToSecrets<br>      Tags:<br>      - Key: com.docker.compose.project<br>        Value: docker<br>      - Key: com.docker.compose.service<br>        Value: frontend<br>    Type: AWS::IAM::Role<br>  LoadBalancer:<br>    Properties:<br>      LoadBalancerAttributes:<br>      - Key: load_balancing.cross_zone.enabled<br>        Value: &quot;true&quot;<br>      Scheme: internet-facing<br>      Subnets:<br>      - subnet-a7d289a9<br>      - subnet-a56b0484<br>      - subnet-22df6b13<br>      - subnet-4c96f72a<br>      - subnet-3bd2b064<br>      - subnet-f486b7b9<br>      Tags:<br>      - Key: com.docker.compose.project<br>        Value: docker<br>      Type: network<br>    Type: AWS::ElasticLoadBalancingV2::LoadBalancer<br>  LogGroup:<br>    Properties:<br>      LogGroupName: /docker-compose/docker<br>    Type: AWS::Logs::LogGroup<br></code></pre><br><p>How would I get this working?</p><br><p>Extra Questions:</p><br><ul><br><li>Is it possible to deploy to EC2 instead of Fargate</li><br><li>Is it possible to specify the AMI and Type used for the instant</li><br></ul><br><p>Notes:<br>I've changed my org name and ARN secret to the values above just to keep them hidden. In my actual file; they are valid.</p><br><p>Links:<br><a href="https://docs.docker.com/cloud/ecs-integration/" rel="nofollow noreferrer">https://docs.docker.com/cloud/ecs-integration/</a></p><br>
0.0,0.0,0.6666666666666666,0.0,0.0,0.6666666666666666,0.0,<h3>Why there is no &quot;CodeDeploy Instance State-change Notification&quot; event for rollback deployment?</h3><p>In CodeDeploy deployment group I have rollback enabled. However I cannot see &quot;CodeDeploy Instance State-change Notification&quot; events for the rollback deployments.</p><br><p>It seems only 'CodeDeploy Deployment State-change Notification' is triggered.</p><br><p>Is this a bug or by design?</p><br>
0.0,0.0,0.0,1.0,0.0,0.0,0.0,<h3>Check presence of a nested attribute with scan in DynamoDB</h3><p>I want to scan items from DynamoDB which contain a certain attribute in a nested map structure.</p><br><p>For example; my item is:</p><br><pre><code>{<br>    &quot;Id&quot;: 1;<br>    &quot;map1&quot;: {<br>        &quot;Random1&quot;: {<br>            &quot;X&quot;: 5;<br>            &quot;Y&quot;: 6<br>        };<br>        &quot;Random2&quot;: {<br>            &quot;Z&quot;: 7;<br>            &quot;Y&quot;: 8<br>        }<br>    }<br>}<br></code></pre><br><p>where <code>Random1</code> and <code>Random2</code> are random strings. Now I want all the items that have <code>X</code> as the key in such such a nested structure.</p><br><p>I tried this:<br><code>aws dynamodb scan --table-name dummy --profile dummy --region us-east-1 --filter-expression &quot;attribute_exists(map1.*.X)&quot;</code> but it doesn't work as wildcards don't work in DynamoDB. How else can I achieve this?</p><br>
0.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.3333333333333333,<h3>Set net.core.somaxconn on elastic beanstalk</h3><p>I have the following ebextension:</p><br><pre><code>files:<br>  &quot;/etc/sysctl.d/50-custom.conf&quot; :<br>    mode: &quot;000644&quot;<br>    owner: root<br>    group: root<br>    content: |<br>        net.core.somaxconn = 20000<br>        net.core.netdev_max_backlog = 65535<br><br>commands:<br>  01:<br>    command: sudo sysctl -p /etc/sysctl.d/50-custom.conf<br></code></pre><br><p>But the values do not set when a node spawns.  If I connect to node and run <code>sudo sysctl -p /etc/sysctl.d/50-custom.conf</code> then the settings take hold.</p><br><p>Why isn't is setting correctly when first starting?</p><br>
0.0,0.0,0.0,0.0,1.0,0.3333333333333333,0.0,<h3>How to install apt-get on AWS EC2 instance?</h3><p>I am using an instance of AWS EC2. I want to use <strong>apt-get</strong> command; but it throws an error: 'apt-get not found'</p><br><p>How do i get to use apt-get command?</p><br>
0.0,0.3333333333333333,0.0,0.0,0.6666666666666666,0.6666666666666666,0.3333333333333333,<h3>MIME type (text/html) mismatch in ngnix docker container deployed to AWS ECS</h3><p>Steps performed at glance:</p><br><ul><br><li>Angular 10 build step</li><br><li>Nginx docker container build and push step</li><br><li>AWS ECS service restart to refresh public container</li><br></ul><br><p>Error received on endpoint:</p><br><pre><code>GEThttp://&lt;aws load balancer endpoint (masked)&gt;/runtime.js<br>[HTTP/1.1 404 Not Found 71ms]<br><br>GEThttp://&lt;aws load balancer endpoint (masked)&gt;/polyfills.js<br>[HTTP/1.1 404 Not Found 55ms]<br><br>GEThttp://&lt;aws load balancer endpoint (masked)&gt;/styles.js<br>[HTTP/1.1 404 Not Found 67ms]<br><br>GEThttp://&lt;aws load balancer endpoint (masked)&gt;/vendor.js<br>[HTTP/1.1 404 Not Found 66ms]<br><br>GEThttp://&lt;aws load balancer endpoint (masked)&gt;/main.js<br>[HTTP/1.1 404 Not Found 105ms]<br><br>The resource from http://&lt;aws load balancer endpoint (masked)&gt;/polyfills.js was blocked due to MIME type (text/html) mismatch (X-Content-Type-Options: nosniff).<br>&lt;aws load balancer endpoint (masked)&gt;<br>Loading failed for the &lt;script&gt; with source http://&lt;aws load balancer endpoint (masked)&gt;/polyfills.js. &lt;aws load balancer endpoint (masked)&gt;:12:1<br><br>The resource from http://&lt;aws load balancer endpoint (masked)&gt;/runtime.js was blocked due to MIME type (text/html) mismatch (X-Content-Type-Options: nosniff).<br>&lt;aws load balancer endpoint (masked)&gt;<br>Loading failed for the &lt;script&gt; with source http://&lt;aws load balancer endpoint (masked)&gt;/runtime.js. &lt;aws load balancer endpoint (masked)&gt;:12:1<br><br>The resource from http://&lt;aws load balancer endpoint (masked)&gt;/styles.js was blocked due to MIME type (text/html) mismatch (X-Content-Type-Options: nosniff).<br>&lt;aws load balancer endpoint (masked)&gt;<br>Loading failed for the &lt;script&gt; with source http://&lt;aws load balancer endpoint (masked)&gt;/styles.js. &lt;aws load balancer endpoint (masked)&gt;:12:1<br><br>The resource from http://&lt;aws load balancer endpoint (masked)&gt;/vendor.js was blocked due to MIME type (text/html) mismatch (X-Content-Type-Options: nosniff).<br>&lt;aws load balancer endpoint (masked)&gt;<br>Loading failed for the &lt;script&gt; with source http://&lt;aws load balancer endpoint (masked)&gt;/vendor.js. &lt;aws load balancer endpoint (masked)&gt;:12:1<br><br>The resource from http://&lt;aws load balancer endpoint (masked)&gt;/main.js was blocked due to MIME type (text/html) mismatch (X-Content-Type-Options: nosniff).<br>&lt;aws load balancer endpoint (masked)&gt;<br>Loading failed for the &lt;script&gt; with source http://&lt;aws load balancer endpoint (masked)&gt;/main.js. &lt;aws load balancer endpoint (masked)&gt;:12:1<br>GEThttp://&lt;aws load balancer endpoint (masked)&gt;/favicon.ico<br></code></pre><br><p>dockerfile:</p><br><pre><code>FROM nginx:1.17.1-alpine<br>COPY nginx.conf /etc/nginx/nginx.conf<br>COPY mime.types /etc/nginx/mime.types<br>COPY security-headers.conf /etc/nginx/security-headers.conf<br>COPY /dist/payload-ng /usr/dist/nginx/html<br></code></pre><br><p>nginx.conf:</p><br><pre><code>user nginx;<br>worker_processes    1;<br>worker_rlimit_nofile    8192;<br><br>events {<br>  worker_connections    1024;<br>}<br><br>http {<br>    include /etc/nginx/mime.types;  <br>    default_type  application/octet-stream;<br>    server_names_hash_bucket_size   128;<br>    sendfile    on; <br>        <br>    server {<br>        listen  80;<br>        server_name localhost;<br>        root    /usr/dist/nginx/html;<br>        index   index.html;<br>        <br>        location / {<br>            try_files $uri$args $uri$args/ /index.html;<br>            add_header Cache-Control 'max-age=86400'; # one day<br>            include /etc/nginx/security-headers.conf; <br>        }<br><br>        location ~ .*\.css$|.*\.js$ {               <br>            add_header Cache-Control 'max-age=31449600'; # one year        <br>            include /etc/nginx/security-headers.conf;        <br>        }<br><br>        location ~ /index.html|.*\.json$ {<br>            expires -1;<br>            add_header Cache-Control 'no-store; no-cache; must-revalidate;<br>            proxy-revalidate; max-age=0';<br>        }<br>    }<br>}<br> <br></code></pre><br><p>mime.types:</p><br><p>types {</p><br><pre><code>  # Data interchange<br><br>    application/atom+xml                  atom;<br>    application/json                      json map topojson;<br>    application/ld+json                   jsonld;<br>    application/rss+xml                   rss;<br>    application/vnd.geo+json              geojson;<br>    application/xml                       rdf xml;<br><br><br>  # JavaScript<br><br>    # Normalize to standard type.<br>    # https://tools.ietf.org/html/rfc4329#section-7.2<br>    application/javascript                js;<br><br><br>  # Manifest files<br><br>    application/manifest+json             webmanifest;<br>    application/x-web-app-manifest+json   webapp;<br>    text/cache-manifest                   appcache;<br><br><br>  # Media files<br><br>    audio/midi                            mid midi kar;<br>    audio/mp4                             aac f4a f4b m4a;<br>    audio/mpeg                            mp3;<br>    audio/ogg                             oga ogg opus;<br>    audio/x-realaudio                     ra;<br>    audio/x-wav                           wav;<br>    image/bmp                             bmp;<br>    image/gif                             gif;<br>    image/jpeg                            jpeg jpg;<br>    image/png                             png;<br>    image/svg+xml                         svg svgz;<br>    image/tiff                            tif tiff;<br>    image/vnd.wap.wbmp                    wbmp;<br>    image/webp                            webp;<br>    image/x-jng                           jng;<br>    video/3gpp                            3gpp 3gp;<br>    video/mp4                             f4v f4p m4v mp4;<br>    video/mpeg                            mpeg mpg;<br>    video/ogg                             ogv;<br>    video/quicktime                       mov;<br>    video/webm                            webm;<br>    video/x-flv                           flv;<br>    video/x-mng                           mng;<br>    video/x-ms-asf                        asx asf;<br>    video/x-ms-wmv                        wmv;<br>    video/x-msvideo                       avi;<br><br>    # Serving `.ico` image files with a different media type<br>    # prevents Internet Explorer from displaying then as images:<br>    # https://github.com/h5bp/html5-boilerplate/commit/37b5fec090d00f38de64b591bcddcb205aadf8ee<br><br>    image/x-icon                          cur ico;<br><br><br>  # Microsoft Office<br><br>    application/msword                                                         doc;<br>    application/vnd.ms-excel                                                   xls;<br>    application/vnd.ms-powerpoint                                              ppt;<br>    application/vnd.openxmlformats-officedocument.wordprocessingml.document    docx;<br>    application/vnd.openxmlformats-officedocument.spreadsheetml.sheet          xlsx;<br>    application/vnd.openxmlformats-officedocument.presentationml.presentation  pptx;<br><br><br>  # Web fonts<br><br>    application/font-woff                 woff;<br>    application/font-woff2                woff2;<br>    application/vnd.ms-fontobject         eot;<br><br>    # Browsers usually ignore the font media types and simply sniff<br>    # the bytes to figure out the font type.<br>    # https://mimesniff.spec.whatwg.org/#matching-a-font-type-pattern<br>    #<br>    # However; Blink and WebKit based browsers will show a warning<br>    # in the console if the following font types are served with any<br>    # other media types.<br><br>    application/x-font-ttf                ttc ttf;<br>    font/opentype                         otf;<br><br><br>  # Other<br><br>    application/java-archive              jar war ear;<br>    application/mac-binhex40              hqx;<br>    application/octet-stream              bin deb dll dmg exe img iso msi msm msp safariextz;<br>    application/pdf                       pdf;<br>    application/postscript                ps eps ai;<br>    application/rtf                       rtf;<br>    application/vnd.google-earth.kml+xml  kml;<br>    application/vnd.google-earth.kmz      kmz;<br>    application/vnd.wap.wmlc              wmlc;<br>    application/x-7z-compressed           7z;<br>    application/x-bb-appworld             bbaw;<br>    application/x-bittorrent              torrent;<br>    application/x-chrome-extension        crx;<br>    application/x-cocoa                   cco;<br>    application/x-java-archive-diff       jardiff;<br>    application/x-java-jnlp-file          jnlp;<br>    application/x-makeself                run;<br>    application/x-opera-extension         oex;<br>    application/x-perl                    pl pm;<br>    application/x-pilot                   prc pdb;<br>    application/x-rar-compressed          rar;<br>    application/x-redhat-package-manager  rpm;<br>    application/x-sea                     sea;<br>    application/x-shockwave-flash         swf;<br>    application/x-stuffit                 sit;<br>    application/x-tcl                     tcl tk;<br>    application/x-x509-ca-cert            der pem crt;<br>    application/x-xpinstall               xpi;<br>    application/xhtml+xml                 xhtml;<br>    application/xslt+xml                  xsl;<br>    application/zip                       zip;<br>    text/css                              css;<br>    text/html                             html htm shtml;<br>    text/mathml                           mml;<br>    text/plain                            txt;<br>    text/vcard                            vcard vcf;<br>    text/vnd.rim.location.xloc            xloc;<br>    text/vnd.sun.j2me.app-descriptor      jad;<br>    text/vnd.wap.wml                      wml;<br>    text/vtt                              vtt;<br>    text/x-component                      htc;<br><br>}<br></code></pre><br><p>security-headers.conf:</p><br><pre><code>add_header X-Content-Type-Options &quot;nosniff&quot; always;<br>add_header Feature-Policy &quot;microphone 'none'; geolocation 'none'; camera 'none'&quot; always;<br></code></pre><br><p>At this point i am not sure what is the cause of getting the mime type error; double checked if the lates image is runing in ECS. Nginx is a new stack i am getting familiar with and lacking of oversight on this matter; and i would appreciate help with it. My best knowledge is it should serv the js files with the proper &quot;text/javascript&quot; header; but i can be easily wrong. I appreciate any input.</p><br>
